{
    "torch->PyTorch Recipes->Loading data in PyTorch": [
        [
            "PyTorch features extensive neural network building blocks with a simple,\nintuitive, and stable API. PyTorch includes packages to prepare and load\ncommon datasets for your model.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Loading data in PyTorch->Introduction": [
        [
            "At the heart of PyTorch data loading utility is the\n\nclass. It represents a Python iterable over a dataset. Libraries in\nPyTorch offer built-in high-quality datasets for you to use in\n.\nThese datasets are currently available in:",
            "markdown"
        ],
        [
            "with more to come.\nUsing the Yesno dataset from torchaudio.datasets.YESNO, we will\ndemonstrate how to effectively and efficiently load data from a PyTorch\nDataset into a PyTorch DataLoader.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Loading data in PyTorch->Setup": [
        [
            "Before we begin, we need to install torchaudio to have access to the\ndataset.",
            "markdown"
        ],
        [
            "# pip install torchaudio",
            "code"
        ],
        [
            "To run in Google Colab, uncomment the following line:",
            "markdown"
        ],
        [
            "# !pip install torchaudio",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Loading data in PyTorch->Steps": [
        [
            "Import all necessary libraries for loading our data",
            "markdown"
        ],
        [
            "Access the data in the dataset",
            "markdown"
        ],
        [
            "Loading the data",
            "markdown"
        ],
        [
            "Iterate over the data",
            "markdown"
        ],
        [
            "[Optional] Visualize the data",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Loading data in PyTorch->1. Import necessary libraries for loading our data": [
        [
            "For this recipe, we will use torch and torchaudio. Depending on\nwhat built-in datasets you use, you can also install and import\ntorchvision or torchtext.",
            "markdown"
        ],
        [
            "import torch\nimport torchaudio",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Loading data in PyTorch->2. Access the data in the dataset": [
        [
            "The Yesno dataset in torchaudio features sixty recordings of one\nindividual saying yes or no in Hebrew; with each recording being eight\nwords long ().",
            "markdown"
        ],
        [
            "torchaudio.datasets.YESNO creates a dataset for YesNo.",
            "markdown"
        ],
        [
            "(\n     root='./',\n     url='http://www.openslr.org/resources/1/waves_yesno.tar.gz',\n     folder_in_archive='waves_yesno',\n     download=True)",
            "code"
        ],
        [
            "Each item in the dataset is a tuple of the form: (waveform, sample_rate,\nlabels).",
            "markdown"
        ],
        [
            "You must set a root for the Yesno dataset, which is where the\ntraining and testing dataset will exist. The other parameters are\noptional, with their default values shown. Here is some additional\nuseful info on the other parameters:",
            "markdown"
        ],
        [
            "# * ``download``: If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.\n#\n# Let\u2019s access our Yesno data:\n#\n\n# A data point in Yesno is a tuple (waveform, sample_rate, labels) where labels\n# is a list of integers with 1 for yes and 0 for no.\nyesno_data = ('./', download=True)\n\n# Pick data point number 3 to see an example of the the yesno_data:\nn = 3\nwaveform, sample_rate, labels = yesno_data[n]\nprint(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(waveform, sample_rate, labels))",
            "code"
        ],
        [
            "When using this data in practice, it is best practice to provision the\ndata into a \u201ctraining\u201d dataset and a \u201ctesting\u201d dataset. This ensures\nthat you have out-of-sample data to test the performance of your model.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Loading data in PyTorch->3. Loading the data": [
        [
            "Now that we have access to the dataset, we must pass it through\ntorch.utils.data.DataLoader. The DataLoader combines the dataset\nand a sampler, returning an iterable over the dataset.",
            "markdown"
        ],
        [
            "data_loader = (yesno_data,\n                                          batch_size=1,\n                                          shuffle=True)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Loading data in PyTorch->4. Iterate over the data": [
        [
            "Our data is now iterable using the data_loader. This will be\nnecessary when we begin training our model! You will notice that now\neach data entry in the data_loader object is converted to a tensor\ncontaining tensors representing our waveform, sample rate, and labels.",
            "markdown"
        ],
        [
            "for data in data_loader:\n  print(\"Data: \", data)\n  print(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(data[0], data[1], data[2]))\n  break",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Loading data in PyTorch->5. [Optional] Visualize the data": [
        [
            "You can optionally visualize your data to further understand the output\nfrom your DataLoader.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nprint(data[0][0].numpy())\n\nplt.figure()\nplt.plot(waveform.t().numpy())",
            "code"
        ],
        [
            "Congratulations! You have successfully loaded data in PyTorch.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Loading data in PyTorch->Learn More": [
        [
            "Take a look at these other recipes to continue your learning:",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Defining a Neural Network in PyTorch": [
        [
            "Deep learning uses artificial neural networks (models), which are\ncomputing systems that are composed of many layers of interconnected\nunits. By passing data through these interconnected units, a neural\nnetwork is able to learn how to approximate the computations required to\ntransform inputs into outputs. In PyTorch, neural networks can be\nconstructed using the torch.nn package.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Defining a Neural Network in PyTorch->Introduction": [
        [
            "PyTorch provides the elegantly designed modules and classes, including\ntorch.nn, to help you create and train neural networks. An\nnn.Module contains layers, and a method forward(input) that\nreturns the output.",
            "markdown"
        ],
        [
            "In this recipe, we will use torch.nn to define a neural network\nintended for the .",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Defining a Neural Network in PyTorch->Setup": [
        [
            "Before we begin, we need to install torch if it isn\u2019t already\navailable.",
            "markdown"
        ],
        [
            "pip install torch",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Defining a Neural Network in PyTorch->Steps": [
        [
            "Import all necessary libraries for loading our data",
            "markdown"
        ],
        [
            "Define and initialize the neural network",
            "markdown"
        ],
        [
            "Specify how data will pass through your model",
            "markdown"
        ],
        [
            "[Optional] Pass data through your model to test",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Defining a Neural Network in PyTorch->Steps->1. Import necessary libraries for loading our data": [
        [
            "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.nn.functional.",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Defining a Neural Network in PyTorch->Steps->2. Define and intialize the neural network": [
        [
            "Our network will recognize images. We will use a process built into\nPyTorch called convolution. Convolution adds each element of an image to\nits local neighbors, weighted by a kernel, or a small matrix, that\nhelps us extract certain features (like edge detection, sharpness,\nblurriness, etc.) from the input image.",
            "markdown"
        ],
        [
            "There are two requirements for defining the Net class of your model.\nThe first is writing an __init__ function that references\nnn.Module. This function is where you define the fully connected\nlayers in your neural network.",
            "markdown"
        ],
        [
            "Using convolution, we will define our model to take 1 input image\nchannel, and output match our target of 10 labels representing numbers 0\nthrough 9. This algorithm is yours to create, we will follow a standard\nMNIST algorithm.",
            "markdown"
        ],
        [
            "class Net():\n    def __init__(self):\n      super(Net, self).__init__()\n\n      # First 2D convolutional layer, taking in 1 input channel (image),\n      # outputting 32 convolutional features, with a square kernel size of 3\n      self.conv1 = (1, 32, 3, 1)\n      # Second 2D convolutional layer, taking in the 32 input layers,\n      # outputting 64 convolutional features, with a square kernel size of 3\n      self.conv2 = (32, 64, 3, 1)\n\n      # Designed to ensure that adjacent pixels are either all 0s or all active\n      # with an input probability\n      self.dropout1 = (0.25)\n      self.dropout2 = (0.5)\n\n      # First fully connected layer\n      self.fc1 = (9216, 128)\n      # Second fully connected layer that outputs our 10 labels\n      self.fc2 = (128, 10)\n\nmy_nn = Net()\nprint(my_nn)",
            "code"
        ],
        [
            "We have finished defining our neural network, now we have to define how\nour data will pass through it.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Defining a Neural Network in PyTorch->Steps->3. Specify how data will pass through your model": [
        [
            "When you use PyTorch to build a model, you just have to define the\nforward function, that will pass the data into the computation graph\n(i.e. our neural network). This will represent our feed-forward\nalgorithm.",
            "markdown"
        ],
        [
            "You can use any of the Tensor operations in the forward function.",
            "markdown"
        ],
        [
            "class Net():\n    def __init__(self):\n      super(Net, self).__init__()\n      self.conv1 = (1, 32, 3, 1)\n      self.conv2 = (32, 64, 3, 1)\n      self.dropout1 = (0.25)\n      self.dropout2 = (0.5)\n      self.fc1 = (9216, 128)\n      self.fc2 = (128, 10)\n\n    # x represents our data\n    def forward(self, x):\n      # Pass data through conv1\n      x = self.conv1(x)\n      # Use the rectified-linear activation function over x\n      x = (x)\n\n      x = self.conv2(x)\n      x = (x)\n\n      # Run max pooling over x\n      x = (x, 2)\n      # Pass data through dropout1\n      x = self.dropout1(x)\n      # Flatten x with start_dim=1\n      x = (x, 1)\n      # Pass data through fc1\n      x = self.fc1(x)\n      x = (x)\n      x = self.dropout2(x)\n      x = self.fc2(x)\n\n      # Apply softmax to x\n      output = (x, dim=1)\n      return output",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Defining a Neural Network in PyTorch->Steps->4. [Optional] Pass data through your model to test": [
        [
            "To ensure we receive our desired output, let\u2019s test our model by passing\nsome random data through it.",
            "markdown"
        ],
        [
            "# Equates to one random 28x28 image\nrandom_data = ((1, 1, 28, 28))\n\nmy_nn = Net()\nresult = my_nn(random_data)\nprint (result)",
            "code"
        ],
        [
            "Each number in this resulting tensor equates to the prediction of the\nlabel the random tensor is associated to.",
            "markdown"
        ],
        [
            "Congratulations! You have successfully defined a neural network in\nPyTorch.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Defining a Neural Network in PyTorch->Learn More": [
        [
            "Take a look at these other recipes to continue your learning:",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->What is a state_dict in PyTorch": [
        [
            "In PyTorch, the learnable parameters (i.e. weights and biases) of a\ntorch.nn.Module model are contained in the model\u2019s parameters\n(accessed with model.parameters()). A state_dict is simply a\nPython dictionary object that maps each layer to its parameter tensor.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->What is a state_dict in PyTorch->Introduction": [
        [
            "A state_dict is an integral entity if you are interested in saving\nor loading models from PyTorch.\nBecause state_dict objects are Python dictionaries, they can be\neasily saved, updated, altered, and restored, adding a great deal of\nmodularity to PyTorch models and optimizers.\nNote that only layers with learnable parameters (convolutional layers,\nlinear layers, etc.) and registered buffers (batchnorm\u2019s running_mean)\nhave entries in the model\u2019s state_dict. Optimizer objects\n(torch.optim) also have a state_dict, which contains information\nabout the optimizer\u2019s state, as well as the hyperparameters used.\nIn this recipe, we will see how state_dict is used with a simple\nmodel.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->What is a state_dict in PyTorch->Setup": [
        [
            "Before we begin, we need to install torch if it isn\u2019t already\navailable.",
            "markdown"
        ],
        [
            "pip install torch",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->What is a state_dict in PyTorch->Steps": [
        [
            "Import all necessary libraries for loading our data",
            "markdown"
        ],
        [
            "Define and intialize the neural network",
            "markdown"
        ],
        [
            "Initialize the optimizer",
            "markdown"
        ],
        [
            "Access the model and optimizer state_dict",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->What is a state_dict in PyTorch->Steps->1. Import necessary libraries for loading our data": [
        [
            "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.optim.",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->What is a state_dict in PyTorch->Steps->2. Define and intialize the neural network": [
        [
            "For sake of example, we will create a neural network for training\nimages. To learn more see the Defining a Neural Network recipe.",
            "markdown"
        ],
        [
            "class Net():\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nprint(net)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->What is a state_dict in PyTorch->Steps->3. Initialize the optimizer": [
        [
            "We will use SGD with momentum.",
            "markdown"
        ],
        [
            "optimizer = (net.parameters(), lr=0.001, momentum=0.9)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->What is a state_dict in PyTorch->Steps->4. Access the model and optimizer state_dict": [
        [
            "Now that we have constructed our model and optimizer, we can understand\nwhat is preserved in their respective state_dict properties.",
            "markdown"
        ],
        [
            "# Print model's state_dict\nprint(\"Model's state_dict:\")\nfor param_tensor in net.state_dict():\n    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n\nprint()\n\n# Print optimizer's state_dict\nprint(\"Optimizer's state_dict:\")\nfor var_name in optimizer.state_dict():\n    print(var_name, \"\\t\", optimizer.state_dict()[var_name])",
            "code"
        ],
        [
            "This information is relevant for saving and loading the model and\noptimizers for future use.",
            "markdown"
        ],
        [
            "Congratulations! You have successfully used state_dict in PyTorch.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->What is a state_dict in PyTorch->Learn More": [
        [
            "Take a look at these other recipes to continue your learning:",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders": [
        [
            "A significant amount of the effort applied to developing machine\nlearning algorithms is related to data preparation. PyTorch provides\nmany tools to make data loading easy and hopefully, makes your code more\nreadable. In this recipe, you will learn how to:\n<blockquote>\n\nCreate a custom dataset leveraging the PyTorch dataset APIs;\nCreate callable custom transforms that can be composable; and\nPut these components together to create a custom dataloader.\n\n</blockquote>",
            "markdown"
        ],
        [
            "Please note, to run this tutorial, ensure the following packages are\ninstalled:\n<blockquote>\n\nscikit-image: For image io and transforms\npandas: For easier csv parsing\n\n</blockquote>",
            "markdown"
        ],
        [
            "As a point of attribution, this recipe is based on the original tutorial\nfrom  and was later\nedited by .",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Setup": [
        [
            "First let\u2019s import all of the needed libraries for this recipe.",
            "markdown"
        ],
        [
            "from __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.ion()   # interactive mode",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset": [
        [
            "The dataset we are going to deal with is that of facial pose. Overall,\n68 different landmark points are annotated for each face.",
            "markdown"
        ],
        [
            "As a next step, please download the dataset from\n so that the\nimages are in a directory named \u2018data/faces/\u2019.",
            "markdown"
        ],
        [
            "<strong>Note:</strong> This dataset was actually generated by applying\n\non images from the imagenet dataset containing the \u2018face\u2019 tag.",
            "markdown"
        ],
        [
            "!wget https://download.pytorch.org/tutorial/faces.zip\n!mkdir data/faces/\nimport zipfile\nwith zipfile.ZipFile(\"faces.zip\",\"r\") as zip_ref:\nzip_ref.extractall(\"/data/faces/\")\n%cd /data/faces/",
            "code"
        ],
        [
            "The dataset comes with a csv file with annotations which looks like\nthis:",
            "markdown"
        ],
        [
            "image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y\n0805personali01.jpg,27,83,27,98, ... 84,134\n1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312",
            "code"
        ],
        [
            "Let\u2019s quickly read the CSV and get the annotations in an (N, 2) array\nwhere N is the number of landmarks.",
            "markdown"
        ],
        [
            "landmarks_frame = pd.read_csv('faces/face_landmarks.csv')\n\nn = 65\nimg_name = landmarks_frame.iloc[n, 0]\nlandmarks = landmarks_frame.iloc[n, 1:]\nlandmarks = np.asarray(landmarks)\nlandmarks = landmarks.astype('float').reshape(-1, 2)\n\nprint('Image name: {}'.format(img_name))\nprint('Landmarks shape: {}'.format(landmarks.shape))\nprint('First 4 Landmarks: {}'.format(landmarks[:4]))",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.1 Write a simple helper function to show an image": [
        [
            "Next let\u2019s write a simple helper function to show an image, its landmarks and use it to show a sample.",
            "markdown"
        ],
        [
            "def show_landmarks(image, landmarks):\n    \"\"\"Show image with landmarks\"\"\"\n    plt.imshow(image)\n    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\nplt.figure()\nshow_landmarks(io.imread(os.path.join('faces/', img_name)),\n               landmarks)\nplt.show()",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.2 Create a dataset class": [
        [
            "Now lets talk about the PyTorch dataset class",
            "markdown"
        ],
        [
            "torch.utils.data.Dataset is an abstract class representing a\ndataset. Your custom dataset should inherit Dataset and override the\nfollowing methods:\n\n__len__ so that len(dataset) returns the size of the dataset.\n__getitem__ to support indexing such that dataset[i] can be\nused to get :math:i\u00a0th sample",
            "markdown"
        ],
        [
            "Let\u2019s create a dataset class for our face landmarks dataset. We will\nread the csv in __init__ but leave the reading of images to\n__getitem__. This is memory efficient because all the images are not\nstored in the memory at once but read as required.",
            "markdown"
        ],
        [
            "Here we show a sample of our dataset in the forma of a dict\n{'image': image, 'landmarks': landmarks}. Our dataset will take an\noptional argument transform so that any required processing can be\napplied on the sample. We will see the usefulness of transform in\nanother recipe.",
            "markdown"
        ],
        [
            "class FaceLandmarksDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.landmarks_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.root_dir,\n                                self.landmarks_frame.iloc[idx, 0])\n        image = io.imread(img_name)\n        landmarks = self.landmarks_frame.iloc[idx, 1:]\n        landmarks = np.array([landmarks])\n        landmarks = landmarks.astype('float').reshape(-1, 2)\n        sample = {'image': image, 'landmarks': landmarks}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples": [
        [
            "Next let\u2019s instantiate this class and iterate through the data samples.\nWe will print the sizes of first 4 samples and show their landmarks.",
            "markdown"
        ],
        [
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 2: Data Tranformations": [
        [
            "Now that we have a dataset to work with and have done some level of\ncustomization, we can move to creating custom transformations. In\ncomputer vision, these come in handy to help generalize algorithms and\nimprove accuracy. A suite of transformations used at training time is\ntypically referred to as data augmentation and is a common practice for\nmodern model development.",
            "markdown"
        ],
        [
            "One issue common in handling datasets is that the samples may not all be\nthe same size. Most neural networks expect the images of a fixed size.\nTherefore, we will need to write some prepocessing code. Let\u2019s create\nthree transforms:\n\nRescale: to scale the image\nRandomCrop: to crop from image randomly. This is data\naugmentation.\nToTensor: to convert the numpy images to torch images (we need to\nswap axes).",
            "markdown"
        ],
        [
            "We will write them as callable classes instead of simple functions so\nthat parameters of the transform need not be passed everytime it\u2019s\ncalled. For this, we just need to implement __call__ method and if\nrequired, __init__ method. We can then use a transform like this:",
            "markdown"
        ],
        [
            "tsfm = Transform(params)\ntransformed_sample = tsfm(sample)",
            "code"
        ],
        [
            "Observe below how these transforms had to be applied both on the image\nand landmarks.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 2: Data Tranformations->2.1 Create callable classes": [
        [
            "Let\u2019s start with creating callable classes for each transform",
            "markdown"
        ],
        [
            "class Rescale(object):\n    \"\"\"Rescale the image in a sample to a given size.\n\n    Args:\n        output_size (tuple or int): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h &gt; w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        img = transform.resize(image, (new_h, new_w))\n\n        # h and w are swapped for landmarks because for images,\n        # x and y axes are axis 1 and 0 respectively\n        landmarks = landmarks * [new_w / w, new_h / h]\n\n        return {'image': img, 'landmarks': landmarks}\n\n\nclass RandomCrop(object):\n    \"\"\"Crop randomly the image in a sample.\n\n    Args:\n        output_size (tuple or int): Desired output size. If int, square crop\n            is made.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n\n        image = image[top: top + new_h,\n                      left: left + new_w]\n\n        landmarks = landmarks - [left, top]\n\n        return {'image': image, 'landmarks': landmarks}\n\n\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n        return {'image': torch.from_numpy(image),\n                'landmarks': torch.from_numpy(landmarks)}",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 2: Data Tranformations->2.2 Compose transforms and apply to a sample": [
        [
            "Next let\u2019s compose these transforms and apply to a sample",
            "markdown"
        ],
        [
            "Let\u2019s say we want to rescale the shorter side of the image to 256 and\nthen randomly crop a square of size 224 from it. i.e, we want to compose\nRescale and RandomCrop transforms.\ntorchvision.transforms.Compose is a simple callable class which\nallows us to do this.",
            "markdown"
        ],
        [
            "scale = Rescale(256)\ncrop = RandomCrop(128)\ncomposed = transforms.Compose([Rescale(256),\n                               RandomCrop(224)])\n\n# Apply each of the above transforms on sample.\nfig = plt.figure()\nsample = face_dataset[65]\nfor i, tsfrm in enumerate([scale, crop, composed]):\n    transformed_sample = tsfrm(sample)\n\n    ax = plt.subplot(1, 3, i + 1)\n    plt.tight_layout()\n    ax.set_title(type(tsfrm).__name__)\n    show_landmarks(**transformed_sample)\n\nplt.show()",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 2: Data Tranformations->2.3 Iterate through the dataset": [
        [
            "Next we will iterate through the dataset",
            "markdown"
        ],
        [
            "Let\u2019s put this all together to create a dataset with composed\ntransforms. To summarize, every time this dataset is sampled:\n\nAn image is read from the file on the fly\nTransforms are applied on the read image\nSince one of the transforms is random, data is augmentated on\nsampling",
            "markdown"
        ],
        [
            "We can iterate over the created dataset with a for i in range loop\nas before.",
            "markdown"
        ],
        [
            "transformed_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                           root_dir='faces/',\n                                           transform=transforms.Compose([\n                                               Rescale(256),\n                                               RandomCrop(224),\n                                               ToTensor()\n                                           ]))\n\nfor i in range(len(transformed_dataset)):\n    sample = transformed_dataset[i]\n\n    print(i, sample['image'].size(), sample['landmarks'].size())\n\n    if i == 3:\n        break",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 3: The Dataloader": [
        [
            "By operating on the dataset directly, we are losing out on a lot of\nfeatures by using a simple for loop to iterate over the data. In\nparticular, we are missing out on:\n\nBatching the data\nShuffling the data\nLoad the data in parallel using multiprocessing workers.",
            "markdown"
        ],
        [
            "torch.utils.data.DataLoader is an iterator which provides all these\nfeatures. Parameters used below should be clear. One parameter of\ninterest is collate_fn. You can specify how exactly the samples need\nto be batched using collate_fn. However, default collate should work\nfine for most use cases.",
            "markdown"
        ],
        [
            "dataloader = DataLoader(transformed_dataset, batch_size=4,\n                        shuffle=True, num_workers=4)\n\n\n# Helper function to show a batch\ndef show_landmarks_batch(sample_batched):\n    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n    images_batch, landmarks_batch = \\\n            sample_batched['image'], sample_batched['landmarks']\n    batch_size = len(images_batch)\n    im_size = images_batch.size(2)\n\n    grid = utils.make_grid(images_batch)\n    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n\n    for i in range(batch_size):\n        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size,\n                    landmarks_batch[i, :, 1].numpy(),\n                    s=10, marker='.', c='r')\n\n        plt.title('Batch from dataloader')\n\nfor i_batch, sample_batched in enumerate(dataloader):\n    print(i_batch, sample_batched['image'].size(),\n          sample_batched['landmarks'].size())\n\n    # observe 4th batch and stop.\n    if i_batch == 3:\n        plt.figure()\n        show_landmarks_batch(sample_batched)\n        plt.axis('off')\n        plt.ioff()\n        plt.show()\n        break",
            "code"
        ],
        [
            "Now that you\u2019ve learned how to create a custom dataloader with PyTorch,\nwe recommend diving deeper into the docs and customizing your workflow\neven further. You can learn more in the torch.utils.data docs\n.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Model Interpretability using Captum": [
        [
            "Captum helps you understand how the data features impact your model\npredictions or neuron activations, shedding light on how your model\noperates.",
            "markdown"
        ],
        [
            "Using Captum, you can apply a wide range of state-of-the-art feature\nattribution algorithms such as Guided GradCam and\nIntegrated Gradients in a unified way.",
            "markdown"
        ],
        [
            "In this recipe you will learn how to use Captum to:",
            "markdown"
        ],
        [
            "Attribute the predictions of an image classifier to their corresponding image features.",
            "markdown"
        ],
        [
            "Visualize the attribution results.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Model Interpretability using Captum->Before you begin": [
        [
            "Make sure Captum is installed in your active Python environment. Captum\nis available both on GitHub, as a pip package, or as a conda\npackage. For detailed instructions, consult the installation guide at",
            "markdown"
        ],
        [
            "For a model, we use a built-in image classifier in PyTorch. Captum can\nreveal which parts of a sample image support certain predictions made by\nthe model.",
            "markdown"
        ],
        [
            "import torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nmodel = (pretrained=True).eval()\n\nresponse = requests.get(\"https://image.freepik.com/free-photo/two-beautiful-puppies-cat-dog_58409-6024.jpg\")\nimg = Image.open(BytesIO(response.content))\n\ncenter_crop = ([\n (256),\n (224),\n])\n\nnormalize = ([\n    (),               # converts the image to a tensor with values between 0 and 1\n    (                # normalize to follow 0-centered imagenet pixel rgb distribution\n     mean=[0.485, 0.456, 0.406],\n     std=[0.229, 0.224, 0.225]\n    )\n])\ninput_img = normalize(center_crop(img)).unsqueeze(0)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Model Interpretability using Captum->Computing Attribution": [
        [
            "Among the top-3 predictions of the models are classes 208 and 283 which\ncorrespond to dog and cat.",
            "markdown"
        ],
        [
            "Let us attribute each of these predictions to the corresponding part of\nthe input, using Captum\u2019s Occlusion algorithm.",
            "markdown"
        ],
        [
            "from captum.attr import Occlusion\n\nocclusion = Occlusion(model)\n\nstrides = (3, 9, 9)               # smaller = more fine-grained attribution but slower\ntarget=208,                       # Labrador index in ImageNet\nsliding_window_shapes=(3,45, 45)  # choose size enough to change object appearance\nbaselines = 0                     # values to occlude the image with. 0 corresponds to gray\n\nattribution_dog = occlusion.attribute(input_img,\n                                       strides = strides,\n                                       target=target,\n                                       sliding_window_shapes=sliding_window_shapes,\n                                       baselines=baselines)\n\n\ntarget=283,                       # Persian cat index in ImageNet\nattribution_cat = occlusion.attribute(input_img,\n                                       strides = strides,\n                                       target=target,\n                                       sliding_window_shapes=sliding_window_shapes,\n                                       baselines=0)",
            "code"
        ],
        [
            "Besides Occlusion, Captum features many algorithms such as\nIntegrated Gradients, Deconvolution,\nGuidedBackprop, Guided GradCam, DeepLift, and\nGradientShap. All of these algorithms are subclasses of\nAttribution which expects your model as a callable forward_func\nupon initialization and has an attribute(...) method which returns\nthe attribution result in a unified format.",
            "markdown"
        ],
        [
            "Let us visualize the computed attribution results in case of images.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Model Interpretability using Captum->Visualizing the Results": [
        [
            "Captum\u2019s visualization utility provides out-of-the-box methods\nto visualize attribution results both for pictorial and for textual\ninputs.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom captum.attr import visualization as viz\n\n# Convert the compute attribution tensor into an image-like numpy array\nattribution_dog = np.transpose(attribution_dog.squeeze().cpu().detach().numpy(), (1,2,0))\n\nvis_types = [\"heat_map\", \"original_image\"]\nvis_signs = [\"all\", \"all\"] # \"positive\", \"negative\", or \"all\" to show both\n# positive attribution indicates that the presence of the area increases the prediction score\n# negative attribution indicates distractor areas whose absence increases the score\n\n_ = viz.visualize_image_attr_multiple(attribution_dog,\n                                      np.array(center_crop(img)),\n                                      vis_types,\n                                      vis_signs,\n                                      [\"attribution for dog\", \"image\"],\n                                      show_colorbar = True\n                                     )\n\n\nattribution_cat = np.transpose(attribution_cat.squeeze().cpu().detach().numpy(), (1,2,0))\n\n_ = viz.visualize_image_attr_multiple(attribution_cat,\n                                      np.array(center_crop(img)),\n                                      [\"heat_map\", \"original_image\"],\n                                      [\"all\", \"all\"], # positive/negative attribution or all\n                                      [\"attribution for cat\", \"image\"],\n                                      show_colorbar = True\n                                     )",
            "code"
        ],
        [
            "If your data is textual, visualization.visualize_text() offers a\ndedicated view to explore attribution on top of the input text. Find out\nmore at ",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Model Interpretability using Captum->Final Notes": [
        [
            "Captum can handle most model types in PyTorch across modalities\nincluding vision, text, and more. With Captum you can: * Attribute a\nspecific output to the model input as illustrated above. * Attribute a\nspecific output to a hidden-layer neuron (see Captum API reference). *\nAttribute a hidden-layer neuron response to the model input (see Captum\nAPI reference).",
            "markdown"
        ],
        [
            "For complete API of the supported methods and a list of tutorials,\nconsult our website ",
            "markdown"
        ],
        [
            "Another useful post by Gilbert Tanner:",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Dynamic Quantization": [
        [
            "In this recipe you will see how to take advantage of Dynamic\nQuantization to accelerate inference on an LSTM-style recurrent neural\nnetwork. This reduces the size of the model weights and speeds up model\nexecution.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Dynamic Quantization->Introduction": [
        [
            "There are a number of trade-offs that can be made when designing neural\nnetworks. During model developmenet and training you can alter the\nnumber of layers and number of parameters in a recurrent neural network\nand trade-off accuracy against model size and/or model latency or\nthroughput. Such changes can take lot of time and compute resources\nbecause you are iterating over the model training. Quantization gives\nyou a way to make a similar trade off between performance and model\naccuracy with a known model after training is completed.",
            "markdown"
        ],
        [
            "You can give it a try in a single session and you will certainly reduce\nyour model size significantly and may get a significant latency\nreduction without losing a lot of accuracy.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Dynamic Quantization->What is dynamic quantization?": [
        [
            "Quantizing a network means converting it to use a reduced precision\ninteger representation for the weights and/or activations. This saves on\nmodel size and allows the use of higher throughput math operations on\nyour CPU or GPU.",
            "markdown"
        ],
        [
            "When converting from floating point to integer values you are\nessentially multiplying the floating point value by some scale factor\nand rounding the result to a whole number. The various quantization\napproaches differ in the way they approach determining that scale\nfactor.",
            "markdown"
        ],
        [
            "The key idea with dynamic quantization as described here is that we are\ngoing to determine the scale factor for activations dynamically based on\nthe data range observed at runtime. This ensures that the scale factor\nis \u201ctuned\u201d so that as much signal as possible about each observed\ndataset is preserved.",
            "markdown"
        ],
        [
            "The model parameters on the other hand are known during model conversion\nand they are converted ahead of time and stored in INT8 form.",
            "markdown"
        ],
        [
            "Arithmetic in the quantized model is done using vectorized INT8\ninstructions. Accumulation is typically done with INT16 or INT32 to\navoid overflow. This higher precision value is scaled back to INT8 if\nthe next layer is quantized or converted to FP32 for output.",
            "markdown"
        ],
        [
            "Dynamic quantization is relatively free of tuning parameters which makes\nit well suited to be added into production pipelines as a standard part\nof converting LSTM models to deployment.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Limitations on the approach taken here",
            "markdown"
        ],
        [
            "This recipe provides a quick introduction to the dynamic quantization\nfeatures in PyTorch and the workflow for using it. Our focus is on\nexplaining the specific functions used to convert the model. We will\nmake a number of significant simplifications in the interest of brevity\nand clarity",
            "markdown"
        ],
        [
            "You will start with a minimal LSTM network",
            "markdown"
        ],
        [
            "You are simply going to initialize the network with a random hidden\nstate",
            "markdown"
        ],
        [
            "You are going to test the network with random inputs",
            "markdown"
        ],
        [
            "You are not going to train the network in this tutorial",
            "markdown"
        ],
        [
            "You will see that the quantized form of this network is smaller and\nruns faster than the floating point network we started with",
            "markdown"
        ],
        [
            "You will see that the output values are generally in the same\nballpark as the output of the FP32 network, but we are not\ndemonstrating here the expected accuracy loss on a real trained\nnetwork",
            "markdown"
        ],
        [
            "You will see how dynamic quantization is done and be able to see\nsuggestive reductions in memory use and latency times. Providing a\ndemonstration that the technique can preserve high levels of model\naccuracy on a trained LSTM is left to a more advanced tutorial. If you\nwant to move right away to that more rigorous treatment please proceed\nto the .",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Dynamic Quantization->Steps": [
        [
            "This recipe has 5 steps.",
            "markdown"
        ],
        [
            "Set Up - Here you define a very simple LSTM, import modules, and establish\nsome random input tensors.",
            "markdown"
        ],
        [
            "Do the Quantization - Here you instantiate a floating point model and then create quantized\nversion of it.",
            "markdown"
        ],
        [
            "Look at Model Size - Here you show that the model size gets smaller.",
            "markdown"
        ],
        [
            "Look at Latency - Here you run the two models and compare model runtime (latency).",
            "markdown"
        ],
        [
            "Look at Accuracy - Here you run the two models and compare outputs.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Dynamic Quantization->Steps->1: Set Up": [
        [
            "This is a straightfoward bit of code to set up for the rest of the\nrecipe.",
            "markdown"
        ],
        [
            "The unique module we are importing here is torch.quantization which\nincludes PyTorch\u2019s quantized operators and conversion functions. We also\ndefine a very simple LSTM model and set up some inputs.",
            "markdown"
        ],
        [
            "# import the modules used here in this recipe\nimport torch\nimport torch.quantization\nimport torch.nn as nn\nimport copy\nimport os\nimport time\n\n# define a very, very simple LSTM for demonstration purposes\n# in this case, we are wrapping nn.LSTM, one layer, no pre or post processing\n# inspired by\n# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html, by Robert Guthrie\n# and https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\nclass lstm_for_demonstration():\n  \"\"\"Elementary Long Short Term Memory style model which simply wraps nn.LSTM\n     Not to be used for anything other than demonstration.\n  \"\"\"\n  def __init__(self,in_dim,out_dim,depth):\n     super(lstm_for_demonstration,self).__init__()\n     self.lstm = (in_dim,out_dim,depth)\n\n  def forward(self,inputs,hidden):\n     out,hidden = self.lstm(inputs,hidden)\n     return out, hidden\n\n\n(29592)  # set the seed for reproducibility\n\n#shape parameters\nmodel_dimension=8\nsequence_length=20\nbatch_size=1\nlstm_depth=1\n\n# random data for input\ninputs = (sequence_length,batch_size,model_dimension)\n# hidden is actually is a tuple of the initial hidden state and the initial cell state\nhidden = ((lstm_depth,batch_size,model_dimension), (lstm_depth,batch_size,model_dimension))",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Dynamic Quantization->Steps->2: Do the Quantization": [
        [
            "Now we get to the fun part. First we create an instance of the model\ncalled float_lstm then we are going to quantize it. We\u2019re going to use\nthe",
            "markdown"
        ],
        [
            "torch.quantization.quantize_dynamic()",
            "code"
        ],
        [
            "function here ()\nwhich takes the model, then a list of the submodules which we want to\nhave quantized if they appear, then the datatype we are targeting. This\nfunction returns a quantized version of the original model as a new\nmodule.",
            "markdown"
        ],
        [
            "That\u2019s all it takes.",
            "markdown"
        ],
        [
            " # here is our floating point instance\nfloat_lstm = lstm_for_demonstration(model_dimension, model_dimension,lstm_depth)\n\n# this is the call that does the work\nquantized_lstm = torch.quantization.quantize_dynamic(\n    float_lstm, {, }, dtype=torch.qint8\n)\n\n# show the changes that were made\nprint('Here is the floating point version of this module:')\nprint(float_lstm)\nprint('')\nprint('and now the quantized version:')\nprint(quantized_lstm)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Dynamic Quantization->Steps->3. Look at Model Size": [
        [
            "Ok, so we\u2019ve quantized the model. What does that get us? Well the first\nbenefit is that we\u2019ve replaced the FP32 model parameters with INT8\nvalues (and some recorded scale factors). This means about 75% less data\nto store and move around. With the default values the reduction shown\nbelow will be less than 75% but if you increase the model size above\n(for example you can set model dimension to something like 80) this will\nconverge towards 4x smaller as the stored model size dominated more and\nmore by the parameter values.",
            "markdown"
        ],
        [
            "def print_size_of_model(model, label=\"\"):\n    (model.state_dict(), \"temp.p\")\n    size=os.path.getsize(\"temp.p\")\n    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n    os.remove('temp.p')\n    return size\n\n# compare the sizes\nf=print_size_of_model(float_lstm,\"fp32\")\nq=print_size_of_model(quantized_lstm,\"int8\")\nprint(\"{0:.2f} times smaller\".format(f/q))",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Dynamic Quantization->Steps->4. Look at Latency": [
        [
            "The second benefit is that the quantized model will typically run\nfaster. This is due to a combinations of effects including at least:",
            "markdown"
        ],
        [
            "Less time spent moving parameter data in",
            "markdown"
        ],
        [
            "Faster INT8 operations",
            "markdown"
        ],
        [
            "As you will see the quantized version of this super-simple network runs\nfaster. This will generally be true of more complex networks but as they\nsay \u201cyour milage may vary\u201d depending on a number of factors including\nthe structure of the model and the hardware you are running on.",
            "markdown"
        ],
        [
            "# compare the performance\nprint(\"Floating point FP32\")\n# %timeit float_lstm.forward(inputs, hidden)\n\nprint(\"Quantized INT8\")\n# %timeit quantized_lstm.forward(inputs,hidden)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Dynamic Quantization->Steps->5: Look at Accuracy": [
        [
            "We are not going to do a careful look at accuracy here because we are\nworking with a randomly initialized network rather than a properly\ntrained one. However, I think it is worth quickly showing that the\nquantized network does produce output tensors that are \u201cin the same\nballpark\u201d as the original one.",
            "markdown"
        ],
        [
            "For a more detailed analysis please see the more advanced tutorials\nreferenced at the end of this recipe.",
            "markdown"
        ],
        [
            "# run the float model\nout1, hidden1 = float_lstm(inputs, hidden)\nmag1 = (abs(out1)).item()\nprint('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n\n# run the quantized model\nout2, hidden2 = quantized_lstm(inputs, hidden)\nmag2 = (abs(out2)).item()\nprint('mean absolute value of output tensor values in the INT8 model is {0:.5f}'.format(mag2))\n\n# compare them\nmag3 = (abs(out1-out2)).item()\nprint('mean absolute value of the difference between the output tensors is {0:.5f} or {1:.2f} percent'.format(mag3,mag3/mag1*100))",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Dynamic Quantization->Learn More": [
        [
            "We\u2019ve explained what dynamic quantization is, what benefits it brings,\nand you have used the torch.quantization.quantize_dynamic() function\nto quickly quantize a simple LSTM model.",
            "markdown"
        ],
        [
            "This was a fast and high level treatment of this material; for more\ndetail please continue learning with .",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models across devices in PyTorch": [
        [
            "There may be instances where you want to save and load your neural\nnetworks across different devices.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models across devices in PyTorch->Introduction": [
        [
            "Saving and loading models across devices is relatively straightforward\nusing PyTorch. In this recipe, we will experiment with saving and\nloading models across CPUs and GPUs.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models across devices in PyTorch->Setup": [
        [
            "In order for every code block to run properly in this recipe, you must\nfirst change the runtime to \u201cGPU\u201d or higher. Once you do, we need to\ninstall torch if it isn\u2019t already available.",
            "markdown"
        ],
        [
            "pip install torch",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models across devices in PyTorch->Steps": [
        [
            "Import all necessary libraries for loading our data",
            "markdown"
        ],
        [
            "Define and intialize the neural network",
            "markdown"
        ],
        [
            "Save on a GPU, load on a CPU",
            "markdown"
        ],
        [
            "Save on a GPU, load on a GPU",
            "markdown"
        ],
        [
            "Save on a CPU, load on a GPU",
            "markdown"
        ],
        [
            "Saving and loading DataParallel models",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models across devices in PyTorch->Steps->1. Import necessary libraries for loading our data": [
        [
            "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.optim.",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models across devices in PyTorch->Steps->2. Define and intialize the neural network": [
        [
            "For sake of example, we will create a neural network for training\nimages. To learn more see the Defining a Neural Network recipe.",
            "markdown"
        ],
        [
            "class Net():\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nprint(net)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models across devices in PyTorch->Steps->3. Save on GPU, Load on CPU": [
        [
            "When loading a model on a CPU that was trained with a GPU, pass\ntorch.device('cpu') to the map_location argument in the\ntorch.load() function.",
            "markdown"
        ],
        [
            "# Specify a path to save to\nPATH = \"model.pt\"\n\n# Save\n(net.state_dict(), PATH)\n\n# Load\ndevice = ('cpu')\nmodel = Net()\nmodel.load_state_dict((PATH, map_location=device))",
            "code"
        ],
        [
            "In this case, the storages underlying the tensors are dynamically\nremapped to the CPU device using the map_location argument.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models across devices in PyTorch->Steps->4. Save on GPU, Load on GPU": [
        [
            "When loading a model on a GPU that was trained and saved on GPU, simply\nconvert the initialized model to a CUDA optimized model using\nmodel.to(torch.device('cuda')).",
            "markdown"
        ],
        [
            "Be sure to use the .to(torch.device('cuda')) function on all model\ninputs to prepare the data for the model.",
            "markdown"
        ],
        [
            "# Save\n(net.state_dict(), PATH)\n\n# Load\ndevice = (\"cuda\")\nmodel = Net()\nmodel.load_state_dict((PATH))\nmodel.to(device)",
            "code"
        ],
        [
            "Note that calling my_tensor.to(device) returns a new copy of\nmy_tensor on GPU. It does NOT overwrite my_tensor. Therefore,\nremember to manually overwrite tensors:\nmy_tensor = my_tensor.to(torch.device('cuda')).",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models across devices in PyTorch->Steps->5. Save on CPU, Load on GPU": [
        [
            "When loading a model on a GPU that was trained and saved on CPU, set the\nmap_location argument in the torch.load() function to\ncuda:device_id. This loads the model to a given GPU device.",
            "markdown"
        ],
        [
            "Be sure to call model.to(torch.device('cuda')) to convert the\nmodel\u2019s parameter tensors to CUDA tensors.",
            "markdown"
        ],
        [
            "Finally, also be sure to use the .to(torch.device('cuda')) function\non all model inputs to prepare the data for the CUDA optimized model.",
            "markdown"
        ],
        [
            "# Save\n(net.state_dict(), PATH)\n\n# Load\ndevice = (\"cuda\")\nmodel = Net()\n# Choose whatever GPU device number you want\nmodel.load_state_dict((PATH, map_location=\"cuda:0\"))\n# Make sure to call input = input.to(device) on any input tensors that you feed to the model\nmodel.to(device)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models across devices in PyTorch->Steps->6. Saving torch.nn.DataParallel Models": [
        [
            "torch.nn.DataParallel is a model wrapper that enables parallel GPU\nutilization.",
            "markdown"
        ],
        [
            "To save a DataParallel model generically, save the\nmodel.module.state_dict(). This way, you have the flexibility to\nload the model any way you want to any device you want.",
            "markdown"
        ],
        [
            "# Save\n(net.module.state_dict(), PATH)\n\n# Load to whatever device you want",
            "code"
        ],
        [
            "Congratulations! You have successfully saved and loaded models across\ndevices in PyTorch.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models across devices in PyTorch->Learn More": [
        [
            "Take a look at these other recipes to continue your learning:",
            "markdown"
        ],
        [
            "TBD",
            "markdown"
        ],
        [
            "TBD",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch": [
        [
            "Saving and loading a general checkpoint model for inference or\nresuming training can be helpful for picking up where you last left off.\nWhen saving a general checkpoint, you must save more than just the\nmodel\u2019s state_dict. It is important to also save the optimizer\u2019s\nstate_dict, as this contains buffers and parameters that are updated as\nthe model trains. Other items that you may want to save are the epoch\nyou left off on, the latest recorded training loss, external\ntorch.nn.Embedding layers, and more, based on your own algorithm.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Introduction": [
        [
            "To save multiple checkpoints, you must organize them in a dictionary and\nuse torch.save() to serialize the dictionary. A common PyTorch\nconvention is to save these checkpoints using the .tar file\nextension. To load the items, first initialize the model and optimizer,\nthen load the dictionary locally using torch.load(). From here, you can\neasily access the saved items by simply querying the dictionary as you\nwould expect.",
            "markdown"
        ],
        [
            "In this recipe, we will explore how to save and load multiple\ncheckpoints.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Setup": [
        [
            "Before we begin, we need to install torch if it isn\u2019t already\navailable.",
            "markdown"
        ],
        [
            "pip install torch",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Steps": [
        [
            "Import all necessary libraries for loading our data",
            "markdown"
        ],
        [
            "Define and initialize the neural network",
            "markdown"
        ],
        [
            "Initialize the optimizer",
            "markdown"
        ],
        [
            "Save the general checkpoint",
            "markdown"
        ],
        [
            "Load the general checkpoint",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Steps->1. Import necessary libraries for loading our data": [
        [
            "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.optim.",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Steps->2. Define and initialize the neural network": [
        [
            "For sake of example, we will create a neural network for training\nimages. To learn more see the Defining a Neural Network recipe.",
            "markdown"
        ],
        [
            "class Net():\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nprint(net)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Steps->3. Initialize the optimizer": [
        [
            "We will use SGD with momentum.",
            "markdown"
        ],
        [
            "optimizer = (net.parameters(), lr=0.001, momentum=0.9)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Steps->4. Save the general checkpoint": [
        [
            "Collect all relevant information and build your dictionary.",
            "markdown"
        ],
        [
            "# Additional information\nEPOCH = 5\nPATH = \"model.pt\"\nLOSS = 0.4\n\n({\n            'epoch': EPOCH,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': LOSS,\n            }, PATH)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Steps->5. Load the general checkpoint": [
        [
            "Remember to first initialize the model and optimizer, then load the\ndictionary locally.",
            "markdown"
        ],
        [
            "model = Net()\noptimizer = (net.parameters(), lr=0.001, momentum=0.9)\n\ncheckpoint = (PATH)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\nmodel.eval()\n# - or -\nmodel.train()",
            "code"
        ],
        [
            "You must call model.eval() to set dropout and batch normalization\nlayers to evaluation mode before running inference. Failing to do this\nwill yield inconsistent inference results.",
            "markdown"
        ],
        [
            "If you wish to resuming training, call model.train() to ensure these\nlayers are in training mode.",
            "markdown"
        ],
        [
            "Congratulations! You have successfully saved and loaded a general\ncheckpoint for inference and/or resuming training in PyTorch.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Learn More": [
        [
            "Take a look at these other recipes to continue your learning:",
            "markdown"
        ],
        [
            "TBD",
            "markdown"
        ],
        [
            "TBD",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch": [
        [
            "There are two approaches for saving and loading models for inference in\nPyTorch. The first is saving and loading the state_dict, and the\nsecond is saving and loading the entire model.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch->Introduction": [
        [
            "Saving the model\u2019s state_dict with the torch.save() function\nwill give you the most flexibility for restoring the model later. This\nis the recommended method for saving models, because it is only really\nnecessary to save the trained model\u2019s learned parameters.\nWhen saving and loading an entire model, you save the entire module\nusing Python\u2019s\n module. Using\nthis approach yields the most intuitive syntax and involves the least\namount of code. The disadvantage of this approach is that the serialized\ndata is bound to the specific classes and the exact directory structure\nused when the model is saved. The reason for this is because pickle does\nnot save the model class itself. Rather, it saves a path to the file\ncontaining the class, which is used during load time. Because of this,\nyour code can break in various ways when used in other projects or after\nrefactors.\nIn this recipe, we will explore both ways on how to save and load models\nfor inference.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch->Setup": [
        [
            "Before we begin, we need to install torch if it isn\u2019t already\navailable.",
            "markdown"
        ],
        [
            "pip install torch",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch->Steps": [
        [
            "Import all necessary libraries for loading our data",
            "markdown"
        ],
        [
            "Define and intialize the neural network",
            "markdown"
        ],
        [
            "Initialize the optimizer",
            "markdown"
        ],
        [
            "Save and load the model via state_dict",
            "markdown"
        ],
        [
            "Save and load the entire model",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch->Steps->1. Import necessary libraries for loading our data": [
        [
            "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.optim.",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch->Steps->2. Define and intialize the neural network": [
        [
            "For sake of example, we will create a neural network for training\nimages. To learn more see the Defining a Neural Network recipe.",
            "markdown"
        ],
        [
            "class Net():\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nprint(net)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch->Steps->3. Initialize the optimizer": [
        [
            "We will use SGD with momentum.",
            "markdown"
        ],
        [
            "optimizer = (net.parameters(), lr=0.001, momentum=0.9)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch->Steps->4. Save and load the model via state_dict": [
        [
            "Let\u2019s save and load our model using just state_dict.",
            "markdown"
        ],
        [
            "# Specify a path\nPATH = \"state_dict_model.pt\"\n\n# Save\n(net.state_dict(), PATH)\n\n# Load\nmodel = Net()\nmodel.load_state_dict((PATH))\nmodel.eval()",
            "code"
        ],
        [
            "A common PyTorch convention is to save models using either a .pt or\n.pth file extension.",
            "markdown"
        ],
        [
            "Notice that the load_state_dict() function takes a dictionary\nobject, NOT a path to a saved object. This means that you must\ndeserialize the saved state_dict before you pass it to the\nload_state_dict() function. For example, you CANNOT load using\nmodel.load_state_dict(PATH).",
            "markdown"
        ],
        [
            "Remember too, that you must call model.eval() to set dropout and\nbatch normalization layers to evaluation mode before running inference.\nFailing to do this will yield inconsistent inference results.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch->Steps->5. Save and load entire model": [
        [
            "Now let\u2019s try the same thing with the entire model.",
            "markdown"
        ],
        [
            "# Specify a path\nPATH = \"entire_model.pt\"\n\n# Save\n(net, PATH)\n\n# Load\nmodel = (PATH)\nmodel.eval()",
            "code"
        ],
        [
            "Again here, remember that you must call model.eval() to set dropout and\nbatch normalization layers to evaluation mode before running inference.",
            "markdown"
        ],
        [
            "Congratulations! You have successfully saved and load models for\ninference in PyTorch.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch->Learn More": [
        [
            "Take a look at these other recipes to continue your learning:",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading multiple models in one file using PyTorch": [
        [
            "Saving and loading multiple models can be helpful for reusing models\nthat you have previously trained.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading multiple models in one file using PyTorch->Introduction": [
        [
            "When saving a model comprised of multiple torch.nn.Modules, such as\na GAN, a sequence-to-sequence model, or an ensemble of models, you must\nsave a dictionary of each model\u2019s state_dict and corresponding\noptimizer. You can also save any other items that may aid you in\nresuming training by simply appending them to the dictionary.\nTo load the models, first initialize the models and optimizers, then\nload the dictionary locally using torch.load(). From here, you can\neasily access the saved items by simply querying the dictionary as you\nwould expect.\nIn this recipe, we will demonstrate how to save multiple models to one\nfile using PyTorch.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading multiple models in one file using PyTorch->Setup": [
        [
            "Before we begin, we need to install torch if it isn\u2019t already\navailable.",
            "markdown"
        ],
        [
            "pip install torch",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading multiple models in one file using PyTorch->Steps": [
        [
            "Import all necessary libraries for loading our data",
            "markdown"
        ],
        [
            "Define and intialize the neural network",
            "markdown"
        ],
        [
            "Initialize the optimizer",
            "markdown"
        ],
        [
            "Save multiple models",
            "markdown"
        ],
        [
            "Load multiple models",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading multiple models in one file using PyTorch->Steps->1. Import necessary libraries for loading our data": [
        [
            "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.optim.",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading multiple models in one file using PyTorch->Steps->2. Define and initialize the neural network": [
        [
            "For sake of example, we will create a neural network for training\nimages. To learn more see the Defining a Neural Network recipe. Build\ntwo variables for the models to eventually save.",
            "markdown"
        ],
        [
            "class Net():\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnetA = Net()\nnetB = Net()",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading multiple models in one file using PyTorch->Steps->3. Initialize the optimizer": [
        [
            "We will use SGD with momentum to build an optimizer for each model we\ncreated.",
            "markdown"
        ],
        [
            "optimizerA = (netA.parameters(), lr=0.001, momentum=0.9)\noptimizerB = (netB.parameters(), lr=0.001, momentum=0.9)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading multiple models in one file using PyTorch->Steps->4. Save multiple models": [
        [
            "Collect all relevant information and build your dictionary.",
            "markdown"
        ],
        [
            "# Specify a path to save to\nPATH = \"model.pt\"\n\n({\n            'modelA_state_dict': netA.state_dict(),\n            'modelB_state_dict': netB.state_dict(),\n            'optimizerA_state_dict': optimizerA.state_dict(),\n            'optimizerB_state_dict': optimizerB.state_dict(),\n            }, PATH)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading multiple models in one file using PyTorch->Steps->4. Load multiple models": [
        [
            "Remember to first initialize the models and optimizers, then load the\ndictionary locally.",
            "markdown"
        ],
        [
            "modelA = Net()\nmodelB = Net()\noptimModelA = (modelA.parameters(), lr=0.001, momentum=0.9)\noptimModelB = (modelB.parameters(), lr=0.001, momentum=0.9)\n\ncheckpoint = (PATH)\nmodelA.load_state_dict(checkpoint['modelA_state_dict'])\nmodelB.load_state_dict(checkpoint['modelB_state_dict'])\noptimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\noptimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])\n\nmodelA.eval()\nmodelB.eval()\n# - or -\nmodelA.train()\nmodelB.train()",
            "code"
        ],
        [
            "You must call model.eval() to set dropout and batch normalization\nlayers to evaluation mode before running inference. Failing to do this\nwill yield inconsistent inference results.",
            "markdown"
        ],
        [
            "If you wish to resuming training, call model.train() to ensure these\nlayers are in training mode.",
            "markdown"
        ],
        [
            "Congratulations! You have successfully saved and loaded multiple models\nin PyTorch.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Saving and loading multiple models in one file using PyTorch->Learn More": [
        [
            "Take a look at these other recipes to continue your learning:",
            "markdown"
        ],
        [
            "TBD",
            "markdown"
        ],
        [
            "TBD",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch": [
        [
            "Partially loading a model or loading a partial model are common\nscenarios when transfer learning or training a new complex model.\nLeveraging trained parameters, even if only a few are usable, will help\nto warmstart the training process and hopefully help your model converge\nmuch faster than training from scratch.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Introduction": [
        [
            "Whether you are loading from a partial state_dict, which is missing\nsome keys, or loading a state_dict with more keys than the model\nthat you are loading into, you can set the strict argument to False\nin the load_state_dict() function to ignore non-matching keys.\nIn this recipe, we will experiment with warmstarting a model using\nparameters of a different model.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Setup": [
        [
            "Before we begin, we need to install torch if it isn\u2019t already\navailable.",
            "markdown"
        ],
        [
            "pip install torch",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Steps": [
        [
            "Import all necessary libraries for loading our data",
            "markdown"
        ],
        [
            "Define and intialize the neural network A and B",
            "markdown"
        ],
        [
            "Save model A",
            "markdown"
        ],
        [
            "Load into model B",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Steps->1. Import necessary libraries for loading our data": [
        [
            "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.optim.",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Steps->2. Define and intialize the neural network A and B": [
        [
            "For sake of example, we will create a neural network for training\nimages. To learn more see the Defining a Neural Network recipe. We will\ncreate two neural networks for sake of loading one parameter of type A\ninto type B.",
            "markdown"
        ],
        [
            "class NetA():\n    def __init__(self):\n        super(NetA, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnetA = NetA()\n\nclass NetB():\n    def __init__(self):\n        super(NetB, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnetB = NetB()",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Steps->3. Save model A": [
        [
            "# Specify a path to save to\nPATH = \"model.pt\"\n\n(netA.state_dict(), PATH)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Steps->4. Load into model B": [
        [
            "If you want to load parameters from one layer to another, but some keys\ndo not match, simply change the name of the parameter keys in the\nstate_dict that you are loading to match the keys in the model that you\nare loading into.",
            "markdown"
        ],
        [
            "netB.load_state_dict((PATH), strict=False)",
            "code"
        ],
        [
            "You can see that all keys matched successfully!",
            "markdown"
        ],
        [
            "Congratulations! You have successfully warmstarted a model using\nparameters from a different model in PyTorch.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Learn More": [
        [
            "Take a look at these other recipes to continue your learning:",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Zeroing out gradients in PyTorch": [
        [
            "It is beneficial to zero out gradients when building a neural network.\nThis is because by default, gradients are accumulated in buffers (i.e,\nnot overwritten) whenever .backward() is called.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Introduction": [
        [
            "When training your neural network, models are able to increase their\naccuracy through gradient descent. In short, gradient descent is the\nprocess of minimizing our loss (or error) by tweaking the weights and\nbiases in our model.",
            "markdown"
        ],
        [
            "torch.Tensor is the central class of PyTorch. When you create a\ntensor, if you set its attribute .requires_grad as True, the\npackage tracks all operations on it. This happens on subsequent backward\npasses. The gradient for this tensor will be accumulated into .grad\nattribute. The accumulation (or sum) of all the gradients is calculated\nwhen .backward() is called on the loss tensor.",
            "markdown"
        ],
        [
            "There are cases where it may be necessary to zero-out the gradients of a\ntensor. For example: when you start your training loop, you should zero\nout the gradients so that you can perform this tracking correctly.\nIn this recipe, we will learn how to zero out gradients using the\nPyTorch library. We will demonstrate how to do this by training a neural\nnetwork on the CIFAR10 dataset built into PyTorch.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Setup": [
        [
            "Since we will be training data in this recipe, if you are in a runable\nnotebook, it is best to switch the runtime to GPU or TPU.\nBefore we begin, we need to install torch and torchvision if\nthey aren\u2019t already available.",
            "markdown"
        ],
        [
            "pip install torchvision",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps": [
        [
            "Steps 1 through 4 set up our data and neural network for training. The\nprocess of zeroing out the gradients happens in step 5. If you already\nhave your data and neural network built, skip to 5.",
            "markdown"
        ],
        [
            "Import all necessary libraries for loading our data",
            "markdown"
        ],
        [
            "Load and normalize the dataset",
            "markdown"
        ],
        [
            "Build the neural network",
            "markdown"
        ],
        [
            "Define the loss function",
            "markdown"
        ],
        [
            "Zero the gradients while training the network",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps->1. Import necessary libraries for loading our data": [
        [
            "For this recipe, we will just be using torch and torchvision to\naccess the dataset.",
            "markdown"
        ],
        [
            "import torch\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.optim as optim\n\nimport torchvision\nimport torchvision.transforms as transforms",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps->2. Load and normalize the dataset": [
        [
            "PyTorch features various built-in datasets (see the Loading Data recipe\nfor more information).",
            "markdown"
        ],
        [
            "transform = (\n    [(),\n     ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = (root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = (trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\ntestset = (root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = (testset, batch_size=4,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps->3. Build the neural network": [
        [
            "We will use a convolutional neural network. To learn more see the\nDefining a Neural Network recipe.",
            "markdown"
        ],
        [
            "class Net():\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool((self.conv1(x)))\n        x = self.pool((self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = (self.fc1(x))\n        x = (self.fc2(x))\n        x = self.fc3(x)\n        return x",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps->4. Define a Loss function and optimizer": [
        [
            "Let\u2019s use a Classification Cross-Entropy loss and SGD with momentum.",
            "markdown"
        ],
        [
            "net = Net()\ncriterion = ()\noptimizer = (net.parameters(), lr=0.001, momentum=0.9)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps->5. Zero the gradients while training the network": [
        [
            "This is when things start to get interesting. We simply have to loop\nover our data iterator, and feed the inputs to the network and optimize.",
            "markdown"
        ],
        [
            "Notice that for each entity of data, we zero out the gradients. This is\nto ensure that we aren\u2019t tracking any unnecessary information when we\ntrain our neural network.",
            "markdown"
        ],
        [
            "for epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')",
            "code"
        ],
        [
            "You can also use model.zero_grad(). This is the same as using\noptimizer.zero_grad() as long as all your model parameters are in\nthat optimizer. Use your best judgement to decide which one to use.",
            "markdown"
        ],
        [
            "Congratulations! You have successfully zeroed out gradients PyTorch.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Learn More": [
        [
            "Take a look at these other recipes to continue your learning:",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Pytorch Mobile Performance Recipes->Introduction": [
        [
            "Performance (aka latency) is crucial to most, if not all,\napplications and use-cases of ML model inference on mobile devices.",
            "markdown"
        ],
        [
            "Today, PyTorch executes the models on the CPU backend pending availability\nof other hardware backends such as GPU, DSP, and NPU.",
            "markdown"
        ],
        [
            "In this recipe, you will learn:",
            "markdown"
        ],
        [
            "How to optimize your model to help decrease execution time (higher performance, lower latency) on the mobile device.",
            "markdown"
        ],
        [
            "How to benchmark (to check if optimizations helped your use case).",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Pytorch Mobile Performance Recipes->Model preparation": [
        [
            "We will start with preparing to optimize your model to help decrease execution time\n(higher performance, lower latency) on the mobile device.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Pytorch Mobile Performance Recipes->Model preparation->Setup": [
        [
            "First we need to installed pytorch using conda or pip with version at least 1.5.0.",
            "markdown"
        ],
        [
            "conda install pytorch torchvision -c pytorch",
            "code"
        ],
        [
            "or",
            "markdown"
        ],
        [
            "pip install torch torchvision",
            "code"
        ],
        [
            "Code your model:",
            "markdown"
        ],
        [
            "import torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nclass AnnotatedConvBnReLUModel(torch.nn.Module):\n    def __init__(self):\n        super(AnnotatedConvBnReLUModel, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 5, 3, bias=False).to(dtype=torch.float)\n        self.bn = torch.nn.BatchNorm2d(5).to(dtype=torch.float)\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.quant = torch.quantization.QuantStub()\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        x = x.contiguous(memory_format=torch.channels_last)\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dequant(x)\n        return x\n\nmodel = AnnotatedConvBnReLUModel()",
            "code"
        ],
        [
            "torch.quantization.QuantStub and torch.quantization.DeQuantStub() are no-op stubs, which will be used for quantization step.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Pytorch Mobile Performance Recipes->Model preparation->1. Fuse operators using torch.quantization.fuse_modules": [
        [
            "Do not be confused that fuse_modules is in the quantization package.\nIt works for all torch.nn.Module.",
            "markdown"
        ],
        [
            "torch.quantization.fuse_modules fuses a list of modules into a single module.\nIt fuses only the following sequence of modules:",
            "markdown"
        ],
        [
            "Convolution, Batch normalization",
            "markdown"
        ],
        [
            "Convolution, Batch normalization, Relu",
            "markdown"
        ],
        [
            "Convolution, Relu",
            "markdown"
        ],
        [
            "Linear, Relu",
            "markdown"
        ],
        [
            "This script will fuse Convolution, Batch Normalization and Relu in previously declared model.",
            "markdown"
        ],
        [
            "torch.quantization.fuse_modules(model, [['conv', 'bn', 'relu']], inplace=True)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Pytorch Mobile Performance Recipes->Model preparation->2. Quantize your model": [
        [
            "You can find more about PyTorch quantization in\n.",
            "markdown"
        ],
        [
            "Quantization of the model not only moves computation to int8,\nbut also reduces the size of your model on a disk.\nThat size reduction helps to reduce disk read operations during the first load of the model and decreases the amount of RAM.\nBoth of those resources can be crucial for the performance of mobile applications.\nThis code does quantization, using stub for model calibration function, you can find more about it .",
            "markdown"
        ],
        [
            "model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\ntorch.quantization.prepare(model, inplace=True)\n# Calibrate your model\ndef calibrate(model, calibration_data):\n    # Your calibration code here\n    return\ncalibrate(model, [])\ntorch.quantization.convert(model, inplace=True)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Pytorch Mobile Performance Recipes->Model preparation->3. Use torch.utils.mobile_optimizer": [
        [
            "Torch mobile_optimizer package does several optimizations with the scripted model,\nwhich will help to conv2d and linear operations.\nIt pre-packs model weights in an optimized format and fuses ops above with relu\nif it is the next operation.",
            "markdown"
        ],
        [
            "First we script the result model from previous step:",
            "markdown"
        ],
        [
            "torchscript_model = torch.jit.script(model)",
            "code"
        ],
        [
            "Next we call optimize_for_mobile and save model on the disk.",
            "markdown"
        ],
        [
            "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\ntorch.jit.save(torchscript_model_optimized, \"model.pt\")",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Pytorch Mobile Performance Recipes->Model preparation->4. Prefer Using Channels Last Tensor memory format": [
        [
            "Channels Last(NHWC) memory format was introduced in PyTorch 1.4.0. It is supported only for four-dimensional tensors. This memory format gives a better memory locality for most operators, especially convolution. Our measurements showed a 3x speedup of MobileNetV2 model compared with the default Channels First(NCHW) format.",
            "markdown"
        ],
        [
            "At the moment of writing this recipe, PyTorch Android java API does not support using inputs in Channels Last memory format. But it can be used on the TorchScript model level, by adding the conversion to it for model inputs.",
            "markdown"
        ],
        [
            "def forward(self, x):\n    x = x.contiguous(memory_format=torch.channels_last)\n    ...",
            "code"
        ],
        [
            "This conversion is zero cost if your input is already in Channels Last memory format. After it, all operators will work preserving ChannelsLast memory format.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Pytorch Mobile Performance Recipes->Model preparation->5. Android - Reusing tensors for forward": [
        [
            "This part of the recipe is Android only.",
            "markdown"
        ],
        [
            "Memory is a critical resource for android performance, especially on old devices.\nTensors can need a significant amount of memory.\nFor example, standard computer vision tensor contains 1*3*224*224 elements,\nassuming that data type is float and will need 588Kb of memory.",
            "markdown"
        ],
        [
            "FloatBuffer buffer = Tensor.allocateFloatBuffer(1*3*224*224);\nTensor tensor = Tensor.fromBlob(buffer, new long[]{1, 3, 224, 224});",
            "code"
        ],
        [
            "Here we allocate native memory as java.nio.FloatBuffer and creating org.pytorch.Tensor which storage will be pointing to the memory of the allocated buffer.",
            "markdown"
        ],
        [
            "For most of the use cases, we do not do model forward only once, repeating it with some frequency or as fast as possible.",
            "markdown"
        ],
        [
            "If we are doing new memory allocation for every module forward - that will be suboptimal.\nInstead of this, we can reuse the same memory that we allocated on the previous step, fill it with new data, and run module forward again on the same tensor object.",
            "markdown"
        ],
        [
            "You can check how it looks in code in .",
            "markdown"
        ],
        [
            "protected AnalysisResult analyzeImage(ImageProxy image, int rotationDegrees) {\n  if (mModule == null) {\n    mModule = Module.load(moduleFileAbsoluteFilePath);\n    mInputTensorBuffer =\n    Tensor.allocateFloatBuffer(3 * 224 * 224);\n    mInputTensor = Tensor.fromBlob(mInputTensorBuffer, new long[]{1, 3, 224, 224});\n  }\n\n  TensorImageUtils.imageYUV420CenterCropToFloatBuffer(\n      image.getImage(), rotationDegrees,\n      224, 224,\n      TensorImageUtils.TORCHVISION_NORM_MEAN_RGB,\n      TensorImageUtils.TORCHVISION_NORM_STD_RGB,\n      mInputTensorBuffer, 0);\n\n  Tensor outputTensor = mModule.forward(IValue.from(mInputTensor)).toTensor();\n}",
            "code"
        ],
        [
            "Member fields mModule, mInputTensorBuffer and mInputTensor are initialized only once\nand buffer is refilled using org.pytorch.torchvision.TensorImageUtils.imageYUV420CenterCropToFloatBuffer.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Pytorch Mobile Performance Recipes->Benchmarking": [
        [
            "The best way to benchmark (to check if optimizations helped your use case) - is to measure your particular use case that you want to optimize, as performance behavior can vary in different environments.",
            "markdown"
        ],
        [
            "PyTorch distribution provides a way to benchmark naked binary that runs the model forward,\nthis approach can give more stable measurements rather than testing inside the application.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Pytorch Mobile Performance Recipes->Benchmarking->Android - Benchmarking Setup": [
        [
            "This part of the recipe is Android only.",
            "markdown"
        ],
        [
            "For this you first need to build benchmark binary:",
            "markdown"
        ],
        [
            "&lt;from-your-root-pytorch-dir&gt;\nrm -rf build_android\nBUILD_PYTORCH_MOBILE=1 ANDROID_ABI=arm64-v8a ./scripts/build_android.sh -DBUILD_BINARY=ON",
            "code"
        ],
        [
            "You should have arm64 binary at: build_android/bin/speed_benchmark_torch.\nThis binary takes --model=&lt;path-to-model&gt;, --input_dim=\"1,3,224,224\" as dimension information for the input and --input_type=\"float\" as the type of the input as arguments.",
            "markdown"
        ],
        [
            "Once you have your android device connected,\npush speedbenchark_torch binary and your model to the phone:",
            "markdown"
        ],
        [
            "adb push &lt;speedbenchmark-torch&gt; /data/local/tmp\nadb push &lt;path-to-scripted-model&gt; /data/local/tmp",
            "code"
        ],
        [
            "Now we are ready to benchmark your model:",
            "markdown"
        ],
        [
            "adb shell \"/data/local/tmp/speed_benchmark_torch --model=/data/local/tmp/model.pt\" --input_dims=\"1,3,224,224\" --input_type=\"float\"\n----- output -----\nStarting benchmark.\nRunning warmup runs.\nMain runs.\nMain run finished. Microseconds per iter: 121318. Iters per second: 8.24281",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Pytorch Mobile Performance Recipes->Benchmarking->iOS - Benchmarking Setup": [
        [
            "For iOS, we\u2019ll be using our  as the benchmarking tool.",
            "markdown"
        ],
        [
            "To begin with, let\u2019s apply the optimize_for_mobile method to our python script located at . Simply modify the code as below.",
            "markdown"
        ],
        [
            "import torch\nimport torchvision\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nmodel = torchvision.models.mobilenet_v2(pretrained=True)\nmodel.eval()\nexample = torch.rand(1, 3, 224, 224)\ntraced_script_module = torch.jit.trace(model, example)\ntorchscript_model_optimized = optimize_for_mobile(traced_script_module)\ntorch.jit.save(torchscript_model_optimized, \"model.pt\")",
            "code"
        ],
        [
            "Now let\u2019s run python trace_model.py. If everything works well, we should be able to generate our optimized model in the benchmark directory.",
            "markdown"
        ],
        [
            "Next, we\u2019re going to build the PyTorch libraries from source.",
            "markdown"
        ],
        [
            "BUILD_PYTORCH_MOBILE=1 IOS_ARCH=arm64 ./scripts/build_ios.sh",
            "code"
        ],
        [
            "Now that we have the optimized model and PyTorch ready, it\u2019s time to generate our XCode project and do benchmarking. To do that, we\u2019ll be using a ruby script - <cite>setup.rb</cite> which does the heavy lifting jobs of setting up the XCode project.",
            "markdown"
        ],
        [
            "ruby setup.rb",
            "code"
        ],
        [
            "Now open the <cite>TestApp.xcodeproj</cite> and plug in your iPhone, you\u2019re ready to go. Below is an example result from iPhoneX",
            "markdown"
        ],
        [
            "TestApp[2121:722447] Main runs\nTestApp[2121:722447] Main run finished. Milliseconds per iter: 28.767\nTestApp[2121:722447] Iters per second: : 34.762\nTestApp[2121:722447] Done.",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Automatic Mixed Precision": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            " provides convenience methods for mixed precision,\nwhere some operations use the torch.float32 (float) datatype and other operations\nuse torch.float16 (half). Some ops, like linear layers and convolutions,\nare much faster in float16 or bfloat16. Other ops, like reductions, often require the dynamic\nrange of float32.  Mixed precision tries to match each op to its appropriate datatype,\nwhich can reduce your network\u2019s runtime and memory footprint.",
            "markdown"
        ],
        [
            "Ordinarily, \u201cautomatic mixed precision training\u201d uses  and\n together.",
            "markdown"
        ],
        [
            "This recipe measures the performance of a simple network in default precision,\nthen walks through adding autocast and GradScaler to run the same network in\nmixed precision with improved performance.",
            "markdown"
        ],
        [
            "You may download and run this recipe as a standalone Python script.\nThe only requirements are PyTorch 1.6 or later and a CUDA-capable GPU.",
            "markdown"
        ],
        [
            "Mixed precision primarily benefits Tensor Core-enabled architectures (Volta, Turing, Ampere).\nThis recipe should show significant (2-3X) speedup on those architectures.\nOn earlier architectures (Kepler, Maxwell, Pascal), you may observe a modest speedup.\nRun nvidia-smi to display your GPU\u2019s architecture.",
            "markdown"
        ],
        [
            "import torch, time, gc\n\n# Timing utilities\nstart_time = None\n\ndef start_timer():\n    global start_time\n    gc.collect()\n    ()\n    ()\n    ()\n    start_time = time.time()\n\ndef end_timer_and_print(local_msg):\n    ()\n    end_time = time.time()\n    print(\"\\n\" + local_msg)\n    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n    print(\"Max memory used by tensors = {} bytes\".format(()))",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Automatic Mixed Precision->A simple network": [
        [
            "The following sequence of linear layers and ReLUs should show a speedup with mixed precision.",
            "markdown"
        ],
        [
            "def make_model(in_size, out_size, num_layers):\n    layers = []\n    for _ in range(num_layers - 1):\n        layers.append((in_size, in_size))\n        layers.append(())\n    layers.append((in_size, out_size))\n    return (*tuple(layers)).cuda()",
            "code"
        ],
        [
            "batch_size, in_size, out_size, and num_layers are chosen to be large enough to saturate the GPU with work.\nTypically, mixed precision provides the greatest speedup when the GPU is saturated.\nSmall networks may be CPU bound, in which case mixed precision won\u2019t improve performance.\nSizes are also chosen such that linear layers\u2019 participating dimensions are multiples of 8,\nto permit Tensor Core usage on Tensor Core-capable GPUs (see  below).",
            "markdown"
        ],
        [
            "Exercise: Vary participating sizes and see how the mixed precision speedup changes.",
            "markdown"
        ],
        [
            "batch_size = 512 # Try, for example, 128, 256, 513.\nin_size = 4096\nout_size = 4096\nnum_layers = 3\nnum_batches = 50\nepochs = 3\n\n# Creates data in default precision.\n# The same data is used for both default and mixed precision trials below.\n# You don't need to manually change inputs' dtype when enabling mixed precision.\ndata = [(batch_size, in_size, device=\"cuda\") for _ in range(num_batches)]\ntargets = [(batch_size, out_size, device=\"cuda\") for _ in range(num_batches)]\n\nloss_fn = ().cuda()",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Automatic Mixed Precision->Default Precision": [
        [
            "Without torch.cuda.amp, the following simple network executes all ops in default precision (torch.float32):",
            "markdown"
        ],
        [
            "net = make_model(in_size, out_size, num_layers)\nopt = (net.parameters(), lr=0.001)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        output = net(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Default precision:\")",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Automatic Mixed Precision->Adding autocast": [
        [
            "Instances of \nserve as context managers that allow regions of your script to run in mixed precision.",
            "markdown"
        ],
        [
            "In these regions, CUDA ops run in a dtype chosen by autocast\nto improve performance while maintaining accuracy.\nSee the \nfor details on what precision autocast chooses for each op, and under what circumstances.",
            "markdown"
        ],
        [
            "for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        # Runs the forward pass under autocast.\n        with (device_type='cuda', dtype=torch.float16):\n            output = net(input)\n            # output is float16 because linear layers autocast to float16.\n            assert output.dtype is torch.float16\n\n            loss = loss_fn(output, target)\n            # loss is float32 because mse_loss layers autocast to float32.\n            assert loss.dtype is torch.float32\n\n        # Exits autocast before backward().\n        # Backward passes under autocast are not recommended.\n        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Automatic Mixed Precision->Adding GradScaler": [
        [
            "helps prevent gradients with small magnitudes from flushing to zero\n(\u201cunderflowing\u201d) when training with mixed precision.",
            "markdown"
        ],
        [
            "performs the steps of gradient scaling conveniently.",
            "markdown"
        ],
        [
            "# Constructs scaler once, at the beginning of the convergence run, using default args.\n# If your network fails to converge with default GradScaler args, please file an issue.\n# The same GradScaler instance should be used for the entire convergence run.\n# If you perform multiple convergence runs in the same script, each run should use\n# a dedicated fresh GradScaler instance.  GradScaler instances are lightweight.\nscaler = ()\n\nfor epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16):\n            output = net(input)\n            loss = loss_fn(output, target)\n\n        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n        scaler.scale(loss).backward()\n\n        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n        # otherwise, optimizer.step() is skipped.\n        scaler.step(opt)\n\n        # Updates the scale for next iteration.\n        scaler.update()\n\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Automatic Mixed Precision->All together: \u201cAutomatic Mixed Precision\u201d": [
        [
            "(The following also demonstrates enabled, an optional convenience argument to autocast and GradScaler.\nIf False, autocast and GradScaler\u2018s calls become no-ops.\nThis allows switching between default precision and mixed precision without if/else statements.)",
            "markdown"
        ],
        [
            "use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = (net.parameters(), lr=0.001)\nscaler = (enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Mixed precision:\")",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Automatic Mixed Precision->Inspecting/modifying gradients (e.g., clipping)": [
        [
            "All gradients produced by scaler.scale(loss).backward() are scaled.  If you wish to modify or inspect\nthe parameters\u2019 .grad attributes between backward() and scaler.step(optimizer), you should\nunscale them first using .",
            "markdown"
        ],
        [
            "for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n\n        # Unscales the gradients of optimizer's assigned params in-place\n        scaler.unscale_(opt)\n\n        # Since the gradients of optimizer's assigned params are now unscaled, clips as usual.\n        # You may use the same value for max_norm here as you would without gradient scaling.\n        (net.parameters(), max_norm=0.1)\n\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Automatic Mixed Precision->Saving/Resuming": [
        [
            "To save/resume Amp-enabled runs with bitwise accuracy, use\n and\n.",
            "markdown"
        ],
        [
            "When saving, save the scaler state dict alongside the usual model and optimizer state dicts.\nDo this either at the beginning of an iteration before any forward passes, or at the end of\nan iteration after scaler.update().",
            "markdown"
        ],
        [
            "checkpoint = {\"model\": net.state_dict(),\n              \"optimizer\": opt.state_dict(),\n              \"scaler\": scaler.state_dict()}\n# Write checkpoint as desired, e.g.,\n# torch.save(checkpoint, \"filename\")",
            "code"
        ],
        [
            "When resuming, load the scaler state dict alongside the model and optimizer state dicts.",
            "markdown"
        ],
        [
            "# Read checkpoint as desired, e.g.,\n# dev = torch.cuda.current_device()\n# checkpoint = torch.load(\"filename\",\n#                         map_location = lambda storage, loc: storage.cuda(dev))\nnet.load_state_dict(checkpoint[\"model\"])\nopt.load_state_dict(checkpoint[\"optimizer\"])\nscaler.load_state_dict(checkpoint[\"scaler\"])",
            "code"
        ],
        [
            "If a checkpoint was created from a run <em>without</em> Amp, and you want to resume training <em>with</em> Amp,\nload model and optimizer states from the checkpoint as usual.  The checkpoint won\u2019t contain a saved scaler state, so\nuse a fresh instance of GradScaler.",
            "markdown"
        ],
        [
            "If a checkpoint was created from a run <em>with</em> Amp and you want to resume training <em>without</em> Amp,\nload model and optimizer states from the checkpoint as usual, and ignore the saved scaler state.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Automatic Mixed Precision->Inference/Evaluation": [
        [
            "autocast may be used by itself to wrap inference or evaluation forward passes. GradScaler is not necessary.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Automatic Mixed Precision->Advanced topics": [
        [
            "See the  for advanced use cases including:",
            "markdown"
        ],
        [
            "Gradient accumulation",
            "markdown"
        ],
        [
            "Gradient penalty/double backward",
            "markdown"
        ],
        [
            "Networks with multiple models, optimizers, or losses",
            "markdown"
        ],
        [
            "Multiple GPUs (torch.nn.DataParallel or torch.nn.parallel.DistributedDataParallel)",
            "markdown"
        ],
        [
            "Custom autograd functions (subclasses of torch.autograd.Function)",
            "markdown"
        ],
        [
            "If you perform multiple convergence runs in the same script, each run should use\na dedicated fresh GradScaler instance.  GradScaler instances are lightweight.",
            "markdown"
        ],
        [
            "If you\u2019re registering a custom C++ op with the dispatcher, see the\n\nof the dispatcher tutorial.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Automatic Mixed Precision->Troubleshooting->Speedup with Amp is minor": [
        [
            "Your network may fail to saturate the GPU(s) with work, and is therefore CPU bound. Amp\u2019s effect on GPU performance\nwon\u2019t matter.",
            "markdown"
        ],
        [
            "A rough rule of thumb to saturate the GPU is to increase batch and/or network size(s)\nas much as you can without running OOM.",
            "markdown"
        ],
        [
            "Try to avoid excessive CPU-GPU synchronization (.item() calls, or printing values from CUDA tensors).",
            "markdown"
        ],
        [
            "Try to avoid sequences of many small CUDA ops (coalesce these into a few large CUDA ops if you can).",
            "markdown"
        ],
        [
            "Your network may be GPU compute bound (lots of matmuls/convolutions) but your GPU does not have Tensor Cores.\nIn this case a reduced speedup is expected.",
            "markdown"
        ],
        [
            "Matmul dimensions are not Tensor Core-friendly.  Make sure matmuls\u2019 participating sizes are multiples of 8.\n(For NLP models with encoders/decoders, this can be subtle.  Also, convolutions used to have similar size constraints\nfor Tensor Core use, but for CuDNN versions 7.3 and later, no such constraints exist.  See\n for guidance.)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Automatic Mixed Precision->Troubleshooting->Loss is inf/NaN": [
        [
            "First, check if your network fits an .\nSee also .",
            "markdown"
        ],
        [
            "If you\u2019re confident your Amp usage is correct, you may need to file an issue, but before doing so, it\u2019s helpful to gather the following information:",
            "markdown"
        ],
        [
            "Disable autocast or GradScaler individually (by passing enabled=False to their constructor) and see if infs/NaNs persist.",
            "markdown"
        ],
        [
            "If you suspect part of your network (e.g., a complicated loss function) overflows , run that forward region in float32\nand see if infs/NaNs persist.\n\u2019s last code snippet\nshows forcing a subregion to run in float32 (by locally disabling autocast and casting the subregion\u2019s inputs).",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Automatic Mixed Precision->Troubleshooting->Type mismatch error (may manifest as CUDNN_STATUS_BAD_PARAM)": [
        [
            "Autocast tries to cover all ops that benefit from or require casting.\n\nare chosen based on numerical properties, but also on experience.\nIf you see a type mismatch error in an autocast-enabled forward region or a backward pass following that region,\nit\u2019s possible autocast missed an op.",
            "markdown"
        ],
        [
            "Please file an issue with the error backtrace.  export TORCH_SHOW_CPP_STACKTRACES=1 before running your script to provide\nfine-grained information on which backend op is failing.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Changing default device": [
        [
            "It is common practice to write PyTorch code in a device-agnostic way,\nand then switch between CPU and CUDA depending on what hardware is available.\nTypically, to do this you might have used if-statements and cuda() calls\nto do this:",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This recipe requires PyTorch 2.0.0 or later.",
            "markdown"
        ],
        [
            "import torch\n\nUSE_CUDA = False\n\n = (20, 30)\nif USE_CUDA:\n    ()\n\ndevice = 'cpu'\nif USE_CUDA:\n    device = 'cuda'\n = (128, 20, device=device)\nprint(().device)",
            "code"
        ],
        [
            "cpu",
            "code"
        ],
        [
            "PyTorch now also has a context manager which can take care of the\ndevice transfer automatically. Here is an example:",
            "markdown"
        ],
        [
            "with ('cuda'):\n     = (20, 30)\n    print()\n    print(((128, 20)).device)",
            "code"
        ],
        [
            "cuda:0\ncuda:0",
            "code"
        ],
        [
            "You can also set it globally like this:",
            "markdown"
        ],
        [
            "('cuda')\n\n = (20, 30)\nprint()\nprint(((128, 20)).device)",
            "code"
        ],
        [
            "cuda:0\ncuda:0",
            "code"
        ],
        [
            "This function imposes a slight performance cost on every Python\ncall to the torch API (not just factory functions). If this\nis causing problems for you, please comment on",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.718 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->How to use TensorBoard with PyTorch": [
        [
            "TensorBoard is a visualization toolkit for machine learning experimentation.\nTensorBoard allows tracking and visualizing metrics such as loss and accuracy,\nvisualizing the model graph, viewing histograms, displaying images and much more.\nIn this tutorial we are going to cover TensorBoard installation,\nbasic usage with PyTorch, and how to visualize data you logged in TensorBoard UI.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Installation": [
        [
            "PyTorch should be installed to log models and metrics into TensorBoard log\ndirectory. The following command will install PyTorch 1.4+ via\nAnaconda (recommended):",
            "markdown"
        ],
        [
            "$ conda install pytorch torchvision -c pytorch",
            "code"
        ],
        [
            "or pip",
            "markdown"
        ],
        [
            "$ pip install torch torchvision",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Using TensorBoard in PyTorch": [
        [
            "Let\u2019s now try using TensorBoard with PyTorch! Before logging anything,\nwe need to create a SummaryWriter instance.",
            "markdown"
        ],
        [
            "import torch\nfrom torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()",
            "code"
        ],
        [
            "Writer will output to ./runs/ directory by default.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Log scalars": [
        [
            "In machine learning, it\u2019s important to understand key metrics such as\nloss and how they change during training. Scalar helps to save\nthe loss value of each training step, or the accuracy after each epoch.",
            "markdown"
        ],
        [
            "To log a scalar value, use\nadd_scalar(tag, scalar_value, global_step=None, walltime=None).\nFor example, lets create a simple linear regression training, and\nlog loss value using add_scalar",
            "markdown"
        ],
        [
            "x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()",
            "code"
        ],
        [
            "Call flush() method to make sure that all pending events\nhave been written to disk.",
            "markdown"
        ],
        [
            "See \nto find more TensorBoard visualization types you can log.",
            "markdown"
        ],
        [
            "If you do not need the summary writer anymore, call close() method.",
            "markdown"
        ],
        [
            "writer.close()",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Run TensorBoard": [
        [
            "Install TensorBoard through the command line to visualize data you logged",
            "markdown"
        ],
        [
            "$ pip install tensorboard",
            "code"
        ],
        [
            "Now, start TensorBoard, specifying the root log directory you used above.\nArgument logdir points to directory where TensorBoard will look to find\nevent files that it can display. TensorBoard will recursively walk\nthe directory structure rooted at logdir, looking for .*tfevents.* files.",
            "markdown"
        ],
        [
            "$ tensorboard --logdir=runs",
            "code"
        ],
        [
            "Go to the URL it provides OR to ",
            "markdown"
        ],
        [
            "This dashboard shows how the loss and accuracy change with every epoch.\nYou can use it to also track training speed, learning rate, and other\nscalar values. It\u2019s helpful to compare these metrics across different\ntraining runs to improve your model.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Share TensorBoard dashboards": [
        [
            " lets you upload and share\nyour ML experiment results with anyone. Use TensorBoard.dev to host,\ntrack, and share your TensorBoard dashboards.",
            "markdown"
        ],
        [
            "Install the latest version of TensorBoard to use the uploader.",
            "markdown"
        ],
        [
            "$ pip install tensorboard --upgrade",
            "code"
        ],
        [
            "Use a simple command to upload and share your TensorBoard.",
            "markdown"
        ],
        [
            "$ tensorboard dev upload --logdir runs \\\n--name \"My latest experiment\" \\ # optional\n--description \"Simple comparison of several hyperparameters\" # optional",
            "code"
        ],
        [
            "For help, run $ tensorboard dev --help.",
            "markdown"
        ],
        [
            "<strong>Note:</strong> Uploaded TensorBoards are public and visible to everyone.\nDo not upload sensitive data.",
            "markdown"
        ],
        [
            "View your TensorBoard live at URL provided in your terminal.\nE.g. ",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "TensorBoard.dev currently supports scalars, graphs, histograms, distributions, hparams, and text dashboards.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Learn More": [
        [
            " docs",
            "markdown"
        ],
        [
            " tutorial",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "Performance Tuning Guide is a set of optimizations and best practices which can\naccelerate training and inference of deep learning models in PyTorch. Presented\ntechniques often can be implemented by changing only a few lines of code and can\nbe applied to a wide range of deep learning models across all domains.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->General optimizations->Enable async data loading and augmentation": [
        [
            "supports asynchronous data loading and data augmentation in separate worker\nsubprocesses. The default setting for DataLoader is num_workers=0,\nwhich means that the data loading is synchronous and done in the main process.\nAs a result the main training process has to wait for the data to be available\nto continue the execution.",
            "markdown"
        ],
        [
            "Setting num_workers &gt; 0 enables asynchronous data loading and overlap\nbetween the training and data loading. num_workers should be tuned\ndepending on the workload, CPU, GPU, and location of training data.",
            "markdown"
        ],
        [
            "DataLoader accepts pin_memory argument, which defaults to False.\nWhen using a GPU it\u2019s better to set pin_memory=True, this instructs\nDataLoader to use pinned memory and enables faster and asynchronous memory\ncopy from the host to the GPU.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->General optimizations->Disable gradient calculation for validation or inference": [
        [
            "PyTorch saves intermediate buffers from all operations which involve tensors\nthat require gradients. Typically gradients aren\u2019t needed for validation or\ninference.\n\ncontext manager can be applied to disable gradient calculation within a\nspecified block of code, this accelerates execution and reduces the amount of\nrequired memory.\n\ncan also be used as a function decorator.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->General optimizations->Disable bias for convolutions directly followed by a batch norm": [
        [
            "has bias parameter which defaults to True (the same is true for\n\nand\n\n).",
            "markdown"
        ],
        [
            "If a nn.Conv2d layer is directly followed by a nn.BatchNorm2d layer,\nthen the bias in the convolution is not needed, instead use\nnn.Conv2d(..., bias=False, ....). Bias is not needed because in the first\nstep BatchNorm subtracts the mean, which effectively cancels out the\neffect of bias.",
            "markdown"
        ],
        [
            "This is also applicable to 1d and 3d convolutions as long as BatchNorm (or\nother normalization layer) normalizes on the same dimension as convolution\u2019s\nbias.",
            "markdown"
        ],
        [
            "Models available from \nalready implement this optimization.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->General optimizations->Use parameter.grad = None instead of model.zero_grad() or optimizer.zero_grad()": [
        [
            "Instead of calling:",
            "markdown"
        ],
        [
            "model.zero_grad()\n# or\noptimizer.zero_grad()",
            "code"
        ],
        [
            "to zero out gradients, use the following method instead:",
            "markdown"
        ],
        [
            "for param in model.parameters():\n    param.grad = None",
            "code"
        ],
        [
            "The second code snippet does not zero the memory of each individual parameter,\nalso the subsequent backward pass uses assignment instead of addition to store\ngradients, this reduces the number of memory operations.",
            "markdown"
        ],
        [
            "Setting gradient to None has a slightly different numerical behavior than\nsetting it to zero, for more details refer to the\n.",
            "markdown"
        ],
        [
            "Alternatively, starting from PyTorch 1.7, call model or\noptimizer.zero_grad(set_to_none=True).",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->General optimizations->Fuse pointwise operations": [
        [
            "Pointwise operations (elementwise addition, multiplication, math functions -\nsin(), cos(), sigmoid() etc.) can be fused into a single kernel\nto amortize memory access time and kernel launch time.",
            "markdown"
        ],
        [
            " can fuse kernels\nautomatically, although there could be additional fusion opportunities not yet\nimplemented in the compiler, and not all device types are supported equally.",
            "markdown"
        ],
        [
            "Pointwise operations are memory-bound, for each operation PyTorch launches a\nseparate kernel. Each kernel loads data from the memory, performs computation\n(this step is usually inexpensive) and stores results back into the memory.",
            "markdown"
        ],
        [
            "Fused operator launches only one kernel for multiple fused pointwise ops and\nloads/stores data only once to the memory. This makes JIT very useful for\nactivation functions, optimizers, custom RNN cells etc.",
            "markdown"
        ],
        [
            "In the simplest case fusion can be enabled by applying\n\ndecorator to the function definition, for example:",
            "markdown"
        ],
        [
            "@torch.jit.script\ndef fused_gelu(x):\n    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))",
            "code"
        ],
        [
            "Refer to\n\nfor more advanced use cases.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->General optimizations->Enable channels_last memory format for computer vision models": [
        [
            "PyTorch 1.5 introduced support for channels_last memory format for\nconvolutional networks. This format is meant to be used in conjunction with\n to further accelerate\nconvolutional neural networks with\n.",
            "markdown"
        ],
        [
            "Support for channels_last is experimental, but it\u2019s expected to work for\nstandard computer vision models (e.g. ResNet-50, SSD). To convert models to\nchannels_last format follow\n.\nThe tutorial includes a section on\n.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->General optimizations->Checkpoint intermediate buffers": [
        [
            "Buffer checkpointing is a technique to mitigate the memory capacity burden of\nmodel training. Instead of storing inputs of all layers to compute upstream\ngradients in backward propagation, it stores the inputs of a few layers and\nthe others are recomputed during backward pass. The reduced memory\nrequirements enables increasing the batch size that can improve utilization.",
            "markdown"
        ],
        [
            "Checkpointing targets should be selected carefully. The best is not to store\nlarge layer outputs that have small re-computation cost. The example target\nlayers are activation functions (e.g. ReLU, Sigmoid, Tanh),\nup/down sampling and matrix-vector operations with small accumulation depth.",
            "markdown"
        ],
        [
            "PyTorch supports a native\n\nAPI to automatically perform checkpointing and recomputation.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->General optimizations->Disable debugging APIs": [
        [
            "Many PyTorch APIs are intended for debugging and should be disabled for\nregular training runs:",
            "markdown"
        ],
        [
            "anomaly detection:\n\nor",
            "markdown"
        ],
        [
            "profiler related:\n,",
            "markdown"
        ],
        [
            "autograd gradcheck:\n\nor",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->CPU specific optimizations->Utilize Non-Uniform Memory Access (NUMA) Controls": [
        [
            "NUMA or non-uniform memory access is a memory layout design used in data center machines meant to take advantage of locality of memory in multi-socket machines with multiple memory controllers and blocks. Generally speaking, all deep learning workloads, training or inference, get better performance without accessing hardware resources across NUMA nodes. Thus, inference can be run with multiple instances, each instance runs on one socket, to raise throughput. For training tasks on single node, distributed training is recommended to make each training process run on one socket.",
            "markdown"
        ],
        [
            "In general cases the following command executes a PyTorch script on cores on the Nth node only, and avoids cross-socket memory access to reduce memory access overhead.",
            "markdown"
        ],
        [
            "# numactl --cpunodebind=N --membind=N python &lt;pytorch_script&gt;",
            "code"
        ],
        [
            "More detailed descriptions can be found .",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->CPU specific optimizations->Utilize OpenMP": [
        [
            "OpenMP is utilized to bring better performance for parallel computation tasks.\nOMP_NUM_THREADS is the easiest switch that can be used to accelerate computations. It determines number of threads used for OpenMP computations.\nCPU affinity setting controls how workloads are distributed over multiple cores. It affects communication overhead, cache line invalidation overhead, or page thrashing, thus proper setting of CPU affinity brings performance benefits. GOMP_CPU_AFFINITY or KMP_AFFINITY determines how to bind OpenMP* threads to physical processing units. Detailed information can be found .",
            "markdown"
        ],
        [
            "With the following command, PyTorch run the task on N OpenMP threads.",
            "markdown"
        ],
        [
            "# export OMP_NUM_THREADS=N",
            "code"
        ],
        [
            "Typically, the following environment variables are used to set for CPU affinity with GNU OpenMP implementation. OMP_PROC_BIND specifies whether threads may be moved between processors. Setting it to CLOSE keeps OpenMP threads close to the primary thread in contiguous place partitions. OMP_SCHEDULE determines how OpenMP threads are scheduled. GOMP_CPU_AFFINITY binds threads to specific CPUs.",
            "markdown"
        ],
        [
            "# export OMP_SCHEDULE=STATIC\n# export OMP_PROC_BIND=CLOSE\n# export GOMP_CPU_AFFINITY=\"N-M\"",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->CPU specific optimizations->Intel OpenMP Runtime Library (libiomp)": [
        [
            "By default, PyTorch uses GNU OpenMP (GNU libgomp) for parallel computation. On Intel platforms, Intel OpenMP Runtime Library (libiomp) provides OpenMP API specification support. It sometimes brings more performance benefits compared to libgomp. Utilizing environment variable LD_PRELOAD can switch OpenMP library to libiomp:",
            "markdown"
        ],
        [
            "# export LD_PRELOAD=&lt;path&gt;/libiomp5.so:$LD_PRELOAD",
            "code"
        ],
        [
            "Similar to CPU affinity settings in GNU OpenMP, environment variables are provided in libiomp to control CPU affinity settings.\nKMP_AFFINITY binds OpenMP threads to physical processing units. KMP_BLOCKTIME sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping. In most cases, setting KMP_BLOCKTIME to 1 or 0 yields good performances.\nThe following commands show a common settings with Intel OpenMP Runtime Library.",
            "markdown"
        ],
        [
            "# export KMP_AFFINITY=granularity=fine,compact,1,0\n# export KMP_BLOCKTIME=1",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->CPU specific optimizations->Switch Memory allocator": [
        [
            "For deep learning workloads, Jemalloc or TCMalloc can get better performance by reusing memory as much as possible than default malloc funtion.  is a general purpose malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support.  also features a couple of optimizations to speed up program executions. One of them is holding memory in caches to speed up access of commonly-used objects. Holding such caches even after deallocation also helps avoid costly system calls if such memory is later re-allocated.\nUse environment variable LD_PRELOAD to take advantage of one of them.",
            "markdown"
        ],
        [
            "# export LD_PRELOAD=&lt;jemalloc.so/tcmalloc.so&gt;:$LD_PRELOAD",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->CPU specific optimizations->Use oneDNN Graph with TorchScript for inference": [
        [
            "oneDNN Graph can significantly boost inference performance. It fuses some compute-intensive operations such as convolution, matmul with their neighbor operations.\nIn PyTorch 2.0, it is supported as a beta feature for Float32 &amp; BFloat16 data-types.\noneDNN Graph receives the model\u2019s graph and identifies candidates for operator-fusion with respect to the shape of the example input.\nA model should be JIT-traced using an example input.\nSpeed-up would then be observed after a couple of warm-up iterations for inputs with the same shape as the example input.\nThe example code-snippets below are for resnet50, but they can very well be extended to use oneDNN Graph with custom models as well.",
            "markdown"
        ],
        [
            "# Only this extra line of code is required to use oneDNN Graph\ntorch.jit.enable_onednn_fusion(True)",
            "code"
        ],
        [
            "Using the oneDNN Graph API requires just one extra line of code for inference with Float32.\nIf you are using oneDNN Graph, please avoid calling torch.jit.optimize_for_inference.",
            "markdown"
        ],
        [
            "# sample input should be of the same shape as expected inputs\nsample_input = [torch.rand(32, 3, 224, 224)]\n# Using resnet50 from TorchVision in this example for illustrative purposes,\n# but the line below can indeed be modified to use custom models as well.\nmodel = getattr(torchvision.models, \"resnet50\")().eval()\n# Tracing the model with example input\ntraced_model = torch.jit.trace(model, sample_input)\n# Invoking torch.jit.freeze\ntraced_model = torch.jit.freeze(traced_model)",
            "code"
        ],
        [
            "Once a model is JIT-traced with a sample input, it can then be used for inference after a couple of warm-up runs.",
            "markdown"
        ],
        [
            "with torch.no_grad():\n    # a couple of warmup runs\n    traced_model(*sample_input)\n    traced_model(*sample_input)\n    # speedup would be observed after warmup runs\n    traced_model(*sample_input)",
            "code"
        ],
        [
            "While the JIT fuser for oneDNN Graph also supports inference with BFloat16 datatype,\nperformance benefit with oneDNN Graph is only exhibited by machines with AVX512_BF16 ISA.\nThe following code snippets serves as an example of using BFloat16 datatype for inference with oneDNN Graph:",
            "markdown"
        ],
        [
            "# AMP for JIT mode is enabled by default, and is divergent with its eager mode counterpart\ntorch._C._jit_set_autocast_mode(False)\n\nwith torch.no_grad(), torch.cpu.amp.autocast(cache_enabled=False, dtype=torch.bfloat16):\n    model = torch.jit.trace(model, (example_input))\n    model = torch.jit.freeze(model)\n    # a couple of warmup runs\n    model(example_input)\n    model(example_input)\n    # speedup would be observed in subsequent runs.\n    model(example_input)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->CPU specific optimizations->Train a model on CPU with PyTorch DistributedDataParallel(DDP) functionality": [
        [
            "For small scale models or memory-bound models, such as DLRM, training on CPU is also a good choice. On a machine with multiple sockets, distributed training brings a high-efficient hardware resource usage to accelerate the training process. , optimized with Intel(R) oneCCL (collective commnications library) for efficient distributed deep learning training implementing such collectives like allreduce, allgather, alltoall, implements PyTorch C10D ProcessGroup API and can be dynamically loaded as external ProcessGroup. Upon optimizations implemented in PyTorch DDP moduel, torhc-ccl accelerates communication operations. Beside the optimizations made to communication kernels, torch-ccl also features simultaneous computation-communication functionality.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->GPU specific optimizations->Enable cuDNN auto-tuner": [
        [
            " supports many algorithms\nto compute a convolution. Autotuner runs a short benchmark and selects the\nkernel with the best performance on a given hardware for a given input size.",
            "markdown"
        ],
        [
            "For convolutional networks (other types currently not supported), enable cuDNN\nautotuner before launching the training loop by setting:",
            "markdown"
        ],
        [
            "torch.backends.cudnn.benchmark = True",
            "code"
        ],
        [
            "the auto-tuner decisions may be non-deterministic; different algorithm may\nbe selected for different runs.  For more details see",
            "markdown"
        ],
        [
            "in some rare cases, such as with highly variable input sizes,  it\u2019s better\nto run convolutional networks with autotuner disabled to avoid the overhead\nassociated with algorithm selection for each input size.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->GPU specific optimizations->Avoid unnecessary CPU-GPU synchronization": [
        [
            "Avoid unnecessary synchronizations, to let the CPU run ahead of the\naccelerator as much as possible to make sure that the accelerator work queue\ncontains many operations.",
            "markdown"
        ],
        [
            "When possible, avoid operations which require synchronizations, for example:",
            "markdown"
        ],
        [
            "print(cuda_tensor)",
            "markdown"
        ],
        [
            "cuda_tensor.item()",
            "markdown"
        ],
        [
            "memory copies: tensor.cuda(),  cuda_tensor.cpu() and equivalent\ntensor.to(device) calls",
            "markdown"
        ],
        [
            "cuda_tensor.nonzero()",
            "markdown"
        ],
        [
            "python control flow which depends on results of operations performed on cuda\ntensors e.g. if (cuda_tensor != 0).all()",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->GPU specific optimizations->Create tensors directly on the target device": [
        [
            "Instead of calling torch.rand(size).cuda() to generate a random tensor,\nproduce the output directly on the target device:\ntorch.rand(size, device=torch.device('cuda')).",
            "markdown"
        ],
        [
            "This is applicable to all functions which create new tensors and accept\ndevice argument:\n,\n,\n\nand similar.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->GPU specific optimizations->Use mixed precision and AMP": [
        [
            "Mixed precision leverages\n\nand offers up to 3x overall speedup on Volta and newer GPU architectures. To\nuse Tensor Cores AMP should be enabled and matrix/tensor dimensions should\nsatisfy requirements for calling kernels that use Tensor Cores.",
            "markdown"
        ],
        [
            "To use Tensor Cores:",
            "markdown"
        ],
        [
            "set sizes to multiples of 8 (to map onto dimensions of Tensor Cores)",
            "markdown"
        ],
        [
            "see\n\nfor more details and guidelines specific to layer type",
            "markdown"
        ],
        [
            "if layer size is derived from other parameters rather than fixed, it can\nstill be explicitly padded e.g. vocabulary size in NLP models",
            "markdown"
        ],
        [
            "enable AMP",
            "markdown"
        ],
        [
            "Introduction to Mixed Precision Training and AMP:\n,",
            "markdown"
        ],
        [
            "native PyTorch AMP is available starting from PyTorch 1.6:\n,\n,",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->GPU specific optimizations->Pre-allocate memory in case of variable input length": [
        [
            "Models for speech recognition or for NLP are often trained on input tensors\nwith variable sequence length. Variable length can be problematic for PyTorch\ncaching allocator and can lead to reduced performance or to unexpected\nout-of-memory errors. If a batch with a short sequence length is followed by\nan another batch with longer sequence length, then PyTorch is forced to\nrelease intermediate buffers from previous iteration and to re-allocate new\nbuffers. This process is time consuming and causes fragmentation in the\ncaching allocator which may result in out-of-memory errors.",
            "markdown"
        ],
        [
            "A typical solution is to implement pre-allocation. It consists of the\nfollowing steps:",
            "markdown"
        ],
        [
            "generate a (usually random) batch of inputs with maximum sequence length\n(either corresponding to max length in the training dataset or to some\npredefined threshold)",
            "markdown"
        ],
        [
            "execute a forward and a backward pass with the generated batch, do not\nexecute an optimizer or a learning rate scheduler, this step pre-allocates\nbuffers of maximum size, which can be reused in subsequent\ntraining iterations",
            "markdown"
        ],
        [
            "zero out gradients",
            "markdown"
        ],
        [
            "proceed to regular training",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->Distributed optimizations->Use efficient data-parallel backend": [
        [
            "PyTorch has two ways to implement data-parallel training:",
            "markdown"
        ],
        [
            "DistributedDataParallel offers much better performance and scaling to\nmultiple-GPUs. For more information refer to the\n\nfrom PyTorch documentation.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->Distributed optimizations->Skip unnecessary all-reduce if training with DistributedDataParallel and gradient accumulation": [
        [
            "By default\n\nexecutes gradient all-reduce after every backward pass to compute the average\ngradient over all workers participating in the training. If training uses\ngradient accumulation over N steps, then all-reduce is not necessary after\nevery training step, it\u2019s only required to perform all-reduce after the last\ncall to backward, just before the execution of the optimizer.",
            "markdown"
        ],
        [
            "DistributedDataParallel provides\n\ncontext manager which disables gradient all-reduce for particular iteration.\nno_sync() should be applied to first N-1 iterations of gradient\naccumulation, the last iteration should follow the default execution and\nperform the required gradient all-reduce.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->Distributed optimizations->Match the order of layers in constructors and during the execution if using DistributedDataParallel(find_unused_parameters=True)": [
        [
            "with find_unused_parameters=True uses the order of layers and parameters\nfrom model constructors to build buckets for DistributedDataParallel\ngradient all-reduce. DistributedDataParallel overlaps all-reduce with the\nbackward pass. All-reduce for a particular bucket is asynchronously triggered\nonly when all gradients for parameters in a given bucket are available.",
            "markdown"
        ],
        [
            "To maximize the amount of overlap, the order in model constructors should\nroughly match the order during the execution. If the order doesn\u2019t match, then\nall-reduce for the entire bucket waits for the gradient which is the last to\narrive, this may reduce the overlap between backward pass and all-reduce,\nall-reduce may end up being exposed, which slows down the training.",
            "markdown"
        ],
        [
            "DistributedDataParallel with find_unused_parameters=False (which is\nthe default setting) relies on automatic bucket formation based on order of\noperations encountered during the backward pass. With\nfind_unused_parameters=False it\u2019s not necessary to reorder layers or\nparameters to achieve optimal performance.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Performance Tuning Guide->Distributed optimizations->Load-balance workload in a distributed setting": [
        [
            "Load imbalance typically may happen for models processing sequential data\n(speech recognition, translation, language models etc.). If one device\nreceives a batch of data with sequence length longer than sequence lengths for\nthe remaining devices, then all devices wait for the worker which finishes\nlast. Backward pass functions as an implicit synchronization point in a\ndistributed setting with\n\nbackend.",
            "markdown"
        ],
        [
            "There are multiple ways to solve the load balancing problem. The core idea is\nto distribute workload over all workers as uniformly as possible within each\nglobal batch. For example Transformer solves imbalance by forming batches with\napproximately constant number of tokens (and variable number of sequences in a\nbatch), other models solve imbalance by bucketing samples with similar\nsequence length or even by sorting dataset by sequence length.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Timer quick start": [
        [
            "In this tutorial, we\u2019re going to cover the primary APIs of\n<cite>torch.utils.benchmark.Timer</cite>. The PyTorch Timer is based on the\n\nAPI, with several PyTorch specific modifications. Familiarity with the\nbuiltin <cite>Timer</cite> class is not required for this tutorial, however we assume\nthat the reader is familiar with the fundamentals of performance work.",
            "markdown"
        ],
        [
            "A more comprehensive performace tuning tutorial is available at:\n<blockquote>",
            "markdown"
        ],
        [
            "</blockquote>\n<dl class=\"simple\">\n<dt><strong>Contents:</strong></dt><dd>",
            "markdown"
        ],
        [
            "</dd>\n</dl>",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Timer quick start->1. Defining a Timer": [
        [
            "A <cite>Timer</cite> serves as a task definition.",
            "markdown"
        ],
        [
            "from torch.utils.benchmark import \n\ntimer = (\n    # The computation which will be run in a loop and timed.\n    stmt=\"x * y\",\n\n    # `setup` will be run before calling the measurement loop, and is used to\n    # populate any state which is needed by `stmt`\n    setup=\"\"\"\n        x = torch.ones((128,))\n        y = torch.ones((128,))\n    \"\"\",\n\n    # Alternately, `globals` can be used to pass variables from the outer scope.\n    # -------------------------------------------------------------------------\n    # globals={\n    #     \"x\": torch.ones((128,)),\n    #     \"y\": torch.ones((128,)),\n    # },\n\n    # Control the number of threads that PyTorch uses. (Default: 1)\n    num_threads=1,\n)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Timer quick start->2. Wall time: Timer.blocked_autorange(\u2026)": [
        [
            "This method will handle details such as picking a suitable number if repeats,\nfixing the number of threads, and providing a convenient representation of\nthe results.",
            "markdown"
        ],
        [
            "# Measurement objects store the results of multiple repeats, and provide\n# various utility features.\nfrom torch.utils.benchmark import \n\nm:  = timer.blocked_autorange(min_run_time=1)\nprint(m)\n\n\n\n<strong>Snippet wall time.</strong>",
            "code"
        ],
        [
            "     &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f1929a38ed0&gt;\n     x * y\n     setup:\n       x = torch.ones((128,))\n       y = torch.ones((128,))\n\n       Median: 2.34 us\n       IQR:    0.07 us (2.31 to 2.38)\n       424 measurements, 1000 runs per measurement, 1 thread",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Timer quick start->3. C++ snippets": [
        [
            "from torch.utils.benchmark import Language\n\ncpp_timer = (\n    \"x * y;\",\n    \"\"\"\n        auto x = torch::ones({128});\n        auto y = torch::ones({128});\n    \"\"\",\n    language=Language.CPP,\n)\n\nprint(cpp_timer.blocked_autorange(min_run_time=1))\n\n\n\n<strong>C++ snippet wall time.</strong>",
            "code"
        ],
        [
            "     &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f192b019ed0&gt;\n     x * y;\n     setup:\n       auto x = torch::ones({128});\n       auto y = torch::ones({128});\n\n       Median: 1.21 us\n       IQR:    0.03 us (1.20 to 1.23)\n       83 measurements, 10000 runs per measurement, 1 thread",
            "code"
        ],
        [
            "Unsurprisingly, the C++ snippet is both faster and has lower variation.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Timer quick start->4. Instruction counts: Timer.collect_callgrind(\u2026)": [
        [
            "For deep dive investigations, <cite>Timer.collect_callgrind</cite> wraps\n<cite>Callgrind &lt;https://valgrind.org/docs/manual/cl-manual.html&gt;</cite> in order to\ncollect instruction counts. These are useful as they offer fine grained and\ndeterministic (or very low noise in the case of Python) insights into how a\nsnippet is run.",
            "markdown"
        ],
        [
            "from torch.utils.benchmark import , FunctionCounts\n\nstats:  = cpp_timer.collect_callgrind()\nprint(stats)\n\n\n\n<strong>C++ Callgrind stats (summary)</strong>",
            "code"
        ],
        [
            "     &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7f1929a35850&gt;\n     x * y;\n     setup:\n       auto x = torch::ones({128});\n       auto y = torch::ones({128});\n\n                             All          Noisy symbols removed\n         Instructions:       563600                     563600\n         Baseline:                0                          0\n     100 runs per measurement, 1 thread",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Timer quick start->5. Instruction counts: Delving deeper": [
        [
            "The string representation of CallgrindStats is similar to that of\nMeasurement. <cite>Noisy symbols</cite> are a Python concept (removing calls in the\nCPython interpreter which are known to be noisy).",
            "markdown"
        ],
        [
            "For more detailed analysis, however, we will want to look at specific calls.\n<cite>CallgrindStats.stats()</cite> returns a FunctionCounts object to make this easier.\nConceptually, FunctionCounts can be thought of as a tuple of pairs with some\nutility methods, where each pair is <cite>(number of instructions, file path and\nfunction name)</cite>.\n<dl>\n<dt>A note on paths:</dt><dd>",
            "markdown"
        ],
        [
            "One generally doesn\u2019t care about absolute path. For instance, the full path\nand function name for a multiply call is something like:\n<blockquote>",
            "markdown"
        ],
        [
            "/the/prefix/to/your/pytorch/install/dir/pytorch/build/aten/src/ATen/core/TensorMethods.cpp:at::Tensor::mul(at::Tensor const&amp;) const [/the/path/to/your/conda/install/miniconda3/envs/ab_ref/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so]\n</blockquote>",
            "markdown"
        ],
        [
            "when in reality, all of the information that we\u2019re interested in can be\nrepresented in:\n<blockquote>",
            "markdown"
        ],
        [
            "build/aten/src/ATen/core/TensorMethods.cpp:at::Tensor::mul(at::Tensor const&amp;) const\n</blockquote>",
            "markdown"
        ],
        [
            "CallgrindStats.as_standardized() makes a best effort to strip low signal\nportions of the file path, as well as the shared object and is generally\nrecommended.\n</dd>\n</dl>",
            "markdown"
        ],
        [
            "inclusive_stats = stats.as_standardized().stats(inclusive=False)\nprint(inclusive_stats[:10])\n\n\n\n<strong>C++ Callgrind stats (detailed)</strong>",
            "code"
        ],
        [
            "     torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7f192a6dfd90&gt;\n       47264  ???:_int_free\n       25963  ???:_int_malloc\n       19900  build/../aten/src/ATen/TensorIter ... (at::TensorIteratorConfig const&amp;)\n       18000  ???:__tls_get_addr\n       13500  ???:malloc\n       11300  build/../c10/util/SmallVector.h:a ... (at::TensorIteratorConfig const&amp;)\n       10345  ???:_int_memalign\n       10000  build/../aten/src/ATen/TensorIter ... (at::TensorIteratorConfig const&amp;)\n        9200  ???:free\n        8000  build/../c10/util/SmallVector.h:a ... IteratorBase::get_strides() const\n\n     Total: 173472",
            "code"
        ],
        [
            "That\u2019s still quite a lot to digest. Let\u2019s use the <cite>FunctionCounts.transform</cite>\nmethod to trim some of the function path, and discard the function called.\nWhen we do, the counts of any collisions (e.g. <cite>foo.h:a()</cite> and <cite>foo.h:b()</cite>\nwill both map to <cite>foo.h</cite>) will be added together.",
            "markdown"
        ],
        [
            "import os\nimport re\n\ndef group_by_file(fn_name: str):\n    if fn_name.startswith(\"???\"):\n        fn_dir, fn_file = fn_name.split(\":\")[:2]\n    else:\n        fn_dir, fn_file = os.path.split(fn_name.split(\":\")[0])\n        fn_dir = re.sub(\"^.*build/../\", \"\", fn_dir)\n        fn_dir = re.sub(\"^.*torch/\", \"torch/\", fn_dir)\n\n    return f\"{fn_dir:&lt;15} {fn_file}\"\n\nprint(inclusive_stats.transform(group_by_file)[:10])\n\n\n\n<strong>Callgrind stats (condensed)</strong>",
            "code"
        ],
        [
            "     &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7f192995d750&gt;\n       118200  aten/src/ATen   TensorIterator.cpp\n        65000  c10/util        SmallVector.h\n        47264  ???             _int_free\n        25963  ???             _int_malloc\n        20900  c10/util        intrusive_ptr.h\n        18000  ???             __tls_get_addr\n        15900  c10/core        TensorImpl.h\n        15100  c10/core        CPUAllocator.cpp\n        13500  ???             malloc\n        12500  c10/core        TensorImpl.cpp\n\n     Total: 352327",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->Timer quick start->6. A/B testing with Callgrind": [
        [
            "One of the most useful features of instruction counts is they allow fine\ngrained comparison of computation, which is critical when analyzing\nperformance.",
            "markdown"
        ],
        [
            "To see this in action, lets compare our multiplication of two size 128\nTensors with a {128} x {1} multiplication, which will broadcast the second\nTensor:\n<blockquote>",
            "markdown"
        ],
        [
            "result = {a0 * b0, a1 * b0, \u2026, a127 * b0}\n</blockquote>",
            "markdown"
        ],
        [
            "broadcasting_stats = (\n    \"x * y;\",\n    \"\"\"\n        auto x = torch::ones({128});\n        auto y = torch::ones({1});\n    \"\"\",\n    language=Language.CPP,\n).collect_callgrind().as_standardized().stats(inclusive=False)",
            "code"
        ],
        [
            "Often we want to A/B test two different environments. (e.g. testing a PR, or\nexperimenting with compile flags.) This is quite simple, as CallgrindStats,\nFunctionCounts, and Measurement are all pickleable. Simply save measurements\nfrom each environment, and load them in a single process for analysis.",
            "markdown"
        ],
        [
            "import pickle\n\n# Let's round trip `broadcasting_stats` just to show that we can.\nbroadcasting_stats = pickle.loads(pickle.dumps(broadcasting_stats))\n\n\n# And now to diff the two tasks:\ndelta = broadcasting_stats - inclusive_stats\n\ndef extract_fn_name(fn: str):\n    \"\"\"Trim everything except the function name.\"\"\"\n    fn = \":\".join(fn.split(\":\")[1:])\n    return re.sub(r\"\\(.+\\)\", \"(...)\", fn)\n\n# We use `.transform` to make the diff readable:\nprint(delta.transform(extract_fn_name))\n\n\n\n<strong>Instruction count delta</strong>",
            "code"
        ],
        [
            "     &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7f192995d750&gt;\n         17600  at::TensorIteratorBase::compute_strides(...)\n         12700  at::TensorIteratorBase::allocate_or_resize_outputs()\n         10200  c10::SmallVectorImpl&lt;long&gt;::operator=(...)\n          7400  at::infer_size(...)\n          6200  at::TensorIteratorBase::invert_perm(...) const\n          6064  _int_free\n          5100  at::TensorIteratorBase::reorder_dimensions()\n          4300  malloc\n          4300  at::TensorIteratorBase::compatible_stride(...) const\n           ...\n           -28  _int_memalign\n          -100  c10::impl::check_tensor_options_and_extract_memory_format(...)\n          -300  __memcmp_avx2_movbe\n          -400  at::detail::empty_cpu(...)\n         -1100  at::TensorIteratorBase::numel() const\n         -1300  void at::native::(...)\n         -2400  c10::TensorImpl::is_contiguous(...) const\n         -6100  at::TensorIteratorBase::compute_fast_setup_type(...)\n        -22600  at::TensorIteratorBase::fast_set_up(...)\n\n     Total: 58091",
            "code"
        ],
        [
            "So the broadcasting version takes an extra 580 instructions per call (recall\nthat we\u2019re collecting 100 runs per sample), or about 10%. There are quite a\nfew TensorIterator calls, so lets drill down to those. FunctionCounts.filter\nmakes this easy.",
            "markdown"
        ],
        [
            "print(delta.transform(extract_fn_name).filter(lambda fn: \"TensorIterator\" in fn))\n\n\n\n<strong>Instruction count delta (filter)</strong>",
            "code"
        ],
        [
            "     &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7f19299544d0&gt;\n         17600  at::TensorIteratorBase::compute_strides(...)\n         12700  at::TensorIteratorBase::allocate_or_resize_outputs()\n          6200  at::TensorIteratorBase::invert_perm(...) const\n          5100  at::TensorIteratorBase::reorder_dimensions()\n          4300  at::TensorIteratorBase::compatible_stride(...) const\n          4000  at::TensorIteratorBase::compute_shape(...)\n          2300  at::TensorIteratorBase::coalesce_dimensions()\n          1600  at::TensorIteratorBase::build(...)\n         -1100  at::TensorIteratorBase::numel() const\n         -6100  at::TensorIteratorBase::compute_fast_setup_type(...)\n        -22600  at::TensorIteratorBase::fast_set_up(...)\n\n     Total: 24000",
            "code"
        ],
        [
            "This makes plain what is going on: there is a fast path in TensorIterator\nsetup, but in the {128} x {1} case we miss it and have to do a more general\nanalysis which is more expensive. The most prominent call omitted by the\nfilter is <cite>c10::SmallVectorImpl&lt;long&gt;::operator=(\u2026)</cite>, which is also part\nof the more general setup.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Timer quick start->7. Wrapping up": [
        [
            "In summary, use <cite>Timer.blocked_autorange</cite> to collect wall times. If timing\nvariation is too high, increase <cite>min_run_time</cite>, or move to C++ snippets if\nconvenient.",
            "markdown"
        ],
        [
            "For fine grained analysis, use <cite>Timer.collect_callgrind</cite> to measure\ninstruction counts and <cite>FunctionCounts.(__add__ / __sub__ / transform / filter)</cite>\nto slice-and-dice them.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->Timer quick start->8. Footnotes": [
        [
            "If <cite>globals</cite> does not contain \u201ctorch\u201d, Timer will automatically\npopulate it. This means that <cite>Timer(\u201ctorch.empty(())\u201d)</cite> will work.\n(Though other imports should be placed in <cite>setup</cite>,\ne.g. <cite>Timer(\u201cnp.zeros(())\u201d, \u201cimport numpy as np\u201d)</cite>)\n</dd>\n</dl>\n\n<dl class=\"simple\">\n<dt>REL_WITH_DEB_INFO</dt><dd>",
            "markdown"
        ],
        [
            "In order to provide full information about the PyTorch internals which\nare executed, Callgrind needs access to C++ debug symbols. This is\naccomplished by setting REL_WITH_DEB_INFO=1 when building PyTorch.\nOtherwise function calls will be opaque. (The resultant CallgrindStats\nwill warn if debug symbols are missing.)\n</dd>\n</dl>\n\n\n</blockquote>",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Profiler": [
        [
            "This recipe explains how to use PyTorch profiler and measure the time and\nmemory consumption of the model\u2019s operators.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Profiler->Introduction": [
        [
            "PyTorch includes a simple profiler API that is useful when user needs\nto determine the most expensive operators in the model.",
            "markdown"
        ],
        [
            "In this recipe, we will use a simple Resnet model to demonstrate how to\nuse profiler to analyze model performance.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Profiler->Setup": [
        [
            "To install torch and torchvision use the following command:",
            "markdown"
        ],
        [
            "pip install torch torchvision",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Profiler->Steps": [
        [
            "Import all necessary libraries",
            "markdown"
        ],
        [
            "Instantiate a simple Resnet model",
            "markdown"
        ],
        [
            "Using profiler to analyze execution time",
            "markdown"
        ],
        [
            "Using profiler to analyze memory consumption",
            "markdown"
        ],
        [
            "Using tracing functionality",
            "markdown"
        ],
        [
            "Examining stack traces",
            "markdown"
        ],
        [
            "Visualizing data as a flamegraph",
            "markdown"
        ],
        [
            "Using profiler to analyze long-running jobs",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Profiler->Steps->1. Import all necessary libraries": [
        [
            "In this recipe we will use torch, torchvision.models\nand profiler modules:",
            "markdown"
        ],
        [
            "import torch\nimport torchvision.models as models\nfrom torch.profiler import , record_function, ProfilerActivity",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Profiler->Steps->2. Instantiate a simple Resnet model": [
        [
            "Let\u2019s create an instance of a Resnet model and prepare an input\nfor it:",
            "markdown"
        ],
        [
            "model = ()\ninputs = (5, 3, 224, 224)",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Profiler->Steps->3. Using profiler to analyze execution time": [
        [
            "PyTorch profiler is enabled through the context manager and accepts\na number of parameters, some of the most useful are:\n\n<dl class=\"simple\">\n<dt>activities - a list of activities to profile:</dt><dd>",
            "markdown"
        ],
        [
            "ProfilerActivity.CPU - PyTorch operators, TorchScript functions and\nuser-defined code labels (see record_function below);",
            "markdown"
        ],
        [
            "ProfilerActivity.CUDA - on-device CUDA kernels;\n\n</dd>\n</dl>",
            "markdown"
        ],
        [
            "record_shapes - whether to record shapes of the operator inputs;",
            "markdown"
        ],
        [
            "profile_memory - whether to report amount of memory consumed by\nmodel\u2019s Tensors;",
            "markdown"
        ],
        [
            "use_cuda - whether to measure execution time of CUDA kernels.",
            "markdown"
        ],
        [
            "Note: when using CUDA, profiler also shows the runtime CUDA events\noccuring on the host.",
            "markdown"
        ],
        [
            "Let\u2019s see how we can use profiler to analyze the execution time:",
            "markdown"
        ],
        [
            "with (activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n    with record_function(\"model_inference\"):\n        model(inputs)",
            "code"
        ],
        [
            "Note that we can use record_function context manager to label\narbitrary code ranges with user provided names\n(model_inference is used as a label in the example above).",
            "markdown"
        ],
        [
            "Profiler allows one to check which operators were called during the\nexecution of a code range wrapped with a profiler context manager.\nIf multiple profiler ranges are active at the same time (e.g. in\nparallel PyTorch threads), each profiling context manager tracks only\nthe operators of its corresponding range.\nProfiler also automatically profiles the async tasks launched\nwith torch.jit._fork and (in case of a backward pass)\nthe backward pass operators launched with backward() call.",
            "markdown"
        ],
        [
            "Let\u2019s print out the stats for the execution above:",
            "markdown"
        ],
        [
            "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))",
            "code"
        ],
        [
            "The output will look like (omitting some columns):",
            "markdown"
        ],
        [
            "# ---------------------------------  ------------  ------------  ------------  ------------\n#                              Name      Self CPU     CPU total  CPU time avg    # of Calls\n# ---------------------------------  ------------  ------------  ------------  ------------\n#                   model_inference       5.509ms      57.503ms      57.503ms             1\n#                      aten::conv2d     231.000us      31.931ms       1.597ms            20\n#                 aten::convolution     250.000us      31.700ms       1.585ms            20\n#                aten::_convolution     336.000us      31.450ms       1.573ms            20\n#          aten::mkldnn_convolution      30.838ms      31.114ms       1.556ms            20\n#                  aten::batch_norm     211.000us      14.693ms     734.650us            20\n#      aten::_batch_norm_impl_index     319.000us      14.482ms     724.100us            20\n#           aten::native_batch_norm       9.229ms      14.109ms     705.450us            20\n#                        aten::mean     332.000us       2.631ms     125.286us            21\n#                      aten::select       1.668ms       2.292ms       8.988us           255\n# ---------------------------------  ------------  ------------  ------------  ------------\n# Self CPU time total: 57.549ms",
            "code"
        ],
        [
            "Here we see that, as expected, most of the time is spent in convolution (and specifically in mkldnn_convolution\nfor PyTorch compiled with MKL-DNN support).\nNote the difference between self cpu time and cpu time - operators can call other operators, self cpu time excludes time\nspent in children operator calls, while total cpu time includes it. You can choose to sort by the self cpu time by passing\nsort_by=\"self_cpu_time_total\" into the table call.",
            "markdown"
        ],
        [
            "To get a finer granularity of results and include operator input shapes, pass group_by_input_shape=True\n(note: this requires running the profiler with record_shapes=True):",
            "markdown"
        ],
        [
            "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  -------------------------------------------\n#                              Name     CPU total                                 Input Shapes\n# ---------------------------------  ------------  -------------------------------------------\n#                   model_inference      57.503ms                                           []\n#                      aten::conv2d       8.008ms      [5,64,56,56], [64,64,3,3], [], ..., []]\n#                 aten::convolution       7.956ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n#                aten::_convolution       7.909ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n#          aten::mkldnn_convolution       7.834ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n#                      aten::conv2d       6.332ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#                 aten::convolution       6.303ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#                aten::_convolution       6.273ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#          aten::mkldnn_convolution       6.233ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#                      aten::conv2d       4.751ms  [[5,256,14,14], [256,256,3,3], [], ..., []]\n# ---------------------------------  ------------  -------------------------------------------\n# Self CPU time total: 57.549ms",
            "code"
        ],
        [
            "Note the occurence of aten::convolution twice with different input shapes.",
            "markdown"
        ],
        [
            "Profiler can also be used to analyze performance of models executed on GPUs:",
            "markdown"
        ],
        [
            "model = ().cuda()\ninputs = (5, 3, 224, 224).cuda()\n\nwith (activities=[\n        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n    with record_function(\"model_inference\"):\n        model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))",
            "code"
        ],
        [
            "(Note: the first use of CUDA profiling may bring an extra overhead.)",
            "markdown"
        ],
        [
            "The resulting table output:",
            "markdown"
        ],
        [
            "# (omitting some columns)\n# -------------------------------------------------------  ------------  ------------\n#                                                    Name     Self CUDA    CUDA total\n# -------------------------------------------------------  ------------  ------------\n#                                         model_inference       0.000us      11.666ms\n#                                            aten::conv2d       0.000us      10.484ms\n#                                       aten::convolution       0.000us      10.484ms\n#                                      aten::_convolution       0.000us      10.484ms\n#                              aten::_convolution_nogroup       0.000us      10.484ms\n#                                       aten::thnn_conv2d       0.000us      10.484ms\n#                               aten::thnn_conv2d_forward      10.484ms      10.484ms\n# void at::native::im2col_kernel&lt;float&gt;(long, float co...       3.844ms       3.844ms\n#                                       sgemm_32x32x32_NN       3.206ms       3.206ms\n#                                   sgemm_32x32x32_NN_vec       3.093ms       3.093ms\n# -------------------------------------------------------  ------------  ------------\n# Self CPU time total: 23.015ms\n# Self CUDA time total: 11.666ms",
            "code"
        ],
        [
            "Note the occurence of on-device kernels in the output (e.g. sgemm_32x32x32_NN).",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Profiler->Steps->4. Using profiler to analyze memory consumption": [
        [
            "PyTorch profiler can also show the amount of memory (used by the model\u2019s tensors)\nthat was allocated (or released) during the execution of the model\u2019s operators.\nIn the output below, \u2018self\u2019 memory corresponds to the memory allocated (released)\nby the operator, excluding the children calls to the other operators.\nTo enable memory profiling functionality pass profile_memory=True.",
            "markdown"
        ],
        [
            "model = ()\ninputs = (5, 3, 224, 224)\n\nwith (activities=[ProfilerActivity.CPU],\n        profile_memory=True, record_shapes=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  ------------  ------------\n#                              Name       CPU Mem  Self CPU Mem    # of Calls\n# ---------------------------------  ------------  ------------  ------------\n#                       aten::empty      94.79 Mb      94.79 Mb           121\n#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n#                       aten::addmm      19.53 Kb      19.53 Kb             1\n#               aten::empty_strided         572 b         572 b            25\n#                     aten::resize_         240 b         240 b             6\n#                         aten::abs         480 b         240 b             4\n#                         aten::add         160 b         160 b            20\n#               aten::masked_select         120 b         112 b             1\n#                          aten::ne         122 b          53 b             6\n#                          aten::eq          60 b          30 b             2\n# ---------------------------------  ------------  ------------  ------------\n# Self CPU time total: 53.064ms\n\nprint(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  ------------  ------------\n#                              Name       CPU Mem  Self CPU Mem    # of Calls\n# ---------------------------------  ------------  ------------  ------------\n#                       aten::empty      94.79 Mb      94.79 Mb           121\n#                  aten::batch_norm      47.41 Mb           0 b            20\n#      aten::_batch_norm_impl_index      47.41 Mb           0 b            20\n#           aten::native_batch_norm      47.41 Mb           0 b            20\n#                      aten::conv2d      47.37 Mb           0 b            20\n#                 aten::convolution      47.37 Mb           0 b            20\n#                aten::_convolution      47.37 Mb           0 b            20\n#          aten::mkldnn_convolution      47.37 Mb           0 b            20\n#                  aten::max_pool2d      11.48 Mb           0 b             1\n#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n# ---------------------------------  ------------  ------------  ------------\n# Self CPU time total: 53.064ms",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Profiler->Steps->5. Using tracing functionality": [
        [
            "Profiling results can be outputted as a .json trace file:",
            "markdown"
        ],
        [
            "model = ().cuda()\ninputs = (5, 3, 224, 224).cuda()\n\nwith (activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n    model(inputs)\n\nprof.export_chrome_trace(\"trace.json\")",
            "code"
        ],
        [
            "You can examine the sequence of profiled operators and CUDA kernels\nin Chrome trace viewer (chrome://tracing):",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Profiler->Steps->6. Examining stack traces": [
        [
            "Profiler can be used to analyze Python and TorchScript stack traces:",
            "markdown"
        ],
        [
            "with (\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    with_stack=True,\n) as prof:\n    model(inputs)\n\n# Print aggregated stats\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=2))\n\n# (omitting some columns)\n# -------------------------  -----------------------------------------------------------\n#                      Name  Source Location\n# -------------------------  -----------------------------------------------------------\n# aten::thnn_conv2d_forward  .../torch/nn/modules/conv.py(439): _conv_forward\n#                            .../torch/nn/modules/conv.py(443): forward\n#                            .../torch/nn/modules/module.py(1051): _call_impl\n#                            .../site-packages/torchvision/models/resnet.py(63): forward\n#                            .../torch/nn/modules/module.py(1051): _call_impl\n#\n# aten::thnn_conv2d_forward  .../torch/nn/modules/conv.py(439): _conv_forward\n#                            .../torch/nn/modules/conv.py(443): forward\n#                            .../torch/nn/modules/module.py(1051): _call_impl\n#                            .../site-packages/torchvision/models/resnet.py(59): forward\n#                            .../torch/nn/modules/module.py(1051): _call_impl\n#\n# -------------------------  -----------------------------------------------------------\n# Self CPU time total: 34.016ms\n# Self CUDA time total: 11.659ms",
            "code"
        ],
        [
            "Note the two convolutions and the two callsites in torchvision/models/resnet.py script.",
            "markdown"
        ],
        [
            "(Warning: stack tracing adds an extra profiling overhead.)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Profiler->Steps->7. Visualizing data as a flamegraph": [
        [
            "Execution time (self_cpu_time_total and self_cuda_time_total metrics) and stack traces\ncan also be visualized as a flame graph. To do this, first export the raw data using export_stacks (requires with_stack=True):",
            "markdown"
        ],
        [
            "prof.export_stacks(\"/tmp/profiler_stacks.txt\", \"self_cuda_time_total\")",
            "code"
        ],
        [
            "We recommend using e.g.  to generate an\ninteractive SVG:",
            "markdown"
        ],
        [
            "# git clone https://github.com/brendangregg/FlameGraph\n# cd FlameGraph\n# ./flamegraph.pl --title \"CUDA time\" --countname \"us.\" /tmp/profiler_stacks.txt &gt; perf_viz.svg",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Profiler->Steps->8. Using profiler to analyze long-running jobs": [
        [
            "PyTorch profiler offers an additional API to handle long-running jobs\n(such as training loops). Tracing all of the execution can be\nslow and result in very large trace files. To avoid this, use optional\narguments:",
            "markdown"
        ],
        [
            "schedule - specifies a function that takes an integer argument (step number)\nas an input and returns an action for the profiler, the best way to use this parameter\nis to use torch.profiler.schedule helper function that can generate a schedule for you;",
            "markdown"
        ],
        [
            "on_trace_ready - specifies a function that takes a reference to the profiler as\nan input and is called by the profiler each time the new trace is ready.",
            "markdown"
        ],
        [
            "To illustrate how the API works, let\u2019s first consider the following example with\ntorch.profiler.schedule helper function:",
            "markdown"
        ],
        [
            "from torch.profiler import \n\nmy_schedule = (\n    skip_first=10,\n    wait=5,\n    warmup=1,\n    active=3,\n    repeat=2)",
            "code"
        ],
        [
            "Profiler assumes that the long-running job is composed of steps, numbered\nstarting from zero. The example above defines the following sequence of actions\nfor the profiler:",
            "markdown"
        ],
        [
            "Parameter skip_first tells profiler that it should ignore the first 10 steps\n(default value of skip_first is zero);",
            "markdown"
        ],
        [
            "After the first skip_first steps, profiler starts executing profiler cycles;",
            "markdown"
        ],
        [
            "Each cycle consists of three phases:",
            "markdown"
        ],
        [
            "idling (wait=5 steps), during this phase profiler is not active;",
            "markdown"
        ],
        [
            "warming up (warmup=1 steps), during this phase profiler starts tracing, but\nthe results are discarded; this phase is used to discard the samples obtained by\nthe profiler at the beginning of the trace since they are usually skewed by an extra\noverhead;",
            "markdown"
        ],
        [
            "active tracing (active=3 steps), during this phase profiler traces and records data;",
            "markdown"
        ],
        [
            "An optional repeat parameter specifies an upper bound on the number of cycles.\nBy default (zero value), profiler will execute cycles as long as the job runs.",
            "markdown"
        ],
        [
            "Thus, in the example above, profiler will skip the first 15 steps, spend the next step on the warm up,\nactively record the next 3 steps, skip another 5 steps, spend the next step on the warm up, actively\nrecord another 3 steps. Since the repeat=2 parameter value is specified, the profiler will stop\nthe recording after the first two cycles.",
            "markdown"
        ],
        [
            "At the end of each cycle profiler calls the specified on_trace_ready function and passes itself as\nan argument. This function is used to process the new trace - either by obtaining the table output or\nby saving the output on disk as a trace file.",
            "markdown"
        ],
        [
            "To send the signal to the profiler that the next step has started, call prof.step() function.\nThe current profiler step is stored in prof.step_num.",
            "markdown"
        ],
        [
            "The following example shows how to use all of the concepts above:",
            "markdown"
        ],
        [
            "def trace_handler(p):\n    output = p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10)\n    print(output)\n    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n\nwith (\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    =(\n        wait=1,\n        warmup=1,\n        active=2),\n    on_trace_ready=trace_handler\n) as p:\n    for idx in range(8):\n        model(inputs)\n        p.step()",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Profiler->Learn More": [
        [
            "Take a look at the following recipes/tutorials to continue your learning:",
            "markdown"
        ],
        [
            " tutorial",
            "markdown"
        ],
        [
            " tutorial",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Benchmark": [
        [
            "This recipe provides a quick-start guide to using PyTorch\nbenchmark module to measure and compare code performance.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Benchmark->Introduction": [
        [
            "Benchmarking is an important step in writing code. It helps\nus validate that our code meets performance expectations,\ncompare different approaches to solving the same problem and\nprevent performance regressions.",
            "markdown"
        ],
        [
            "There are many options when it comes to benchmarking PyTorch code\nincluding the Python builtin timeit module. However, benchmarking\nPyTorch code has many caveats that can be easily overlooked such as\nmanaging the number of threads and synchronizing CUDA devices. Moreover,\ngenerating Tensor inputs for benchmarking can be quite tedious.",
            "markdown"
        ],
        [
            "This recipe demonstrates how to use PyTorch benchmark module to avoid\ncommon mistakes while making it easier to compare performance of\ndifferent code, generate input for benchmarking and more.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Benchmark->Setup": [
        [
            "Before we begin, install torch if it isn\u2019t already available.",
            "markdown"
        ],
        [
            "pip install torch",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Benchmark->Steps": [
        [
            "Defining functions to benchmark",
            "markdown"
        ],
        [
            "Benchmarking with timeit.Timer",
            "markdown"
        ],
        [
            "Benchmarking with torch.utils.benchmark.Timer",
            "markdown"
        ],
        [
            "Benchmarking with <cite>Blocked Autorange</cite>",
            "markdown"
        ],
        [
            "Comparing benchmark results",
            "markdown"
        ],
        [
            "Saving/Loading benchmark results",
            "markdown"
        ],
        [
            "Generating inputs with <cite>Fuzzed Parameters</cite>",
            "markdown"
        ],
        [
            "Collecting instruction counts with <cite>Callgrind</cite>",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Benchmark->Steps->1. Defining functions to benchmark": [
        [
            "As of the time of this writing, \ndoes not support batched mode, so we will compare two approaches to\nimplementing it using existing torch operators: one approach uses a\ncombination of mul and sum while the other reduces the problem to bmm.",
            "markdown"
        ],
        [
            "import torch\n\n\ndef batched_dot_mul_sum(a, b):\n    '''Computes batched dot by multiplying and summing'''\n    return a.mul(b).sum(-1)\n\n\ndef batched_dot_bmm(a, b):\n    '''Computes batched dot by reducing to bmm'''\n    a = a.reshape(-1, 1, a.shape[-1])\n    b = b.reshape(-1, b.shape[-1], 1)\n    return (a, b).flatten(-3)\n\n\n# Input for benchmarking\nx = (10000, 64)\n\n# Ensure that both functions compute the same output\nassert batched_dot_mul_sum(x, x).allclose(batched_dot_bmm(x, x))",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Benchmark->Steps->2. Benchmarking with timeit.Timer": [
        [
            "First, let\u2019s benchmark the code using Python\u2019s builtin timeit module.\nWe keep the benchmark code simple here so we can compare the defaults\nof timeit and torch.utils.benchmark.",
            "markdown"
        ],
        [
            "import timeit\n\nt0 = timeit.Timer(\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x})\n\nt1 = timeit.Timer(\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x})\n\nprint(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:&gt;5.1f} us')\nprint(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:&gt;5.1f} us')\n\n\n\nOutput",
            "code"
        ],
        [
            " mul_sum(x, x):  111.6 us\n bmm(x, x):       70.0 us",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Benchmark->Steps->3. Benchmarking with torch.utils.benchmark.Timer": [
        [
            "PyTorch benchmark module was designed to be familiar to those who\nhave used the timeit module before. However, its defaults make it\neasier and safer to use for benchmarking PyTorch code. Let\u2019s first\ncompare the same basic API as above.",
            "markdown"
        ],
        [
            "import torch.utils.benchmark as benchmark\n\nt0 = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x})\n\nt1 = (\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x})\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\n\n\nOutput",
            "code"
        ],
        [
            " &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0&gt;\n batched_dot_mul_sum(x, x)\n setup: from __main__ import batched_dot_mul_sum\n   379.29 us\n   1 measurement, 100 runs , 1 thread\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb103d67048&gt;\n batched_dot_bmm(x, x)\n setup: from __main__ import batched_dot_bmm\n   716.42 us\n   1 measurement, 100 runs , 1 thread",
            "code"
        ],
        [
            "Even though the APIs are the same for the basic functionality, there\nare some important differences. benchmark.Timer.timeit() returns the\ntime per run as opposed to the total runtime like timeit.Timer.timeit()\ndoes. PyTorch benchmark module also provides formatted string\nrepresentations for printing the results.",
            "markdown"
        ],
        [
            "Another important difference, and the reason why the results diverge\nis that PyTorch benchmark module runs in a single thread by default.\nWe can change the number of threads with the num_threads arg.",
            "markdown"
        ],
        [
            "torch.utils.benchmark.Timer takes several additional arguments\nincluding: <cite>label</cite>, <cite>sub_label</cite>, <cite>description</cite> and <cite>env</cite> which change\nthe __repr__ of the measurement object returned and are used for\ngrouping the results (more on this later).",
            "markdown"
        ],
        [
            "num_threads = ()\nprint(f'Benchmarking on {num_threads} threads')\n\nt0 = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using mul and sum')\n\nt1 = (\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using bmm')\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\n\n\nOutput",
            "code"
        ],
        [
            " Benchmarking on 40 threads\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb103d54080&gt;\n Multithreaded batch dot: Implemented using mul and sum\n setup: from __main__ import batched_dot_mul_sum\n   118.47 us\n   1 measurement, 100 runs , 40 threads\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;\n Multithreaded batch dot: Implemented using bmm\n setup: from __main__ import batched_dot_bmm\n   68.21 us\n   1 measurement, 100 runs , 40 threads",
            "code"
        ],
        [
            "Running benchmark with all threads available gives similar results\nas the timeit module. More importantly, which version is faster\ndepends on how many threads we run the code with. This is why it\u2019s\nimportant to benchmark the code with thread settings that are\nrepresentative of real use cases. Another important thing to remember\nis to synchronize CPU and CUDA when benchmarking on the GPU. Let\u2019s run\nthe above benchmarks again on a CUDA tensor and see what happens.",
            "markdown"
        ],
        [
            "x = (10000, 1024, device='cuda')\n\nt0 = timeit.Timer(\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x})\n\nt1 = timeit.Timer(\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x})\n\n# Ran each twice to show difference before/after warmup\nprint(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:&gt;5.1f} us')\nprint(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:&gt;5.1f} us')\nprint(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:&gt;5.1f} us')\nprint(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:&gt;5.1f} us')\n\n\n\nOutput",
            "code"
        ],
        [
            " mul_sum(x, x):   27.6 us\n mul_sum(x, x):   25.3 us\n bmm(x, x):      2775.5 us\n bmm(x, x):       22.4 us",
            "code"
        ],
        [
            "t0 = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x})\n\nt1 = (\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x})\n\n# Run only once since benchmark module does warmup for us\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\n\n\nOutput",
            "code"
        ],
        [
            " &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d080&gt;\n batched_dot_mul_sum(x, x)\n setup: from __main__ import batched_dot_mul_sum\n   232.93 us\n   1 measurement, 100 runs , 1 thread\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0&gt;\n batched_dot_bmm(x, x)\n setup: from __main__ import batched_dot_bmm\n   181.04 us\n   1 measurement, 100 runs , 1 thread",
            "code"
        ],
        [
            "The results reveal something interesting. The first run of the bmm\nversion using the timeit module takes much longer than the second\nrun. This is because bmm calls into <cite>cuBLAS</cite> which needs to be\nloaded the first time it\u2019s called which takes some time. This is why\nit\u2019s important to do a warmup run before benchmarking, luckily for\nus, PyTorch\u2019s benchmark module takes care of that.",
            "markdown"
        ],
        [
            "The difference in the results between timeit and benchmark modules\nis because the <cite>timeit</cite> module is not synchronizing CUDA and is thus only\ntiming the time to launch the kernel. PyTorch\u2019s benchmark module does\nthe synchronization for us.",
            "markdown"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Benchmark->Steps->4. Benchmarking with Blocked Autorange": [
        [
            "While timeit.Timer.autorange takes a single continuous measurement\nof at least 0.2 seconds, <cite>torch.utils.benchmark.blocked_autorange</cite>\ntakes many measurements whose times total at least 0.2 seconds (which\ncan be changed by the <cite>min_run_time</cite> parameter) subject to the constraint\nthat timing overhead is a small fraction of the overall measurement.\nThis is accomplished by first running with an increasing number of runs\nper loop until the runtime is much larger than measurement overhead\n(which also serves as a warm up), and then taking measurements until\nthe target time is reached. This has the useful properties that it wastes\nless data and allows us to compute statistics to estimate the reliability\nof the measurements.",
            "markdown"
        ],
        [
            "m0 = t0.blocked_autorange()\nm1 = t1.blocked_autorange()\n\nprint(m0)\nprint(m1)\n\n\n\nOutput",
            "code"
        ],
        [
            " &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0&gt;\n batched_dot_mul_sum(x, x)\n setup: from __main__ import batched_dot_mul_sum\n   231.79 us\n   1 measurement, 1000 runs , 1 thread\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d080&gt;\n batched_dot_bmm(x, x)\n setup: from __main__ import batched_dot_bmm\n   Median: 162.08 us\n   2 measurements, 1000 runs per measurement, 1 thread",
            "code"
        ],
        [
            "We can also inspect the individual statistics from the returned\nmeasurements object.",
            "markdown"
        ],
        [
            "print(f\"Mean:   {m0.mean * 1e6:6.2f} us\")\nprint(f\"Median: {m0.median * 1e6:6.2f} us\")\n\n\n\nOutput",
            "code"
        ],
        [
            " Mean:   231.79 us\n Median: 231.79 us",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results": [
        [
            "So far we\u2019ve been comparing our two versions of batched dot against a\nsingle input. In practice, we want to try a combination of inputs as\nwell as different number of threads. The Compare class helps display\nthe results of many measurements in a formatted table. It uses the\nannotations described above (<cite>label</cite>, <cite>sub_label</cite>, <cite>num_threads</cite>, etc.) as\nwell as <cite>description</cite> to group and organize the table. Let\u2019s use\nCompare to see how our functions perform for different input sizes\nand number of threads.",
            "markdown"
        ],
        [
            "from itertools import product\n\n# Compare takes a list of measurements which we'll save in results.\nresults = []\n\nsizes = [1, 64, 1024, 10000]\nfor b, n in product(sizes, sizes):\n    # label and sub_label are the rows\n    # description is the column\n    label = 'Batched dot'\n    sub_label = f'[{b}, {n}]'\n    x = ((b, n))\n    for num_threads in [1, 4, 16, 32]:\n        results.append((\n            stmt='batched_dot_mul_sum(x, x)',\n            setup='from __main__ import batched_dot_mul_sum',\n            globals={'x': x},\n            num_threads=num_threads,\n            label=label,\n            sub_label=sub_label,\n            description='mul/sum',\n        ).blocked_autorange(min_run_time=1))\n        results.append((\n            stmt='batched_dot_bmm(x, x)',\n            setup='from __main__ import batched_dot_bmm',\n            globals={'x': x},\n            num_threads=num_threads,\n            label=label,\n            sub_label=sub_label,\n            description='bmm',\n        ).blocked_autorange(min_run_time=1))\n\ncompare = benchmark.Compare(results)\ncompare.print()\n\n\n\nOutput",
            "code"
        ],
        [
            " [--------------- Batched dot ----------------]\n                       |  mul/sum   |    bmm\n 1 threads: -----------------------------------\n       [1, 1]          |       5.9  |      11.2\n       [1, 64]         |       6.4  |      11.4\n       [1, 1024]       |       6.7  |      14.2\n       [1, 10000]      |      10.2  |      23.7\n       [64, 1]         |       6.3  |      11.5\n       [64, 64]        |       8.6  |      15.4\n       [64, 1024]      |      39.4  |     204.4\n       [64, 10000]     |     274.9  |     748.5\n       [1024, 1]       |       7.7  |      17.8\n       [1024, 64]      |      40.3  |      76.4\n       [1024, 1024]    |     432.4  |    2795.9\n       [1024, 10000]   |   22657.3  |   11899.5\n       [10000, 1]      |      16.9  |      74.8\n       [10000, 64]     |     300.3  |     609.4\n       [10000, 1024]   |   23098.6  |   27246.1\n       [10000, 10000]  |  267073.7  |  118823.7\n 4 threads: -----------------------------------\n       [1, 1]          |       6.0  |      11.5\n       [1, 64]         |       6.2  |      11.2\n       [1, 1024]       |       6.8  |      14.3\n       [1, 10000]      |      10.2  |      23.7\n       [64, 1]         |       6.3  |      16.2\n       [64, 64]        |       8.8  |      18.2\n       [64, 1024]      |      41.5  |     189.1\n       [64, 10000]     |      91.7  |     849.1\n       [1024, 1]       |       7.6  |      17.4\n       [1024, 64]      |      43.5  |      33.5\n       [1024, 1024]    |     135.4  |    2782.3\n       [1024, 10000]   |    7471.1  |   11874.0\n       [10000, 1]      |      16.8  |      33.9\n       [10000, 64]     |     118.7  |     173.2\n       [10000, 1024]   |    7264.6  |   27824.7\n       [10000, 10000]  |  100060.9  |  121499.0\n 16 threads: ----------------------------------\n       [1, 1]          |       6.0  |      11.3\n       [1, 64]         |       6.2  |      11.2\n       [1, 1024]       |       6.9  |      14.2\n       [1, 10000]      |      10.3  |      23.8\n       [64, 1]         |       6.4  |      24.1\n       [64, 64]        |       9.0  |      23.8\n       [64, 1024]      |      54.1  |     188.5\n       [64, 10000]     |      49.9  |     748.0\n       [1024, 1]       |       7.6  |      23.4\n       [1024, 64]      |      55.5  |      28.2\n       [1024, 1024]    |      66.9  |    2773.9\n       [1024, 10000]   |    6111.5  |   12833.7\n       [10000, 1]      |      16.9  |      27.5\n       [10000, 64]     |      59.5  |      73.7\n       [10000, 1024]   |    6295.9  |   27062.0\n       [10000, 10000]  |   71804.5  |  120365.8\n 32 threads: ----------------------------------\n       [1, 1]          |       5.9  |      11.3\n       [1, 64]         |       6.2  |      11.3\n       [1, 1024]       |       6.7  |      14.2\n       [1, 10000]      |      10.5  |      23.8\n       [64, 1]         |       6.3  |      31.7\n       [64, 64]        |       9.1  |      30.4\n       [64, 1024]      |      72.0  |     190.4\n       [64, 10000]     |     103.1  |     746.9\n       [1024, 1]       |       7.6  |      28.4\n       [1024, 64]      |      70.5  |      31.9\n       [1024, 1024]    |      65.6  |    2804.6\n       [1024, 10000]   |    6764.0  |   11871.4\n       [10000, 1]      |      17.8  |      31.8\n       [10000, 64]     |     110.3  |      56.0\n       [10000, 1024]   |    6640.2  |   27592.2\n       [10000, 10000]  |   73003.4  |  120083.2\n\n Times are in microseconds (us).",
            "code"
        ],
        [
            "The results above indicate that the version which reduces to bmm\nis better for larger tensors running on multiple threads, while for\nsmaller and/or single thread code, the other version is better.",
            "markdown"
        ],
        [
            "Compare also provides functions for changing the table format",
            "markdown"
        ],
        [
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Benchmark->Steps->6. Saving/Loading benchmark results": [
        [
            "<cite>Measurements</cite> (and <cite>CallgrindStats</cite> which are described in section 8)\nare pickleable. This makes A/B testing easy, as you can collect\nmeasurements from two separate environments, pickle them, and then\nload both in a single environment. Timer even takes an <cite>env</cite>\nconstructor argument so that such A/B testing works seamlessly.",
            "markdown"
        ],
        [
            "Let\u2019s imagine that rather than two Python functions, the add/sum\nand bmm approaches were in two different builds of PyTorch.\nThe example below demonstrates how one might A/B test them. For\nsimplicity, we only use a subset of shapes, and simply round trip\nresults through pickle rather than actually using multiple environments\nand writing results to disk.",
            "markdown"
        ],
        [
            "import pickle\n\nab_test_results = []\nfor env in ('environment A: mul/sum', 'environment B: bmm'):\n    for b, n in ((1, 1), (1024, 10000), (10000, 1)):\n        x = ((b, n))\n        dot_fn = (batched_dot_mul_sum if env == 'environment A: mul/sum' else batched_dot_bmm)\n        m = (\n            stmt='batched_dot(x, x)',\n            globals={'x': x, 'batched_dot': dot_fn},\n            num_threads=1,\n            label='Batched dot',\n            description=f'[{b}, {n}]',\n            env=env,\n        ).blocked_autorange(min_run_time=1)\n        ab_test_results.append(pickle.dumps(m))\n\nab_results = [pickle.loads(i) for i in ab_test_results]\ncompare = benchmark.Compare(ab_results)\ncompare.trim_significant_figures()\ncompare.colorize()\ncompare.print()\n\n\n\nOutput",
            "code"
        ],
        [
            " [------------------------------------- Batched dot -------------------------------------]\n                                                |  [1, 1]  |  [1024, 10000]  |  [10000, 1]\n 1 threads: ------------------------------------------------------------------------------\n   (environment A: mul/sum)  batched_dot(x, x)  |     7    |      36000      |      21\n   (environment B: bmm)      batched_dot(x, x)  |    14    |      40000      |      85\n\n Times are in microseconds (us).",
            "code"
        ],
        [
            "# And just to show that we can round trip all of the results from earlier:\nround_tripped_results = pickle.loads(pickle.dumps(results))\nassert(str(benchmark.Compare(results)) == str(benchmark.Compare(round_tripped_results)))",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Benchmark->Steps->7. Generating inputs with Fuzzed Parameters": [
        [
            "As we\u2019ve seen in the previous section, there can be some stark\nperformance differences depending on the input tensors. Hence, it\nis a good idea to run benchmarks on a number of different inputs.\nHowever, creating all these input tensors can be tedious which is\nwhere torch.utils.benchmark.Fuzzer and related classes come in.\nLet\u2019s take a look at how we can use the Fuzzer to create some test\ncases for the benchmark.",
            "markdown"
        ],
        [
            "from torch.utils.benchmark import Fuzzer, FuzzedParameter, FuzzedTensor, ParameterAlias\n\n# Generates random tensors with 128 to 10000000 elements and sizes k0 and k1 chosen from a\n# loguniform distribution in [1, 10000], 40% of which will be discontiguous on average.\nexample_fuzzer = Fuzzer(\n    parameters = [\n        FuzzedParameter('k0', minval=1, maxval=10000, distribution='loguniform'),\n        FuzzedParameter('k1', minval=1, maxval=10000, distribution='loguniform'),\n    ],\n    tensors = [\n        FuzzedTensor('x', size=('k0', 'k1'), min_elements=128, max_elements=10000000, probability_contiguous=0.6)\n    ],\n    seed=0,\n)\n\nresults = []\nfor tensors, tensor_params, params in example_fuzzer.take(10):\n    # description is the column label\n    sub_label=f\"{params['k0']:&lt;6} x {params['k1']:&lt;4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n    results.append((\n        stmt='batched_dot_mul_sum(x, x)',\n        setup='from __main__ import batched_dot_mul_sum',\n        globals=tensors,\n        label='Batched dot',\n        sub_label=sub_label,\n        description='mul/sum',\n    ).blocked_autorange(min_run_time=1))\n    results.append((\n        stmt='batched_dot_bmm(x, x)',\n        setup='from __main__ import batched_dot_bmm',\n        globals=tensors,\n        label='Batched dot',\n        sub_label=sub_label,\n        description='bmm',\n    ).blocked_autorange(min_run_time=1))\n\ncompare = benchmark.Compare(results)\ncompare.trim_significant_figures()\ncompare.print()\n\n\n\nOutput",
            "code"
        ],
        [
            " [--------------------- Batched dot ---------------------]\n                                      |  mul/sum  |   bmm\n 1 threads: ----------------------------------------------\n       725    x 257                   |      87   |    180\n       49     x 383                   |      15   |     30\n       34     x 1468                  |      30   |    118\n       187    x 5039                  |     400   |   1200\n       2140   x 1296 (discontiguous)  |    2000   |  41000\n       78     x 1598                  |      74   |    310\n       519    x 763                   |     190   |   1500\n       141    x 1082                  |      87   |    500\n       78     x 5    (discontiguous)  |       9   |     20\n       187    x 1                     |      12   |     10\n\n Times are in microseconds (us).",
            "code"
        ],
        [
            "There is a lot of flexibility for defining your own Fuzzers which\nis great for creating a powerful set of inputs to benchmark. But to\nmake things even simpler, PyTorch benchmark module comes with some\nbuitin Fuzzers for common benchmarking needs. Let\u2019s take a look at\nhow we can use one of these builtin fuzzers.",
            "markdown"
        ],
        [
            "from torch.utils.benchmark.op_fuzzers import binary\n\nresults = []\nfor tensors, tensor_params, params in binary.BinaryOpFuzzer(seed=0).take(10):\n    sub_label=f\"{params['k0']:&lt;6} x {params['k1']:&lt;4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n    results.append((\n        stmt='batched_dot_mul_sum(x, x)',\n        setup='from __main__ import batched_dot_mul_sum',\n        globals=tensors,\n        label='Batched dot',\n        sub_label=sub_label,\n        description='mul/sum',\n    ).blocked_autorange(min_run_time=1))\n    results.append((\n        stmt='batched_dot_bmm(x, x)',\n        setup='from __main__ import batched_dot_bmm',\n        globals=tensors,\n        label='Batched dot',\n        sub_label=sub_label,\n        description='bmm',\n    ).blocked_autorange(min_run_time=1))\n\ncompare = benchmark.Compare(results)\ncompare.trim_significant_figures()\ncompare.colorize(rowwise=True)\ncompare.print()\n\n\n\nOutput",
            "code"
        ],
        [
            " [----------------------- Batched dot ------------------------]\n                                          |  mul/sum  |   bmm\n 1 threads: ---------------------------------------------------\n       64     x 473  (discontiguous)      |    10000  |   40000\n       16384  x 12642115 (discontiguous)  |       31  |      78\n       8192   x 892                       |     4800  |   20400\n       512    x 64   (discontiguous)      |   110000  |  400000\n       493    x 27   (discontiguous)      |     1100  |    2440\n       118    x 32   (discontiguous)      |      870  |    2030\n       16     x 495  (discontiguous)      |    23600  |   24000\n       488    x 62374                     |    90000  |  100000\n       240372 x 69                        |    40000  |   16000\n       40156  x 32   (discontiguous)      |     2670  |    5000\n\n Times are in microseconds (us).",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Benchmark->Steps->8. Collecting instruction counts with Callgrind": [
        [
            "One of the challenges of optimizing code is the variation and opacity of\nwall time. There are many sources of non-determinism, from adaptive clock\nspeeds to resource contention with other processes. Furthermore, end-to-end\ntime gives no insight into where time is being spent, which is really what\nwe\u2019re interested in when optimizing code.",
            "markdown"
        ],
        [
            "A complementary approach is to also collect instruction counts. These counts\nare a proxy metric and do not capture all aspects of performance\n(e.g. memory or I/O bound tasks), however they do have several useful\nproperties. Instruction counts are reproducible, insensitive to environmental\nvariation, and offer fine grained insight into where a program is spending\ncycles.",
            "markdown"
        ],
        [
            "To see the utility of instruction counts, let us look at how we might\nreduce the overhead of <cite>batched_dot_mul_sum</cite>. The obvious solution is to\nmove it to C++, so we avoid going between Python and C++ multiple times.",
            "markdown"
        ],
        [
            "Fortunately, the source is nearly identical. One question that we have to ask\nin C++ is whether we should take arguments by value or reference.",
            "markdown"
        ],
        [
            "batched_dot_src = \"\"\"\\\n/* ---- Python ---- */\n// def batched_dot_mul_sum(a, b):\n//     return a.mul(b).sum(-1)\n\ntorch::Tensor batched_dot_mul_sum_v0(\n    const torch::Tensor a,\n    const torch::Tensor b) {\n  return a.mul(b).sum(-1);\n}\n\ntorch::Tensor batched_dot_mul_sum_v1(\n    const torch::Tensor&amp; a,\n    const torch::Tensor&amp; b) {\n  return a.mul(b).sum(-1);\n}\n\"\"\"\n\n\n# PyTorch makes it easy to test our C++ implementations by providing a utility\n# to JIT compile C++ source into Python extensions:\nimport os\nfrom torch.utils import cpp_extension\ncpp_lib = (\n    name='cpp_lib',\n    cpp_sources=batched_dot_src,\n    extra_cflags=['-O3'],\n    extra_include_paths=[\n        # `load_inline` needs to know where to find Pybind11 headers.\n        os.path.join(os.getenv('CONDA_PREFIX'), 'include')\n    ],\n    functions=['batched_dot_mul_sum_v0', 'batched_dot_mul_sum_v1']\n)\n\n# `load_inline` will create a shared object that is loaded into Python. When we collect\n# instruction counts Timer will create a subprocess, so we need to re-import it. The\n# import process is slightly more complicated for C extensions, but that's all we're\n# doing here.\nmodule_import_str = f\"\"\"\\\n# https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path\nimport importlib.util\nspec = importlib.util.spec_from_file_location(\"cpp_lib\", {repr(cpp_lib.__file__)})\ncpp_lib = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(cpp_lib)\"\"\"\n\nimport textwrap\ndef pretty_print(result):\n    \"\"\"Import machinery for cpp_lib.so can get repetitive to look at.\"\"\"\n    print(repr(result).replace(textwrap.indent(module_import_str, \"  \"), \"  import cpp_lib\"))\n\n\nt_baseline = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='''\\\nfrom __main__ import batched_dot_mul_sum\nx = torch.randn(2, 2)''')\n\nt0 = (\n    stmt='cpp_lib.batched_dot_mul_sum_v0(x, x)',\n    setup=f'''\\\n{module_import_str}\nx = torch.randn(2, 2)''')\n\nt1 = (\n    stmt='cpp_lib.batched_dot_mul_sum_v1(x, x)',\n    setup=f'''\\\n{module_import_str}\nx = torch.randn(2, 2)''')\n\n# Moving to C++ did indeed reduce overhead, but it's hard to tell which\n# calling convention is more efficient. v1 (call with references) seems to\n# be a bit faster, but it's within measurement error.\npretty_print(t_baseline.blocked_autorange())\npretty_print(t0.blocked_autorange())\npretty_print(t1.blocked_autorange())\n\n\n\nOutput",
            "code"
        ],
        [
            " &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;\n batched_dot_mul_sum(x, x)\n setup:\n   from __main__ import batched_dot_mul_sum\n   x = torch.randn(2, 2)\n\n   6.92 us\n   1 measurement, 100000 runs , 1 thread\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;\n cpp_lib.batched_dot_mul_sum_v0(x, x)\n setup:\n   import cpp_lib\n   x = torch.randn(2, 2)\n\n   5.29 us\n   1 measurement, 100000 runs , 1 thread\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;\n cpp_lib.batched_dot_mul_sum_v1(x, x)\n setup:\n   import cpp_lib\n   x = torch.randn(2, 2)\n\n   5.22 us\n   1 measurement, 100000 runs , 1 thread",
            "code"
        ],
        [
            "# Let's use Callgrind to determine which is better.\nstats_v0 = t0.collect_callgrind()\nstats_v1 = t1.collect_callgrind()\n\npretty_print(stats_v0)\npretty_print(stats_v1)\n\n# `.as_standardized` removes file names and some path prefixes, and makes\n# it easier to read the function symbols.\nstats_v0 = stats_v0.as_standardized()\nstats_v1 = stats_v1.as_standardized()\n\n# `.delta` diffs the instruction counts, and `.denoise` removes several\n# functions in the Python interpreter that are known to have significant\n# jitter.\ndelta = stats_v1.delta(stats_v0).denoise()\n\n# `.transform` is a convenience API for transforming function names. It is\n# useful for increasing cancelation when diff-ing instructions, as well as\n# just generally improving readability.\nreplacements = (\n    (\"???:void pybind11\", \"pybind11\"),\n    (\"batched_dot_mul_sum_v0\", \"batched_dot_mul_sum_v1\"),\n    (\"at::Tensor, at::Tensor\", \"...\"),\n    (\"at::Tensor const&amp;, at::Tensor const&amp;\", \"...\"),\n    (\"auto torch::detail::wrap_pybind_function_impl_\", \"wrap_pybind_function_impl_\"),\n)\nfor before, after in replacements:\n    delta = delta.transform(lambda l: l.replace(before, after))\n\n# We can use print options to control how much of the function to display.\n(linewidth=160)\n\n# Once parsed, the instruction counts make clear that passing `a` and `b`\n# by reference is more efficient as it skips some c10::TensorImpl bookkeeping\n# for the intermediate Tensors, and is also works better with PyBind11. This\n# is consistent with our noisy wall time observations.\nprint(delta)\n\n\n\nOutput",
            "code"
        ],
        [
            " &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fb0f06e7630&gt;\n cpp_lib.batched_dot_mul_sum_v0(x, x)\n setup:\n   import cpp_lib\n   x = torch.randn(2, 2)\n\n                            All          Noisy symbols removed\n     Instructions:      2392671                    2392671\n     Baseline:             4367                       4367\n 100 runs per measurement, 1 thread\n Warning: PyTorch was not built with debug symbols.\n          Source information may be limited. Rebuild with\n          REL_WITH_DEB_INFO=1 for more detailed results.\n &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fb10400d208&gt;\n cpp_lib.batched_dot_mul_sum_v1(x, x)\n setup:\n   import cpp_lib\n   x = torch.randn(2, 2)\n\n                            All          Noisy symbols removed\n     Instructions:      2378978                    2378978\n     Baseline:             4367                       4367\n     100 runs per measurement, 1 thread\n     Warning: PyTorch was not built with debug symbols.\n              Source information may be limited. Rebuild with\n              REL_WITH_DEB_INFO=1 for more detailed results.\n     &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7fb1000ab358&gt;\n           86  ???:0x000000000020d9e0\n       56  ???:0x000000000020db10\n    -1100  pybind11::cpp_function::initialize&lt;wrap_pybind_function_impl_&lt;at::Tensor ... r (&amp;)(...), std::integer_sequence&lt;unsigned long, 0ul, 1ul&gt;)::{lambda(...)\n    -1600  ???:wrap_pybind_function_impl_&lt;at::Tensor (&amp;)(...), 0ul, 1ul&gt;(at::Tensor (&amp;)(...), std::integer_sequence&lt;unsigned long, 0ul, 1ul&gt;)::{lambda(...)\n    -5200  ???:c10::intrusive_ptr&lt;c10::TensorImpl, c10::UndefinedTensorImpl&gt;::reset_()\n    -5935  ???:0x000000000022c0e0\n\n Total: -13693",
            "code"
        ]
    ],
    "torch->PyTorch Recipes->PyTorch Benchmark->Learn More": [
        [
            "Take a look at these other recipes to continue your learning:",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Learn the Basics": [
        [
            "Authors:\n,\n,\n,\n,",
            "markdown"
        ],
        [
            "Most machine learning workflows involve working with data, creating models, optimizing model\nparameters, and saving the trained models. This tutorial introduces you to a complete ML workflow\nimplemented in PyTorch, with links to learn more about each of these concepts.",
            "markdown"
        ],
        [
            "We\u2019ll use the FashionMNIST dataset to train a neural network that predicts if an input image belongs\nto one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker,\nBag, or Ankle boot.",
            "markdown"
        ],
        [
            "<cite>This tutorial assumes a basic familiarity with Python and Deep Learning concepts.</cite>",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Learn the Basics->Running the Tutorial Code": [
        [
            "You can run this tutorial in a couple of ways:",
            "markdown"
        ],
        [
            "<strong>In the cloud</strong>: This is the easiest way to get started! Each section has a \u201cRun in Microsoft Learn\u201d and \u201cRun in Google Colab\u201d link at the top, which opens an integrated notebook in Microsoft Learn or Google Colab, respectively, with the code in a fully-hosted environment.",
            "markdown"
        ],
        [
            "<strong>Locally</strong>: This option requires you to setup PyTorch and TorchVision first on your local machine (). Download the notebook or copy the code into your favorite IDE.",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Learn the Basics->How to Use this Guide": [
        [
            "If you\u2019re familiar with other deep learning frameworks, check out the  first\nto quickly familiarize yourself with PyTorch\u2019s API.",
            "markdown"
        ],
        [
            "If you\u2019re new to deep learning frameworks, head right into the first section of our step-by-step guide: .\n\n0. \n1. \n2. \n3. \n4. \n5. \n6. \n7. ",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Quickstart": [
        [
            "This section runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Quickstart->Working with data": [
        [
            "PyTorch has two :\ntorch.utils.data.DataLoader and torch.utils.data.Dataset.\nDataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around\nthe Dataset.",
            "markdown"
        ],
        [
            "import torch\nfrom torch import nn\nfrom torch.utils.data import \nfrom torchvision import datasets\nfrom torchvision.transforms import ",
            "code"
        ],
        [
            "PyTorch offers domain-specific libraries such as ,\n, and ,\nall of which include datasets. For this tutorial, we  will be using a TorchVision dataset.",
            "markdown"
        ],
        [
            "The torchvision.datasets module contains Dataset objects for many real-world vision data like\nCIFAR, COCO (). In this tutorial, we\nuse the FashionMNIST dataset. Every TorchVision Dataset includes two arguments: transform and\ntarget_transform to modify the samples and labels respectively.",
            "markdown"
        ],
        [
            "# Download training data from open datasets.\n = (\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=(),\n)\n\n# Download test data from open datasets.\n = (\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=(),\n)",
            "code"
        ],
        [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/26421880 [00:00&lt;?, ?it/s]\n  0%|          | 32768/26421880 [00:00&lt;01:28, 299753.88it/s]\n  0%|          | 65536/26421880 [00:00&lt;01:29, 295294.13it/s]\n  0%|          | 131072/26421880 [00:00&lt;01:01, 425672.21it/s]\n  1%|          | 229376/26421880 [00:00&lt;00:43, 602749.34it/s]\n  2%|1         | 425984/26421880 [00:00&lt;00:25, 1020255.52it/s]\n  2%|2         | 589824/26421880 [00:00&lt;00:22, 1169971.20it/s]\n  4%|3         | 983040/26421880 [00:00&lt;00:13, 1930260.95it/s]\n  6%|6         | 1703936/26421880 [00:00&lt;00:07, 3363722.84it/s]\n 12%|#1        | 3145728/26421880 [00:01&lt;00:03, 6338540.79it/s]\n 23%|##2       | 6062080/26421880 [00:01&lt;00:01, 12413292.37it/s]\n 35%|###4      | 9207808/26421880 [00:01&lt;00:01, 17148848.13it/s]\n 46%|####5     | 12025856/26421880 [00:01&lt;00:00, 19444941.06it/s]\n 57%|#####6    | 14974976/26421880 [00:01&lt;00:00, 21433382.45it/s]\n 67%|######6   | 17629184/26421880 [00:01&lt;00:00, 22108810.91it/s]\n 79%|#######8  | 20742144/26421880 [00:01&lt;00:00, 23708973.23it/s]\n 90%|######### | 23855104/26421880 [00:01&lt;00:00, 24886110.73it/s]\n100%|##########| 26421880/26421880 [00:01&lt;00:00, 14551885.44it/s]\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/29515 [00:00&lt;?, ?it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 273566.55it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 271935.02it/s]\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/4422102 [00:00&lt;?, ?it/s]\n  1%|          | 32768/4422102 [00:00&lt;00:14, 299418.88it/s]\n  1%|1         | 65536/4422102 [00:00&lt;00:14, 298442.09it/s]\n  3%|2         | 131072/4422102 [00:00&lt;00:09, 433828.47it/s]\n  5%|5         | 229376/4422102 [00:00&lt;00:06, 615340.99it/s]\n 11%|#1        | 491520/4422102 [00:00&lt;00:03, 1251385.57it/s]\n 21%|##1       | 950272/4422102 [00:00&lt;00:01, 2243412.69it/s]\n 44%|####3     | 1933312/4422102 [00:00&lt;00:00, 4423447.14it/s]\n 87%|########6 | 3833856/4422102 [00:00&lt;00:00, 8515899.82it/s]\n100%|##########| 4422102/4422102 [00:00&lt;00:00, 5009664.10it/s]\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/5148 [00:00&lt;?, ?it/s]\n100%|##########| 5148/5148 [00:00&lt;00:00, 23780040.74it/s]\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw",
            "code"
        ],
        [
            "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports\nautomatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element\nin the dataloader iterable will return a batch of 64 features and labels.",
            "markdown"
        ],
        [
            "batch_size = 64\n\n# Create data loaders.\n = (, batch_size=batch_size)\n = (, batch_size=batch_size)\n\nfor , y in :\n    print(f\"Shape of X [N, C, H, W]: {.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break",
            "code"
        ],
        [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64",
            "code"
        ],
        [
            "Read more about .",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Quickstart->Creating Models": [
        [
            "To define a neural network in PyTorch, we create a class that inherits\nfrom . We define the layers of the network\nin the __init__ function and specify how data will pass through the network in the forward function. To accelerate\noperations in the neural network, we move it to the GPU if available.",
            "markdown"
        ],
        [
            "# Get cpu or gpu device for training.\ndevice = \"cuda\" if () else \"mps\" if () else \"cpu\"\nprint(f\"Using {device} device\")\n\n# Define model\nclass NeuralNetwork():\n    def __init__(self):\n        super().__init__()\n        self.flatten = ()\n        self.linear_relu_stack = (\n            (28*28, 512),\n            (),\n            (512, 512),\n            (),\n            (512, 10)\n        )\n\n    def forward(self, ):\n         = self.flatten()\n        logits = self.linear_relu_stack()\n        return logits\n\nmodel = ().to(device)\nprint(model)",
            "code"
        ],
        [
            "Using cuda device\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)",
            "code"
        ],
        [
            "Read more about .",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters": [
        [
            "To train a model, we need a \nand an .",
            "markdown"
        ],
        [
            " = ()\n = ((), lr=1e-3)",
            "code"
        ],
        [
            "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and\nbackpropagates the prediction error to adjust the model\u2019s parameters.",
            "markdown"
        ],
        [
            "def train(dataloader, model, , ):\n    size = len(dataloader.dataset)\n    ()\n    for batch, (, y) in enumerate(dataloader):\n        , y = .to(device), y.to(device)\n\n        # Compute prediction error\n         = model()\n        loss = (, y)\n\n        # Backpropagation\n        ()\n        loss.backward()\n        ()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len()\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")",
            "code"
        ],
        [
            "We also check the model\u2019s performance against the test dataset to ensure it is learning.",
            "markdown"
        ],
        [
            "def test(dataloader, model, ):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    ()\n    test_loss, correct = 0, 0\n    with ():\n        for , y in dataloader:\n            , y = .to(device), y.to(device)\n             = model()\n            test_loss += (, y).item()\n            correct += (.argmax(1) == y).type().sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")",
            "code"
        ],
        [
            "The training process is conducted over several iterations (<em>epochs</em>). During each epoch, the model learns\nparameters to make better predictions. We print the model\u2019s accuracy and loss at each epoch; we\u2019d like to see the\naccuracy increase and the loss decrease with every epoch.",
            "markdown"
        ],
        [
            "epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "code"
        ],
        [
            "Epoch 1\n-------------------------------\nloss: 2.296901  [   64/60000]\nloss: 2.276506  [ 6464/60000]\nloss: 2.256961  [12864/60000]\nloss: 2.258251  [19264/60000]\nloss: 2.231319  [25664/60000]\nloss: 2.210411  [32064/60000]\nloss: 2.218347  [38464/60000]\nloss: 2.173729  [44864/60000]\nloss: 2.179739  [51264/60000]\nloss: 2.154059  [57664/60000]\nTest Error:\n Accuracy: 46.7%, Avg loss: 2.135564\n\nEpoch 2\n-------------------------------\nloss: 2.144533  [   64/60000]\nloss: 2.123762  [ 6464/60000]\nloss: 2.059470  [12864/60000]\nloss: 2.091630  [19264/60000]\nloss: 2.025001  [25664/60000]\nloss: 1.967701  [32064/60000]\nloss: 2.001757  [38464/60000]\nloss: 1.900702  [44864/60000]\nloss: 1.923718  [51264/60000]\nloss: 1.860082  [57664/60000]\nTest Error:\n Accuracy: 54.1%, Avg loss: 1.845273\n\nEpoch 3\n-------------------------------\nloss: 1.876047  [   64/60000]\nloss: 1.834949  [ 6464/60000]\nloss: 1.711895  [12864/60000]\nloss: 1.777222  [19264/60000]\nloss: 1.656412  [25664/60000]\nloss: 1.615331  [32064/60000]\nloss: 1.642911  [38464/60000]\nloss: 1.530612  [44864/60000]\nloss: 1.575070  [51264/60000]\nloss: 1.480866  [57664/60000]\nTest Error:\n Accuracy: 60.6%, Avg loss: 1.491688\n\nEpoch 4\n-------------------------------\nloss: 1.552643  [   64/60000]\nloss: 1.514459  [ 6464/60000]\nloss: 1.366315  [12864/60000]\nloss: 1.456150  [19264/60000]\nloss: 1.334569  [25664/60000]\nloss: 1.333665  [32064/60000]\nloss: 1.346708  [38464/60000]\nloss: 1.263595  [44864/60000]\nloss: 1.315365  [51264/60000]\nloss: 1.223128  [57664/60000]\nTest Error:\n Accuracy: 62.4%, Avg loss: 1.244198\n\nEpoch 5\n-------------------------------\nloss: 1.311815  [   64/60000]\nloss: 1.292572  [ 6464/60000]\nloss: 1.132950  [12864/60000]\nloss: 1.249755  [19264/60000]\nloss: 1.121590  [25664/60000]\nloss: 1.146547  [32064/60000]\nloss: 1.162641  [38464/60000]\nloss: 1.094516  [44864/60000]\nloss: 1.152009  [51264/60000]\nloss: 1.070775  [57664/60000]\nTest Error:\n Accuracy: 64.2%, Avg loss: 1.087599\n\nDone!",
            "code"
        ],
        [
            "Read more about .",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Quickstart->Saving Models": [
        [
            "A common way to save a model is to serialize the internal state dictionary (containing the model parameters).",
            "markdown"
        ],
        [
            "((), \"model.pth\")\nprint(\"Saved PyTorch Model State to model.pth\")",
            "code"
        ],
        [
            "Saved PyTorch Model State to model.pth",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Quickstart->Loading Models": [
        [
            "The process for loading a model includes re-creating the model structure and loading\nthe state dictionary into it.",
            "markdown"
        ],
        [
            "model = ()\n((\"model.pth\"))",
            "code"
        ],
        [
            "&lt;All keys matched successfully&gt;",
            "code"
        ],
        [
            "This model can now be used to make predictions.",
            "markdown"
        ],
        [
            "classes = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\n()\n, y = [0][0], [0][1]\nwith ():\n     = model()\n    predicted, actual = classes[[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')",
            "code"
        ],
        [
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"",
            "code"
        ],
        [
            "Read more about .",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  51.560 seconds)",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Tensors": [
        [
            "Tensors are a specialized data structure that are very similar to arrays and matrices.\nIn PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model\u2019s parameters.",
            "markdown"
        ],
        [
            "Tensors are similar to  ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and\nNumPy arrays can often share the same underlying memory, eliminating the need to copy data (see ). Tensors\nare also optimized for automatic differentiation (we\u2019ll see more about that later in the \nsection). If you\u2019re familiar with ndarrays, you\u2019ll be right at home with the Tensor API. If not, follow along!",
            "markdown"
        ],
        [
            "import torch\nimport numpy as np",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Tensors->Initializing a Tensor": [
        [
            "Tensors can be initialized in various ways. Take a look at the following examples:",
            "markdown"
        ],
        [
            "<strong>Directly from data</strong>",
            "markdown"
        ],
        [
            "Tensors can be created directly from data. The data type is automatically inferred.",
            "markdown"
        ],
        [
            "data = [[1, 2],[3, 4]]\n = (data)",
            "code"
        ],
        [
            "<strong>From a NumPy array</strong>",
            "markdown"
        ],
        [
            "Tensors can be created from NumPy arrays (and vice versa - see ).",
            "markdown"
        ],
        [
            "np_array = np.array(data)\n = (np_array)",
            "code"
        ],
        [
            "<strong>From another tensor:</strong>",
            "markdown"
        ],
        [
            "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.",
            "markdown"
        ],
        [
            " = () # retains the properties of x_data\nprint(f\"Ones Tensor: \\n {} \\n\")\n\n = (, dtype=) # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n {} \\n\")",
            "code"
        ],
        [
            "Ones Tensor:\n tensor([[1, 1],\n        [1, 1]])\n\nRandom Tensor:\n tensor([[0.1539, 0.6085],\n        [0.9970, 0.2234]])",
            "code"
        ],
        [
            "<strong>With random or constant values:</strong>",
            "markdown"
        ],
        [
            "shape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.",
            "markdown"
        ],
        [
            "shape = (2,3,)\n = (shape)\n = (shape)\n = (shape)\n\nprint(f\"Random Tensor: \\n {} \\n\")\nprint(f\"Ones Tensor: \\n {} \\n\")\nprint(f\"Zeros Tensor: \\n {}\")",
            "code"
        ],
        [
            "Random Tensor:\n tensor([[0.0614, 0.3925, 0.6718],\n        [0.0782, 0.7854, 0.7167]])\n\nOnes Tensor:\n tensor([[1., 1., 1.],\n        [1., 1., 1.]])\n\nZeros Tensor:\n tensor([[0., 0., 0.],\n        [0., 0., 0.]])",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Tensors->Attributes of a Tensor": [
        [
            "Tensor attributes describe their shape, datatype, and the device on which they are stored.",
            "markdown"
        ],
        [
            " = (3,4)\n\nprint(f\"Shape of tensor: {.shape}\")\nprint(f\"Datatype of tensor: {}\")\nprint(f\"Device tensor is stored on: {}\")",
            "code"
        ],
        [
            "Shape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Tensors->Operations on Tensors": [
        [
            "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing,\nindexing, slicing), sampling and more are\ncomprehensively described .",
            "markdown"
        ],
        [
            "Each of these operations can be run on the GPU (at typically higher speeds than on a\nCPU). If you\u2019re using Colab, allocate a GPU by going to Runtime &gt; Change runtime type &gt; GPU.",
            "markdown"
        ],
        [
            "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using\n.to method (after checking for GPU availability). Keep in mind that copying large tensors\nacross devices can be expensive in terms of time and memory!",
            "markdown"
        ],
        [
            "# We move our tensor to the GPU if available\nif ():\n     = .to(\"cuda\")",
            "code"
        ],
        [
            "Try out some of the operations from the list.\nIf you\u2019re familiar with the NumPy API, you\u2019ll find the Tensor API a breeze to use.",
            "markdown"
        ],
        [
            "<strong>Standard numpy-like indexing and slicing:</strong>",
            "markdown"
        ],
        [
            " = (4, 4)\nprint(f\"First row: {[0]}\")\nprint(f\"First column: {[:, 0]}\")\nprint(f\"Last column: {[..., -1]}\")\n[:,1] = 0\nprint()",
            "code"
        ],
        [
            "First row: tensor([1., 1., 1., 1.])\nFirst column: tensor([1., 1., 1., 1.])\nLast column: tensor([1., 1., 1., 1.])\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])",
            "code"
        ],
        [
            "<strong>Joining tensors</strong> You can use torch.cat to concatenate a sequence of tensors along a given dimension.\nSee also ,\nanother tensor joining option that is subtly different from torch.cat.",
            "markdown"
        ],
        [
            " = ([, , ], dim=1)\nprint()",
            "code"
        ],
        [
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])",
            "code"
        ],
        [
            "<strong>Arithmetic operations</strong>",
            "markdown"
        ],
        [
            "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n# ``tensor.T`` returns the transpose of a tensor\n =  @ \n = .matmul()\n\n = ()\n(, , out=)\n\n\n# This computes the element-wise product. z1, z2, z3 will have the same value\n =  * \n = .mul()\n\n = ()\n(, , out=)",
            "code"
        ],
        [
            "tensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])",
            "code"
        ],
        [
            "<strong>Single-element tensors</strong> If you have a one-element tensor, for example by aggregating all\nvalues of a tensor into one value, you can convert it to a Python\nnumerical value using item():",
            "markdown"
        ],
        [
            " = .sum()\nagg_item = .item()\nprint(agg_item, type(agg_item))",
            "code"
        ],
        [
            "12.0 &lt;class 'float'&gt;",
            "code"
        ],
        [
            "<strong>In-place operations</strong>\nOperations that store the result into the operand are called in-place. They are denoted by a _ suffix.\nFor example: x.copy_(y), x.t_(), will change x.",
            "markdown"
        ],
        [
            "print(f\"{} \\n\")\n.add_(5)\nprint()",
            "code"
        ],
        [
            "tensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\n\ntensor([[6., 5., 6., 6.],\n        [6., 5., 6., 6.],\n        [6., 5., 6., 6.],\n        [6., 5., 6., 6.]])",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss\nof history. Hence, their use is discouraged.",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Tensors->Bridge with NumPy": [
        [
            "Tensors on the CPU and NumPy arrays can share their underlying memory\nlocations, and changing one will change the other.",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Tensors->Bridge with NumPy->Tensor to NumPy array": [
        [
            " = (5)\nprint(f\"t: {}\")\nn = .numpy()\nprint(f\"n: {n}\")",
            "code"
        ],
        [
            "t: tensor([1., 1., 1., 1., 1.])\nn: [1. 1. 1. 1. 1.]",
            "code"
        ],
        [
            "A change in the tensor reflects in the NumPy array.",
            "markdown"
        ],
        [
            ".add_(1)\nprint(f\"t: {}\")\nprint(f\"n: {n}\")",
            "code"
        ],
        [
            "t: tensor([2., 2., 2., 2., 2.])\nn: [2. 2. 2. 2. 2.]",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Tensors->Bridge with NumPy->NumPy array to Tensor": [
        [
            "n = np.ones(5)\n = (n)",
            "code"
        ],
        [
            "Changes in the NumPy array reflects in the tensor.",
            "markdown"
        ],
        [
            "np.add(n, 1, out=n)\nprint(f\"t: {}\")\nprint(f\"n: {n}\")",
            "code"
        ],
        [
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\nn: [2. 2. 2. 2. 2.]",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.052 seconds)",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Datasets & DataLoaders": [
        [
            "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code\nto be decoupled from our model training code for better readability and modularity.\nPyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset\nthat allow you to use pre-loaded datasets as well as your own data.\nDataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around\nthe Dataset to enable easy access to the samples.",
            "markdown"
        ],
        [
            "PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that\nsubclass torch.utils.data.Dataset and implement functions specific to the particular data.\nThey can be used to prototype and benchmark your model. You can find them\nhere: ,\n, and",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Datasets & DataLoaders->Loading a Dataset": [
        [
            "Here is an example of how to load the  dataset from TorchVision.\nFashion-MNIST is a dataset of Zalando\u2019s article images consisting of 60,000 training examples and 10,000 test examples.\nEach example comprises a 28\u00d728 grayscale image and an associated label from one of 10 classes.\n<dl class=\"simple\">\n<dt>We load the  with the following parameters:</dt><dd>",
            "markdown"
        ],
        [
            "root is the path where the train/test data is stored,",
            "markdown"
        ],
        [
            "train specifies training or test dataset,",
            "markdown"
        ],
        [
            "download=True downloads the data from the internet if it\u2019s not available at root.",
            "markdown"
        ],
        [
            "transform and target_transform specify the feature and label transformations\n\n</dd>\n</dl>",
            "markdown"
        ],
        [
            "import torch\nfrom torch.utils.data import \nfrom torchvision import datasets\nfrom torchvision.transforms import \nimport matplotlib.pyplot as plt\n\n\n = (\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=()\n)\n\n = (\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=()\n)",
            "code"
        ],
        [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/26421880 [00:00&lt;?, ?it/s]\n  0%|          | 32768/26421880 [00:00&lt;01:24, 310618.97it/s]\n  0%|          | 65536/26421880 [00:00&lt;01:26, 305195.85it/s]\n  0%|          | 131072/26421880 [00:00&lt;00:59, 442431.56it/s]\n  1%|          | 229376/26421880 [00:00&lt;00:41, 625119.29it/s]\n  2%|1         | 491520/26421880 [00:00&lt;00:20, 1272491.28it/s]\n  4%|3         | 950272/26421880 [00:00&lt;00:11, 2282203.01it/s]\n  7%|7         | 1933312/26421880 [00:00&lt;00:05, 4499420.27it/s]\n 15%|#4        | 3833856/26421880 [00:00&lt;00:02, 8653918.88it/s]\n 26%|##5       | 6782976/26421880 [00:00&lt;00:01, 14441084.20it/s]\n 34%|###4      | 9109504/26421880 [00:01&lt;00:01, 16595929.65it/s]\n 46%|####6     | 12222464/26421880 [00:01&lt;00:00, 20260355.92it/s]\n 58%|#####7    | 15302656/26421880 [00:01&lt;00:00, 22762341.82it/s]\n 70%|######9   | 18415616/26421880 [00:01&lt;00:00, 24544593.90it/s]\n 81%|########1 | 21528576/26421880 [00:01&lt;00:00, 25781543.68it/s]\n 93%|#########3| 24674304/26421880 [00:01&lt;00:00, 26665755.25it/s]\n100%|##########| 26421880/26421880 [00:01&lt;00:00, 16091903.18it/s]\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/29515 [00:00&lt;?, ?it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 272663.94it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 271309.25it/s]\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/4422102 [00:00&lt;?, ?it/s]\n  1%|          | 32768/4422102 [00:00&lt;00:14, 306014.93it/s]\n  1%|1         | 65536/4422102 [00:00&lt;00:14, 304736.93it/s]\n  3%|2         | 131072/4422102 [00:00&lt;00:09, 442373.51it/s]\n  5%|5         | 229376/4422102 [00:00&lt;00:06, 628505.08it/s]\n 11%|#1        | 491520/4422102 [00:00&lt;00:03, 1277413.09it/s]\n 21%|##1       | 950272/4422102 [00:00&lt;00:01, 2289352.69it/s]\n 44%|####3     | 1933312/4422102 [00:00&lt;00:00, 4490802.21it/s]\n 87%|########6 | 3833856/4422102 [00:00&lt;00:00, 8707933.22it/s]\n100%|##########| 4422102/4422102 [00:00&lt;00:00, 5107828.86it/s]\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/5148 [00:00&lt;?, ?it/s]\n100%|##########| 5148/5148 [00:00&lt;00:00, 21965693.79it/s]\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Datasets & DataLoaders->Iterating and Visualizing the Dataset": [
        [
            "We can index Datasets manually like a list: training_data[index].\nWe use matplotlib to visualize some samples in our training data.",
            "markdown"
        ],
        [
            "labels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = (len(), size=(1,)).item()\n    ,  = [sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[])\n    plt.axis(\"off\")\n    plt.imshow(.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n<img alt=\"Pullover, Sandal, Bag, Bag, Sneaker, Bag, Pullover, Sandal, Sandal\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_001.png\" srcset=\"../../_images/sphx_glr_data_tutorial_001.png\"/>",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Datasets & DataLoaders->Creating a Custom Dataset for your files": [
        [
            "A custom Dataset class must implement three functions: <cite>__init__</cite>, <cite>__len__</cite>, and <cite>__getitem__</cite>.\nTake a look at this implementation; the FashionMNIST images are stored\nin a directory img_dir, and their labels are stored separately in a CSV file annotations_file.",
            "markdown"
        ],
        [
            "In the next sections, we\u2019ll break down what\u2019s happening in each of these functions.",
            "markdown"
        ],
        [
            "import os\nimport pandas as pd\nfrom torchvision.io import \n\nclass CustomImageDataset():\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = (img_path)\n         = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n             = self.target_transform()\n        return image, ",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Datasets & DataLoaders->Creating a Custom Dataset for your files->__init__": [
        [
            "The __init__ function is run once when instantiating the Dataset object. We initialize\nthe directory containing the images, the annotations file, and both transforms (covered\nin more detail in the next section).",
            "markdown"
        ],
        [
            "The labels.csv file looks like:",
            "markdown"
        ],
        [
            "tshirt1.jpg, 0\ntshirt2.jpg, 0\n......\nankleboot999.jpg, 9",
            "code"
        ],
        [
            "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n    self.img_labels = pd.read_csv(annotations_file)\n    self.img_dir = img_dir\n    self.transform = transform\n    self.target_transform = target_transform",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Datasets & DataLoaders->Creating a Custom Dataset for your files->__len__": [
        [
            "The __len__ function returns the number of samples in our dataset.",
            "markdown"
        ],
        [
            "Example:",
            "markdown"
        ],
        [
            "def __len__(self):\n    return len(self.img_labels)",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Datasets & DataLoaders->Creating a Custom Dataset for your files->__getitem__": [
        [
            "The __getitem__ function loads and returns a sample from the dataset at the given index idx.\nBased on the index, it identifies the image\u2019s location on disk, converts that to a tensor using read_image, retrieves the\ncorresponding label from the csv data in self.img_labels, calls the transform functions on them (if applicable), and returns the\ntensor image and corresponding label in a tuple.",
            "markdown"
        ],
        [
            "def __getitem__(self, idx):\n    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n    image = (img_path)\n     = self.img_labels.iloc[idx, 1]\n    if self.transform:\n        image = self.transform(image)\n    if self.target_transform:\n         = self.target_transform()\n    return image, ",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Datasets & DataLoaders->Preparing your data for training with DataLoaders": [
        [
            "The Dataset retrieves our dataset\u2019s features and labels one sample at a time. While training a model, we typically want to\npass samples in \u201cminibatches\u201d, reshuffle the data at every epoch to reduce model overfitting, and use Python\u2019s multiprocessing to\nspeed up data retrieval.",
            "markdown"
        ],
        [
            "DataLoader is an iterable that abstracts this complexity for us in an easy API.",
            "markdown"
        ],
        [
            "from torch.utils.data import \n\n = (, batch_size=64, shuffle=True)\n = (, batch_size=64, shuffle=True)",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Datasets & DataLoaders->Iterate through the DataLoader": [
        [
            "We have loaded that dataset into the DataLoader and can iterate through the dataset as needed.\nEach iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively).\nBecause we specified shuffle=True, after we iterate over all batches the data is shuffled (for finer-grained control over\nthe data loading order, take a look at ).",
            "markdown"
        ],
        [
            "# Display image and label.\n,  = next(iter())\nprint(f\"Feature batch shape: {.size()}\")\nprint(f\"Labels batch shape: {.size()}\")\n = [0].squeeze()\n = [0]\nplt.imshow(, cmap=\"gray\")\nplt.show()\nprint(f\"Label: {}\")\n\n\n<img alt=\"data tutorial\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_002.png\" srcset=\"../../_images/sphx_glr_data_tutorial_002.png\"/>",
            "code"
        ],
        [
            "Feature batch shape: torch.Size([64, 1, 28, 28])\nLabels batch shape: torch.Size([64])\nLabel: 5",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Datasets & DataLoaders->Further Reading": [
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  6.518 seconds)",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Transforms": [
        [
            "Data does not always come in its final processed form that is required for\ntraining machine learning algorithms. We use <strong>transforms</strong> to perform some\nmanipulation of the data and make it suitable for training.",
            "markdown"
        ],
        [
            "All TorchVision datasets have two parameters -transform to modify the features and\ntarget_transform to modify the labels - that accept callables containing the transformation logic.\nThe  module offers\nseveral commonly-used transforms out of the box.",
            "markdown"
        ],
        [
            "The FashionMNIST features are in PIL Image format, and the labels are integers.\nFor training, we need the features as normalized tensors, and the labels as one-hot encoded tensors.\nTo make these transformations, we use ToTensor and Lambda.",
            "markdown"
        ],
        [
            "import torch\nfrom torchvision import datasets\nfrom torchvision.transforms import , \n\n = (\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=(),\n    =(lambda y: (10, dtype=).scatter_(0, (y), value=1))\n)",
            "code"
        ],
        [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/26421880 [00:00&lt;?, ?it/s]\n  0%|          | 32768/26421880 [00:00&lt;01:28, 296688.04it/s]\n  0%|          | 65536/26421880 [00:00&lt;01:29, 295179.74it/s]\n  0%|          | 131072/26421880 [00:00&lt;01:01, 429092.71it/s]\n  1%|          | 229376/26421880 [00:00&lt;00:42, 609327.21it/s]\n  2%|1         | 491520/26421880 [00:00&lt;00:20, 1235121.14it/s]\n  4%|3         | 950272/26421880 [00:00&lt;00:11, 2220353.87it/s]\n  7%|7         | 1933312/26421880 [00:00&lt;00:05, 4376227.17it/s]\n 15%|#4        | 3833856/26421880 [00:00&lt;00:02, 8429427.68it/s]\n 26%|##5       | 6848512/26421880 [00:01&lt;00:01, 14260366.18it/s]\n 37%|###6      | 9732096/26421880 [00:01&lt;00:00, 17771879.75it/s]\n 49%|####8     | 12845056/26421880 [00:01&lt;00:00, 20850503.68it/s]\n 59%|#####9    | 15630336/26421880 [00:01&lt;00:00, 22063757.95it/s]\n 71%|#######   | 18743296/26421880 [00:01&lt;00:00, 23831016.47it/s]\n 83%|########2 | 21823488/26421880 [00:01&lt;00:00, 24969364.88it/s]\n 94%|#########4| 24936448/26421880 [00:01&lt;00:00, 25802545.97it/s]\n100%|##########| 26421880/26421880 [00:01&lt;00:00, 15634638.40it/s]\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/29515 [00:00&lt;?, ?it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 271609.85it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 270390.73it/s]\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/4422102 [00:00&lt;?, ?it/s]\n  1%|          | 32768/4422102 [00:00&lt;00:14, 295442.48it/s]\n  1%|1         | 65536/4422102 [00:00&lt;00:14, 294316.69it/s]\n  3%|2         | 131072/4422102 [00:00&lt;00:10, 428063.45it/s]\n  4%|4         | 196608/4422102 [00:00&lt;00:08, 490174.10it/s]\n 10%|9         | 425984/4422102 [00:00&lt;00:03, 1054556.38it/s]\n 19%|#8        | 819200/4422102 [00:00&lt;00:01, 1895885.94it/s]\n 37%|###7      | 1638400/4422102 [00:00&lt;00:00, 3676846.97it/s]\n 75%|#######4  | 3309568/4422102 [00:00&lt;00:00, 7256005.57it/s]\n100%|##########| 4422102/4422102 [00:00&lt;00:00, 4929402.27it/s]\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/5148 [00:00&lt;?, ?it/s]\n100%|##########| 5148/5148 [00:00&lt;00:00, 21004160.50it/s]\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Transforms->ToTensor()": [
        [
            "converts a PIL image or NumPy ndarray into a FloatTensor. and scales\nthe image\u2019s pixel intensity values in the range [0., 1.]",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Transforms->Lambda Transforms": [
        [
            "Lambda transforms apply any user-defined lambda function. Here, we define a function\nto turn the integer into a one-hot encoded tensor.\nIt first creates a zero tensor of size 10 (the number of labels in our dataset) and calls\n which assigns a\nvalue=1 on the index as given by the label y.",
            "markdown"
        ],
        [
            " = (lambda y: (\n    10, dtype=).scatter_(dim=0, index=(y), value=1))",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Transforms->Lambda Transforms->Further Reading": [
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  5.203 seconds)",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Build the Neural Network": [
        [
            "Neural networks comprise of layers/modules that perform operations on data.\nThe  namespace provides all the building blocks you need to\nbuild your own neural network. Every module in PyTorch subclasses the .\nA neural network is a module itself that consists of other modules (layers). This nested structure allows for\nbuilding and managing complex architectures easily.",
            "markdown"
        ],
        [
            "In the following sections, we\u2019ll build a neural network to classify images in the FashionMNIST dataset.",
            "markdown"
        ],
        [
            "import os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Build the Neural Network->Get Device for Training": [
        [
            "We want to be able to train our model on a hardware accelerator like the GPU,\nif it is available. Let\u2019s check to see if\n is available, else we\ncontinue to use the CPU.",
            "markdown"
        ],
        [
            "device = \"cuda\" if () else \"cpu\"\nprint(f\"Using {device} device\")",
            "code"
        ],
        [
            "Using cuda device",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Build the Neural Network->Define the Class": [
        [
            "We define our neural network by subclassing nn.Module, and\ninitialize the neural network layers in __init__. Every nn.Module subclass implements\nthe operations on input data in the forward method.",
            "markdown"
        ],
        [
            "class NeuralNetwork():\n    def __init__(self):\n        super().__init__()\n        self. = ()\n        self.linear_relu_stack = (\n            (28*28, 512),\n            (),\n            (512, 512),\n            (),\n            (512, 10),\n        )\n\n    def forward(self, x):\n        x = self.(x)\n         = self.linear_relu_stack(x)\n        return ",
            "code"
        ],
        [
            "We create an instance of NeuralNetwork, and move it to the device, and print\nits structure.",
            "markdown"
        ],
        [
            "model = ().to(device)\nprint(model)",
            "code"
        ],
        [
            "NeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)",
            "code"
        ],
        [
            "To use the model, we pass it the input data. This executes the model\u2019s forward,\nalong with some .\nDo not call model.forward() directly!",
            "markdown"
        ],
        [
            "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output.\nWe get the prediction probabilities by passing it through an instance of the nn.Softmax module.",
            "markdown"
        ],
        [
            " = (1, 28, 28, device=device)\n = model()\n = (dim=1)()\n = .argmax(1)\nprint(f\"Predicted class: {}\")",
            "code"
        ],
        [
            "Predicted class: tensor([1], device='cuda:0')",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Build the Neural Network->Model Layers": [
        [
            "Let\u2019s break down the layers in the FashionMNIST model. To illustrate it, we\nwill take a sample minibatch of 3 images of size 28x28 and see what happens to it as\nwe pass it through the network.",
            "markdown"
        ],
        [
            " = (3,28,28)\nprint(.size())",
            "code"
        ],
        [
            "torch.Size([3, 28, 28])",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Build the Neural Network->Model Layers->nn.Flatten": [
        [
            "We initialize the \nlayer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (\nthe minibatch dimension (at dim=0) is maintained).",
            "markdown"
        ],
        [
            " = ()\n = ()\nprint(.size())",
            "code"
        ],
        [
            "torch.Size([3, 784])",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Build the Neural Network->Model Layers->nn.Linear": [
        [
            "The \nis a module that applies a linear transformation on the input using its stored weights and biases.",
            "markdown"
        ],
        [
            " = (in_features=28*28, out_features=20)\n = ()\nprint(.size())",
            "code"
        ],
        [
            "torch.Size([3, 20])",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Build the Neural Network->Model Layers->nn.ReLU": [
        [
            "Non-linear activations are what create the complex mappings between the model\u2019s inputs and outputs.\nThey are applied after linear transformations to introduce <em>nonlinearity</em>, helping neural networks\nlearn a wide variety of phenomena.",
            "markdown"
        ],
        [
            "In this model, we use  between our\nlinear layers, but there\u2019s other activations to introduce non-linearity in your model.",
            "markdown"
        ],
        [
            "print(f\"Before ReLU: {}\\n\\n\")\n = ()()\nprint(f\"After ReLU: {}\")",
            "code"
        ],
        [
            "Before ReLU: tensor([[ 0.1789,  0.1301, -0.1497,  0.5988,  0.0768, -0.0540, -0.2174, -0.3324,\n         -0.5413, -0.1044,  0.5615, -0.2167,  0.0709, -0.0665, -0.1761,  0.5780,\n         -0.2716, -0.2387, -0.1427,  0.2420],\n        [-0.3099, -0.2667,  0.0975,  0.3754,  0.2276, -0.6589, -0.4287, -0.0892,\n         -0.6801, -0.3452,  0.2156, -0.1987,  0.0013,  0.3602, -0.2906,  0.5432,\n         -0.2488, -0.1862, -0.0737,  0.0740],\n        [ 0.1613, -0.0959, -0.1895,  0.4352,  0.3404, -0.2839, -0.4150, -0.4820,\n         -0.4538, -0.3342,  0.5201, -0.3329, -0.1264,  0.1956, -0.2441,  0.2632,\n         -0.1860, -0.2550, -0.1155,  0.2239]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nAfter ReLU: tensor([[0.1789, 0.1301, 0.0000, 0.5988, 0.0768, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.5615, 0.0000, 0.0709, 0.0000, 0.0000, 0.5780, 0.0000, 0.0000,\n         0.0000, 0.2420],\n        [0.0000, 0.0000, 0.0975, 0.3754, 0.2276, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.2156, 0.0000, 0.0013, 0.3602, 0.0000, 0.5432, 0.0000, 0.0000,\n         0.0000, 0.0740],\n        [0.1613, 0.0000, 0.0000, 0.4352, 0.3404, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.5201, 0.0000, 0.0000, 0.1956, 0.0000, 0.2632, 0.0000, 0.0000,\n         0.0000, 0.2239]], grad_fn=&lt;ReluBackward0&gt;)",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Build the Neural Network->Model Layers->nn.Sequential": [
        [
            " is an ordered\ncontainer of modules. The data is passed through all the modules in the same order as defined. You can use\nsequential containers to put together a quick network like seq_modules.",
            "markdown"
        ],
        [
            " = (\n    ,\n    ,\n    (),\n    (20, 10)\n)\n = (3,28,28)\n = ()",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Build the Neural Network->Model Layers->nn.Softmax": [
        [
            "The last linear layer of the neural network returns <cite>logits</cite> - raw values in [-infty, infty] - which are passed to the\n module. The logits are scaled to values\n[0, 1] representing the model\u2019s predicted probabilities for each class. dim parameter indicates the dimension along\nwhich the values must sum to 1.",
            "markdown"
        ],
        [
            " = (dim=1)\n = ()",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Build the Neural Network->Model Parameters": [
        [
            "Many layers inside a neural network are <em>parameterized</em>, i.e. have associated weights\nand biases that are optimized during training. Subclassing nn.Module automatically\ntracks all fields defined inside your model object, and makes all parameters\naccessible using your model\u2019s parameters() or named_parameters() methods.",
            "markdown"
        ],
        [
            "In this example, we iterate over each parameter, and print its size and a preview of its values.",
            "markdown"
        ],
        [
            "print(f\"Model structure: {model}\\n\\n\")\n\nfor name,  in ():\n    print(f\"Layer: {name} | Size: {.size()} | Values : {[:2]} \\n\")",
            "code"
        ],
        [
            "Model structure: NeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\n\nLayer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0110,  0.0225,  0.0328,  ..., -0.0281,  0.0314,  0.0240],\n        [ 0.0006, -0.0191, -0.0094,  ..., -0.0357,  0.0324, -0.0114]],\n       device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nLayer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0270, -0.0135], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nLayer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0410,  0.0228, -0.0157,  ...,  0.0431,  0.0202, -0.0306],\n        [ 0.0050,  0.0058, -0.0246,  ...,  0.0314,  0.0082, -0.0298]],\n       device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nLayer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0380, 0.0157], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nLayer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0139,  0.0366, -0.0057,  ..., -0.0234,  0.0136,  0.0403],\n        [-0.0083,  0.0247, -0.0343,  ...,  0.0433,  0.0206,  0.0151]],\n       device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nLayer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0422, 0.0322], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Build the Neural Network->Further Reading": [
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.053 seconds)",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Automatic Differentiation with torch.autograd": [
        [
            "When training neural networks, the most frequently used algorithm is\n<strong>back propagation</strong>. In this algorithm, parameters (model weights) are\nadjusted according to the <strong>gradient</strong> of the loss function with respect\nto the given parameter.",
            "markdown"
        ],
        [
            "To compute those gradients, PyTorch has a built-in differentiation engine\ncalled torch.autograd. It supports automatic computation of gradient for any\ncomputational graph.",
            "markdown"
        ],
        [
            "Consider the simplest one-layer neural network, with input x,\nparameters w and b, and some loss function. It can be defined in\nPyTorch in the following manner:",
            "markdown"
        ],
        [
            "import torch\n\n = (5)  # input tensor\n = (3)  # expected output\n = (5, 3, requires_grad=True)\n = (3, requires_grad=True)\n = (, )+\n = (, )",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Automatic Differentiation with torch.autograd->Tensors, Functions and Computational graph": [
        [
            "This code defines the following <strong>computational graph</strong>:\n\n<img alt=\"\" src=\"../../_images/comp-graph.png\"/>",
            "markdown"
        ],
        [
            "In this network, w and b are <strong>parameters</strong>, which we need to\noptimize. Thus, we need to be able to compute the gradients of loss\nfunction with respect to those variables. In order to do that, we set\nthe requires_grad property of those tensors.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "You can set the value of requires_grad when creating a\ntensor, or later by using x.requires_grad_(True) method.",
            "markdown"
        ],
        [
            "A function that we apply to tensors to construct computational graph is\nin fact an object of class Function. This object knows how to\ncompute the function in the <em>forward</em> direction, and also how to compute\nits derivative during the <em>backward propagation</em> step. A reference to\nthe backward propagation function is stored in grad_fn property of a\ntensor. You can find more information of Function .",
            "markdown"
        ],
        [
            "print(f\"Gradient function for z = {.grad_fn}\")\nprint(f\"Gradient function for loss = {.grad_fn}\")",
            "code"
        ],
        [
            "Gradient function for z = &lt;AddBackward0 object at 0x7f64949ec940&gt;\nGradient function for loss = &lt;BinaryCrossEntropyWithLogitsBackward0 object at 0x7f64949efa90&gt;",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Automatic Differentiation with torch.autograd->Computing Gradients": [
        [
            "To optimize weights of parameters in the neural network, we need to\ncompute the derivatives of our loss function with respect to parameters,\nnamely, we need \\(\\frac{\\partial loss}{\\partial w}\\) and\n\\(\\frac{\\partial loss}{\\partial b}\\) under some fixed values of\nx and y. To compute those derivatives, we call\nloss.backward(), and then retrieve the values from w.grad and\nb.grad:",
            "markdown"
        ],
        [
            "()\nprint()\nprint()",
            "code"
        ],
        [
            "tensor([[0.0795, 0.1649, 0.2479],\n        [0.0795, 0.1649, 0.2479],\n        [0.0795, 0.1649, 0.2479],\n        [0.0795, 0.1649, 0.2479],\n        [0.0795, 0.1649, 0.2479]])\ntensor([0.0795, 0.1649, 0.2479])",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "We can only obtain the grad properties for the leaf\nnodes of the computational graph, which have requires_grad property\nset to True. For all other nodes in our graph, gradients will not be\navailable.",
            "markdown"
        ],
        [
            "We can only perform gradient calculations using\nbackward once on a given graph, for performance reasons. If we need\nto do several backward calls on the same graph, we need to pass\nretain_graph=True to the backward call.",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Automatic Differentiation with torch.autograd->Disabling Gradient Tracking": [
        [
            "By default, all tensors with requires_grad=True are tracking their\ncomputational history and support gradient computation. However, there\nare some cases when we do not need to do that, for example, when we have\ntrained the model and just want to apply it to some input data, i.e. we\nonly want to do <em>forward</em> computations through the network. We can stop\ntracking computations by surrounding our computation code with\ntorch.no_grad() block:",
            "markdown"
        ],
        [
            " = (, )+\nprint(.requires_grad)\n\nwith ():\n     = (, )+\nprint(.requires_grad)",
            "code"
        ],
        [
            "True\nFalse",
            "code"
        ],
        [
            "Another way to achieve the same result is to use the detach() method\non the tensor:",
            "markdown"
        ],
        [
            " = (, )+\n = .detach()\nprint(.requires_grad)",
            "code"
        ],
        [
            "False\n\n\n<dl class=\"simple\">\n<dt>There are reasons you might want to disable gradient tracking:</dt><dd>",
            "code"
        ],
        [
            "To mark some parameters in your neural network as <strong>frozen parameters</strong>.",
            "markdown"
        ],
        [
            "To <strong>speed up computations</strong> when you are only doing forward pass, because computations on tensors that do\nnot track gradients would be more efficient.\n\n</dd>\n</dl>",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Automatic Differentiation with torch.autograd->More on Computational Graphs": [
        [
            "Conceptually, autograd keeps a record of data (tensors) and all executed\noperations (along with the resulting new tensors) in a directed acyclic\ngraph (DAG) consisting of\n\nobjects. In this DAG, leaves are the input tensors, roots are the output\ntensors. By tracing this graph from roots to leaves, you can\nautomatically compute the gradients using the chain rule.",
            "markdown"
        ],
        [
            "In a forward pass, autograd does two things simultaneously:",
            "markdown"
        ],
        [
            "run the requested operation to compute a resulting tensor",
            "markdown"
        ],
        [
            "maintain the operation\u2019s <em>gradient function</em> in the DAG.",
            "markdown"
        ],
        [
            "The backward pass kicks off when .backward() is called on the DAG\nroot. autograd then:",
            "markdown"
        ],
        [
            "computes the gradients from each .grad_fn,",
            "markdown"
        ],
        [
            "accumulates them in the respective tensor\u2019s .grad attribute",
            "markdown"
        ],
        [
            "using the chain rule, propagates all the way to the leaf tensors.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "<strong>DAGs are dynamic in PyTorch</strong>\nAn important thing to note is that the graph is recreated from scratch; after each\n.backward() call, autograd starts populating a new graph. This is\nexactly what allows you to use control flow statements in your model;\nyou can change the shape, size and operations at every iteration if\nneeded.",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Automatic Differentiation with torch.autograd->Optional Reading: Tensor Gradients and Jacobian Products": [
        [
            "In many cases, we have a scalar loss function, and we need to compute\nthe gradient with respect to some parameters. However, there are cases\nwhen the output function is an arbitrary tensor. In this case, PyTorch\nallows you to compute so-called <strong>Jacobian product</strong>, and not the actual\ngradient.",
            "markdown"
        ],
        [
            "For a vector function \\(\\vec{y}=f(\\vec{x})\\), where\n\\(\\vec{x}=\\langle x_1,\\dots,x_n\\rangle\\) and\n\\(\\vec{y}=\\langle y_1,\\dots,y_m\\rangle\\), a gradient of\n\\(\\vec{y}\\) with respect to \\(\\vec{x}\\) is given by <strong>Jacobian\nmatrix</strong>:\n\n\\[J=\\left(\\begin{array}{ccc}\n   \\frac{\\partial y_{1}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n   \\vdots &amp; \\ddots &amp; \\vdots\\\\\n   \\frac{\\partial y_{m}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial y_{m}}{\\partial x_{n}}\n   \\end{array}\\right)\\]",
            "markdown"
        ],
        [
            "Instead of computing the Jacobian matrix itself, PyTorch allows you to\ncompute <strong>Jacobian Product</strong> \\(v^T\\cdot J\\) for a given input vector\n\\(v=(v_1 \\dots v_m)\\). This is achieved by calling backward with\n\\(v\\) as an argument. The size of \\(v\\) should be the same as\nthe size of the original tensor, with respect to which we want to\ncompute the product:",
            "markdown"
        ],
        [
            " = (4, 5, requires_grad=True)\n = (+1).pow(2).t()\n((), retain_graph=True)\nprint(f\"First call\\n{}\")\n((), retain_graph=True)\nprint(f\"\\nSecond call\\n{}\")\n.zero_()\n((), retain_graph=True)\nprint(f\"\\nCall after zeroing gradients\\n{}\")",
            "code"
        ],
        [
            "First call\ntensor([[4., 2., 2., 2., 2.],\n        [2., 4., 2., 2., 2.],\n        [2., 2., 4., 2., 2.],\n        [2., 2., 2., 4., 2.]])\n\nSecond call\ntensor([[8., 4., 4., 4., 4.],\n        [4., 8., 4., 4., 4.],\n        [4., 4., 8., 4., 4.],\n        [4., 4., 4., 8., 4.]])\n\nCall after zeroing gradients\ntensor([[4., 2., 2., 2., 2.],\n        [2., 4., 2., 2., 2.],\n        [2., 2., 4., 2., 2.],\n        [2., 2., 2., 4., 2.]])",
            "code"
        ],
        [
            "Notice that when we call backward for the second time with the same\nargument, the value of the gradient is different. This happens because\nwhen doing backward propagation, PyTorch <strong>accumulates the\ngradients</strong>, i.e. the value of computed gradients is added to the\ngrad property of all leaf nodes of computational graph. If you want\nto compute the proper gradients, you need to zero out the grad\nproperty before. In real-life training an <em>optimizer</em> helps us to do\nthis.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Previously we were calling backward() function without\nparameters. This is essentially equivalent to calling\nbackward(torch.tensor(1.0)), which is a useful way to compute the\ngradients in case of a scalar-valued function, such as loss during\nneural network training.",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Automatic Differentiation with torch.autograd->Optional Reading: Tensor Gradients and Jacobian Products->Further Reading": [
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.054 seconds)",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Optimizing Model Parameters": [
        [
            "Now that we have a model and data it\u2019s time to train, validate and test our model by optimizing its parameters on\nour data. Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates\nthe error in its guess (<em>loss</em>), collects the derivatives of the error with respect to its parameters (as we saw in\nthe ), and <strong>optimizes</strong> these parameters using gradient descent. For a more\ndetailed walkthrough of this process, check out this video on .",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Optimizing Model Parameters->Prerequisite Code": [
        [
            "We load the code from the previous sections on \nand .",
            "markdown"
        ],
        [
            "import torch\nfrom torch import nn\nfrom torch.utils.data import \nfrom torchvision import datasets\nfrom torchvision.transforms import \n\n = (\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=()\n)\n\n = (\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=()\n)\n\n = (, batch_size=64)\n = (, batch_size=64)\n\nclass NeuralNetwork():\n    def __init__(self):\n        super(, self).__init__()\n        self.flatten = ()\n        self.linear_relu_stack = (\n            (28*28, 512),\n            (),\n            (512, 512),\n            (),\n            (512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = ()",
            "code"
        ],
        [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/26421880 [00:00&lt;?, ?it/s]\n  0%|          | 32768/26421880 [00:00&lt;01:25, 308533.34it/s]\n  0%|          | 65536/26421880 [00:00&lt;01:26, 303989.64it/s]\n  0%|          | 131072/26421880 [00:00&lt;00:59, 440126.28it/s]\n  1%|          | 229376/26421880 [00:00&lt;00:41, 623857.36it/s]\n  2%|1         | 491520/26421880 [00:00&lt;00:20, 1266466.02it/s]\n  4%|3         | 950272/26421880 [00:00&lt;00:11, 2269763.66it/s]\n  7%|7         | 1933312/26421880 [00:00&lt;00:05, 4471185.06it/s]\n 15%|#4        | 3833856/26421880 [00:00&lt;00:02, 8607269.59it/s]\n 26%|##6       | 6946816/26421880 [00:00&lt;00:01, 14841015.69it/s]\n 37%|###6      | 9732096/26421880 [00:01&lt;00:00, 18064398.08it/s]\n 49%|####8     | 12877824/26421880 [00:01&lt;00:00, 21318997.08it/s]\n 59%|#####9    | 15695872/26421880 [00:01&lt;00:00, 22636137.26it/s]\n 71%|#######1  | 18808832/26421880 [00:01&lt;00:00, 24427652.64it/s]\n 83%|########2 | 21889024/26421880 [00:01&lt;00:00, 25578163.49it/s]\n 95%|#########4| 25001984/26421880 [00:01&lt;00:00, 26420546.75it/s]\n100%|##########| 26421880/26421880 [00:01&lt;00:00, 16087653.96it/s]\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/29515 [00:00&lt;?, ?it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 266477.70it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 265043.97it/s]\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/4422102 [00:00&lt;?, ?it/s]\n  1%|          | 32768/4422102 [00:00&lt;00:14, 301419.72it/s]\n  1%|1         | 65536/4422102 [00:00&lt;00:14, 299886.54it/s]\n  3%|2         | 131072/4422102 [00:00&lt;00:09, 435724.08it/s]\n  5%|5         | 229376/4422102 [00:00&lt;00:06, 617548.80it/s]\n 11%|#1        | 491520/4422102 [00:00&lt;00:03, 1257879.50it/s]\n 21%|##1       | 950272/4422102 [00:00&lt;00:01, 2253560.03it/s]\n 44%|####3     | 1933312/4422102 [00:00&lt;00:00, 4446841.74it/s]\n 87%|########6 | 3833856/4422102 [00:00&lt;00:00, 8563990.63it/s]\n100%|##########| 4422102/4422102 [00:00&lt;00:00, 5030366.42it/s]\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/5148 [00:00&lt;?, ?it/s]\n100%|##########| 5148/5148 [00:00&lt;00:00, 24508827.46it/s]\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Optimizing Model Parameters->Hyperparameters": [
        [
            "Hyperparameters are adjustable parameters that let you control the model optimization process.\nDifferent hyperparameter values can impact model training and convergence rates\n( about hyperparameter tuning)\n<dl class=\"simple\">\n<dt>We define the following hyperparameters for training:</dt><dd>",
            "markdown"
        ],
        [
            "<strong>Number of Epochs</strong> - the number times to iterate over the dataset",
            "markdown"
        ],
        [
            "<strong>Batch Size</strong> - the number of data samples propagated through the network before the parameters are updated",
            "markdown"
        ],
        [
            "<strong>Learning Rate</strong> - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.\n\n</dd>\n</dl>",
            "markdown"
        ],
        [
            "learning_rate = 1e-3\nbatch_size = 64\nepochs = 5",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Optimizing Model Parameters->Optimization Loop": [
        [
            "Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each\niteration of the optimization loop is called an <strong>epoch</strong>.\n<dl class=\"simple\">\n<dt>Each epoch consists of two main parts:</dt><dd>",
            "markdown"
        ],
        [
            "<strong>The Train Loop</strong> - iterate over the training dataset and try to converge to optimal parameters.",
            "markdown"
        ],
        [
            "<strong>The Validation/Test Loop</strong> - iterate over the test dataset to check if model performance is improving.\n\n</dd>\n</dl>",
            "markdown"
        ],
        [
            "Let\u2019s briefly familiarize ourselves with some of the concepts used in the training loop. Jump ahead to\nsee the  of the optimization loop.",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Optimizing Model Parameters->Optimization Loop->Loss Function": [
        [
            "When presented with some training data, our untrained network is likely not to give the correct\nanswer. <strong>Loss function</strong> measures the degree of dissimilarity of obtained result to the target value,\nand it is the loss function that we want to minimize during training. To calculate the loss we make a\nprediction using the inputs of our given data sample and compare it against the true data label value.",
            "markdown"
        ],
        [
            "Common loss functions include  (Mean Square Error) for regression tasks, and\n (Negative Log Likelihood) for classification.\n combines nn.LogSoftmax and nn.NLLLoss.",
            "markdown"
        ],
        [
            "We pass our model\u2019s output logits to nn.CrossEntropyLoss, which will normalize the logits and compute the prediction error.",
            "markdown"
        ],
        [
            "# Initialize the loss function\n = ()",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Optimizing Model Parameters->Optimization Loop->Optimizer": [
        [
            "Optimization is the process of adjusting model parameters to reduce model error in each training step. <strong>Optimization algorithms</strong> define how this process is performed (in this example we use Stochastic Gradient Descent).\nAll optimization logic is encapsulated in  the optimizer object. Here, we use the SGD optimizer; additionally, there are many \navailable in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.",
            "markdown"
        ],
        [
            "We initialize the optimizer by registering the model\u2019s parameters that need to be trained, and passing in the learning rate hyperparameter.",
            "markdown"
        ],
        [
            " = ((), lr=learning_rate)\n\n\n<dl class=\"simple\">\n<dt>Inside the training loop, optimization happens in three steps:</dt><dd>",
            "code"
        ],
        [
            "Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.",
            "markdown"
        ],
        [
            "Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.",
            "markdown"
        ],
        [
            "Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass.\n\n</dd>\n</dl>",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Optimizing Model Parameters->Full Implementation": [
        [
            "We define train_loop that loops over our optimization code, and test_loop that\nevaluates the model\u2019s performance against our test data.",
            "markdown"
        ],
        [
            "def train_loop(dataloader, model, , ):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = (pred, y)\n\n        # Backpropagation\n        ()\n        loss.backward()\n        ()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, ):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with ():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += (pred, y).item()\n            correct += (pred.argmax(1) == y).type().sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")",
            "code"
        ],
        [
            "We initialize the loss function and optimizer, and pass it to train_loop and test_loop.\nFeel free to increase the number of epochs to track the model\u2019s improving performance.",
            "markdown"
        ],
        [
            " = ()\n = ((), lr=learning_rate)\n\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(, model, , )\n    test_loop(, model, )\nprint(\"Done!\")",
            "code"
        ],
        [
            "Epoch 1\n-------------------------------\nloss: 2.306274  [   64/60000]\nloss: 2.285838  [ 6464/60000]\nloss: 2.269943  [12864/60000]\nloss: 2.261921  [19264/60000]\nloss: 2.245953  [25664/60000]\nloss: 2.216744  [32064/60000]\nloss: 2.223908  [38464/60000]\nloss: 2.186029  [44864/60000]\nloss: 2.199185  [51264/60000]\nloss: 2.156675  [57664/60000]\nTest Error:\n Accuracy: 43.2%, Avg loss: 2.147765\n\nEpoch 2\n-------------------------------\nloss: 2.169220  [   64/60000]\nloss: 2.154036  [ 6464/60000]\nloss: 2.094223  [12864/60000]\nloss: 2.105725  [19264/60000]\nloss: 2.057751  [25664/60000]\nloss: 1.993958  [32064/60000]\nloss: 2.032755  [38464/60000]\nloss: 1.943888  [44864/60000]\nloss: 1.964005  [51264/60000]\nloss: 1.880819  [57664/60000]\nTest Error:\n Accuracy: 49.6%, Avg loss: 1.877261\n\nEpoch 3\n-------------------------------\nloss: 1.923636  [   64/60000]\nloss: 1.892730  [ 6464/60000]\nloss: 1.766222  [12864/60000]\nloss: 1.803750  [19264/60000]\nloss: 1.695832  [25664/60000]\nloss: 1.644898  [32064/60000]\nloss: 1.681978  [38464/60000]\nloss: 1.567808  [44864/60000]\nloss: 1.608559  [51264/60000]\nloss: 1.498805  [57664/60000]\nTest Error:\n Accuracy: 58.4%, Avg loss: 1.512764\n\nEpoch 4\n-------------------------------\nloss: 1.588199  [   64/60000]\nloss: 1.555499  [ 6464/60000]\nloss: 1.396250  [12864/60000]\nloss: 1.469044  [19264/60000]\nloss: 1.354479  [25664/60000]\nloss: 1.347111  [32064/60000]\nloss: 1.376084  [38464/60000]\nloss: 1.281405  [44864/60000]\nloss: 1.330694  [51264/60000]\nloss: 1.232292  [57664/60000]\nTest Error:\n Accuracy: 62.6%, Avg loss: 1.253344\n\nEpoch 5\n-------------------------------\nloss: 1.334092  [   64/60000]\nloss: 1.319183  [ 6464/60000]\nloss: 1.146310  [12864/60000]\nloss: 1.253771  [19264/60000]\nloss: 1.136652  [25664/60000]\nloss: 1.153831  [32064/60000]\nloss: 1.188763  [38464/60000]\nloss: 1.104697  [44864/60000]\nloss: 1.157200  [51264/60000]\nloss: 1.075848  [57664/60000]\nTest Error:\n Accuracy: 64.3%, Avg loss: 1.092215\n\nEpoch 6\n-------------------------------\nloss: 1.164995  [   64/60000]\nloss: 1.170451  [ 6464/60000]\nloss: 0.982307  [12864/60000]\nloss: 1.118346  [19264/60000]\nloss: 1.000636  [25664/60000]\nloss: 1.022203  [32064/60000]\nloss: 1.070253  [38464/60000]\nloss: 0.991670  [44864/60000]\nloss: 1.042103  [51264/60000]\nloss: 0.976573  [57664/60000]\nTest Error:\n Accuracy: 65.3%, Avg loss: 0.986656\n\nEpoch 7\n-------------------------------\nloss: 1.046831  [   64/60000]\nloss: 1.072952  [ 6464/60000]\nloss: 0.869186  [12864/60000]\nloss: 1.026576  [19264/60000]\nloss: 0.912367  [25664/60000]\nloss: 0.928342  [32064/60000]\nloss: 0.991693  [38464/60000]\nloss: 0.918040  [44864/60000]\nloss: 0.961682  [51264/60000]\nloss: 0.910086  [57664/60000]\nTest Error:\n Accuracy: 66.7%, Avg loss: 0.914412\n\nEpoch 8\n-------------------------------\nloss: 0.960576  [   64/60000]\nloss: 1.005463  [ 6464/60000]\nloss: 0.788259  [12864/60000]\nloss: 0.960789  [19264/60000]\nloss: 0.852388  [25664/60000]\nloss: 0.859124  [32064/60000]\nloss: 0.936759  [38464/60000]\nloss: 0.868817  [44864/60000]\nloss: 0.903424  [51264/60000]\nloss: 0.862730  [57664/60000]\nTest Error:\n Accuracy: 68.1%, Avg loss: 0.862345\n\nEpoch 9\n-------------------------------\nloss: 0.894723  [   64/60000]\nloss: 0.954783  [ 6464/60000]\nloss: 0.727722  [12864/60000]\nloss: 0.911476  [19264/60000]\nloss: 0.809471  [25664/60000]\nloss: 0.806541  [32064/60000]\nloss: 0.895423  [38464/60000]\nloss: 0.834586  [44864/60000]\nloss: 0.859842  [51264/60000]\nloss: 0.826616  [57664/60000]\nTest Error:\n Accuracy: 69.3%, Avg loss: 0.822960\n\nEpoch 10\n-------------------------------\nloss: 0.842345  [   64/60000]\nloss: 0.914199  [ 6464/60000]\nloss: 0.680164  [12864/60000]\nloss: 0.873395  [19264/60000]\nloss: 0.776440  [25664/60000]\nloss: 0.765965  [32064/60000]\nloss: 0.861726  [38464/60000]\nloss: 0.809308  [44864/60000]\nloss: 0.825955  [51264/60000]\nloss: 0.797490  [57664/60000]\nTest Error:\n Accuracy: 70.6%, Avg loss: 0.791684\n\nDone!",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Optimizing Model Parameters->Further Reading": [
        [
            "<strong>Total running time of the script:</strong> ( 1 minutes  43.533 seconds)",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Save and Load the Model": [
        [
            "In this section we will look at how to persist model state with saving, loading and running model predictions.",
            "markdown"
        ],
        [
            "import torch\nimport torchvision.models as models",
            "code"
        ]
    ],
    "torch->Introduction to PyTorch->Save and Load the Model->Saving and Loading Model Weights": [
        [
            "PyTorch models store the learned parameters in an internal\nstate dictionary, called state_dict. These can be persisted via the torch.save\nmethod:",
            "markdown"
        ],
        [
            "model = (pretrained=True)\n((), 'model_weights.pth')",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning:\n\nThe parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning:\n\nArguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /var/lib/jenkins/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n\n  0%|          | 0.00/528M [00:00&lt;?, ?B/s]\n  0%|          | 1.99M/528M [00:00&lt;00:26, 20.7MB/s]\n  1%|1         | 5.68M/528M [00:00&lt;00:18, 28.8MB/s]\n  2%|1         | 10.4M/528M [00:00&lt;00:14, 36.9MB/s]\n  3%|2         | 14.7M/528M [00:00&lt;00:13, 40.0MB/s]\n  4%|3         | 19.0M/528M [00:00&lt;00:12, 41.7MB/s]\n  4%|4         | 23.0M/528M [00:00&lt;00:12, 41.5MB/s]\n  5%|5         | 28.1M/528M [00:00&lt;00:11, 45.2MB/s]\n  7%|6         | 34.3M/528M [00:00&lt;00:10, 51.6MB/s]\n  8%|7         | 39.8M/528M [00:00&lt;00:09, 53.2MB/s]\n  8%|8         | 44.8M/528M [00:01&lt;00:10, 49.2MB/s]\n 10%|9         | 51.4M/528M [00:01&lt;00:09, 54.9MB/s]\n 11%|#         | 56.7M/528M [00:01&lt;00:10, 47.5MB/s]\n 12%|#1        | 61.5M/528M [00:01&lt;00:12, 38.8MB/s]\n 12%|#2        | 65.6M/528M [00:01&lt;00:12, 39.3MB/s]\n 14%|#3        | 71.4M/528M [00:01&lt;00:10, 44.6MB/s]\n 14%|#4        | 76.0M/528M [00:01&lt;00:11, 41.8MB/s]\n 15%|#5        | 81.0M/528M [00:01&lt;00:10, 44.6MB/s]\n 16%|#6        | 86.5M/528M [00:02&lt;00:09, 47.9MB/s]\n 17%|#7        | 91.2M/528M [00:02&lt;00:10, 42.2MB/s]\n 18%|#8        | 95.7M/528M [00:02&lt;00:10, 43.3MB/s]\n 19%|#9        | 101M/528M [00:02&lt;00:09, 45.4MB/s]\n 20%|#9        | 105M/528M [00:02&lt;00:10, 44.2MB/s]\n 21%|##1       | 111M/528M [00:02&lt;00:08, 48.6MB/s]\n 22%|##1       | 116M/528M [00:02&lt;00:08, 48.6MB/s]\n 23%|##3       | 122M/528M [00:02&lt;00:08, 50.3MB/s]\n 24%|##4       | 127M/528M [00:03&lt;00:09, 42.4MB/s]\n 25%|##5       | 133M/528M [00:03&lt;00:08, 47.9MB/s]\n 26%|##6       | 138M/528M [00:03&lt;00:10, 38.2MB/s]\n 27%|##6       | 142M/528M [00:03&lt;00:13, 30.1MB/s]\n 28%|##8       | 148M/528M [00:03&lt;00:10, 36.5MB/s]\n 29%|##9       | 154M/528M [00:03&lt;00:09, 42.3MB/s]\n 30%|###       | 159M/528M [00:03&lt;00:08, 43.4MB/s]\n 31%|###       | 163M/528M [00:03&lt;00:09, 40.4MB/s]\n 32%|###2      | 169M/528M [00:04&lt;00:08, 44.9MB/s]\n 33%|###2      | 174M/528M [00:04&lt;00:08, 43.5MB/s]\n 34%|###3      | 179M/528M [00:04&lt;00:07, 46.5MB/s]\n 35%|###4      | 183M/528M [00:04&lt;00:07, 45.3MB/s]\n 36%|###5      | 188M/528M [00:04&lt;00:10, 35.2MB/s]\n 37%|###6      | 194M/528M [00:04&lt;00:08, 41.3MB/s]\n 38%|###7      | 198M/528M [00:04&lt;00:08, 38.5MB/s]\n 38%|###8      | 202M/528M [00:04&lt;00:08, 39.5MB/s]\n 39%|###9      | 206M/528M [00:05&lt;00:08, 39.7MB/s]\n 40%|###9      | 210M/528M [00:05&lt;00:08, 40.0MB/s]\n 41%|####1     | 217M/528M [00:05&lt;00:06, 47.6MB/s]\n 42%|####2     | 222M/528M [00:05&lt;00:06, 49.2MB/s]\n 43%|####2     | 227M/528M [00:05&lt;00:06, 48.3MB/s]\n 44%|####3     | 232M/528M [00:05&lt;00:06, 49.3MB/s]\n 45%|####4     | 237M/528M [00:05&lt;00:06, 45.0MB/s]\n 46%|####5     | 241M/528M [00:05&lt;00:06, 45.7MB/s]\n 47%|####6     | 246M/528M [00:05&lt;00:06, 47.6MB/s]\n 48%|####7     | 251M/528M [00:06&lt;00:06, 42.3MB/s]\n 49%|####8     | 257M/528M [00:06&lt;00:06, 47.1MB/s]\n 50%|####9     | 262M/528M [00:06&lt;00:05, 49.6MB/s]\n 51%|#####     | 267M/528M [00:06&lt;00:05, 51.4MB/s]\n 52%|#####1    | 273M/528M [00:06&lt;00:04, 53.8MB/s]\n 53%|#####2    | 279M/528M [00:06&lt;00:04, 57.0MB/s]\n 54%|#####3    | 285M/528M [00:06&lt;00:05, 50.3MB/s]\n 55%|#####4    | 290M/528M [00:06&lt;00:05, 47.5MB/s]\n 56%|#####5    | 295M/528M [00:06&lt;00:05, 47.8MB/s]\n 57%|#####6    | 300M/528M [00:07&lt;00:04, 48.7MB/s]\n 58%|#####7    | 304M/528M [00:07&lt;00:04, 47.6MB/s]\n 59%|#####8    | 309M/528M [00:07&lt;00:06, 37.6MB/s]\n 59%|#####9    | 313M/528M [00:07&lt;00:05, 37.9MB/s]\n 60%|######    | 318M/528M [00:07&lt;00:05, 37.3MB/s]\n 61%|######1   | 322M/528M [00:07&lt;00:06, 34.6MB/s]\n 62%|######2   | 328M/528M [00:07&lt;00:05, 41.1MB/s]\n 63%|######3   | 335M/528M [00:07&lt;00:04, 48.9MB/s]\n 65%|######4   | 341M/528M [00:08&lt;00:03, 52.9MB/s]\n 66%|######5   | 347M/528M [00:08&lt;00:03, 55.3MB/s]\n 67%|######6   | 352M/528M [00:08&lt;00:03, 48.1MB/s]\n 68%|######7   | 358M/528M [00:08&lt;00:03, 50.4MB/s]\n 69%|######8   | 364M/528M [00:08&lt;00:03, 50.3MB/s]\n 70%|######9   | 369M/528M [00:08&lt;00:03, 47.6MB/s]\n 71%|#######1  | 375M/528M [00:08&lt;00:03, 52.6MB/s]\n 72%|#######2  | 380M/528M [00:08&lt;00:03, 47.4MB/s]\n 73%|#######2  | 385M/528M [00:09&lt;00:03, 47.8MB/s]\n 74%|#######3  | 390M/528M [00:09&lt;00:03, 43.6MB/s]\n 75%|#######4  | 395M/528M [00:09&lt;00:03, 43.8MB/s]\n 76%|#######5  | 400M/528M [00:09&lt;00:03, 42.7MB/s]\n 77%|#######6  | 404M/528M [00:09&lt;00:03, 42.7MB/s]\n 78%|#######7  | 409M/528M [00:09&lt;00:02, 45.8MB/s]\n 78%|#######8  | 414M/528M [00:09&lt;00:02, 45.8MB/s]\n 79%|#######9  | 418M/528M [00:09&lt;00:03, 36.5MB/s]\n 80%|########  | 423M/528M [00:10&lt;00:02, 40.5MB/s]\n 81%|########1 | 428M/528M [00:10&lt;00:03, 34.5MB/s]\n 82%|########1 | 431M/528M [00:10&lt;00:03, 30.7MB/s]\n 82%|########2 | 434M/528M [00:10&lt;00:03, 29.4MB/s]\n 83%|########2 | 437M/528M [00:10&lt;00:03, 26.6MB/s]\n 83%|########3 | 440M/528M [00:10&lt;00:03, 26.3MB/s]\n 84%|########4 | 446M/528M [00:10&lt;00:02, 34.2MB/s]\n 85%|########5 | 451M/528M [00:10&lt;00:02, 39.3MB/s]\n 86%|########6 | 455M/528M [00:11&lt;00:02, 30.5MB/s]\n 87%|########7 | 460M/528M [00:11&lt;00:02, 34.7MB/s]\n 88%|########7 | 463M/528M [00:11&lt;00:02, 32.6MB/s]\n 88%|########8 | 467M/528M [00:11&lt;00:01, 32.8MB/s]\n 89%|########9 | 471M/528M [00:11&lt;00:01, 35.4MB/s]\n 90%|########9 | 474M/528M [00:11&lt;00:01, 34.2MB/s]\n 91%|######### | 478M/528M [00:11&lt;00:01, 34.6MB/s]\n 91%|#########1| 483M/528M [00:11&lt;00:01, 39.1MB/s]\n 92%|#########2| 487M/528M [00:12&lt;00:01, 33.9MB/s]\n 93%|#########2| 490M/528M [00:12&lt;00:01, 30.3MB/s]\n 94%|#########3| 496M/528M [00:12&lt;00:00, 34.2MB/s]\n 95%|#########4| 500M/528M [00:12&lt;00:00, 35.4MB/s]\n 96%|#########5| 505M/528M [00:12&lt;00:00, 41.3MB/s]\n 97%|#########6| 512M/528M [00:12&lt;00:00, 48.2MB/s]\n 98%|#########7| 516M/528M [00:12&lt;00:00, 46.8MB/s]\n 99%|#########8| 521M/528M [00:12&lt;00:00, 46.6MB/s]\n100%|#########9| 526M/528M [00:13&lt;00:00, 13.3MB/s]\n100%|##########| 528M/528M [00:14&lt;00:00, 39.1MB/s]",
            "code"
        ],
        [
            "To load model weights, you need to create an instance of the same model first, and then load the parameters\nusing load_state_dict() method.",
            "markdown"
        ],
        [
            "model = () # we do not specify pretrained=True, i.e. do not load default weights\n(('model_weights.pth'))\n()",
            "code"
        ],
        [
            "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results.",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Save and Load the Model->Saving and Loading Models with Shapes": [
        [
            "When loading model weights, we needed to instantiate the model class first, because the class\ndefines the structure of a network. We might want to save the structure of this class together with\nthe model, in which case we can pass model (and not model.state_dict()) to the saving function:",
            "markdown"
        ],
        [
            "(model, 'model.pth')",
            "code"
        ],
        [
            "We can then load the model like this:",
            "markdown"
        ],
        [
            "model = ('model.pth')",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This approach uses Python  module when serializing the model, thus it relies on the actual class definition to be available when loading the model.",
            "markdown"
        ]
    ],
    "torch->Introduction to PyTorch->Save and Load the Model->Related Tutorials": [
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  20.151 seconds)",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->Deep Learning with PyTorch: A 60 Minute Blitz": [
        [
            "<strong>Author</strong>: \n\n<iframe allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/u7x8RXwLKcA\" width=\"560\"></iframe>",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->Deep Learning with PyTorch: A 60 Minute Blitz->What is PyTorch?": [
        [
            "PyTorch is a Python-based scientific computing package serving two broad purposes:",
            "markdown"
        ],
        [
            "A replacement for NumPy to use the power of GPUs and other accelerators.",
            "markdown"
        ],
        [
            "An automatic differentiation library that is useful to implement neural networks.",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->Deep Learning with PyTorch: A 60 Minute Blitz->Goal of this tutorial:": [
        [
            "Understand PyTorch\u2019s Tensor library and neural networks at a high level.",
            "markdown"
        ],
        [
            "Train a small neural network to classify images",
            "markdown"
        ],
        [
            "To run the tutorials below, make sure you have the  and \npackages installed.\n\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg> Tensors",
            "markdown"
        ],
        [
            "In this tutorial, you will learn the basics of PyTorch tensors.",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg> A Gentle Introduction to torch.autograd",
            "markdown"
        ],
        [
            "Learn about autograd.",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg> Neural Networks",
            "markdown"
        ],
        [
            "This tutorial demonstrates how you can train neural networks in PyTorch.",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg> Training a Classifier",
            "markdown"
        ],
        [
            "Learn how to train an image classifier in PyTorch by using the\nCIFAR10 dataset.",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->Learning PyTorch with Examples": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This is one of our older PyTorch tutorials. You can view our latest\nbeginner content in\n.",
            "markdown"
        ],
        [
            "This tutorial introduces the fundamental concepts of\n through self-contained\nexamples.",
            "markdown"
        ],
        [
            "At its core, PyTorch provides two main features:",
            "markdown"
        ],
        [
            "An n-dimensional Tensor, similar to numpy but can run on GPUs",
            "markdown"
        ],
        [
            "Automatic differentiation for building and training neural networks",
            "markdown"
        ],
        [
            "We will use a problem of fitting \\(y=\\sin(x)\\) with a third order polynomial\nas our running example. The network will have four parameters, and will be trained with\ngradient descent to fit random data by minimizing the Euclidean distance\nbetween the network output and the true output.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "You can browse the individual examples at the\n.",
            "markdown"
        ],
        [
            "Table of Contents",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->Learning PyTorch with Examples->->": [
        [
            "Before introducing PyTorch, we will first implement the network using\nnumpy.",
            "markdown"
        ],
        [
            "Numpy provides an n-dimensional array object, and many functions for\nmanipulating these arrays. Numpy is a generic framework for scientific\ncomputing; it does not know anything about computation graphs, or deep\nlearning, or gradients. However we can easily use numpy to fit a\nthird order polynomial to sine function by manually implementing the forward\nand backward passes through the network using numpy operations:",
            "markdown"
        ],
        [
            "# -*- coding: utf-8 -*-\nimport numpy as np\nimport math\n\n# Create random input and output data\nx = np.linspace(-math.pi, math.pi, 2000)\ny = np.sin(x)\n\n# Randomly initialize weights\na = np.random.randn()\nb = np.random.randn()\nc = np.random.randn()\nd = np.random.randn()\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    # y = a + b x + c x^2 + d x^3\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = np.square(y_pred - y).sum()\n    if t % 100 == 99:\n        print(t, loss)\n\n    # Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\nprint(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')",
            "code"
        ],
        [
            "Numpy is a great framework, but it cannot utilize GPUs to accelerate its\nnumerical computations. For modern deep neural networks, GPUs often\nprovide speedups of , so\nunfortunately numpy won\u2019t be enough for modern deep learning.",
            "markdown"
        ],
        [
            "Here we introduce the most fundamental PyTorch concept: the <strong>Tensor</strong>.\nA PyTorch Tensor is conceptually identical to a numpy array: a Tensor is\nan n-dimensional array, and PyTorch provides many functions for\noperating on these Tensors. Behind the scenes, Tensors can keep track of\na computational graph and gradients, but they\u2019re also useful as a\ngeneric tool for scientific computing.",
            "markdown"
        ],
        [
            "Also unlike numpy, PyTorch Tensors can utilize GPUs to accelerate\ntheir numeric computations. To run a PyTorch Tensor on GPU, you simply\nneed to specify the correct device.",
            "markdown"
        ],
        [
            "Here we use PyTorch Tensors to fit a third order polynomial to sine function.\nLike the numpy example above we need to manually implement the forward\nand backward passes through the network:",
            "markdown"
        ],
        [
            "# -*- coding: utf-8 -*-\n\nimport torch\nimport math\n\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n\n# Create random input and output data\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Randomly initialize weights\na = torch.randn((), device=device, dtype=dtype)\nb = torch.randn((), device=device, dtype=dtype)\nc = torch.randn((), device=device, dtype=dtype)\nd = torch.randn((), device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    if t % 100 == 99:\n        print(t, loss)\n\n    # Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights using gradient descent\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')",
            "code"
        ],
        [
            "In the above examples, we had to manually implement both the forward and\nbackward passes of our neural network. Manually implementing the\nbackward pass is not a big deal for a small two-layer network, but can\nquickly get very hairy for large complex networks.",
            "markdown"
        ],
        [
            "Thankfully, we can use \nto automate the computation of backward passes in neural networks. The\n<strong>autograd</strong> package in PyTorch provides exactly this functionality.\nWhen using autograd, the forward pass of your network will define a\n<strong>computational graph</strong>; nodes in the graph will be Tensors, and edges\nwill be functions that produce output Tensors from input Tensors.\nBackpropagating through this graph then allows you to easily compute\ngradients.",
            "markdown"
        ],
        [
            "This sounds complicated, it\u2019s pretty simple to use in practice. Each Tensor\nrepresents a node in a computational graph. If x is a Tensor that has\nx.requires_grad=True then x.grad is another Tensor holding the\ngradient of x with respect to some scalar value.",
            "markdown"
        ],
        [
            "Here we use PyTorch Tensors and autograd to implement our fitting sine wave\nwith third order polynomial example; now we no longer need to manually\nimplement the backward pass through the network:",
            "markdown"
        ],
        [
            "# -*- coding: utf-8 -*-\nimport torch\nimport math\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n\n# Create Tensors to hold input and outputs.\n# By default, requires_grad=False, which indicates that we do not need to\n# compute gradients with respect to these Tensors during the backward pass.\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Create random Tensors for weights. For a third order polynomial, we need\n# 4 weights: y = a + b x + c x^2 + d x^3\n# Setting requires_grad=True indicates that we want to compute gradients with\n# respect to these Tensors during the backward pass.\na = torch.randn((), device=device, dtype=dtype, requires_grad=True)\nb = torch.randn((), device=device, dtype=dtype, requires_grad=True)\nc = torch.randn((), device=device, dtype=dtype, requires_grad=True)\nd = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y using operations on Tensors.\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss using operations on Tensors.\n    # Now loss is a Tensor of shape (1,)\n    # loss.item() gets the scalar value held in the loss.\n    loss = (y_pred - y).pow(2).sum()\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Use autograd to compute the backward pass. This call will compute the\n    # gradient of loss with respect to all Tensors with requires_grad=True.\n    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n    # the gradient of the loss with respect to a, b, c, d respectively.\n    loss.backward()\n\n    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n    # because weights have requires_grad=True, but we don't need to track this\n    # in autograd.\n    with torch.no_grad():\n        a -= learning_rate * a.grad\n        b -= learning_rate * b.grad\n        c -= learning_rate * c.grad\n        d -= learning_rate * d.grad\n\n        # Manually zero the gradients after updating weights\n        a.grad = None\n        b.grad = None\n        c.grad = None\n        d.grad = None\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')",
            "code"
        ],
        [
            "Under the hood, each primitive autograd operator is really two functions\nthat operate on Tensors. The <strong>forward</strong> function computes output\nTensors from input Tensors. The <strong>backward</strong> function receives the\ngradient of the output Tensors with respect to some scalar value, and\ncomputes the gradient of the input Tensors with respect to that same\nscalar value.",
            "markdown"
        ],
        [
            "In PyTorch we can easily define our own autograd operator by defining a\nsubclass of torch.autograd.Function and implementing the forward\nand backward functions. We can then use our new autograd operator by\nconstructing an instance and calling it like a function, passing\nTensors containing input data.",
            "markdown"
        ],
        [
            "In this example we define our model as \\(y=a+b P_3(c+dx)\\) instead of\n\\(y=a+bx+cx^2+dx^3\\), where \\(P_3(x)=\\frac{1}{2}\\left(5x^3-3x\\right)\\)\nis the  of degree three. We write our own custom autograd\nfunction for computing forward and backward of \\(P_3\\), and use it to implement\nour model:",
            "markdown"
        ],
        [
            "# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\nclass LegendrePolynomial3(torch.autograd.Function):\n    \"\"\"\n    We can implement our own custom autograd Functions by subclassing\n    torch.autograd.Function and implementing the forward and backward passes\n    which operate on Tensors.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        \"\"\"\n        In the forward pass we receive a Tensor containing the input and return\n        a Tensor containing the output. ctx is a context object that can be used\n        to stash information for backward computation. You can cache arbitrary\n        objects for use in the backward pass using the ctx.save_for_backward method.\n        \"\"\"\n        ctx.save_for_backward(input)\n        return 0.5 * (5 * input ** 3 - 3 * input)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        \"\"\"\n        input, = ctx.saved_tensors\n        return grad_output * 1.5 * (5 * input ** 2 - 1)\n\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n\n# Create Tensors to hold input and outputs.\n# By default, requires_grad=False, which indicates that we do not need to\n# compute gradients with respect to these Tensors during the backward pass.\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Create random Tensors for weights. For this example, we need\n# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n# not too far from the correct result to ensure convergence.\n# Setting requires_grad=True indicates that we want to compute gradients with\n# respect to these Tensors during the backward pass.\na = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\nb = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\nc = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\nd = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n\nlearning_rate = 5e-6\nfor t in range(2000):\n    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n    P3 = LegendrePolynomial3.apply\n\n    # Forward pass: compute predicted y using operations; we compute\n    # P3 using our custom autograd operation.\n    y_pred = a + b * P3(c + d * x)\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum()\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Use autograd to compute the backward pass.\n    loss.backward()\n\n    # Update weights using gradient descent\n    with torch.no_grad():\n        a -= learning_rate * a.grad\n        b -= learning_rate * b.grad\n        c -= learning_rate * c.grad\n        d -= learning_rate * d.grad\n\n        # Manually zero the gradients after updating weights\n        a.grad = None\n        b.grad = None\n        c.grad = None\n        d.grad = None\n\nprint(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')",
            "code"
        ],
        [
            "Computational graphs and autograd are a very powerful paradigm for\ndefining complex operators and automatically taking derivatives; however\nfor large neural networks raw autograd can be a bit too low-level.",
            "markdown"
        ],
        [
            "When building neural networks we frequently think of arranging the\ncomputation into <strong>layers</strong>, some of which have <strong>learnable parameters</strong>\nwhich will be optimized during learning.",
            "markdown"
        ],
        [
            "In TensorFlow, packages like\n,\n,\nand  provide higher-level abstractions\nover raw computational graphs that are useful for building neural\nnetworks.",
            "markdown"
        ],
        [
            "In PyTorch, the nn package serves this same purpose. The nn\npackage defines a set of <strong>Modules</strong>, which are roughly equivalent to\nneural network layers. A Module receives input Tensors and computes\noutput Tensors, but may also hold internal state such as Tensors\ncontaining learnable parameters. The nn package also defines a set\nof useful loss functions that are commonly used when training neural\nnetworks.",
            "markdown"
        ],
        [
            "In this example we use the nn package to implement our polynomial model\nnetwork:",
            "markdown"
        ],
        [
            "# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# For this example, the output y is a linear function of (x, x^2, x^3), so\n# we can consider it as a linear layer neural network. Let's prepare the\n# tensor (x, x^2, x^3).\np = torch.tensor([1, 2, 3])\nxx = x.unsqueeze(-1).pow(p)\n\n# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n# of shape (2000, 3) \n\n# Use the nn package to define our model as a sequence of layers. nn.Sequential\n# is a Module which contains other Modules, and applies them in sequence to\n# produce its output. The Linear Module computes output from input using a\n# linear function, and holds internal Tensors for its weight and bias.\n# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n# to match the shape of `y`.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3, 1),\n    torch.nn.Flatten(0, 1)\n)\n\n# The nn package also contains definitions of popular loss functions; in this\n# case we will use Mean Squared Error (MSE) as our loss function.\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\nlearning_rate = 1e-6\nfor t in range(2000):\n\n    # Forward pass: compute predicted y by passing x to the model. Module objects\n    # override the __call__ operator so you can call them like functions. When\n    # doing so you pass a Tensor of input data to the Module and it produces\n    # a Tensor of output data.\n    y_pred = model(xx)\n\n    # Compute and print loss. We pass Tensors containing the predicted and true\n    # values of y, and the loss function returns a Tensor containing the\n    # loss.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Zero the gradients before running the backward pass.\n    model.zero_grad()\n\n    # Backward pass: compute gradient of the loss with respect to all the learnable\n    # parameters of the model. Internally, the parameters of each Module are stored\n    # in Tensors with requires_grad=True, so this call will compute gradients for\n    # all learnable parameters in the model.\n    loss.backward()\n\n    # Update the weights using gradient descent. Each parameter is a Tensor, so\n    # we can access its gradients like we did before.\n    with torch.no_grad():\n        for param in model.parameters():\n            param -= learning_rate * param.grad\n\n# You can access the first layer of `model` like accessing the first item of a list\nlinear_layer = model[0]\n\n# For linear layer, its parameters are stored as `weight` and `bias`.\nprint(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')",
            "code"
        ],
        [
            "Up to this point we have updated the weights of our models by manually\nmutating the Tensors holding learnable parameters with torch.no_grad().\nThis is not a huge burden for simple optimization algorithms like stochastic\ngradient descent, but in practice we often train neural networks using more\nsophisticated optimizers like AdaGrad, RMSProp, Adam, etc.",
            "markdown"
        ],
        [
            "The optim package in PyTorch abstracts the idea of an optimization\nalgorithm and provides implementations of commonly used optimization\nalgorithms.",
            "markdown"
        ],
        [
            "In this example we will use the nn package to define our model as\nbefore, but we will optimize the model using the RMSprop algorithm provided\nby the optim package:",
            "markdown"
        ],
        [
            "# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# Prepare the input tensor (x, x^2, x^3).\np = torch.tensor([1, 2, 3])\nxx = x.unsqueeze(-1).pow(p)\n\n# Use the nn package to define our model and loss function.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3, 1),\n    torch.nn.Flatten(0, 1)\n)\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\n# Use the optim package to define an Optimizer that will update the weights of\n# the model for us. Here we will use RMSprop; the optim package contains many other\n# optimization algorithms. The first argument to the RMSprop constructor tells the\n# optimizer which Tensors it should update.\nlearning_rate = 1e-3\noptimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\nfor t in range(2000):\n    # Forward pass: compute predicted y by passing x to the model.\n    y_pred = model(xx)\n\n    # Compute and print loss.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Before the backward pass, use the optimizer object to zero all of the\n    # gradients for the variables it will update (which are the learnable\n    # weights of the model). This is because by default, gradients are\n    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n    # is called. Checkout docs of torch.autograd.backward for more details.\n    optimizer.zero_grad()\n\n    # Backward pass: compute gradient of the loss with respect to model\n    # parameters\n    loss.backward()\n\n    # Calling the step function on an Optimizer makes an update to its\n    # parameters\n    optimizer.step()\n\n\nlinear_layer = model[0]\nprint(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')",
            "code"
        ],
        [
            "Sometimes you will want to specify models that are more complex than a\nsequence of existing Modules; for these cases you can define your own\nModules by subclassing nn.Module and defining a forward which\nreceives input Tensors and produces output Tensors using other\nmodules or other autograd operations on Tensors.",
            "markdown"
        ],
        [
            "In this example we implement our third order polynomial as a custom Module\nsubclass:",
            "markdown"
        ],
        [
            "# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\nclass Polynomial3(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate four parameters and assign them as\n        member parameters.\n        \"\"\"\n        super().__init__()\n        self.a = torch.nn.Parameter(torch.randn(()))\n        self.b = torch.nn.Parameter(torch.randn(()))\n        self.c = torch.nn.Parameter(torch.randn(()))\n        self.d = torch.nn.Parameter(torch.randn(()))\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Tensor of input data and we must return\n        a Tensor of output data. We can use Modules defined in the constructor as\n        well as arbitrary operators on Tensors.\n        \"\"\"\n        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n\n    def string(self):\n        \"\"\"\n        Just like any class in Python, you can also define custom method on PyTorch modules\n        \"\"\"\n        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# Construct our model by instantiating the class defined above\nmodel = Polynomial3()\n\n# Construct our loss function and an Optimizer. The call to model.parameters()\n# in the SGD constructor will contain the learnable parameters (defined \n# with torch.nn.Parameter) which are members of the model.\ncriterion = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\nfor t in range(2000):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f'Result: {model.string()}')",
            "code"
        ],
        [
            "As an example of dynamic graphs and weight sharing, we implement a very\nstrange model: a third-fifth order polynomial that on each forward pass\nchooses a random number between 3 and 5 and uses that many orders, reusing\nthe same weights multiple times to compute the fourth and fifth order.",
            "markdown"
        ],
        [
            "For this model we can use normal Python flow control to implement the loop,\nand we can implement weight sharing by simply reusing the same parameter multiple\ntimes when defining the forward pass.",
            "markdown"
        ],
        [
            "We can easily implement this model as a Module subclass:",
            "markdown"
        ],
        [
            "# -*- coding: utf-8 -*-\nimport random\nimport torch\nimport math\n\n\nclass DynamicNet(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate five parameters and assign them as members.\n        \"\"\"\n        super().__init__()\n        self.a = torch.nn.Parameter(torch.randn(()))\n        self.b = torch.nn.Parameter(torch.randn(()))\n        self.c = torch.nn.Parameter(torch.randn(()))\n        self.d = torch.nn.Parameter(torch.randn(()))\n        self.e = torch.nn.Parameter(torch.randn(()))\n\n    def forward(self, x):\n        \"\"\"\n        For the forward pass of the model, we randomly choose either 4, 5\n        and reuse the e parameter to compute the contribution of these orders.\n\n        Since each forward pass builds a dynamic computation graph, we can use normal\n        Python control-flow operators like loops or conditional statements when\n        defining the forward pass of the model.\n\n        Here we also see that it is perfectly safe to reuse the same parameter many\n        times when defining a computational graph.\n        \"\"\"\n        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n        for exp in range(4, random.randint(4, 6)):\n            y = y + self.e * x ** exp\n        return y\n\n    def string(self):\n        \"\"\"\n        Just like any class in Python, you can also define custom method on PyTorch modules\n        \"\"\"\n        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# Construct our model by instantiating the class defined above\nmodel = DynamicNet()\n\n# Construct our loss function and an Optimizer. Training this strange model with\n# vanilla stochastic gradient descent is tough, so we use momentum\ncriterion = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\nfor t in range(30000):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y)\n    if t % 2000 == 1999:\n        print(t, loss.item())\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f'Result: {model.string()}')",
            "code"
        ]
    ],
    "torch->Learning PyTorch->Learning PyTorch with Examples->": [
        [
            "You can browse the above examples here.",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?": [
        [
            "by Jeremy Howard, . Thanks to Rachel Thomas and Francisco Ingham.",
            "markdown"
        ],
        [
            "We recommend running this tutorial as a notebook, not a script. To download the notebook (.ipynb) file,\nclick the link at the top of the page.",
            "markdown"
        ],
        [
            "PyTorch provides the elegantly designed modules and classes  ,\n ,\n ,\nand \nto help you create and train neural networks.\nIn order to fully utilize their power and customize\nthem for your problem, you need to really understand exactly what they\u2019re\ndoing. To develop this understanding, we will first train basic neural net\non the MNIST data set without using any features from these models; we will\ninitially only use the most basic PyTorch tensor functionality. Then, we will\nincrementally add one feature from torch.nn, torch.optim, Dataset, or\nDataLoader at a time, showing exactly what each piece does, and how it\nworks to make the code either more concise, or more flexible.",
            "markdown"
        ],
        [
            "<strong>This tutorial assumes you already have PyTorch installed, and are familiar\nwith the basics of tensor operations.</strong> (If you\u2019re familiar with Numpy array\noperations, you\u2019ll find the PyTorch tensor operations used here nearly identical).",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->MNIST data setup": [
        [
            "We will use the classic  dataset,\nwhich consists of black-and-white images of hand-drawn digits (between 0 and 9).",
            "markdown"
        ],
        [
            "We will use \nfor dealing with paths (part of the Python 3 standard library), and will\ndownload the dataset using\n. We will only\nimport modules when we use them, so you can see exactly what\u2019s being\nused at each point.",
            "markdown"
        ],
        [
            "from pathlib import Path\nimport requests\n\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)",
            "code"
        ],
        [
            "This dataset is in numpy array format, and has been stored using pickle,\na python-specific format for serializing data.",
            "markdown"
        ],
        [
            "import pickle\nimport gzip\n\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((, ), (, ), _) = pickle.load(f, encoding=\"latin-1\")",
            "code"
        ],
        [
            "Each image is 28 x 28, and is being stored as a flattened row of length\n784 (=28x28). Let\u2019s take a look at one; we need to reshape it to 2d\nfirst.",
            "markdown"
        ],
        [
            "from matplotlib import pyplot\nimport numpy as np\n\npyplot.imshow([0].reshape((28, 28)), cmap=\"gray\")\nprint(.shape)\n\n\n<img alt=\"nn tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_nn_tutorial_001.png\" srcset=\"../_images/sphx_glr_nn_tutorial_001.png\"/>",
            "code"
        ],
        [
            "(50000, 784)",
            "code"
        ],
        [
            "PyTorch uses torch.tensor, rather than numpy arrays, so we need to\nconvert our data.",
            "markdown"
        ],
        [
            "import torch\n\n, , ,  = map(\n    , (, , , )\n)\nn, c = .shape\nprint(, )\nprint(.shape)\nprint(.min(), .max())",
            "code"
        ],
        [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\ntorch.Size([50000, 784])\ntensor(0) tensor(9)",
            "code"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->Neural net from scratch (no torch.nn)": [
        [
            "Let\u2019s first create a model using nothing but PyTorch tensor operations. We\u2019re assuming\nyou\u2019re already familiar with the basics of neural networks. (If you\u2019re not, you can\nlearn them at ).",
            "markdown"
        ],
        [
            "PyTorch provides methods to create random or zero-filled tensors, which we will\nuse to create our weights and bias for a simple linear model. These are just regular\ntensors, with one very special addition: we tell PyTorch that they require a\ngradient. This causes PyTorch to record all of the operations done on the tensor,\nso that it can calculate the gradient during back-propagation <em>automatically</em>!",
            "markdown"
        ],
        [
            "For the weights, we set requires_grad <strong>after</strong> the initialization, since we\ndon\u2019t want that step included in the gradient. (Note that a trailing _ in\nPyTorch signifies that the operation is performed in-place.)",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "We are initializing the weights here with\n\n(by multiplying with 1/sqrt(n)).",
            "markdown"
        ],
        [
            "import math\n\n = (784, 10) / math.sqrt(784)\n.requires_grad_()\n = (10, requires_grad=True)",
            "code"
        ],
        [
            "Thanks to PyTorch\u2019s ability to calculate gradients automatically, we can\nuse any standard Python function (or callable object) as a model! So\nlet\u2019s just write a plain matrix multiplication and broadcasted addition\nto create a simple linear model. We also need an activation function, so\nwe\u2019ll write <cite>log_softmax</cite> and use it. Remember: although PyTorch\nprovides lots of pre-written loss functions, activation functions, and\nso forth, you can easily write your own using plain python. PyTorch will\neven create fast GPU or vectorized CPU code for your function\nautomatically.",
            "markdown"
        ],
        [
            "def log_softmax(x):\n    return x - x.exp().sum(-1).log().unsqueeze(-1)\n\ndef model():\n    return log_softmax( @  + )",
            "code"
        ],
        [
            "In the above, the @ stands for the matrix multiplication operation. We will call\nour function on one batch of data (in this case, 64 images).  This is\none <em>forward pass</em>.  Note that our predictions won\u2019t be any better than\nrandom at this stage, since we start with random weights.",
            "markdown"
        ],
        [
            "bs = 64  # batch size\n\n = [0:bs]  # a mini-batch from x\n = ()  # predictions\n[0], .shape\nprint([0], .shape)",
            "code"
        ],
        [
            "tensor([-2.2877, -2.4242, -2.4204, -2.2774, -2.2933, -2.4228, -1.8680, -2.6085,\n        -2.9591, -1.9041], grad_fn=&lt;SelectBackward0&gt;) torch.Size([64, 10])",
            "code"
        ],
        [
            "As you see, the preds tensor contains not only the tensor values, but also a\ngradient function. We\u2019ll use this later to do backprop.",
            "markdown"
        ],
        [
            "Let\u2019s implement negative log-likelihood to use as the loss function\n(again, we can just use standard Python):",
            "markdown"
        ],
        [
            "def nll(input, target):\n    return -input[range(target.shape[0]), target].mean()\n\nloss_func = nll",
            "code"
        ],
        [
            "Let\u2019s check our loss with our random model, so we can see if we improve\nafter a backprop pass later.",
            "markdown"
        ],
        [
            " = [0:bs]\nprint(loss_func(, ))",
            "code"
        ],
        [
            "tensor(2.3459, grad_fn=&lt;NegBackward0&gt;)",
            "code"
        ],
        [
            "Let\u2019s also implement a function to calculate the accuracy of our model.\nFor each prediction, if the index with the largest value matches the\ntarget value, then the prediction was correct.",
            "markdown"
        ],
        [
            "def accuracy(out, ):\n     = (out, dim=1)\n    return ( == ).float().mean()",
            "code"
        ],
        [
            "Let\u2019s check the accuracy of our random model, so we can see if our\naccuracy improves as our loss improves.",
            "markdown"
        ],
        [
            "print(accuracy(, ))",
            "code"
        ],
        [
            "tensor(0.1094)",
            "code"
        ],
        [
            "We can now run a training loop.  For each iteration, we will:",
            "markdown"
        ],
        [
            "select a mini-batch of data (of size bs)",
            "markdown"
        ],
        [
            "use the model to make predictions",
            "markdown"
        ],
        [
            "calculate the loss",
            "markdown"
        ],
        [
            "loss.backward() updates the gradients of the model, in this case, weights\nand bias.",
            "markdown"
        ],
        [
            "We now use these gradients to update the weights and bias.  We do this\nwithin the torch.no_grad() context manager, because we do not want these\nactions to be recorded for our next calculation of the gradient.  You can read\nmore about how PyTorch\u2019s Autograd records operations\n.",
            "markdown"
        ],
        [
            "We then set the\ngradients to zero, so that we are ready for the next loop.\nOtherwise, our gradients would record a running tally of all the operations\nthat had happened (i.e. loss.backward() <em>adds</em> the gradients to whatever is\nalready stored, rather than replacing them).",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "You can use the standard python debugger to step through PyTorch\ncode, allowing you to check the various variable values at each step.\nUncomment set_trace() below to try it out.",
            "markdown"
        ],
        [
            "from IPython.core.debugger import set_trace\n\nlr = 0.5  # learning rate\nepochs = 2  # how many epochs to train for\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        #         set_trace()\n        start_i = i * bs\n        end_i = start_i + bs\n         = [start_i:end_i]\n         = [start_i:end_i]\n         = ()\n         = loss_func(, )\n\n        ()\n        with ():\n             -=  * lr\n             -=  * lr\n            .zero_()\n            .zero_()",
            "code"
        ],
        [
            "That\u2019s it: we\u2019ve created and trained a minimal neural network (in this case, a\nlogistic regression, since we have no hidden layers) entirely from scratch!",
            "markdown"
        ],
        [
            "Let\u2019s check the loss and accuracy and compare those to what we got\nearlier. We expect that the loss will have decreased and accuracy to\nhave increased, and they have.",
            "markdown"
        ],
        [
            "print(loss_func((), ), accuracy((), ))",
            "code"
        ],
        [
            "tensor(0.0807, grad_fn=&lt;NegBackward0&gt;) tensor(1.)",
            "code"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->Using torch.nn.functional": [
        [
            "We will now refactor our code, so that it does the same thing as before, only\nwe\u2019ll start taking advantage of PyTorch\u2019s nn classes to make it more concise\nand flexible. At each step from here, we should be making our code one or more\nof: shorter, more understandable, and/or more flexible.",
            "markdown"
        ],
        [
            "The first and easiest step is to make our code shorter by replacing our\nhand-written activation and loss functions with those from torch.nn.functional\n(which is generally imported into the namespace F by convention). This module\ncontains all the functions in the torch.nn library (whereas other parts of the\nlibrary contain classes). As well as a wide range of loss and activation\nfunctions, you\u2019ll also find here some convenient functions for creating neural\nnets, such as pooling functions. (There are also functions for doing convolutions,\nlinear layers, etc, but as we\u2019ll see, these are usually better handled using\nother parts of the library.)",
            "markdown"
        ],
        [
            "If you\u2019re using negative log likelihood loss and log softmax activation,\nthen Pytorch provides a single function F.cross_entropy that combines\nthe two. So we can even remove the activation function from our model.",
            "markdown"
        ],
        [
            "import torch.nn.functional as F\n\nloss_func = \n\ndef model():\n    return  @  + ",
            "code"
        ],
        [
            "Note that we no longer call log_softmax in the model function. Let\u2019s\nconfirm that our loss and accuracy are the same as before:",
            "markdown"
        ],
        [
            "print(loss_func((), ), accuracy((), ))",
            "code"
        ],
        [
            "tensor(0.0807, grad_fn=&lt;NllLossBackward0&gt;) tensor(1.)",
            "code"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->Refactor using nn.Module": [
        [
            "Next up, we\u2019ll use nn.Module and nn.Parameter, for a clearer and more\nconcise training loop. We subclass nn.Module (which itself is a class and\nable to keep track of state).  In this case, we want to create a class that\nholds our weights, bias, and method for the forward step.  nn.Module has a\nnumber of attributes and methods (such as .parameters() and .zero_grad())\nwhich we will be using.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "nn.Module (uppercase M) is a PyTorch specific concept, and is a\nclass we\u2019ll be using a lot. nn.Module is not to be confused with the Python\nconcept of a (lowercase m) ,\nwhich is a file of Python code that can be imported.",
            "markdown"
        ],
        [
            "from torch import nn\n\nclass Mnist_Logistic():\n    def __init__(self):\n        super().__init__()\n        self. = ((784, 10) / math.sqrt(784))\n        self. = ((10))\n\n    def forward(self, ):\n        return  @ self. + self.",
            "code"
        ],
        [
            "Since we\u2019re now using an object instead of just using a function, we\nfirst have to instantiate our model:",
            "markdown"
        ],
        [
            " = ()",
            "code"
        ],
        [
            "Now we can calculate the loss in the same way as before. Note that\nnn.Module objects are used as if they are functions (i.e they are\n<em>callable</em>), but behind the scenes Pytorch will call our forward\nmethod automatically.",
            "markdown"
        ],
        [
            "print(loss_func((), ))",
            "code"
        ],
        [
            "tensor(2.4012, grad_fn=&lt;NllLossBackward0&gt;)",
            "code"
        ],
        [
            "Previously for our training loop we had to update the values for each parameter\nby name, and manually zero out the grads for each parameter separately, like this:",
            "markdown"
        ],
        [
            "with ():\n     -=  * lr\n     -=  * lr\n    .zero_()\n    .zero_()",
            "code"
        ],
        [
            "Now we can take advantage of model.parameters() and model.zero_grad() (which\nare both defined by PyTorch for nn.Module) to make those steps more concise\nand less prone to the error of forgetting some of our parameters, particularly\nif we had a more complicated model:",
            "markdown"
        ],
        [
            "with ():\n    for p in (): p -= p.grad * lr\n    ()",
            "code"
        ],
        [
            "We\u2019ll wrap our little training loop in a fit function so we can run it\nagain later.",
            "markdown"
        ],
        [
            "def fit():\n    for epoch in range(epochs):\n        for i in range((n - 1) // bs + 1):\n            start_i = i * bs\n            end_i = start_i + bs\n             = [start_i:end_i]\n             = [start_i:end_i]\n             = ()\n             = loss_func(, )\n\n            ()\n            with ():\n                for p in ():\n                    p -= p.grad * lr\n                ()\n\nfit()",
            "code"
        ],
        [
            "Let\u2019s double-check that our loss has gone down:",
            "markdown"
        ],
        [
            "print(loss_func((), ))",
            "code"
        ],
        [
            "tensor(0.0809, grad_fn=&lt;NllLossBackward0&gt;)",
            "code"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->Refactor using nn.Linear": [
        [
            "We continue to refactor our code.  Instead of manually defining and\ninitializing self.weights and self.bias, and calculating xb\u00a0 @\nself.weights + self.bias, we will instead use the Pytorch class\n for a\nlinear layer, which does all that for us. Pytorch has many types of\npredefined layers that can greatly simplify our code, and often makes it\nfaster too.",
            "markdown"
        ],
        [
            "class Mnist_Logistic():\n    def __init__(self):\n        super().__init__()\n        self.lin = (784, 10)\n\n    def forward(self, ):\n        return self.lin()",
            "code"
        ],
        [
            "We instantiate our model and calculate the loss in the same way as before:",
            "markdown"
        ],
        [
            " = ()\nprint(loss_func((), ))",
            "code"
        ],
        [
            "tensor(2.2902, grad_fn=&lt;NllLossBackward0&gt;)",
            "code"
        ],
        [
            "We are still able to use our same fit method as before.",
            "markdown"
        ],
        [
            "fit()\n\nprint(loss_func((), ))",
            "code"
        ],
        [
            "tensor(0.0802, grad_fn=&lt;NllLossBackward0&gt;)",
            "code"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->Refactor using optim": [
        [
            "Pytorch also has a package with various optimization algorithms, torch.optim.\nWe can use the step method from our optimizer to take a forward step, instead\nof manually updating each parameter.",
            "markdown"
        ],
        [
            "This will let us replace our previous manually coded optimization step:",
            "markdown"
        ],
        [
            "with ():\n    for p in (): p -= p.grad * lr\n    ()",
            "code"
        ],
        [
            "and instead use just:",
            "markdown"
        ],
        [
            "()\n()",
            "code"
        ],
        [
            "(optim.zero_grad() resets the gradient to 0 and we need to call it before\ncomputing the gradient for the next minibatch.)",
            "markdown"
        ],
        [
            "from torch import optim",
            "code"
        ],
        [
            "We\u2019ll define a little function to create our model and optimizer so we\ncan reuse it in the future.",
            "markdown"
        ],
        [
            "def get_model():\n     = ()\n    return , ((), lr=lr)\n\n,  = get_model()\nprint(loss_func((), ))\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        start_i = i * bs\n        end_i = start_i + bs\n         = [start_i:end_i]\n         = [start_i:end_i]\n         = ()\n         = loss_func(, )\n\n        ()\n        ()\n        ()\n\nprint(loss_func((), ))",
            "code"
        ],
        [
            "tensor(2.2155, grad_fn=&lt;NllLossBackward0&gt;)\ntensor(0.0802, grad_fn=&lt;NllLossBackward0&gt;)",
            "code"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->Refactor using Dataset": [
        [
            "PyTorch has an abstract Dataset class.  A Dataset can be anything that has\na __len__ function (called by Python\u2019s standard len function) and\na __getitem__ function as a way of indexing into it.\n\nwalks through a nice example of creating a custom FacialLandmarkDataset class\nas a subclass of Dataset.",
            "markdown"
        ],
        [
            "PyTorch\u2019s \nis a Dataset wrapping tensors. By defining a length and way of indexing,\nthis also gives us a way to iterate, index, and slice along the first\ndimension of a tensor. This will make it easier to access both the\nindependent and dependent variables in the same line as we train.",
            "markdown"
        ],
        [
            "from torch.utils.data import ",
            "code"
        ],
        [
            "Both x_train and y_train can be combined in a single TensorDataset,\nwhich will be easier to iterate over and slice.",
            "markdown"
        ],
        [
            " = (, )",
            "code"
        ],
        [
            "Previously, we had to iterate through minibatches of x and y values separately:",
            "markdown"
        ],
        [
            " = [start_i:end_i]\n = [start_i:end_i]",
            "code"
        ],
        [
            "Now, we can do these two steps together:",
            "markdown"
        ],
        [
            ", = [i*bs : i*bs+bs]",
            "code"
        ],
        [
            ",  = get_model()\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        ,  = [i * bs: i * bs + bs]\n         = ()\n         = loss_func(, )\n\n        ()\n        ()\n        ()\n\nprint(loss_func((), ))",
            "code"
        ],
        [
            "tensor(0.0818, grad_fn=&lt;NllLossBackward0&gt;)",
            "code"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->Refactor using DataLoader": [
        [
            "Pytorch\u2019s DataLoader is responsible for managing batches. You can\ncreate a DataLoader from any Dataset. DataLoader makes it easier\nto iterate over batches. Rather than having to use train_ds[i*bs : i*bs+bs],\nthe DataLoader gives us each minibatch automatically.",
            "markdown"
        ],
        [
            "from torch.utils.data import \n\n = (, )\ntrain_dl = (, batch_size=bs)",
            "code"
        ],
        [
            "Previously, our loop iterated over batches (xb, yb) like this:",
            "markdown"
        ],
        [
            "for i in range((n-1)//bs + 1):\n    , = [i*bs : i*bs+bs]\n     = ()",
            "code"
        ],
        [
            "Now, our loop is much cleaner, as (xb, yb) are loaded automatically from the data loader:",
            "markdown"
        ],
        [
            "for , in train_dl:\n     = ()",
            "code"
        ],
        [
            ",  = get_model()\n\nfor epoch in range(epochs):\n    for ,  in train_dl:\n         = ()\n         = loss_func(, )\n\n        ()\n        ()\n        ()\n\nprint(loss_func((), ))",
            "code"
        ],
        [
            "tensor(0.0808, grad_fn=&lt;NllLossBackward0&gt;)",
            "code"
        ],
        [
            "Thanks to Pytorch\u2019s nn.Module, nn.Parameter, Dataset, and DataLoader,\nour training loop is now dramatically smaller and easier to understand. Let\u2019s\nnow try to add the basic features necessary to create effective models in practice.",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->Add validation": [
        [
            "In section 1, we were just trying to get a reasonable training loop set up for\nuse on our training data.  In reality, you <strong>always</strong> should also have\na , in order\nto identify if you are overfitting.",
            "markdown"
        ],
        [
            "Shuffling the training data is\n\nto prevent correlation between batches and overfitting. On the other hand, the\nvalidation loss will be identical whether we shuffle the validation set or not.\nSince shuffling takes extra time, it makes no sense to shuffle the validation data.",
            "markdown"
        ],
        [
            "We\u2019ll use a batch size for the validation set that is twice as large as\nthat for the training set. This is because the validation set does not\nneed backpropagation and thus takes less memory (it doesn\u2019t need to\nstore the gradients). We take advantage of this to use a larger batch\nsize and compute the loss more quickly.",
            "markdown"
        ],
        [
            " = (, )\ntrain_dl = (, batch_size=bs, shuffle=True)\n\n = (, )\nvalid_dl = (, batch_size=bs * 2)",
            "code"
        ],
        [
            "We will calculate and print the validation loss at the end of each epoch.",
            "markdown"
        ],
        [
            "(Note that we always call model.train() before training, and model.eval()\nbefore inference, because these are used by layers such as nn.BatchNorm2d\nand nn.Dropout to ensure appropriate behaviour for these different phases.)",
            "markdown"
        ],
        [
            ",  = get_model()\n\nfor epoch in range(epochs):\n    ()\n    for ,  in train_dl:\n         = ()\n         = loss_func(, )\n\n        ()\n        ()\n        ()\n\n    ()\n    with ():\n         = sum(loss_func((), ) for ,  in valid_dl)\n\n    print(epoch,  / len(valid_dl))",
            "code"
        ],
        [
            "0 tensor(0.2961)\n1 tensor(0.2847)",
            "code"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->Create fit() and get_data()": [
        [
            "We\u2019ll now do a little refactoring of our own. Since we go through a similar\nprocess twice of calculating the loss for both the training set and the\nvalidation set, let\u2019s make that into its own function, loss_batch, which\ncomputes the loss for one batch.",
            "markdown"
        ],
        [
            "We pass an optimizer in for the training set, and use it to perform\nbackprop.  For the validation set, we don\u2019t pass an optimizer, so the\nmethod doesn\u2019t perform backprop.",
            "markdown"
        ],
        [
            "def loss_batch(, loss_func, , , =None):\n     = loss_func((), )\n\n    if  is not None:\n        ()\n        ()\n        ()\n\n    return .item(), len()",
            "code"
        ],
        [
            "fit runs the necessary operations to train our model and compute the\ntraining and validation losses for each epoch.",
            "markdown"
        ],
        [
            "import numpy as np\n\ndef fit(epochs, , loss_func, , train_dl, valid_dl):\n    for epoch in range(epochs):\n        ()\n        for ,  in train_dl:\n            loss_batch(, loss_func, , , )\n\n        ()\n        with ():\n            losses, nums = zip(\n                *[loss_batch(, loss_func, , ) for ,  in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n\n        print(epoch, val_loss)",
            "code"
        ],
        [
            "get_data returns dataloaders for the training and validation sets.",
            "markdown"
        ],
        [
            "def get_data(, , bs):\n    return (\n        (, batch_size=bs, shuffle=True),\n        (, batch_size=bs * 2),\n    )",
            "code"
        ],
        [
            "Now, our whole process of obtaining the data loaders and fitting the\nmodel can be run in 3 lines of code:",
            "markdown"
        ],
        [
            "train_dl, valid_dl = get_data(, , bs)\n,  = get_model()\nfit(epochs, , loss_func, , train_dl, valid_dl)",
            "code"
        ],
        [
            "0 0.42506868764162065\n1 0.3510864285647869",
            "code"
        ],
        [
            "You can use these basic 3 lines of code to train a wide variety of models.\nLet\u2019s see if we can use them to train a convolutional neural network (CNN)!",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->Switch to CNN": [
        [
            "We are now going to build our neural network with three convolutional layers.\nBecause none of the functions in the previous section assume anything about\nthe model form, we\u2019ll be able to use them to train a CNN without any modification.",
            "markdown"
        ],
        [
            "We will use Pytorch\u2019s predefined\n class\nas our convolutional layer. We define a CNN with 3 convolutional layers.\nEach convolution is followed by a ReLU.  At the end, we perform an\naverage pooling.  (Note that view is PyTorch\u2019s version of numpy\u2019s\nreshape)",
            "markdown"
        ],
        [
            "class Mnist_CNN():\n    def __init__(self):\n        super().__init__()\n        self.conv1 = (1, 16, kernel_size=3, stride=2, padding=1)\n        self.conv2 = (16, 16, kernel_size=3, stride=2, padding=1)\n        self.conv3 = (16, 10, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, ):\n         = .view(-1, 1, 28, 28)\n         = (self.conv1())\n         = (self.conv2())\n         = (self.conv3())\n         = (, 4)\n        return .view(-1, .size(1))\n\nlr = 0.1",
            "code"
        ],
        [
            " is a variation on\nstochastic gradient descent that takes previous updates into account as well\nand generally leads to faster training.",
            "markdown"
        ],
        [
            " = ()\n = ((), lr=lr, momentum=0.9)\n\nfit(epochs, , loss_func, , train_dl, valid_dl)",
            "code"
        ],
        [
            "0 0.34581177258491513\n1 0.291131191277504",
            "code"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->nn.Sequential": [
        [
            "torch.nn has another handy class we can use to simplify our code:\n .\nA Sequential object runs each of the modules contained within it, in a\nsequential manner. This is a simpler way of writing our neural network.",
            "markdown"
        ],
        [
            "To take advantage of this, we need to be able to easily define a\n<strong>custom layer</strong> from a given function.  For instance, PyTorch doesn\u2019t\nhave a <cite>view</cite> layer, and we need to create one for our network. Lambda\nwill create a layer that we can then use when defining a network with\nSequential.",
            "markdown"
        ],
        [
            "class Lambda():\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x):\n        return self.func(x)\n\n\ndef preprocess(x):\n    return x.view(-1, 1, 28, 28)",
            "code"
        ],
        [
            "The model created with Sequential is simply:",
            "markdown"
        ],
        [
            " = (\n    (preprocess),\n    (1, 16, kernel_size=3, stride=2, padding=1),\n    (),\n    (16, 16, kernel_size=3, stride=2, padding=1),\n    (),\n    (16, 10, kernel_size=3, stride=2, padding=1),\n    (),\n    (4),\n    (lambda x: x.view(x.size(0), -1)),\n)\n\n = ((), lr=lr, momentum=0.9)\n\nfit(epochs, , loss_func, , train_dl, valid_dl)",
            "code"
        ],
        [
            "0 0.40341693930625916\n1 0.2864704542160034",
            "code"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->Wrapping DataLoader": [
        [
            "It assumes the input is a 28*28 long vector",
            "markdown"
        ],
        [
            "It assumes that the final CNN grid size is 4*4 (since that\u2019s the average pooling kernel size we used)\n\n</dd>\n</dl>",
            "markdown"
        ],
        [
            "Let\u2019s get rid of these two assumptions, so our model works with any 2d\nsingle channel image. First, we can remove the initial Lambda layer by\nmoving the data preprocessing into a generator:",
            "markdown"
        ],
        [
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "code"
        ],
        [
            "Next, we can replace nn.AvgPool2d with nn.AdaptiveAvgPool2d, which\nallows us to define the size of the <em>output</em> tensor we want, rather than\nthe <em>input</em> tensor we have. As a result, our model will work with any\nsize input.",
            "markdown"
        ],
        [
            " = (\n    (1, 16, kernel_size=3, stride=2, padding=1),\n    (),\n    (16, 16, kernel_size=3, stride=2, padding=1),\n    (),\n    (16, 10, kernel_size=3, stride=2, padding=1),\n    (),\n    (1),\n    (lambda x: x.view(x.size(0), -1)),\n)\n\n = ((), lr=lr, momentum=0.9)",
            "code"
        ],
        [
            "Let\u2019s try it out:",
            "markdown"
        ],
        [
            "fit(epochs, , loss_func, , train_dl, valid_dl)",
            "code"
        ],
        [
            "0 0.4155500603556633\n1 0.25355035238265994",
            "code"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->Using your GPU": [
        [
            "If you\u2019re lucky enough to have access to a CUDA-capable GPU (you can\nrent one for about $0.50/hour from most cloud providers) you can\nuse it to speed up your code. First check that your GPU is working in\nPytorch:",
            "markdown"
        ],
        [
            "print(())",
            "code"
        ],
        [
            "True",
            "code"
        ],
        [
            "And then create a device object for it:",
            "markdown"
        ],
        [
            " = (\n    \"cuda\") if () else (\"cpu\")",
            "code"
        ],
        [
            "Let\u2019s update preprocess to move batches to the GPU:",
            "markdown"
        ],
        [
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(), y.to()\n\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "code"
        ],
        [
            "Finally, we can move our model to the GPU.",
            "markdown"
        ],
        [
            "()\n = ((), lr=lr, momentum=0.9)",
            "code"
        ],
        [
            "You should find it runs faster now:",
            "markdown"
        ],
        [
            "fit(epochs, , loss_func, , train_dl, valid_dl)",
            "code"
        ],
        [
            "0 0.20518756081461906\n1 0.1734702249288559",
            "code"
        ]
    ],
    "torch->Learning PyTorch->What is torch.nn really?->Closing thoughts": [
        [
            "We now have a general data pipeline and training loop which you can use for\ntraining many types of models using Pytorch. To see how simple training a model\ncan now be, take a look at the .",
            "markdown"
        ],
        [
            "Of course, there are many things you\u2019ll want to add, such as data augmentation,\nhyperparameter tuning, monitoring training, transfer learning, and so forth.\nThese features are available in the fastai library, which has been developed\nusing the same design approach shown in this tutorial, providing a natural\nnext step for practitioners looking to take their models further.",
            "markdown"
        ],
        [
            "We promised at the start of this tutorial we\u2019d explain through example each of\ntorch.nn, torch.optim, Dataset, and DataLoader. So let\u2019s summarize\nwhat we\u2019ve seen:\n<blockquote>",
            "markdown"
        ],
        [
            "<strong>torch.nn</strong>",
            "markdown"
        ],
        [
            "Module: creates a callable which behaves like a function, but can also\ncontain state(such as neural net layer weights). It knows what Parameter (s) it\ncontains and can zero all their gradients, loop through them for weight updates, etc.",
            "markdown"
        ],
        [
            "Parameter: a wrapper for a tensor that tells a Module that it has weights\nthat need updating during backprop. Only tensors with the <cite>requires_grad</cite> attribute set are updated",
            "markdown"
        ],
        [
            "functional: a module(usually imported into the F namespace by convention)\nwhich contains activation functions, loss functions, etc, as well as non-stateful\nversions of layers such as convolutional and linear layers.",
            "markdown"
        ],
        [
            "torch.optim: Contains optimizers such as SGD, which update the weights\nof Parameter during the backward step",
            "markdown"
        ],
        [
            "Dataset: An abstract interface of objects with a __len__ and a __getitem__,\nincluding classes provided with Pytorch such as TensorDataset",
            "markdown"
        ],
        [
            "DataLoader: Takes any Dataset and creates an iterator which returns batches of data.\n\n</blockquote>",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  39.874 seconds)",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard": [
        [
            "In the ,\nwe show you how to load in data,\nfeed it through a model we define as a subclass of nn.Module,\ntrain this model on training data, and test it on test data.\nTo see what\u2019s happening, we print out some statistics as the model\nis training to get a sense for whether training is progressing.\nHowever, we can do much better than that: PyTorch integrates with\nTensorBoard, a tool designed for visualizing the results of neural\nnetwork training runs. This tutorial illustrates some of its\nfunctionality, using the\n\nwhich can be read into PyTorch using <cite>torchvision.datasets</cite>.",
            "markdown"
        ],
        [
            "In this tutorial, we\u2019ll learn how to:\n<blockquote>",
            "markdown"
        ],
        [
            "Read in data and with appropriate transforms (nearly identical to the prior tutorial).",
            "markdown"
        ],
        [
            "Set up TensorBoard.",
            "markdown"
        ],
        [
            "Write to TensorBoard.",
            "markdown"
        ],
        [
            "Inspect a model architecture using TensorBoard.",
            "markdown"
        ],
        [
            "Use TensorBoard to create interactive versions of the visualizations we created in last tutorial, with less code\n\n</blockquote>",
            "markdown"
        ],
        [
            "Specifically, on point #5, we\u2019ll see:\n<blockquote>",
            "markdown"
        ],
        [
            "A couple of ways to inspect our training data",
            "markdown"
        ],
        [
            "How to track our model\u2019s performance as it trains",
            "markdown"
        ],
        [
            "How to assess our model\u2019s performance once it is trained.\n\n</blockquote>",
            "markdown"
        ],
        [
            "We\u2019ll begin with similar boilerplate code as in the :",
            "markdown"
        ],
        [
            "# imports\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# transforms\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))])\n\n# datasets\ntrainset = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=True,\n    transform=transform)\ntestset = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=False,\n    transform=transform)\n\n# dataloaders\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                        shuffle=True, num_workers=2)\n\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                        shuffle=False, num_workers=2)\n\n# constant for classes\nclasses = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n\n# helper function to show an image\n# (used in the `plot_classes_preds` function below)\ndef matplotlib_imshow(img, one_channel=False):\n    if one_channel:\n        img = img.mean(dim=0)\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    if one_channel:\n        plt.imshow(npimg, cmap=\"Greys\")\n    else:\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))",
            "code"
        ],
        [
            "We\u2019ll define a similar model architecture from that tutorial, making only\nminor modifications to account for the fact that the images are now\none channel instead of three and 28x28 instead of 32x32:",
            "markdown"
        ],
        [
            "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()",
            "code"
        ],
        [
            "We\u2019ll define the same optimizer and criterion from before:",
            "markdown"
        ],
        [
            "criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)",
            "code"
        ]
    ],
    "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->1. TensorBoard setup": [
        [
            "Now we\u2019ll set up TensorBoard, importing tensorboard from torch.utils and defining a\nSummaryWriter, our key object for writing information to TensorBoard.",
            "markdown"
        ],
        [
            "from torch.utils.tensorboard import SummaryWriter\n\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter('runs/fashion_mnist_experiment_1')",
            "code"
        ],
        [
            "Note that this line alone creates a runs/fashion_mnist_experiment_1\nfolder.",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->2. Writing to TensorBoard": [
        [
            "Now let\u2019s write an image to our TensorBoard - specifically, a grid -\nusing .",
            "markdown"
        ],
        [
            "# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# create grid of images\nimg_grid = torchvision.utils.make_grid(images)\n\n# show images\nmatplotlib_imshow(img_grid, one_channel=True)\n\n# write to tensorboard\nwriter.add_image('four_fashion_mnist_images', img_grid)",
            "code"
        ],
        [
            "Now running",
            "markdown"
        ],
        [
            "tensorboard --logdir=runs",
            "code"
        ],
        [
            "from the command line and then navigating to \nshould show the following.\n<img alt=\"../_static/img/tensorboard_first_view.png\" src=\"../_static/img/tensorboard_first_view.png\"/>",
            "markdown"
        ],
        [
            "Now you know how to use TensorBoard! This example, however, could be\ndone in a Jupyter Notebook - where TensorBoard really excels is in\ncreating interactive visualizations. We\u2019ll cover one of those next,\nand several more by the end of the tutorial.",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->3. Inspect the model using TensorBoard": [
        [
            "One of TensorBoard\u2019s strengths is its ability to visualize complex model\nstructures. Let\u2019s visualize the model we built.",
            "markdown"
        ],
        [
            "writer.add_graph(net, images)\nwriter.close()",
            "code"
        ],
        [
            "Now upon refreshing TensorBoard you should see a \u201cGraphs\u201d tab that\nlooks like this:\n<img alt=\"../_static/img/tensorboard_model_viz.png\" src=\"../_static/img/tensorboard_model_viz.png\"/>",
            "markdown"
        ],
        [
            "Go ahead and double click on \u201cNet\u201d to see it expand, seeing a\ndetailed view of the individual operations that make up the model.",
            "markdown"
        ],
        [
            "TensorBoard has a very handy feature for visualizing high dimensional\ndata such as image data in a lower dimensional space; we\u2019ll cover this\nnext.",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->4. Adding a \u201cProjector\u201d to TensorBoard": [
        [
            "We can visualize the lower dimensional representation of higher\ndimensional data via the  method",
            "markdown"
        ],
        [
            "# helper function\ndef select_n_random(data, labels, n=100):\n    '''\n    Selects n random datapoints and their corresponding labels from a dataset\n    '''\n    assert len(data) == len(labels)\n\n    perm = torch.randperm(len(data))\n    return data[perm][:n], labels[perm][:n]\n\n# select random images and their target indices\nimages, labels = select_n_random(trainset.data, trainset.targets)\n\n# get the class labels for each image\nclass_labels = [classes[lab] for lab in labels]\n\n# log embeddings\nfeatures = images.view(-1, 28 * 28)\nwriter.add_embedding(features,\n                    metadata=class_labels,\n                    label_img=images.unsqueeze(1))\nwriter.close()",
            "code"
        ],
        [
            "Now in the \u201cProjector\u201d tab of TensorBoard, you can see these 100\nimages - each of which is 784 dimensional - projected down into three\ndimensional space. Furthermore, this is interactive: you can click\nand drag to rotate the three dimensional projection. Finally, a couple\nof tips to make the visualization easier to see: select \u201ccolor: label\u201d\non the top left, as well as enabling \u201cnight mode\u201d, which will make the\nimages easier to see since their background is white:\n<img alt=\"../_static/img/tensorboard_projector.png\" src=\"../_static/img/tensorboard_projector.png\"/>",
            "markdown"
        ],
        [
            "Now we\u2019ve thoroughly inspected our data, let\u2019s show how TensorBoard\ncan make tracking model training and evaluation clearer, starting with\ntraining.",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->5. Tracking model training with TensorBoard": [
        [
            "In the previous example, we simply <em>printed</em> the model\u2019s running loss\nevery 2000 iterations. Now, we\u2019ll instead log the running loss to\nTensorBoard, along with a view into the predictions the model is\nmaking via the plot_classes_preds function.",
            "markdown"
        ],
        [
            "# helper functions\n\ndef images_to_probs(net, images):\n    '''\n    Generates predictions and corresponding probabilities from a trained\n    network and a list of images\n    '''\n    output = net(images)\n    # convert output probabilities to predicted class\n    _, preds_tensor = torch.max(output, 1)\n    preds = np.squeeze(preds_tensor.numpy())\n    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n\n\ndef plot_classes_preds(net, images, labels):\n    '''\n    Generates matplotlib Figure using a trained network, along with images\n    and labels from a batch, that shows the network's top prediction along\n    with its probability, alongside the actual label, coloring this\n    information based on whether the prediction was correct or not.\n    Uses the \"images_to_probs\" function.\n    '''\n    preds, probs = images_to_probs(net, images)\n    # plot the images in the batch, along with predicted and true labels\n    fig = plt.figure(figsize=(12, 48))\n    for idx in np.arange(4):\n        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n        matplotlib_imshow(images[idx], one_channel=True)\n        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n            classes[preds[idx]],\n            probs[idx] * 100.0,\n            classes[labels[idx]]),\n                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n    return fig",
            "code"
        ],
        [
            "Finally, let\u2019s train the model using the same model training code from\nthe prior tutorial, but writing results to TensorBoard every 1000\nbatches instead of printing to console; this is done using the\n\nfunction.",
            "markdown"
        ],
        [
            "In addition, as we train, we\u2019ll generate an image showing the model\u2019s\npredictions vs. the actual results on the four images included in that\nbatch.",
            "markdown"
        ],
        [
            "running_loss = 0.0\nfor epoch in range(1):  # loop over the dataset multiple times\n\n    for i, data in enumerate(trainloader, 0):\n\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 1000 == 999:    # every 1000 mini-batches...\n\n            # ...log the running loss\n            writer.add_scalar('training loss',\n                            running_loss / 1000,\n                            epoch * len(trainloader) + i)\n\n            # ...log a Matplotlib Figure showing the model's predictions on a\n            # random mini-batch\n            writer.add_figure('predictions vs. actuals',\n                            plot_classes_preds(net, inputs, labels),\n                            global_step=epoch * len(trainloader) + i)\n            running_loss = 0.0\nprint('Finished Training')",
            "code"
        ],
        [
            "You can now look at the scalars tab to see the running loss plotted\nover the 15,000 iterations of training:\n<img alt=\"../_static/img/tensorboard_scalar_runs.png\" src=\"../_static/img/tensorboard_scalar_runs.png\"/>",
            "markdown"
        ],
        [
            "In addition, we can look at the predictions the model made on\narbitrary batches throughout learning. See the \u201cImages\u201d tab and scroll\ndown under the \u201cpredictions vs. actuals\u201d visualization to see this;\nthis shows us that, for example, after just 3000 training iterations,\nthe model was already able to distinguish between visually distinct\nclasses such as shirts, sneakers, and coats, though it isn\u2019t as\nconfident as it becomes later on in training:\n<img alt=\"../_static/img/tensorboard_images.png\" src=\"../_static/img/tensorboard_images.png\"/>",
            "markdown"
        ],
        [
            "In the prior tutorial, we looked at per-class accuracy once the model\nhad been trained; here, we\u2019ll use TensorBoard to plot precision-recall\ncurves (good explanation\n)\nfor each class.",
            "markdown"
        ]
    ],
    "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard": [
        [
            "# 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
            "code"
        ],
        [
            "You will now see a \u201cPR Curves\u201d tab that contains the precision-recall\ncurves for each class. Go ahead and poke around; you\u2019ll see that on\nsome classes the model has nearly 100% \u201carea under the curve\u201d,\nwhereas on others this area is lower:\n<img alt=\"../_static/img/tensorboard_pr_curves.png\" src=\"../_static/img/tensorboard_pr_curves.png\"/>",
            "markdown"
        ],
        [
            "And that\u2019s an intro to TensorBoard and PyTorch\u2019s integration with it.\nOf course, you could do everything TensorBoard does in your Jupyter\nNotebook, but with TensorBoard, you gets visuals that are interactive\nby default.",
            "markdown"
        ]
    ],
    "torch->Image and Video->TorchVision Object Detection Finetuning Tutorial": [
        [
            "Tip",
            "markdown"
        ],
        [
            "To get the most of this tutorial, we suggest using this\n.\nThis will allow you to experiment with the information presented below.",
            "markdown"
        ],
        [
            "For this tutorial, we will be finetuning a pre-trained  model in the . It contains\n170 images with 345 instances of pedestrians, and we will use it to\nillustrate how to use the new features in torchvision in order to train\nan instance segmentation model on a custom dataset.",
            "markdown"
        ]
    ],
    "torch->Image and Video->TorchVision Object Detection Finetuning Tutorial->Defining the Dataset": [
        [
            "The reference scripts for training object detection, instance\nsegmentation and person keypoint detection allows for easily supporting\nadding new custom datasets. The dataset should inherit from the standard\ntorch.utils.data.Dataset class, and implement __len__ and\n__getitem__.",
            "markdown"
        ],
        [
            "The only specificity that we require is that the dataset __getitem__\nshould return:",
            "markdown"
        ],
        [
            "image: a PIL Image of size (H, W)",
            "markdown"
        ],
        [
            "target: a dict containing the following fields",
            "markdown"
        ],
        [
            "boxes (FloatTensor[N, 4]): the coordinates of the N\nbounding boxes in [x0, y0, x1, y1] format, ranging from 0\nto W and 0 to H",
            "markdown"
        ],
        [
            "labels (Int64Tensor[N]): the label for each bounding box. 0 represents always the background class.",
            "markdown"
        ],
        [
            "image_id (Int64Tensor[1]): an image identifier. It should be\nunique between all the images in the dataset, and is used during\nevaluation",
            "markdown"
        ],
        [
            "area (Tensor[N]): The area of the bounding box. This is used\nduring evaluation with the COCO metric, to separate the metric\nscores between small, medium and large boxes.",
            "markdown"
        ],
        [
            "iscrowd (UInt8Tensor[N]): instances with iscrowd=True will be\nignored during evaluation.",
            "markdown"
        ],
        [
            "(optionally) masks (UInt8Tensor[N, H, W]): The segmentation\nmasks for each one of the objects",
            "markdown"
        ],
        [
            "(optionally) keypoints (FloatTensor[N, K, 3]): For each one of\nthe N objects, it contains the K keypoints in\n[x, y, visibility] format, defining the object. visibility=0\nmeans that the keypoint is not visible. Note that for data\naugmentation, the notion of flipping a keypoint is dependent on\nthe data representation, and you should probably adapt\nreferences/detection/transforms.py for your new keypoint\nrepresentation",
            "markdown"
        ],
        [
            "If your model returns the above methods, they will make it work for both\ntraining and evaluation, and will use the evaluation scripts from\npycocotools which can be installed with pip install pycocotools.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "For Windows, please install pycocotools from  with command",
            "markdown"
        ],
        [
            "pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI",
            "markdown"
        ],
        [
            "One note on the labels. The model considers class 0 as background. If your dataset does not contain the background class, you should not have 0 in your labels. For example, assuming you have just two classes, <em>cat</em> and <em>dog</em>, you can define 1 (not 0) to represent <em>cats</em> and 2 to represent <em>dogs</em>. So, for instance, if one of the images has both classes, your labels tensor should look like [1,2].",
            "markdown"
        ],
        [
            "Additionally, if you want to use aspect ratio grouping during training\n(so that each batch only contains images with similar aspect ratios),\nthen it is recommended to also implement a get_height_and_width\nmethod, which returns the height and the width of the image. If this\nmethod is not provided, we query all elements of the dataset via\n__getitem__ , which loads the image in memory and is slower than if\na custom method is provided.",
            "markdown"
        ]
    ],
    "torch->Image and Video->TorchVision Object Detection Finetuning Tutorial->Defining the Dataset->Writing a custom dataset for PennFudan": [
        [
            "Let\u2019s write a dataset for the PennFudan dataset. After , we\nhave the following folder structure:",
            "markdown"
        ],
        [
            "PennFudanPed/\n  PedMasks/\n    FudanPed00001_mask.png\n    FudanPed00002_mask.png\n    FudanPed00003_mask.png\n    FudanPed00004_mask.png\n    ...\n  PNGImages/\n    FudanPed00001.png\n    FudanPed00002.png\n    FudanPed00003.png\n    FudanPed00004.png",
            "code"
        ],
        [
            "Here is one example of a pair of images and segmentation masks\n<img alt=\"../_static/img/tv_tutorial/tv_image01.png\" src=\"../_static/img/tv_tutorial/tv_image01.png\"/>\n<img alt=\"../_static/img/tv_tutorial/tv_image02.png\" src=\"../_static/img/tv_tutorial/tv_image02.png\"/>",
            "markdown"
        ],
        [
            "So each image has a corresponding\nsegmentation mask, where each color correspond to a different instance.\nLet\u2019s write a torch.utils.data.Dataset class for this dataset.",
            "markdown"
        ],
        [
            "import os\nimport numpy as np\nimport torch\nfrom PIL import Image\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        # because each color corresponds to a different instance\n        # with 0 being background\n        mask = Image.open(mask_path)\n        # convert the PIL Image into a numpy array\n        mask = np.array(mask)\n        # instances are encoded as different colors\n        obj_ids = np.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        # convert everything into a torch.Tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)",
            "code"
        ],
        [
            "That\u2019s all for the dataset. Now let\u2019s define a model that can perform\npredictions on this dataset.",
            "markdown"
        ]
    ],
    "torch->Image and Video->TorchVision Object Detection Finetuning Tutorial->Defining your model": [
        [
            "In this tutorial, we will be using , which is based on top of\n. Faster R-CNN is a\nmodel that predicts both bounding boxes and class scores for potential\nobjects in the image.\n<img alt=\"../_static/img/tv_tutorial/tv_image03.png\" src=\"../_static/img/tv_tutorial/tv_image03.png\"/>",
            "markdown"
        ],
        [
            "Mask R-CNN adds an extra branch\ninto Faster R-CNN, which also predicts segmentation masks for each\ninstance.\n<img alt=\"../_static/img/tv_tutorial/tv_image04.png\" src=\"../_static/img/tv_tutorial/tv_image04.png\"/>",
            "markdown"
        ],
        [
            "There are two common\nsituations where one might want\nto modify one of the available models in torchvision modelzoo. The first\nis when we want to start from a pre-trained model, and just finetune the\nlast layer. The other is when we want to replace the backbone of the\nmodel with a different one (for faster predictions, for example).",
            "markdown"
        ],
        [
            "Let\u2019s go see how we would do one or another in the following sections.",
            "markdown"
        ]
    ],
    "torch->Image and Video->TorchVision Object Detection Finetuning Tutorial->Defining your model->1 - Finetuning from a pretrained model": [
        [
            "Let\u2019s suppose that you want to start from a model pre-trained on COCO\nand want to finetune it for your particular classes. Here is a possible\nway of doing it:",
            "markdown"
        ],
        [
            "import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# load a model pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)",
            "code"
        ]
    ],
    "torch->Image and Video->TorchVision Object Detection Finetuning Tutorial->Defining your model->2 - Modifying the model to add a different backbone": [
        [
            "import torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\n# load a pre-trained model for classification and return\n# only the features\nbackbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n# FasterRCNN needs to know the number of\n# output channels in a backbone. For mobilenet_v2, it's 1280\n# so we need to add it here\nbackbone.out_channels = 1280\n\n# let's make the RPN generate 5 x 3 anchors per spatial\n# location, with 5 different sizes and 3 different aspect\n# ratios. We have a Tuple[Tuple[int]] because each feature\n# map could potentially have different sizes and\n# aspect ratios\nanchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))\n\n# let's define what are the feature maps that we will\n# use to perform the region of interest cropping, as well as\n# the size of the crop after rescaling.\n# if your backbone returns a Tensor, featmap_names is expected to\n# be [0]. More generally, the backbone should return an\n# OrderedDict[Tensor], and in featmap_names you can choose which\n# feature maps to use.\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n                                                output_size=7,\n                                                sampling_ratio=2)\n\n# put the pieces together inside a FasterRCNN model\nmodel = FasterRCNN(backbone,\n                   num_classes=2,\n                   rpn_anchor_generator=anchor_generator,\n                   box_roi_pool=roi_pooler)",
            "code"
        ]
    ],
    "torch->Image and Video->TorchVision Object Detection Finetuning Tutorial->Defining your model->An Instance segmentation model for PennFudan Dataset": [
        [
            "In our case, we want to fine-tune from a pre-trained model, given that\nour dataset is very small, so we will be following approach number 1.",
            "markdown"
        ],
        [
            "Here we want to also compute the instance segmentation masks, so we will\nbe using Mask R-CNN:",
            "markdown"
        ],
        [
            "import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n\ndef get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,\n                                                       num_classes)\n\n    return model",
            "code"
        ],
        [
            "That\u2019s it, this will make model be ready to be trained and evaluated\non your custom dataset.",
            "markdown"
        ]
    ],
    "torch->Image and Video->TorchVision Object Detection Finetuning Tutorial->Putting everything together": [
        [
            "In references/detection/, we have a number of helper functions to\nsimplify training and evaluating detection models. Here, we will use\nreferences/detection/engine.py, references/detection/utils.py\nand references/detection/transforms.py. Just copy everything under\nreferences/detection to your folder and use them here.",
            "markdown"
        ],
        [
            "Let\u2019s write some helper functions for data augmentation /\ntransformation:",
            "markdown"
        ],
        [
            "import transforms as T\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.PILToTensor())\n    transforms.append(T.ConvertImageDtype(torch.float))\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)",
            "code"
        ]
    ],
    "torch->Image and Video->TorchVision Object Detection Finetuning Tutorial->Testing forward() method (Optional)": [
        [
            "Before iterating over the dataset, it\u2019s good to see what the model\nexpects during training and inference time on sample data.",
            "markdown"
        ],
        [
            "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\ndataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\ndata_loader = torch.utils.data.DataLoader(\n dataset, batch_size=2, shuffle=True, num_workers=4,\n collate_fn=utils.collate_fn)\n# For Training\nimages,targets = next(iter(data_loader))\nimages = list(image for image in images)\ntargets = [{k: v for k, v in t.items()} for t in targets]\noutput = model(images,targets)   # Returns losses and detections\n# For inference\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)           # Returns predictions",
            "code"
        ],
        [
            "Let\u2019s now write the main function which performs the training and the\nvalidation:",
            "markdown"
        ],
        [
            "from engine import train_one_epoch, evaluate\nimport utils\n\n\ndef main():\n    # train on the GPU or on the CPU, if a GPU is not available\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    # our dataset has two classes only - background and person\n    num_classes = 2\n    # use our dataset and defined transformations\n    dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n    dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n\n    # split the dataset in train and test set\n    indices = torch.randperm(len(dataset)).tolist()\n    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n    # define training and validation data loaders\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=2, shuffle=True, num_workers=4,\n        collate_fn=utils.collate_fn)\n\n    data_loader_test = torch.utils.data.DataLoader(\n        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n        collate_fn=utils.collate_fn)\n\n    # get the model using our helper function\n    model = get_model_instance_segmentation(num_classes)\n\n    # move model to the right device\n    model.to(device)\n\n    # construct an optimizer\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.005,\n                                momentum=0.9, weight_decay=0.0005)\n    # and a learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                                   step_size=3,\n                                                   gamma=0.1)\n\n    # let's train it for 10 epochs\n    num_epochs = 10\n\n    for epoch in range(num_epochs):\n        # train for one epoch, printing every 10 iterations\n        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n        # update the learning rate\n        lr_scheduler.step()\n        # evaluate on the test dataset\n        evaluate(model, data_loader_test, device=device)\n\n    print(\"That's it!\")",
            "code"
        ],
        [
            "You should get as output for the first epoch:",
            "markdown"
        ],
        [
            "Epoch: [0]  [ 0/60]  eta: 0:01:18  lr: 0.000090  loss: 2.5213 (2.5213)  loss_classifier: 0.8025 (0.8025)  loss_box_reg: 0.2634 (0.2634)  loss_mask: 1.4265 (1.4265)  loss_objectness: 0.0190 (0.0190)  loss_rpn_box_reg: 0.0099 (0.0099)  time: 1.3121  data: 0.3024  max mem: 3485\nEpoch: [0]  [10/60]  eta: 0:00:20  lr: 0.000936  loss: 1.3007 (1.5313)  loss_classifier: 0.3979 (0.4719)  loss_box_reg: 0.2454 (0.2272)  loss_mask: 0.6089 (0.7953)  loss_objectness: 0.0197 (0.0228)  loss_rpn_box_reg: 0.0121 (0.0141)  time: 0.4198  data: 0.0298  max mem: 5081\nEpoch: [0]  [20/60]  eta: 0:00:15  lr: 0.001783  loss: 0.7567 (1.1056)  loss_classifier: 0.2221 (0.3319)  loss_box_reg: 0.2002 (0.2106)  loss_mask: 0.2904 (0.5332)  loss_objectness: 0.0146 (0.0176)  loss_rpn_box_reg: 0.0094 (0.0123)  time: 0.3293  data: 0.0035  max mem: 5081\nEpoch: [0]  [30/60]  eta: 0:00:11  lr: 0.002629  loss: 0.4705 (0.8935)  loss_classifier: 0.0991 (0.2517)  loss_box_reg: 0.1578 (0.1957)  loss_mask: 0.1970 (0.4204)  loss_objectness: 0.0061 (0.0140)  loss_rpn_box_reg: 0.0075 (0.0118)  time: 0.3403  data: 0.0044  max mem: 5081\nEpoch: [0]  [40/60]  eta: 0:00:07  lr: 0.003476  loss: 0.3901 (0.7568)  loss_classifier: 0.0648 (0.2022)  loss_box_reg: 0.1207 (0.1736)  loss_mask: 0.1705 (0.3585)  loss_objectness: 0.0018 (0.0113)  loss_rpn_box_reg: 0.0075 (0.0112)  time: 0.3407  data: 0.0044  max mem: 5081\nEpoch: [0]  [50/60]  eta: 0:00:03  lr: 0.004323  loss: 0.3237 (0.6703)  loss_classifier: 0.0474 (0.1731)  loss_box_reg: 0.1109 (0.1561)  loss_mask: 0.1658 (0.3201)  loss_objectness: 0.0015 (0.0093)  loss_rpn_box_reg: 0.0093 (0.0116)  time: 0.3379  data: 0.0043  max mem: 5081\nEpoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2540 (0.6082)  loss_classifier: 0.0309 (0.1526)  loss_box_reg: 0.0463 (0.1405)  loss_mask: 0.1568 (0.2945)  loss_objectness: 0.0012 (0.0083)  loss_rpn_box_reg: 0.0093 (0.0123)  time: 0.3489  data: 0.0042  max mem: 5081\nEpoch: [0] Total time: 0:00:21 (0.3570 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:19  model_time: 0.2152 (0.2152)  evaluator_time: 0.0133 (0.0133)  time: 0.4000  data: 0.1701  max mem: 5081\nTest:  [49/50]  eta: 0:00:00  model_time: 0.0628 (0.0687)  evaluator_time: 0.0039 (0.0064)  time: 0.0735  data: 0.0022  max mem: 5081\nTest: Total time: 0:00:04 (0.0828 s / it)\nAveraged stats: model_time: 0.0628 (0.0687)  evaluator_time: 0.0039 (0.0064)\nAccumulating evaluation results...\nDONE (t=0.01s).\nAccumulating evaluation results...\nDONE (t=0.01s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.606\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.984\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.780\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.313\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.582\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.612\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.270\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.672\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.672\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.755\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.664\nIoU metric: segm\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.704\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.871\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.325\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.488\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.727\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.316\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.748\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.749\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.673\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.758",
            "code"
        ],
        [
            "So after one epoch of training, we obtain a COCO-style mAP of 60.6, and\na mask mAP of 70.4.",
            "markdown"
        ],
        [
            "After training for 10 epochs, I got the following metrics",
            "markdown"
        ],
        [
            "IoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.799\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.969\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.935\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.349\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.592\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.324\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.844\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.844\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.777\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.870\nIoU metric: segm\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.761\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.969\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.919\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.341\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.464\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.788\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.303\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.799\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.799\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.769\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.818",
            "code"
        ],
        [
            "But what do the predictions look like? Let\u2019s take one image in the\ndataset and verify\n<img alt=\"../_static/img/tv_tutorial/tv_image05.png\" src=\"../_static/img/tv_tutorial/tv_image05.png\"/>",
            "markdown"
        ],
        [
            "The trained model predicts 9\ninstances of person in this image, let\u2019s see a couple of them:\n<img alt=\"../_static/img/tv_tutorial/tv_image06.png\" src=\"../_static/img/tv_tutorial/tv_image06.png\"/>\n<img alt=\"../_static/img/tv_tutorial/tv_image07.png\" src=\"../_static/img/tv_tutorial/tv_image07.png\"/>",
            "markdown"
        ],
        [
            "The results look pretty good!",
            "markdown"
        ]
    ],
    "torch->Image and Video->TorchVision Object Detection Finetuning Tutorial->Wrapping up": [
        [
            "In this tutorial, you have learned how to create your own training\npipeline for instance segmentation models, on a custom dataset. For\nthat, you wrote a torch.utils.data.Dataset class that returns the\nimages and the ground truth boxes and segmentation masks. You also\nleveraged a Mask R-CNN model pre-trained on COCO train2017 in order to\nperform transfer learning on this new dataset.",
            "markdown"
        ],
        [
            "For a more complete example, which includes multi-machine / multi-gpu\ntraining, check references/detection/train.py, which is present in\nthe torchvision repo.",
            "markdown"
        ],
        [
            "You can download a full source file for this tutorial\n.",
            "markdown"
        ]
    ],
    "torch->Image and Video->Transfer Learning for Computer Vision Tutorial": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "In this tutorial, you will learn how to train a convolutional neural network for\nimage classification using transfer learning. You can read more about the transfer\nlearning at ",
            "markdown"
        ],
        [
            "Quoting these notes,\n<blockquote>",
            "markdown"
        ],
        [
            "In practice, very few people train an entire Convolutional Network\nfrom scratch (with random initialization), because it is relatively\nrare to have a dataset of sufficient size. Instead, it is common to\npretrain a ConvNet on a very large dataset (e.g. ImageNet, which\ncontains 1.2 million images with 1000 categories), and then use the\nConvNet either as an initialization or a fixed feature extractor for\nthe task of interest.\n</blockquote>",
            "markdown"
        ],
        [
            "These two major transfer learning scenarios look as follows:",
            "markdown"
        ],
        [
            "<strong>Finetuning the convnet</strong>: Instead of random initialization, we\ninitialize the network with a pretrained network, like the one that is\ntrained on imagenet 1000 dataset. Rest of the training looks as\nusual.",
            "markdown"
        ],
        [
            "<strong>ConvNet as fixed feature extractor</strong>: Here, we will freeze the weights\nfor all of the network except that of the final fully connected\nlayer. This last fully connected layer is replaced with a new one\nwith random weights and only this layer is trained.",
            "markdown"
        ],
        [
            "# License: BSD\n# Author: Sasank Chilamkurthy\n\nfrom __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\n\ncudnn.benchmark = True\nplt.ion()   # interactive mode",
            "code"
        ],
        [
            "&lt;contextlib.ExitStack object at 0x7ff9449a8160&gt;",
            "code"
        ]
    ],
    "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data": [
        [
            "We will use torchvision and torch.utils.data packages for loading the\ndata.",
            "markdown"
        ],
        [
            "The problem we\u2019re going to solve today is to train a model to classify\n<strong>ants</strong> and <strong>bees</strong>. We have about 120 training images each for ants and bees.\nThere are 75 validation images for each class. Usually, this is a very\nsmall dataset to generalize upon, if trained from scratch. Since we\nare using transfer learning, we should be able to generalize reasonably\nwell.",
            "markdown"
        ],
        [
            "This dataset is a very small subset of imagenet.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Download the data from\n\nand extract it to the current directory.",
            "markdown"
        ],
        [
            "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "code"
        ]
    ],
    "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data->Visualize a few images": [
        [
            "Let\u2019s visualize a few training images so as to understand the data\naugmentations.",
            "markdown"
        ],
        [
            "def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\n,  = next(iter(dataloaders['train']))\n\n# Make a grid from batch\n = ()\n\nimshow(, title=[class_names[x] for x in ])\n\n\n<img alt=\"['bees', 'bees', 'ants', 'bees']\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\" srcset=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\"/>",
            "code"
        ]
    ],
    "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Training the model": [
        [
            "Now, let\u2019s write a general function to train a model. Here, we will\nillustrate:",
            "markdown"
        ],
        [
            "Scheduling the learning rate",
            "markdown"
        ],
        [
            "Saving the best model",
            "markdown"
        ],
        [
            "In the following, parameter scheduler is an LR scheduler object from\ntorch.optim.lr_scheduler.",
            "markdown"
        ],
        [
            "def train_model(model, , optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for , labels in dataloaders[phase]:\n                 = .to()\n                labels = labels.to()\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with (phase == 'train'):\n                    outputs = model()\n                    _, preds = (outputs, 1)\n                    loss = (outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * .size(0)\n                running_corrects += (preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:4f}')\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model",
            "code"
        ]
    ],
    "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Training the model->Visualizing the model predictions": [
        [
            "Generic function to display predictions for a few images",
            "markdown"
        ],
        [
            "def visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with ():\n        for i, (, labels) in enumerate(dataloaders['val']):\n             = .to()\n            labels = labels.to()\n\n            outputs = model()\n            _, preds = (outputs, 1)\n\n            for j in range(.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images//2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title(f'predicted: {class_names[preds[j]]}')\n                imshow(.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)",
            "code"
        ]
    ],
    "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Finetuning the convnet": [
        [
            "Load a pretrained model and reset final fully connected layer.",
            "markdown"
        ],
        [
            "model_ft = (pretrained=True)\nnum_ftrs = .in_features\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n = (num_ftrs, 2)\n\nmodel_ft = ()\n\n = ()\n\n# Observe that all parameters are being optimized\n = ((), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\n = (, step_size=7, gamma=0.1)",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning:\n\nThe parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning:\n\nArguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /var/lib/jenkins/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n  0%|          | 0.00/44.7M [00:00&lt;?, ?B/s]\n 21%|##        | 9.28M/44.7M [00:00&lt;00:00, 97.0MB/s]\n 66%|######6   | 29.5M/44.7M [00:00&lt;00:00, 165MB/s]\n100%|##########| 44.7M/44.7M [00:00&lt;00:00, 172MB/s]",
            "code"
        ]
    ],
    "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Finetuning the convnet->Train and evaluate": [
        [
            "It should take around 15-25 min on CPU. On GPU though, it takes less than a\nminute.",
            "markdown"
        ],
        [
            "model_ft = train_model(model_ft, , , ,\n                       num_epochs=25)",
            "code"
        ],
        [
            "Epoch 0/24\n----------\ntrain Loss: 0.5726 Acc: 0.7172\nval Loss: 0.1992 Acc: 0.9216\n\nEpoch 1/24\n----------\ntrain Loss: 0.6180 Acc: 0.7418\nval Loss: 0.2637 Acc: 0.8954\n\nEpoch 2/24\n----------\ntrain Loss: 0.6153 Acc: 0.7664\nval Loss: 0.4100 Acc: 0.8235\n\nEpoch 3/24\n----------\ntrain Loss: 0.6259 Acc: 0.7746\nval Loss: 0.2515 Acc: 0.9150\n\nEpoch 4/24\n----------\ntrain Loss: 0.4605 Acc: 0.8361\nval Loss: 0.1442 Acc: 0.9412\n\nEpoch 5/24\n----------\ntrain Loss: 0.5114 Acc: 0.8238\nval Loss: 0.4190 Acc: 0.8693\n\nEpoch 6/24\n----------\ntrain Loss: 0.4694 Acc: 0.8238\nval Loss: 0.5706 Acc: 0.8627\n\nEpoch 7/24\n----------\ntrain Loss: 0.3649 Acc: 0.8402\nval Loss: 0.3057 Acc: 0.9150\n\nEpoch 8/24\n----------\ntrain Loss: 0.3285 Acc: 0.8648\nval Loss: 0.2728 Acc: 0.9216\n\nEpoch 9/24\n----------\ntrain Loss: 0.1921 Acc: 0.9426\nval Loss: 0.3099 Acc: 0.9216\n\nEpoch 10/24\n----------\ntrain Loss: 0.3274 Acc: 0.8648\nval Loss: 0.3046 Acc: 0.8954\n\nEpoch 11/24\n----------\ntrain Loss: 0.3464 Acc: 0.8443\nval Loss: 0.2354 Acc: 0.9346\n\nEpoch 12/24\n----------\ntrain Loss: 0.2984 Acc: 0.8730\nval Loss: 0.2999 Acc: 0.9020\n\nEpoch 13/24\n----------\ntrain Loss: 0.3737 Acc: 0.8525\nval Loss: 0.2356 Acc: 0.9281\n\nEpoch 14/24\n----------\ntrain Loss: 0.3435 Acc: 0.8566\nval Loss: 0.2334 Acc: 0.9477\n\nEpoch 15/24\n----------\ntrain Loss: 0.2781 Acc: 0.8730\nval Loss: 0.2462 Acc: 0.9216\n\nEpoch 16/24\n----------\ntrain Loss: 0.1656 Acc: 0.9385\nval Loss: 0.2380 Acc: 0.9216\n\nEpoch 17/24\n----------\ntrain Loss: 0.2829 Acc: 0.9016\nval Loss: 0.2423 Acc: 0.9346\n\nEpoch 18/24\n----------\ntrain Loss: 0.2433 Acc: 0.8852\nval Loss: 0.2419 Acc: 0.9216\n\nEpoch 19/24\n----------\ntrain Loss: 0.3778 Acc: 0.8279\nval Loss: 0.2537 Acc: 0.9281\n\nEpoch 20/24\n----------\ntrain Loss: 0.2816 Acc: 0.8770\nval Loss: 0.2284 Acc: 0.9281\n\nEpoch 21/24\n----------\ntrain Loss: 0.2312 Acc: 0.8852\nval Loss: 0.2293 Acc: 0.9412\n\nEpoch 22/24\n----------\ntrain Loss: 0.2367 Acc: 0.9139\nval Loss: 0.2405 Acc: 0.9216\n\nEpoch 23/24\n----------\ntrain Loss: 0.3230 Acc: 0.8730\nval Loss: 0.2436 Acc: 0.9281\n\nEpoch 24/24\n----------\ntrain Loss: 0.1921 Acc: 0.9016\nval Loss: 0.2392 Acc: 0.9216\n\nTraining complete in 1m 10s\nBest val Acc: 0.947712",
            "code"
        ],
        [
            "visualize_model(model_ft)\n\n\n<img alt=\"predicted: bees, predicted: bees, predicted: bees, predicted: ants, predicted: ants, predicted: ants\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_transfer_learning_tutorial_002.png\" srcset=\"../_images/sphx_glr_transfer_learning_tutorial_002.png\"/>",
            "code"
        ]
    ],
    "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->ConvNet as fixed feature extractor": [
        [
            "Here, we need to freeze all the network except the final layer. We need\nto set requires_grad = False to freeze the parameters so that the\ngradients are not computed in backward().",
            "markdown"
        ],
        [
            "You can read more about this in the documentation\n.",
            "markdown"
        ],
        [
            "model_conv = (pretrained=True)\nfor  in ():\n    .requires_grad = False\n\n# Parameters of newly constructed modules have requires_grad=True by default\nnum_ftrs = .in_features\n = (num_ftrs, 2)\n\nmodel_conv = ()\n\n = ()\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\n = ((), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\n = (, step_size=7, gamma=0.1)",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning:\n\nThe parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning:\n\nArguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.",
            "code"
        ]
    ],
    "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->ConvNet as fixed feature extractor->Train and evaluate": [
        [
            "On CPU this will take about half the time compared to previous scenario.\nThis is expected as gradients don\u2019t need to be computed for most of the\nnetwork. However, forward does need to be computed.",
            "markdown"
        ],
        [
            "model_conv = train_model(model_conv, , ,\n                         , num_epochs=25)",
            "code"
        ],
        [
            "Epoch 0/24\n----------\ntrain Loss: 0.7290 Acc: 0.5902\nval Loss: 0.2586 Acc: 0.8954\n\nEpoch 1/24\n----------\ntrain Loss: 0.6689 Acc: 0.7295\nval Loss: 0.3950 Acc: 0.8497\n\nEpoch 2/24\n----------\ntrain Loss: 0.4358 Acc: 0.8115\nval Loss: 0.1802 Acc: 0.9412\n\nEpoch 3/24\n----------\ntrain Loss: 0.4473 Acc: 0.7910\nval Loss: 0.2035 Acc: 0.9346\n\nEpoch 4/24\n----------\ntrain Loss: 0.3695 Acc: 0.8156\nval Loss: 0.2172 Acc: 0.9150\n\nEpoch 5/24\n----------\ntrain Loss: 0.4376 Acc: 0.8197\nval Loss: 0.1986 Acc: 0.9412\n\nEpoch 6/24\n----------\ntrain Loss: 0.3912 Acc: 0.8156\nval Loss: 0.1906 Acc: 0.9412\n\nEpoch 7/24\n----------\ntrain Loss: 0.4135 Acc: 0.8238\nval Loss: 0.1573 Acc: 0.9412\n\nEpoch 8/24\n----------\ntrain Loss: 0.3502 Acc: 0.8443\nval Loss: 0.1837 Acc: 0.9412\n\nEpoch 9/24\n----------\ntrain Loss: 0.3378 Acc: 0.8566\nval Loss: 0.1689 Acc: 0.9346\n\nEpoch 10/24\n----------\ntrain Loss: 0.3615 Acc: 0.8402\nval Loss: 0.1795 Acc: 0.9477\n\nEpoch 11/24\n----------\ntrain Loss: 0.2932 Acc: 0.8648\nval Loss: 0.1722 Acc: 0.9346\n\nEpoch 12/24\n----------\ntrain Loss: 0.2824 Acc: 0.8648\nval Loss: 0.1795 Acc: 0.9412\n\nEpoch 13/24\n----------\ntrain Loss: 0.3392 Acc: 0.8525\nval Loss: 0.1894 Acc: 0.9412\n\nEpoch 14/24\n----------\ntrain Loss: 0.3772 Acc: 0.8566\nval Loss: 0.2044 Acc: 0.9412\n\nEpoch 15/24\n----------\ntrain Loss: 0.3700 Acc: 0.8033\nval Loss: 0.2390 Acc: 0.9412\n\nEpoch 16/24\n----------\ntrain Loss: 0.3259 Acc: 0.8443\nval Loss: 0.1807 Acc: 0.9477\n\nEpoch 17/24\n----------\ntrain Loss: 0.3933 Acc: 0.8525\nval Loss: 0.1865 Acc: 0.9477\n\nEpoch 18/24\n----------\ntrain Loss: 0.3499 Acc: 0.8443\nval Loss: 0.1757 Acc: 0.9346\n\nEpoch 19/24\n----------\ntrain Loss: 0.3927 Acc: 0.8033\nval Loss: 0.1748 Acc: 0.9477\n\nEpoch 20/24\n----------\ntrain Loss: 0.2217 Acc: 0.9139\nval Loss: 0.1768 Acc: 0.9477\n\nEpoch 21/24\n----------\ntrain Loss: 0.3716 Acc: 0.8443\nval Loss: 0.1914 Acc: 0.9412\n\nEpoch 22/24\n----------\ntrain Loss: 0.3795 Acc: 0.8074\nval Loss: 0.1907 Acc: 0.9412\n\nEpoch 23/24\n----------\ntrain Loss: 0.3627 Acc: 0.8320\nval Loss: 0.1690 Acc: 0.9412\n\nEpoch 24/24\n----------\ntrain Loss: 0.2867 Acc: 0.8811\nval Loss: 0.1908 Acc: 0.9477\n\nTraining complete in 0m 44s\nBest val Acc: 0.947712",
            "code"
        ],
        [
            "visualize_model(model_conv)\n\nplt.ioff()\nplt.show()\n\n\n<img alt=\"predicted: ants, predicted: ants, predicted: ants, predicted: ants, predicted: bees, predicted: bees\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_transfer_learning_tutorial_003.png\" srcset=\"../_images/sphx_glr_transfer_learning_tutorial_003.png\"/>",
            "code"
        ]
    ],
    "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Further Learning": [
        [
            "If you would like to learn more about the applications of transfer learning,\ncheckout our .",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 2 minutes  0.817 seconds)",
            "markdown"
        ]
    ],
    "torch->Image and Video->Adversarial Example Generation": [
        [
            "<strong>Author:</strong> ",
            "markdown"
        ],
        [
            "If you are reading this, hopefully you can appreciate how effective some\nmachine learning models are. Research is constantly pushing ML models to\nbe faster, more accurate, and more efficient. However, an often\noverlooked aspect of designing and training models is security and\nrobustness, especially in the face of an adversary who wishes to fool\nthe model.",
            "markdown"
        ],
        [
            "This tutorial will raise your awareness to the security vulnerabilities\nof ML models, and will give insight into the hot topic of adversarial\nmachine learning. You may be surprised to find that adding imperceptible\nperturbations to an image <em>can</em> cause drastically different model\nperformance. Given that this is a tutorial, we will explore the topic\nvia example on an image classifier. Specifically, we will use one of the\nfirst and most popular attack methods, the Fast Gradient Sign Attack\n(FGSM), to fool an MNIST classifier.",
            "markdown"
        ]
    ],
    "torch->Image and Video->Adversarial Example Generation->Threat Model": [
        [
            "For context, there are many categories of adversarial attacks, each with\na different goal and assumption of the attacker\u2019s knowledge. However, in\ngeneral the overarching goal is to add the least amount of perturbation\nto the input data to cause the desired misclassification. There are\nseveral kinds of assumptions of the attacker\u2019s knowledge, two of which\nare: <strong>white-box</strong> and <strong>black-box</strong>. A <em>white-box</em> attack assumes the\nattacker has full knowledge and access to the model, including\narchitecture, inputs, outputs, and weights. A <em>black-box</em> attack assumes\nthe attacker only has access to the inputs and outputs of the model, and\nknows nothing about the underlying architecture or weights. There are\nalso several types of goals, including <strong>misclassification</strong> and\n<strong>source/target misclassification</strong>. A goal of <em>misclassification</em> means\nthe adversary only wants the output classification to be wrong but does\nnot care what the new classification is. A <em>source/target\nmisclassification</em> means the adversary wants to alter an image that is\noriginally of a specific source class so that it is classified as a\nspecific target class.",
            "markdown"
        ],
        [
            "In this case, the FGSM attack is a <em>white-box</em> attack with the goal of\n<em>misclassification</em>. With this background information, we can now\ndiscuss the attack in detail.",
            "markdown"
        ]
    ],
    "torch->Image and Video->Adversarial Example Generation->Fast Gradient Sign Attack": [
        [
            "One of the first and most popular adversarial attacks to date is\nreferred to as the <em>Fast Gradient Sign Attack (FGSM)</em> and is described\nby Goodfellow et. al.\u00a0in . The attack is remarkably\npowerful, and yet intuitive. It is designed to attack neural networks by\nleveraging the way they learn, <em>gradients</em>. The idea is simple, rather\nthan working to minimize the loss by adjusting the weights based on the\nbackpropagated gradients, the attack <em>adjusts the input data to maximize\nthe loss</em> based on the same backpropagated gradients. In other words,\nthe attack uses the gradient of the loss w.r.t the input data, then\nadjusts the input data to maximize the loss.",
            "markdown"
        ],
        [
            "Before we jump into the code, let\u2019s look at the famous\n panda example and extract\nsome notation.\n\n<img alt=\"fgsm_panda_image\" src=\"../_images/fgsm_panda_image.png\"/>",
            "markdown"
        ],
        [
            "From the figure, \\(\\mathbf{x}\\) is the original input image\ncorrectly classified as a \u201cpanda\u201d, \\(y\\) is the ground truth label\nfor \\(\\mathbf{x}\\), \\(\\mathbf{\\theta}\\) represents the model\nparameters, and \\(J(\\mathbf{\\theta}, \\mathbf{x}, y)\\) is the loss\nthat is used to train the network. The attack backpropagates the\ngradient back to the input data to calculate\n\\(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)\\). Then, it adjusts\nthe input data by a small step (\\(\\epsilon\\) or \\(0.007\\) in the\npicture) in the direction (i.e.\n\\(sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))\\)) that will\nmaximize the loss. The resulting perturbed image, \\(x'\\), is then\n<em>misclassified</em> by the target network as a \u201cgibbon\u201d when it is still\nclearly a \u201cpanda\u201d.",
            "markdown"
        ],
        [
            "Hopefully now the motivation for this tutorial is clear, so lets jump\ninto the implementation.",
            "markdown"
        ],
        [
            "from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# NOTE: This is a hack to get around \"User-agent\" limitations when downloading MNIST datasets\n#       see, https://github.com/pytorch/vision/issues/3497 for more information\nfrom six.moves import urllib\nopener = urllib.request.build_opener()\nopener.addheaders = [('User-agent', 'Mozilla/5.0')]\nurllib.request.install_opener(opener)",
            "code"
        ]
    ],
    "torch->Image and Video->Adversarial Example Generation->Implementation": [
        [
            "In this section, we will discuss the input parameters for the tutorial,\ndefine the model under attack, then code the attack and run some tests.",
            "markdown"
        ]
    ],
    "torch->Image and Video->Adversarial Example Generation->Implementation->Inputs": [
        [
            "There are only three inputs for this tutorial, and are defined as\nfollows:",
            "markdown"
        ],
        [
            "<strong>epsilons</strong> - List of epsilon values to use for the run. It is\nimportant to keep 0 in the list because it represents the model\nperformance on the original test set. Also, intuitively we would\nexpect the larger the epsilon, the more noticeable the perturbations\nbut the more effective the attack in terms of degrading model\naccuracy. Since the data range here is \\([0,1]\\), no epsilon\nvalue should exceed 1.",
            "markdown"
        ],
        [
            "<strong>pretrained_model</strong> - path to the pretrained MNIST model which was\ntrained with\n.\nFor simplicity, download the pretrained model .",
            "markdown"
        ],
        [
            "<strong>use_cuda</strong> - boolean flag to use CUDA if desired and available.\nNote, a GPU with CUDA is not critical for this tutorial as a CPU will\nnot take much time.",
            "markdown"
        ],
        [
            "epsilons = [0, .05, .1, .15, .2, .25, .3]\npretrained_model = \"data/lenet_mnist_model.pth\"\nuse_cuda=True",
            "code"
        ]
    ],
    "torch->Image and Video->Adversarial Example Generation->Implementation->Model Under Attack": [
        [
            "As mentioned, the model under attack is the same MNIST model from\n.\nYou may train and save your own MNIST model or you can download and use\nthe provided model. The <em>Net</em> definition and test dataloader here have\nbeen copied from the MNIST example. The purpose of this section is to\ndefine the model and dataloader, then initialize the model and load the\npretrained weights.",
            "markdown"
        ],
        [
            "# LeNet Model definition\nclass Net():\n    def __init__(self):\n        super(, self).__init__()\n        self.conv1 = (1, 10, kernel_size=5)\n        self.conv2 = (10, 20, kernel_size=5)\n        self.conv2_drop = ()\n        self.fc1 = (320, 50)\n        self.fc2 = (50, 10)\n\n    def forward(self, x):\n        x = ((self.conv1(x), 2))\n        x = ((self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = (self.fc1(x))\n        x = (x, training=self.training)\n        x = self.fc2(x)\n        return (x, dim=1)\n\n# MNIST Test dataset and dataloader declaration\n = (\n    ('../data', train=False, download=True, transform=([\n            (),\n            ])),\n        batch_size=1, shuffle=True)\n\n# Define what device we are using\nprint(\"CUDA Available: \",())\n = (\"cuda\" if (use_cuda and ()) else \"cpu\")\n\n# Initialize the network\nmodel = ().to()\n\n# Load the pretrained model\n((pretrained_model, map_location='cpu'))\n\n# Set the model in evaluation mode. In this case this is for the Dropout layers\n()",
            "code"
        ],
        [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/9912422 [00:00&lt;?, ?it/s]\n 90%|########9 | 8880128/9912422 [00:00&lt;00:00, 88669659.28it/s]\n100%|##########| 9912422/9912422 [00:00&lt;00:00, 94623370.71it/s]\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/28881 [00:00&lt;?, ?it/s]\n100%|##########| 28881/28881 [00:00&lt;00:00, 26611532.04it/s]\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/1648877 [00:00&lt;?, ?it/s]\n100%|##########| 1648877/1648877 [00:00&lt;00:00, 25857956.22it/s]\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/4542 [00:00&lt;?, ?it/s]\n100%|##########| 4542/4542 [00:00&lt;00:00, 30143241.72it/s]\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nCUDA Available:  True\n\nNet(\n  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n  (fc1): Linear(in_features=320, out_features=50, bias=True)\n  (fc2): Linear(in_features=50, out_features=10, bias=True)\n)",
            "code"
        ]
    ],
    "torch->Image and Video->Adversarial Example Generation->Implementation->FGSM Attack": [
        [
            "Now, we can define the function that creates the adversarial examples by\nperturbing the original inputs. The fgsm_attack function takes three\ninputs, <em>image</em> is the original clean image (\\(x\\)), <em>epsilon</em> is\nthe pixel-wise perturbation amount (\\(\\epsilon\\)), and <em>data_grad</em>\nis gradient of the loss w.r.t the input image\n(\\(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)\\)). The function\nthen creates perturbed image as\n\n\\[perturbed\\_image = image + epsilon*sign(data\\_grad) = x + \\epsilon * sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))\n\n\\]",
            "markdown"
        ],
        [
            "Finally, in order to maintain the original range of the data, the\nperturbed image is clipped to range \\([0,1]\\).",
            "markdown"
        ],
        [
            "# FGSM attack code\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = (perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image",
            "code"
        ]
    ],
    "torch->Image and Video->Adversarial Example Generation->Implementation->Testing Function": [
        [
            "Finally, the central result of this tutorial comes from the test\nfunction. Each call to this test function performs a full test step on\nthe MNIST test set and reports a final accuracy. However, notice that\nthis function also takes an <em>epsilon</em> input. This is because the\ntest function reports the accuracy of a model that is under attack\nfrom an adversary with strength \\(\\epsilon\\). More specifically, for\neach sample in the test set, the function computes the gradient of the\nloss w.r.t the input data (\\(data\\_grad\\)), creates a perturbed\nimage with fgsm_attack (\\(perturbed\\_data\\)), then checks to see\nif the perturbed example is adversarial. In addition to testing the\naccuracy of the model, the function also saves and returns some\nsuccessful adversarial examples to be visualized later.",
            "markdown"
        ],
        [
            "def test( model, , , epsilon ):\n\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n\n    # Loop over all examples in test set\n    for data, target in :\n\n        # Send the data and label to the device\n        data, target = data.to(), target.to()\n\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n\n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = (output, target)\n\n        # Zero all existing gradients\n        ()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        output = model(perturbed_data)\n\n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        if final_pred.item() == target.item():\n            correct += 1\n            # Special case for saving 0 epsilon examples\n            if (epsilon == 0) and (len(adv_examples) &lt; 5):\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        else:\n            # Save some adv examples for visualization later\n            if len(adv_examples) &lt; 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    final_acc = correct/float(len())\n    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(), final_acc))\n\n    # Return the accuracy and an adversarial example\n    return final_acc, adv_examples",
            "code"
        ]
    ],
    "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack": [
        [
            "The last part of the implementation is to actually run the attack. Here,\nwe run a full test step for each epsilon value in the <em>epsilons</em> input.\nFor each epsilon we also save the final accuracy and some successful\nadversarial examples to be plotted in the coming sections. Notice how\nthe printed accuracies decrease as the epsilon value increases. Also,\nnote the \\(\\epsilon=0\\) case represents the original test accuracy,\nwith no attack.",
            "markdown"
        ],
        [
            "accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "code"
        ],
        [
            "Epsilon: 0      Test Accuracy = 9810 / 10000 = 0.981\nEpsilon: 0.05   Test Accuracy = 9426 / 10000 = 0.9426\nEpsilon: 0.1    Test Accuracy = 8510 / 10000 = 0.851\nEpsilon: 0.15   Test Accuracy = 6826 / 10000 = 0.6826\nEpsilon: 0.2    Test Accuracy = 4301 / 10000 = 0.4301\nEpsilon: 0.25   Test Accuracy = 2082 / 10000 = 0.2082\nEpsilon: 0.3    Test Accuracy = 869 / 10000 = 0.0869",
            "code"
        ]
    ],
    "torch->Image and Video->Adversarial Example Generation->Results->Accuracy vs Epsilon": [
        [
            "The first result is the accuracy versus epsilon plot. As alluded to\nearlier, as epsilon increases we expect the test accuracy to decrease.\nThis is because larger epsilons mean we take a larger step in the\ndirection that will maximize the loss. Notice the trend in the curve is\nnot linear even though the epsilon values are linearly spaced. For\nexample, the accuracy at \\(\\epsilon=0.05\\) is only about 4% lower\nthan \\(\\epsilon=0\\), but the accuracy at \\(\\epsilon=0.2\\) is 25%\nlower than \\(\\epsilon=0.15\\). Also, notice the accuracy of the model\nhits random accuracy for a 10-class classifier between\n\\(\\epsilon=0.25\\) and \\(\\epsilon=0.3\\).",
            "markdown"
        ],
        [
            "plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\n<img alt=\"Accuracy vs Epsilon\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_001.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_001.png\"/>",
            "code"
        ]
    ],
    "torch->Image and Video->Adversarial Example Generation->Results->Sample Adversarial Examples": [
        [
            "Remember the idea of no free lunch? In this case, as epsilon increases\nthe test accuracy decreases <strong>BUT</strong> the perturbations become more easily\nperceptible. In reality, there is a tradeoff between accuracy\ndegredation and perceptibility that an attacker must consider. Here, we\nshow some examples of successful adversarial examples at each epsilon\nvalue. Each row of the plot shows a different epsilon value. The first\nrow is the \\(\\epsilon=0\\) examples which represent the original\n\u201cclean\u201d images with no perturbation. The title of each image shows the\n\u201coriginal classification -&gt; adversarial classification.\u201d Notice, the\nperturbations start to become evident at \\(\\epsilon=0.15\\) and are\nquite evident at \\(\\epsilon=0.3\\). However, in all cases humans are\nstill capable of identifying the correct class despite the added noise.",
            "markdown"
        ],
        [
            "# Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>",
            "code"
        ]
    ],
    "torch->Image and Video->Adversarial Example Generation->Where to go next?": [
        [
            "Hopefully this tutorial gives some insight into the topic of adversarial\nmachine learning. There are many potential directions to go from here.\nThis attack represents the very beginning of adversarial attack research\nand since there have been many subsequent ideas for how to attack and\ndefend ML models from an adversary. In fact, at NIPS 2017 there was an\nadversarial attack and defense competition and many of the methods used\nin the competition are described in this paper: . The work\non defense also leads into the idea of making machine learning models\nmore <em>robust</em> in general, to both naturally perturbed and adversarially\ncrafted inputs.",
            "markdown"
        ],
        [
            "Another direction to go is adversarial attacks and defense in different\ndomains. Adversarial research is not limited to the image domain, check\nout  attack on\nspeech-to-text models. But perhaps the best way to learn more about\nadversarial machine learning is to get your hands dirty. Try to\nimplement a different attack from the NIPS 2017 competition, and see how\nit differs from FGSM. Then, try to defend the model from your own\nattacks.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 2 minutes  38.625 seconds)",
            "markdown"
        ]
    ],
    "torch->Image and Video->Spatial Transformer Networks Tutorial": [
        [
            "<strong>Author</strong>: \n\n<img alt=\"../_images/FSeq.png\" src=\"../_images/FSeq.png\"/>",
            "markdown"
        ],
        [
            "In this tutorial, you will learn how to augment your network using\na visual attention mechanism called spatial transformer\nnetworks. You can read more about the spatial transformer\nnetworks in the ",
            "markdown"
        ],
        [
            "Spatial transformer networks are a generalization of differentiable\nattention to any spatial transformation. Spatial transformer networks\n(STN for short) allow a neural network to learn how to perform spatial\ntransformations on the input image in order to enhance the geometric\ninvariance of the model.\nFor example, it can crop a region of interest, scale and correct\nthe orientation of an image. It can be a useful mechanism because CNNs\nare not invariant to rotation and scale and more general affine\ntransformations.",
            "markdown"
        ],
        [
            "One of the best things about STN is the ability to simply plug it into\nany existing CNN with very little modification.",
            "markdown"
        ],
        [
            "# License: BSD\n# Author: Ghassen Hamrouni\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.ion()   # interactive mode",
            "code"
        ],
        [
            "&lt;contextlib.ExitStack object at 0x7f2d90743cd0&gt;",
            "code"
        ]
    ],
    "torch->Image and Video->Spatial Transformer Networks Tutorial->Loading the data": [
        [
            "In this post we experiment with the classic MNIST dataset. Using a\nstandard convolutional network augmented with a spatial transformer\nnetwork.",
            "markdown"
        ],
        [
            "from six.moves import urllib\nopener = urllib.request.build_opener()\nopener.addheaders = [('User-agent', 'Mozilla/5.0')]\nurllib.request.install_opener(opener)\n\n = (\"cuda\" if () else \"cpu\")\n\n# Training dataset\n = (\n    (root='.', train=True, download=True,\n                   transform=([\n                       (),\n                       ((0.1307,), (0.3081,))\n                   ])), batch_size=64, shuffle=True, num_workers=4)\n# Test dataset\n = (\n    (root='.', train=False, transform=([\n        (),\n        ((0.1307,), (0.3081,))\n    ])), batch_size=64, shuffle=True, num_workers=4)",
            "code"
        ],
        [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/9912422 [00:00&lt;?, ?it/s]\n 85%|########4 | 8388608/9912422 [00:00&lt;00:00, 83763738.69it/s]\n100%|##########| 9912422/9912422 [00:00&lt;00:00, 96041985.73it/s]\nExtracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/28881 [00:00&lt;?, ?it/s]\n100%|##########| 28881/28881 [00:00&lt;00:00, 133262589.47it/s]\nExtracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/1648877 [00:00&lt;?, ?it/s]\n100%|##########| 1648877/1648877 [00:00&lt;00:00, 25862694.45it/s]\nExtracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/4542 [00:00&lt;?, ?it/s]\n100%|##########| 4542/4542 [00:00&lt;00:00, 28139628.90it/s]\nExtracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw",
            "code"
        ]
    ],
    "torch->Image and Video->Spatial Transformer Networks Tutorial->Depicting spatial transformer networks": [
        [
            "Spatial transformer networks boils down to three main components :",
            "markdown"
        ],
        [
            "The localization network is a regular CNN which regresses the\ntransformation parameters. The transformation is never learned\nexplicitly from this dataset, instead the network learns automatically\nthe spatial transformations that enhances the global accuracy.",
            "markdown"
        ],
        [
            "The grid generator generates a grid of coordinates in the input\nimage corresponding to each pixel from the output image.",
            "markdown"
        ],
        [
            "The sampler uses the parameters of the transformation and applies\nit to the input image.\n\n\n<img alt=\"../_images/stn-arch.png\" src=\"../_images/stn-arch.png\"/>",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "We need the latest version of PyTorch that contains\naffine_grid and grid_sample modules.",
            "markdown"
        ],
        [
            "class Net():\n    def __init__(self):\n        super(, self).__init__()\n        self.conv1 = (1, 10, kernel_size=5)\n        self.conv2 = (10, 20, kernel_size=5)\n        self.conv2_drop = ()\n        self.fc1 = (320, 50)\n        self.fc2 = (50, 10)\n\n        # Spatial transformer localization-network\n        self.localization = (\n            (1, 8, kernel_size=7),\n            (2, stride=2),\n            (True),\n            (8, 10, kernel_size=5),\n            (2, stride=2),\n            (True)\n        )\n\n        # Regressor for the 3 * 2 affine matrix\n        self.fc_loc = (\n            (10 * 3 * 3, 32),\n            (True),\n            (32, 3 * 2)\n        )\n\n        # Initialize the weights/bias with identity transformation\n        self.fc_loc[2].weight.data.zero_()\n        self.fc_loc[2].bias.data.copy_(([1, 0, 0, 0, 1, 0], dtype=))\n\n    # Spatial transformer network forward function\n    def stn(self, x):\n        xs = self.localization(x)\n        xs = xs.view(-1, 10 * 3 * 3)\n        theta = self.fc_loc(xs)\n        theta = theta.view(-1, 2, 3)\n\n        grid = (theta, x.size())\n        x = (x, grid)\n\n        return x\n\n    def forward(self, x):\n        # transform the input\n        x = self.stn(x)\n\n        # Perform the usual forward pass\n        x = ((self.conv1(x), 2))\n        x = ((self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = (self.fc1(x))\n        x = (x, training=self.training)\n        x = self.fc2(x)\n        return (x, dim=1)\n\n\nmodel = ().to()",
            "code"
        ]
    ],
    "torch->Image and Video->Spatial Transformer Networks Tutorial->Training the model": [
        [
            "Now, let\u2019s use the SGD algorithm to train the model. The network is\nlearning the classification task in a supervised way. In the same time\nthe model is learning STN automatically in an end-to-end fashion.",
            "markdown"
        ],
        [
            " = ((), lr=0.01)\n\n\ndef train(epoch):\n    ()\n    for batch_idx, (data, target) in enumerate():\n        data, target = data.to(), target.to()\n\n        ()\n        output = model(data)\n        loss = (output, target)\n        loss.backward()\n        ()\n        if batch_idx % 500 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(),\n                100. * batch_idx / len(), loss.item()))\n#\n# A simple test procedure to measure the STN performances on MNIST.\n#\n\n\ndef test():\n    with ():\n        ()\n        test_loss = 0\n        correct = 0\n        for data, target in :\n            data, target = data.to(), target.to()\n            output = model(data)\n\n            # sum up batch loss\n            test_loss += (output, target, size_average=False).item()\n            # get the index of the max log-probability\n            pred = output.max(1, keepdim=True)[1]\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n        test_loss /= len()\n        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n              .format(test_loss, correct, len(),\n                      100. * correct / len()))",
            "code"
        ]
    ],
    "torch->Image and Video->Spatial Transformer Networks Tutorial->Visualizing the STN results": [
        [
            "Now, we will inspect the results of our learned visual attention\nmechanism.",
            "markdown"
        ],
        [
            "We define a small helper function in order to visualize the\ntransformations while training.",
            "markdown"
        ],
        [
            "def convert_image_np(inp):\n    \"\"\"Convert a Tensor to numpy image.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    return inp\n\n# We want to visualize the output of the spatial transformers layer\n# after the training, we visualize a batch of input images and\n# the corresponding transformed batch using STN.\n\n\ndef visualize_stn():\n    with ():\n        # Get a batch of training data\n        data = next(iter())[0].to()\n\n        input_tensor = data.cpu()\n        transformed_input_tensor = model.stn(data).cpu()\n\n        in_grid = convert_image_np(\n            (input_tensor))\n\n        out_grid = convert_image_np(\n            (transformed_input_tensor))\n\n        # Plot the results side-by-side\n        f, axarr = plt.subplots(1, 2)\n        axarr[0].imshow(in_grid)\n        axarr[0].set_title('Dataset Images')\n\n        axarr[1].imshow(out_grid)\n        axarr[1].set_title('Transformed Images')\n\nfor epoch in range(1, 20 + 1):\n    train(epoch)\n    test()\n\n# Visualize the STN transformation on some input batch\nvisualize_stn()\n\nplt.ioff()\nplt.show()\n\n\n<img alt=\"Dataset Images, Transformed Images\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_spatial_transformer_tutorial_001.png\" srcset=\"../_images/sphx_glr_spatial_transformer_tutorial_001.png\"/>",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4298: UserWarning:\n\nDefault grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4236: UserWarning:\n\nDefault grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 2.283169\nTrain Epoch: 1 [32000/60000 (53%)]      Loss: 0.815205\n/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning:\n\nsize_average and reduce args will be deprecated, please use reduction='sum' instead.\n\n\nTest set: Average loss: 0.1749, Accuracy: 9508/10000 (95%)\n\nTrain Epoch: 2 [0/60000 (0%)]   Loss: 0.338278\nTrain Epoch: 2 [32000/60000 (53%)]      Loss: 0.422044\n\nTest set: Average loss: 0.1444, Accuracy: 9563/10000 (96%)\n\nTrain Epoch: 3 [0/60000 (0%)]   Loss: 0.489171\nTrain Epoch: 3 [32000/60000 (53%)]      Loss: 0.283197\n\nTest set: Average loss: 0.1098, Accuracy: 9675/10000 (97%)\n\nTrain Epoch: 4 [0/60000 (0%)]   Loss: 0.226815\nTrain Epoch: 4 [32000/60000 (53%)]      Loss: 0.326167\n\nTest set: Average loss: 0.0650, Accuracy: 9817/10000 (98%)\n\nTrain Epoch: 5 [0/60000 (0%)]   Loss: 0.065320\nTrain Epoch: 5 [32000/60000 (53%)]      Loss: 0.154336\n\nTest set: Average loss: 0.0557, Accuracy: 9833/10000 (98%)\n\nTrain Epoch: 6 [0/60000 (0%)]   Loss: 0.081050\nTrain Epoch: 6 [32000/60000 (53%)]      Loss: 0.180084\n\nTest set: Average loss: 0.0575, Accuracy: 9833/10000 (98%)\n\nTrain Epoch: 7 [0/60000 (0%)]   Loss: 0.122517\nTrain Epoch: 7 [32000/60000 (53%)]      Loss: 0.224352\n\nTest set: Average loss: 0.0530, Accuracy: 9848/10000 (98%)\n\nTrain Epoch: 8 [0/60000 (0%)]   Loss: 0.130049\nTrain Epoch: 8 [32000/60000 (53%)]      Loss: 0.136648\n\nTest set: Average loss: 0.0474, Accuracy: 9857/10000 (99%)\n\nTrain Epoch: 9 [0/60000 (0%)]   Loss: 0.093681\nTrain Epoch: 9 [32000/60000 (53%)]      Loss: 0.050301\n\nTest set: Average loss: 0.0454, Accuracy: 9868/10000 (99%)\n\nTrain Epoch: 10 [0/60000 (0%)]  Loss: 0.103837\nTrain Epoch: 10 [32000/60000 (53%)]     Loss: 0.047425\n\nTest set: Average loss: 0.0772, Accuracy: 9784/10000 (98%)\n\nTrain Epoch: 11 [0/60000 (0%)]  Loss: 0.149770\nTrain Epoch: 11 [32000/60000 (53%)]     Loss: 0.025939\n\nTest set: Average loss: 0.0440, Accuracy: 9877/10000 (99%)\n\nTrain Epoch: 12 [0/60000 (0%)]  Loss: 0.211523\nTrain Epoch: 12 [32000/60000 (53%)]     Loss: 0.126906\n\nTest set: Average loss: 0.0396, Accuracy: 9881/10000 (99%)\n\nTrain Epoch: 13 [0/60000 (0%)]  Loss: 0.065644\nTrain Epoch: 13 [32000/60000 (53%)]     Loss: 0.080548\n\nTest set: Average loss: 0.0409, Accuracy: 9865/10000 (99%)\n\nTrain Epoch: 14 [0/60000 (0%)]  Loss: 0.097975\nTrain Epoch: 14 [32000/60000 (53%)]     Loss: 0.199458\n\nTest set: Average loss: 0.0355, Accuracy: 9896/10000 (99%)\n\nTrain Epoch: 15 [0/60000 (0%)]  Loss: 0.163116\nTrain Epoch: 15 [32000/60000 (53%)]     Loss: 0.105829\n\nTest set: Average loss: 0.0371, Accuracy: 9890/10000 (99%)\n\nTrain Epoch: 16 [0/60000 (0%)]  Loss: 0.117419\nTrain Epoch: 16 [32000/60000 (53%)]     Loss: 0.111820\n\nTest set: Average loss: 0.0476, Accuracy: 9860/10000 (99%)\n\nTrain Epoch: 17 [0/60000 (0%)]  Loss: 0.325285\nTrain Epoch: 17 [32000/60000 (53%)]     Loss: 0.079091\n\nTest set: Average loss: 0.0432, Accuracy: 9871/10000 (99%)\n\nTrain Epoch: 18 [0/60000 (0%)]  Loss: 0.111252\nTrain Epoch: 18 [32000/60000 (53%)]     Loss: 0.039592\n\nTest set: Average loss: 0.0437, Accuracy: 9861/10000 (99%)\n\nTrain Epoch: 19 [0/60000 (0%)]  Loss: 0.051505\nTrain Epoch: 19 [32000/60000 (53%)]     Loss: 0.143836\n\nTest set: Average loss: 0.0453, Accuracy: 9866/10000 (99%)\n\nTrain Epoch: 20 [0/60000 (0%)]  Loss: 0.159451\nTrain Epoch: 20 [32000/60000 (53%)]     Loss: 0.101160\n\nTest set: Average loss: 0.0390, Accuracy: 9880/10000 (99%)",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 3 minutes  27.639 seconds)",
            "markdown"
        ]
    ],
    "torch->Image and Video->Optimizing Vision Transformer Model for Deployment": [
        [
            ",",
            "markdown"
        ],
        [
            "Vision Transformer models apply the cutting-edge attention-based\ntransformer models, introduced in Natural Language Processing to achieve\nall kinds of the state of the art (SOTA) results, to Computer Vision\ntasks. Facebook Data-efficient Image Transformers \nis a Vision Transformer model trained on ImageNet for image\nclassification.",
            "markdown"
        ],
        [
            "In this tutorial, we will first cover what DeiT is and how to use it,\nthen go through the complete steps of scripting, quantizing, optimizing,\nand using the model in iOS and Android apps. We will also compare the\nperformance of quantized, optimized and non-quantized, non-optimized\nmodels, and show the benefits of applying quantization and optimization\nto the model along the steps.",
            "markdown"
        ]
    ],
    "torch->Image and Video->Optimizing Vision Transformer Model for Deployment->What is DeiT": [
        [
            "Convolutional Neural Networks (CNNs) have been the main models for image\nclassification since deep learning took off in 2012, but CNNs typically\nrequire hundreds of millions of images for training to achieve the\nSOTAresults. DeiT is a vision transformer model that requires a lot less\ndata and computing resources for training to compete with the leading\nCNNs in performing image classification, which is made possible by two\nkey components of of DeiT:",
            "markdown"
        ],
        [
            "Data augmentation that simulates training on a much larger dataset;",
            "markdown"
        ],
        [
            "Native distillation that allows the transformer network to learn from\na CNN\u2019s output.",
            "markdown"
        ],
        [
            "DeiT shows that Transformers can be successfully applied to computer\nvision tasks, with limited access to data and resources. For more\ndetails on DeiT, see the \nand .",
            "markdown"
        ]
    ],
    "torch->Image and Video->Optimizing Vision Transformer Model for Deployment->Classifying Images with DeiT": [
        [
            "Follow the README at the DeiT repo for detailed information on how to\nclassify images using DeiT, or for a quick test, first install the\nrequired packages:",
            "markdown"
        ],
        [
            "# pip install torch torchvision timm pandas requests",
            "code"
        ],
        [
            "To run in Google Colab, uncomment the following line:",
            "markdown"
        ],
        [
            "# !pip install timm pandas requests",
            "code"
        ],
        [
            "then run the script below:",
            "markdown"
        ],
        [
            "from PIL import Image\nimport torch\nimport timm\nimport requests\nimport torchvision.transforms as transforms\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\nprint(torch.__version__)\n# should be 1.8.0\n\n\nmodel = ('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n()\n\n = ([\n    (256, interpolation=3),\n    (224),\n    (),\n    (IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n])\n\n = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n = ()[None,]\n = model()\n = ()\nprint(.item())",
            "code"
        ],
        [
            "2.0.0+cu117\nDownloading: \"https://github.com/facebookresearch/deit/zipball/main\" to /var/lib/jenkins/.cache/torch/hub/main.zip\nDownloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /var/lib/jenkins/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n\n  0%|          | 0.00/330M [00:00&lt;?, ?B/s]\n  0%|          | 56.0k/330M [00:00&lt;10:58, 526kB/s]\n  0%|          | 256k/330M [00:00&lt;04:20, 1.33MB/s]\n  0%|          | 1.13M/330M [00:00&lt;01:15, 4.57MB/s]\n  1%|1         | 4.59M/330M [00:00&lt;00:21, 16.0MB/s]\n  4%|3         | 12.4M/330M [00:00&lt;00:09, 36.3MB/s]\n  6%|6         | 19.9M/330M [00:00&lt;00:06, 47.0MB/s]\n  8%|8         | 27.5M/330M [00:00&lt;00:05, 55.1MB/s]\n 11%|#         | 35.3M/330M [00:00&lt;00:05, 59.8MB/s]\n 13%|#2        | 42.9M/330M [00:01&lt;00:04, 62.9MB/s]\n 15%|#5        | 50.7M/330M [00:01&lt;00:04, 64.7MB/s]\n 18%|#7        | 58.4M/330M [00:01&lt;00:04, 67.6MB/s]\n 20%|#9        | 65.9M/330M [00:01&lt;00:04, 68.2MB/s]\n 22%|##2       | 73.6M/330M [00:01&lt;00:03, 68.7MB/s]\n 25%|##4       | 81.4M/330M [00:01&lt;00:03, 69.3MB/s]\n 27%|##6       | 89.0M/330M [00:01&lt;00:03, 68.8MB/s]\n 29%|##9       | 96.5M/330M [00:01&lt;00:03, 66.1MB/s]\n 32%|###1      | 104M/330M [00:01&lt;00:03, 66.9MB/s]\n 34%|###3      | 112M/330M [00:02&lt;00:03, 68.4MB/s]\n 36%|###5      | 118M/330M [00:02&lt;00:03, 65.9MB/s]\n 38%|###8      | 126M/330M [00:02&lt;00:03, 66.8MB/s]\n 41%|####      | 134M/330M [00:02&lt;00:03, 68.1MB/s]\n 43%|####2     | 142M/330M [00:02&lt;00:02, 67.2MB/s]\n 45%|####5     | 149M/330M [00:02&lt;00:02, 67.7MB/s]\n 47%|####7     | 156M/330M [00:02&lt;00:02, 65.5MB/s]\n 49%|####9     | 162M/330M [00:02&lt;00:03, 54.0MB/s]\n 51%|#####     | 168M/330M [00:03&lt;00:03, 54.9MB/s]\n 53%|#####3    | 176M/330M [00:03&lt;00:02, 59.3MB/s]\n 55%|#####5    | 183M/330M [00:03&lt;00:02, 62.6MB/s]\n 58%|#####7    | 191M/330M [00:03&lt;00:02, 63.2MB/s]\n 60%|#####9    | 198M/330M [00:03&lt;00:02, 65.3MB/s]\n 62%|######1   | 204M/330M [00:03&lt;00:02, 60.0MB/s]\n 65%|######4   | 213M/330M [00:03&lt;00:01, 66.8MB/s]\n 67%|######6   | 221M/330M [00:03&lt;00:01, 68.0MB/s]\n 69%|######9   | 229M/330M [00:03&lt;00:01, 69.0MB/s]\n 71%|#######1  | 236M/330M [00:04&lt;00:01, 69.2MB/s]\n 74%|#######3  | 244M/330M [00:04&lt;00:01, 67.8MB/s]\n 76%|#######5  | 251M/330M [00:04&lt;00:01, 67.4MB/s]\n 78%|#######8  | 258M/330M [00:04&lt;00:01, 67.6MB/s]\n 81%|########  | 266M/330M [00:04&lt;00:01, 67.4MB/s]\n 83%|########2 | 274M/330M [00:04&lt;00:00, 68.4MB/s]\n 85%|########5 | 281M/330M [00:04&lt;00:00, 68.4MB/s]\n 87%|########7 | 289M/330M [00:04&lt;00:00, 69.6MB/s]\n 90%|########9 | 297M/330M [00:05&lt;00:00, 69.9MB/s]\n 92%|#########2| 304M/330M [00:05&lt;00:00, 69.0MB/s]\n 94%|#########4| 312M/330M [00:05&lt;00:00, 69.6MB/s]\n 97%|#########6| 319M/330M [00:05&lt;00:00, 69.4MB/s]\n 99%|#########9| 327M/330M [00:05&lt;00:00, 70.3MB/s]\n100%|##########| 330M/330M [00:05&lt;00:00, 63.0MB/s]\n269",
            "code"
        ],
        [
            "The output should be 269, which, according to the ImageNet list of class\nindex to , maps to \u2018timber\nwolf, grey wolf, gray wolf, Canis lupus\u2019.",
            "markdown"
        ],
        [
            "Now that we have verified that we can use the DeiT model to classify\nimages, let\u2019s see how to modify the model so it can run on iOS and\nAndroid apps.",
            "markdown"
        ]
    ],
    "torch->Image and Video->Optimizing Vision Transformer Model for Deployment->Scripting DeiT": [
        [
            "To use the model on mobile, we first need to script the\nmodel. See the  for a\nquick overview. Run the code below to convert the DeiT model used in the\nprevious step to the TorchScript format that can run on mobile.",
            "markdown"
        ],
        [
            "model = ('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n()\nscripted_model = (model)\n(\"fbdeit_scripted.pt\")",
            "code"
        ],
        [
            "Using cache found in /var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main",
            "code"
        ],
        [
            "The scripted model file fbdeit_scripted.pt of size about 346MB is\ngenerated.",
            "markdown"
        ]
    ],
    "torch->Image and Video->Optimizing Vision Transformer Model for Deployment->Quantizing DeiT": [
        [
            "To reduce the trained model size significantly while\nkeeping the inference accuracy about the same, quantization can be\napplied to the model. Thanks to the transformer model used in DeiT, we\ncan easily apply dynamic-quantization to the model, because dynamic\nquantization works best for LSTM and transformer models (see \nfor more details).",
            "markdown"
        ],
        [
            "Now run the code below:",
            "markdown"
        ],
        [
            "# Use 'x86' for server inference (the old 'fbgemm' is still available but 'x86' is the recommended default) and 'qnnpack' for mobile inference.\nbackend = \"x86\" # replaced with qnnpack causing much worse inference speed for quantized model on this notebook\n = torch.quantization.get_default_qconfig(backend)\ntorch.backends.quantized.engine = backend\n\nquantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={}, dtype=)\nscripted_quantized_model = (quantized_model)\n(\"fbdeit_scripted_quantized.pt\")",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning:\n\nPlease use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.",
            "code"
        ],
        [
            "This generates the scripted and quantized version of the model\nfbdeit_quantized_scripted.pt, with size about 89MB, a 74% reduction of\nthe non-quantized model size of 346MB!",
            "markdown"
        ],
        [
            "You can use the scripted_quantized_model to generate the same\ninference result:",
            "markdown"
        ],
        [
            " = scripted_quantized_model()\n = ()\nprint(.item())\n# The same output 269 should be printed",
            "code"
        ],
        [
            "269",
            "code"
        ]
    ],
    "torch->Image and Video->Optimizing Vision Transformer Model for Deployment->Optimizing DeiT": [
        [
            "The final step before using the quantized and scripted\nmodel on mobile is to optimize it:",
            "markdown"
        ],
        [
            "from torch.utils.mobile_optimizer import \noptimized_scripted_quantized_model = (scripted_quantized_model)\n(\"fbdeit_optimized_scripted_quantized.pt\")",
            "code"
        ],
        [
            "The generated fbdeit_optimized_scripted_quantized.pt file has about the\nsame size as the quantized, scripted, but non-optimized model. The\ninference result remains the same.",
            "markdown"
        ],
        [
            " = optimized_scripted_quantized_model()\n = ()\nprint(.item())\n# Again, the same output 269 should be printed",
            "code"
        ],
        [
            "269",
            "code"
        ]
    ],
    "torch->Image and Video->Optimizing Vision Transformer Model for Deployment->Using Lite Interpreter": [
        [
            "To see how much model size reduction and inference speed up the Lite\nInterpreter can result in, let\u2019s create the lite version of the model.",
            "markdown"
        ],
        [
            "optimized_scripted_quantized_model._save_for_lite_interpreter(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\nptl = (\"fbdeit_optimized_scripted_quantized_lite.ptl\")",
            "code"
        ],
        [
            "Although the lite model size is comparable to the non-lite version, when\nrunning the lite version on mobile, the inference speed up is expected.",
            "markdown"
        ]
    ],
    "torch->Image and Video->Optimizing Vision Transformer Model for Deployment->Comparing Inference Speed": [
        [
            "To see how the inference speed differs for the four models - the\noriginal model, the scripted model, the quantized-and-scripted model,\nthe optimized-quantized-and-scripted model - run the code below:",
            "markdown"
        ],
        [
            "with (use_cuda=False) as :\n     = model()\nwith (use_cuda=False) as :\n     = scripted_model()\nwith (use_cuda=False) as :\n     = scripted_quantized_model()\nwith (use_cuda=False) as :\n     = optimized_scripted_quantized_model()\nwith (use_cuda=False) as :\n     = ptl()\n\nprint(\"original model: {:.2f}ms\".format(/1000))\nprint(\"scripted model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized &amp; optimized model: {:.2f}ms\".format(/1000))\nprint(\"lite model: {:.2f}ms\".format(/1000))",
            "code"
        ],
        [
            "original model: 309.06ms\nscripted model: 335.36ms\nscripted &amp; quantized model: 240.60ms\nscripted &amp; quantized &amp; optimized model: 201.37ms\nlite model: 206.63ms",
            "code"
        ],
        [
            "The results running on a Google Colab are:",
            "markdown"
        ],
        [
            "original model: 1236.69ms\nscripted model: 1226.72ms\nscripted &amp; quantized model: 593.19ms\nscripted &amp; quantized &amp; optimized model: 598.01ms\nlite model: 600.72ms",
            "code"
        ],
        [
            "The following results summarize the inference time taken by each model\nand the percentage reduction of each model relative to the original\nmodel.",
            "markdown"
        ],
        [
            "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Model': ['original model','scripted model', 'scripted &amp; quantized model', 'scripted &amp; quantized &amp; optimized model', 'lite model']})\ndf = pd.concat([df, pd.DataFrame([\n    [\"{:.2f}ms\".format(/1000), \"0%\"],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)]],\n    columns=['Inference Time', 'Reduction'])], axis=1)\n\nprint(df)\n\n\"\"\"\n        Model                             Inference Time    Reduction\n0   original model                             1236.69ms           0%\n1   scripted model                             1226.72ms        0.81%\n2   scripted &amp; quantized model                  593.19ms       52.03%\n3   scripted &amp; quantized &amp; optimized model      598.01ms       51.64%\n4   lite model                                  600.72ms       51.43%\n\"\"\"",
            "code"
        ],
        [
            "                                    Model Inference Time Reduction\n0                          original model       309.06ms        0%\n1                          scripted model       335.36ms    -8.51%\n2              scripted &amp; quantized model       240.60ms    22.15%\n3  scripted &amp; quantized &amp; optimized model       201.37ms    34.84%\n4                              lite model       206.63ms    33.14%\n\n'\\n        Model                             Inference Time    Reduction\\n0\\toriginal model                             1236.69ms           0%\\n1\\tscripted model                             1226.72ms        0.81%\\n2\\tscripted &amp; quantized model                  593.19ms       52.03%\\n3\\tscripted &amp; quantized &amp; optimized model      598.01ms       51.64%\\n4\\tlite model                                  600.72ms       51.43%\\n'",
            "code"
        ]
    ],
    "torch->Image and Video->Optimizing Vision Transformer Model for Deployment->Comparing Inference Speed->Learn More": [
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  26.562 seconds)",
            "markdown"
        ]
    ],
    "torch->Audio->Audio I/O": [
        [
            "This tutorial has been moved to ",
            "markdown"
        ],
        [
            "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_io_tutorial.html'\" http-equiv=\"Refresh\"/>",
            "markdown"
        ]
    ],
    "torch->Audio->Audio Resampling": [
        [
            "This tutorial has been moved to \nYou will be redirected in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_resampling_tutorial.html'\" http-equiv=\"Refresh\"/>",
            "markdown"
        ]
    ],
    "torch->Audio->Audio Data Augmentation": [
        [
            "This tutorial has been moved to ",
            "markdown"
        ],
        [
            "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_data_augmentation_tutorial.html'\" http-equiv=\"Refresh\"/>",
            "markdown"
        ]
    ],
    "torch->Audio->Audio Feature Extractions": [
        [
            "This tutorial has been moved to ",
            "markdown"
        ],
        [
            "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html'\" http-equiv=\"Refresh\"/>",
            "markdown"
        ]
    ],
    "torch->Audio->Audio Feature Augmentation": [
        [
            "This tutorial has been moved to ",
            "markdown"
        ],
        [
            "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_data_augmentation_tutorial.html'\" http-equiv=\"Refresh\"/>",
            "markdown"
        ]
    ],
    "torch->Audio->Audio Datasets": [
        [
            "This tutorial has been moved to ",
            "markdown"
        ],
        [
            "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/tutorials/beginner/audio_datasets_tutorial.html'\" http-equiv=\"Refresh\"/>",
            "markdown"
        ]
    ],
    "torch->Audio->Speech Recognition with Wav2Vec2": [
        [
            "This tutorial has been moved to ",
            "markdown"
        ],
        [
            "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html'\" http-equiv=\"Refresh\"/>",
            "markdown"
        ]
    ],
    "torch->Audio->Text-to-speech with Tacotron2": [
        [
            "This tutorial has been moved to ",
            "markdown"
        ],
        [
            "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/tacotron2_pipeline_tutorial.html'\" http-equiv=\"Refresh\"/>",
            "markdown"
        ]
    ],
    "torch->Audio->Forced Alignment with Wav2Vec2": [
        [
            "This tutorial has been moved to ",
            "markdown"
        ],
        [
            "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/forced_alignment_tutorial.html'\" http-equiv=\"Refresh\"/>",
            "markdown"
        ]
    ],
    "torch->Text->Language Modeling with nn.Transformer and TorchText": [
        [
            "This is a tutorial on training a sequence-to-sequence model that uses the\n module.",
            "markdown"
        ],
        [
            "The PyTorch 1.2 release includes a standard transformer module based on the\npaper .\nCompared to Recurrent Neural Networks (RNNs), the transformer model has proven\nto be superior in quality for many sequence-to-sequence tasks while being more\nparallelizable. The nn.Transformer module relies entirely on an attention\nmechanism (implemented as\n)\nto draw global dependencies between input and output. The nn.Transformer\nmodule is highly modularized such that a single component (e.g.,\n)\ncan be easily adapted/composed.\n<img alt=\"../_images/transformer_architecture.jpg\" src=\"../_images/transformer_architecture.jpg\"/>",
            "markdown"
        ]
    ],
    "torch->Text->Language Modeling with nn.Transformer and TorchText->Define the model": [
        [
            "In this tutorial, we train a nn.TransformerEncoder model on a\nlanguage modeling task. The language modeling task is to assign a\nprobability for the likelihood of a given word (or a sequence of words)\nto follow a sequence of words. A sequence of tokens are passed to the embedding\nlayer first, followed by a positional encoding layer to account for the order\nof the word (see the next paragraph for more details). The\nnn.TransformerEncoder consists of multiple layers of\n.\nAlong with the input sequence, a square attention mask is required because the\nself-attention layers in nn.TransformerEncoder are only allowed to attend\nthe earlier positions in the sequence. For the language modeling task, any\ntokens on the future positions should be masked. To produce a probability\ndistribution over output words, the output of the nn.TransformerEncoder\nmodel is passed through a linear layer followed by a log-softmax function.",
            "markdown"
        ],
        [
            "import math\nimport os\nfrom tempfile import TemporaryDirectory\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn, \nimport torch.nn.functional as F\nfrom torch.nn import , \nfrom torch.utils.data import dataset\n\nclass TransformerModel():\n\n    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n                 nlayers: int, dropout: float = 0.5):\n        super().__init__()\n        self.model_type = 'Transformer'\n        self.pos_encoder = (d_model, dropout)\n        encoder_layers = (d_model, nhead, d_hid, dropout)\n        self.transformer_encoder = (encoder_layers, nlayers)\n        self.encoder = (ntoken, d_model)\n        self.d_model = d_model\n        self.decoder = (d_model, ntoken)\n\n        self.init_weights()\n\n    def init_weights(self) -&gt; None:\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src: , src_mask: ) -&gt; :\n        \"\"\"\n        Args:\n            src: Tensor, shape [seq_len, batch_size]\n            src_mask: Tensor, shape [seq_len, seq_len]\n\n        Returns:\n            output Tensor of shape [seq_len, batch_size, ntoken]\n        \"\"\"\n        src = self.encoder(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, src_mask)\n        output = self.decoder(output)\n        return output\n\n\ndef generate_square_subsequent_mask(sz: int) -&gt; :\n    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n    return ((sz, sz) * float('-inf'), diagonal=1)",
            "code"
        ],
        [
            "PositionalEncoding module injects some information about the\nrelative or absolute position of the tokens in the sequence. The\npositional encodings have the same dimension as the embeddings so that\nthe two can be summed. Here, we use sine and cosine functions of\ndifferent frequencies.",
            "markdown"
        ],
        [
            "class PositionalEncoding():\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = (p=dropout)\n\n        position = (max_len).unsqueeze(1)\n        div_term = ((0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = (max_len, 1, d_model)\n        pe[:, 0, 0::2] = (position * div_term)\n        pe[:, 0, 1::2] = (position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: ) -&gt; :\n        \"\"\"\n        Args:\n            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)",
            "code"
        ]
    ],
    "torch->Text->Language Modeling with nn.Transformer and TorchText->Load and batch data": [
        [
            "This tutorial uses torchtext to generate Wikitext-2 dataset.\nTo access torchtext datasets, please install torchdata following instructions at .\n%%\n<blockquote>",
            "markdown"
        ],
        [
            "%%bash\npip install torchdata\n\n\n</blockquote>",
            "code"
        ],
        [
            "The vocab object is built based on the train dataset and is used to numericalize\ntokens into tensors. Wikitext-2 represents rare tokens as <cite>&lt;unk&gt;</cite>.",
            "markdown"
        ],
        [
            "Given a 1-D vector of sequential data, batchify() arranges the data\ninto batch_size columns. If the data does not divide evenly into\nbatch_size columns, then the data is trimmed to fit. For instance, with\nthe alphabet as the data (total length of 26) and batch_size=4, we would\ndivide the alphabet into 4 sequences of length 6:\n\n\\[\\begin{bmatrix}\n\\text{A} &amp; \\text{B} &amp; \\text{C} &amp; \\ldots &amp; \\text{X} &amp; \\text{Y} &amp; \\text{Z}\n\\end{bmatrix}\n\\Rightarrow\n\\begin{bmatrix}\n\\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n\\end{bmatrix}\n\n\\]",
            "markdown"
        ],
        [
            "Batching enables more parallelizable processing. However, batching means that\nthe model treats each column independently; for example, the dependence of\nG and F can not be learned in the example above.",
            "markdown"
        ],
        [
            "from torchtext.datasets import \nfrom torchtext.data.utils import \nfrom torchtext.vocab import \n\ntrain_iter = (split='train')\ntokenizer = ('basic_english')\n = (map(tokenizer, train_iter), specials=['&lt;unk&gt;'])\n(['&lt;unk&gt;'])\n\ndef data_process(raw_text_iter: ) -&gt; :\n    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n    data = [((tokenizer(item)), dtype=) for item in raw_text_iter]\n    return (tuple(filter(lambda t: t.numel() &gt; 0, data)))\n\n# train_iter was \"consumed\" by the process of building the vocab,\n# so we have to create it again\ntrain_iter, val_iter, test_iter = ()\n = data_process(train_iter)\n = data_process(val_iter)\n = data_process(test_iter)\n\n = ('cuda' if () else 'cpu')\n\ndef batchify(data: , bsz: int) -&gt; :\n    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n    that wouldn't cleanly fit.\n\n    Args:\n        data: Tensor, shape [N]\n        bsz: int, batch size\n\n    Returns:\n        Tensor of shape [N // bsz, bsz]\n    \"\"\"\n    seq_len = data.size(0) // bsz\n    data = data[:seq_len * bsz]\n    data = data.view(bsz, seq_len).t().contiguous()\n    return data.to()\n\nbatch_size = 20\neval_batch_size = 10\n = batchify(, batch_size)  # shape [seq_len, batch_size]\n = batchify(, eval_batch_size)\n = batchify(, eval_batch_size)",
            "code"
        ]
    ],
    "torch->Text->Language Modeling with nn.Transformer and TorchText->Load and batch data->Functions to generate input and target sequence": [
        [
            "get_batch() generates a pair of input-target sequences for\nthe transformer model. It subdivides the source data into chunks of\nlength bptt. For the language modeling task, the model needs the\nfollowing words as Target. For example, with a bptt value of 2,\nwe\u2019d get the following two Variables for i = 0:\n<img alt=\"../_images/transformer_input_target.png\" src=\"../_images/transformer_input_target.png\"/>",
            "markdown"
        ],
        [
            "It should be noted that the chunks are along dimension 0, consistent\nwith the S dimension in the Transformer model. The batch dimension\nN is along dimension 1.",
            "markdown"
        ],
        [
            "bptt = 35\ndef get_batch(source: , i: int) -&gt; Tuple[, ]:\n    \"\"\"\n    Args:\n        source: Tensor, shape [full_seq_len, batch_size]\n        i: int\n\n    Returns:\n        tuple (data, target), where data has shape [seq_len, batch_size] and\n        target has shape [seq_len * batch_size]\n    \"\"\"\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].reshape(-1)\n    return data, target",
            "code"
        ]
    ],
    "torch->Text->Language Modeling with nn.Transformer and TorchText->Initiate an instance": [
        [
            "The model hyperparameters are defined below. The vocab size is\nequal to the length of the vocab object.",
            "markdown"
        ],
        [
            "ntokens = len()  # size of vocabulary\nemsize = 200  # embedding dimension\nd_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 2  # number of heads in nn.MultiheadAttention\ndropout = 0.2  # dropout probability\nmodel = (ntokens, emsize, nhead, d_hid, nlayers, dropout).to()",
            "code"
        ]
    ],
    "torch->Text->Language Modeling with nn.Transformer and TorchText->Run the model": [
        [
            "We use \nwith the \n(stochastic gradient descent) optimizer. The learning rate is initially set to\n5.0 and follows a \nschedule. During training, we use \nto prevent gradients from exploding.",
            "markdown"
        ],
        [
            "import copy\nimport time\n\n = ()\nlr = 5.0  # learning rate\n = ((), lr=lr)\n = (, 1.0, gamma=0.95)\n\ndef train(model: ) -&gt; None:\n    ()  # turn on train mode\n    total_loss = 0.\n    log_interval = 200\n    start_time = time.time()\n    src_mask = generate_square_subsequent_mask(bptt).to()\n\n    num_batches = len() // bptt\n    for batch, i in enumerate(range(0, .size(0) - 1, bptt)):\n        data, targets = get_batch(, i)\n        seq_len = data.size(0)\n        if seq_len != bptt:  # only on last batch\n            src_mask = src_mask[:seq_len, :seq_len]\n        output = model(data, src_mask)\n        loss = (output.view(-1, ntokens), targets)\n\n        ()\n        loss.backward()\n        ((), 0.5)\n        .step()\n\n        total_loss += loss.item()\n        if batch % log_interval == 0 and batch &gt; 0:\n            lr = ()[0]\n            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n            cur_loss = total_loss / log_interval\n            ppl = math.exp(cur_loss)\n            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n            total_loss = 0\n            start_time = time.time()\n\ndef evaluate(model: , eval_data: ) -&gt; float:\n    ()  # turn on evaluation mode\n    total_loss = 0.\n    src_mask = generate_square_subsequent_mask(bptt).to()\n    with ():\n        for i in range(0, eval_data.size(0) - 1, bptt):\n            data, targets = get_batch(eval_data, i)\n            seq_len = data.size(0)\n            if seq_len != bptt:\n                src_mask = src_mask[:seq_len, :seq_len]\n            output = model(data, src_mask)\n            output_flat = output.view(-1, ntokens)\n            total_loss += seq_len * (output_flat, targets).item()\n    return total_loss / (len(eval_data) - 1)",
            "code"
        ],
        [
            "Loop over epochs. Save the model if the validation loss is the best\nwe\u2019ve seen so far. Adjust the learning rate after each epoch.",
            "markdown"
        ],
        [
            "best_val_loss = float('inf')\nepochs = 3\n\nwith TemporaryDirectory() as tempdir:\n    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n\n    for epoch in range(1, epochs + 1):\n        epoch_start_time = time.time()\n        train(model)\n        val_loss = evaluate(model, )\n        val_ppl = math.exp(val_loss)\n        elapsed = time.time() - epoch_start_time\n        print('-' * 89)\n        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n        print('-' * 89)\n\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            ((), best_model_params_path)\n\n        .step()\n    ((best_model_params_path)) # load best model states",
            "code"
        ],
        [
            "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 31.17 | loss  8.07 | ppl  3197.87\n| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 22.10 | loss  6.87 | ppl   960.80\n| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 22.12 | loss  6.44 | ppl   624.70\n| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 22.06 | loss  6.29 | ppl   541.16\n| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 22.14 | loss  6.19 | ppl   489.94\n| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 22.16 | loss  6.15 | ppl   469.48\n| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 22.10 | loss  6.12 | ppl   453.14\n| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 22.10 | loss  6.11 | ppl   452.01\n| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 22.12 | loss  6.03 | ppl   415.23\n| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 22.07 | loss  6.03 | ppl   414.24\n| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 22.10 | loss  5.90 | ppl   366.14\n| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 22.09 | loss  5.97 | ppl   391.54\n| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 22.08 | loss  5.95 | ppl   384.65\n| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 22.08 | loss  5.89 | ppl   359.97\n-----------------------------------------------------------------------------------------\n| end of epoch   1 | time: 69.80s | valid loss  5.84 | valid ppl   344.17\n-----------------------------------------------------------------------------------------\n| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 22.21 | loss  5.86 | ppl   351.29\n| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 22.08 | loss  5.85 | ppl   346.20\n| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 22.12 | loss  5.67 | ppl   289.41\n| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 22.08 | loss  5.70 | ppl   300.23\n| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 22.08 | loss  5.66 | ppl   286.99\n| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 22.14 | loss  5.69 | ppl   296.29\n| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 22.11 | loss  5.69 | ppl   296.85\n| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 22.11 | loss  5.71 | ppl   303.34\n| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 22.11 | loss  5.66 | ppl   286.84\n| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 22.08 | loss  5.67 | ppl   290.62\n| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 22.13 | loss  5.56 | ppl   258.76\n| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 22.10 | loss  5.65 | ppl   283.83\n| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 22.08 | loss  5.65 | ppl   283.45\n| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 22.05 | loss  5.58 | ppl   264.29\n-----------------------------------------------------------------------------------------\n| end of epoch   2 | time: 67.98s | valid loss  5.65 | valid ppl   285.44\n-----------------------------------------------------------------------------------------\n| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 22.20 | loss  5.60 | ppl   270.05\n| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 22.12 | loss  5.61 | ppl   274.27\n| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 22.12 | loss  5.41 | ppl   224.12\n| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 22.09 | loss  5.48 | ppl   239.00\n| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 22.08 | loss  5.43 | ppl   227.12\n| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 22.08 | loss  5.47 | ppl   237.65\n| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 22.09 | loss  5.49 | ppl   241.54\n| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 22.08 | loss  5.51 | ppl   248.00\n| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 22.09 | loss  5.46 | ppl   235.99\n| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 22.14 | loss  5.48 | ppl   239.26\n| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 22.11 | loss  5.35 | ppl   210.16\n| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 22.14 | loss  5.46 | ppl   234.99\n| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 22.19 | loss  5.47 | ppl   237.56\n| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 22.23 | loss  5.40 | ppl   222.09\n-----------------------------------------------------------------------------------------\n| end of epoch   3 | time: 68.06s | valid loss  5.62 | valid ppl   275.14\n-----------------------------------------------------------------------------------------",
            "code"
        ]
    ],
    "torch->Text->Language Modeling with nn.Transformer and TorchText->Evaluate the best model on the test dataset": [
        [
            "test_loss = evaluate(model, )\ntest_ppl = math.exp(test_loss)\nprint('=' * 89)\nprint(f'| End of training | test loss {test_loss:5.2f} | '\n      f'test ppl {test_ppl:8.2f}')\nprint('=' * 89)",
            "code"
        ],
        [
            "=========================================================================================\n| End of training | test loss  5.52 | test ppl   250.61\n=========================================================================================",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 3 minutes  41.329 seconds)",
            "markdown"
        ]
    ],
    "torch->Text->Fast Transformer Inference with Better Transformer": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "This tutorial introduces Better Transformer (BT) as part of the PyTorch 1.12 release.\nIn this tutorial, we show how to use Better Transformer for production\ninference with torchtext.  Better Transformer is a production ready fastpath to\naccelerate deployment of Transformer models with high performance on CPU and GPU.\nThe fastpath feature works transparently for models based either directly on\nPyTorch core nn.module or with torchtext.",
            "markdown"
        ],
        [
            "Models which can be accelerated by Better Transformer fastpath execution are those\nusing the following PyTorch core <cite>torch.nn.module</cite> classes <cite>TransformerEncoder</cite>,\n<cite>TransformerEncoderLayer</cite>, and <cite>MultiHeadAttention</cite>.  In addition, torchtext has\nbeen updated to use the core library modules to benefit from fastpath acceleration.\n(Additional modules may be enabled with fastpath execution in the future.)",
            "markdown"
        ],
        [
            "Better Transformer offers two types of acceleration:",
            "markdown"
        ],
        [
            "Native multihead attention (MHA) implementation for CPU and GPU to improve overall execution efficiency.",
            "markdown"
        ],
        [
            "Exploiting sparsity in NLP inference.  Because of variable input lengths, input\ntokens may contain a large number of padding tokens for which processing may be\nskipped, delivering significant speedups.",
            "markdown"
        ],
        [
            "Fastpath execution is subject to some criteria. Most importantly, the model\nmust be executed in inference mode and operate on input tensors that do not collect\ngradient tape information (e.g., running with torch.no_grad).",
            "markdown"
        ],
        [
            "To follow this example in Google Colab, .",
            "markdown"
        ]
    ],
    "torch->Text->Fast Transformer Inference with Better Transformer->Better Transformer Features in This Tutorial": [
        [
            "Load pre-trained models (pre-1.12 created without Better Transformer)",
            "markdown"
        ],
        [
            "Run and benchmark inference on CPU with and without BT fastpath (native MHA only)",
            "markdown"
        ],
        [
            "Run and benchmark inference on (configurable) DEVICE with and without BT fastpath (native MHA only)",
            "markdown"
        ],
        [
            "Enable sparsity support",
            "markdown"
        ],
        [
            "Run and benchmark inference on (configurable) DEVICE with and without BT fastpath (native MHA + sparsity)",
            "markdown"
        ]
    ],
    "torch->Text->Fast Transformer Inference with Better Transformer->Additional Information": [
        [
            "Additional information about Better Transformer may be found in the PyTorch.Org blog\n.",
            "markdown"
        ],
        [
            "Setup",
            "markdown"
        ],
        [
            "1.1 Load pre-trained models",
            "markdown"
        ],
        [
            "We download the XLM-R model from the pre-defined torchtext models by following the instructions in\n.  We also set the DEVICE to execute\non-accelerator tests.  (Enable GPU execution for your environment as appropriate.)",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\n\nprint(f\"torch version: {torch.__version__}\")\n\nDEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nprint(f\"torch cuda available: {torch.cuda.is_available()}\")\n\nimport torch, torchtext\nfrom torchtext.models import RobertaClassificationHead\nfrom torchtext.functional import to_tensor\nxlmr_large = torchtext.models.XLMR_LARGE_ENCODER\nclassifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\nmodel = xlmr_large.get_model(head=classifier_head)\ntransform = xlmr_large.transform()",
            "code"
        ],
        [
            "1.2 Dataset Setup",
            "markdown"
        ],
        [
            "We set up two types of inputs: a small input batch and a big input batch with sparsity.",
            "markdown"
        ],
        [
            "small_input_batch = [\n               \"Hello world\",\n               \"How are you!\"\n]\nbig_input_batch = [\n               \"Hello world\",\n               \"How are you!\",\n               \"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\nBuonapartes. But I warn you, if you don't tell me that this means war,\nif you still try to defend the infamies and horrors perpetrated by\nthat Antichrist- I really believe he is Antichrist- I will have\nnothing more to do with you and you are no longer my friend, no longer\nmy 'faithful slave,' as you call yourself! But how do you do? I see\nI have frightened you- sit down and tell me all the news.`\n\nIt was in July, 1805, and the speaker was the well-known Anna\nPavlovna Scherer, maid of honor and favorite of the Empress Marya\nFedorovna. With these words she greeted Prince Vasili Kuragin, a man\nof high rank and importance, who was the first to arrive at her\nreception. Anna Pavlovna had had a cough for some days. She was, as\nshe said, suffering from la grippe; grippe being then a new word in\nSt. Petersburg, used only by the elite.\"\"\"\n]",
            "code"
        ],
        [
            "Next, we select either the small or large input batch, preprocess the inputs and test the model.",
            "markdown"
        ],
        [
            "input_batch=big_input_batch\n\nmodel_input = to_tensor(transform(input_batch), padding_value=1)\noutput = model(model_input)\noutput.shape",
            "code"
        ],
        [
            "Finally, we set the benchmark iteration count:",
            "markdown"
        ],
        [
            "ITERATIONS=10",
            "code"
        ],
        [
            "Execution",
            "markdown"
        ],
        [
            "2.1  Run and benchmark inference on CPU with and without BT fastpath (native MHA only)",
            "markdown"
        ],
        [
            "We run the model on CPU, and collect profile information:",
            "markdown"
        ],
        [
            "The first run uses traditional (\u201cslow path\u201d) execution.",
            "markdown"
        ],
        [
            "The second run enables BT fastpath execution by putting the model in inference mode using <cite>model.eval()</cite> and disables gradient collection with <cite>torch.no_grad()</cite>.",
            "markdown"
        ],
        [
            "You can see an improvement (whose magnitude will depend on the CPU model) when the model is executing on CPU.  Notice that the fastpath profile shows most of the execution time\nin the native <cite>TransformerEncoderLayer</cite> implementation <cite>aten::_transformer_encoder_layer_fwd</cite>.",
            "markdown"
        ],
        [
            "print(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=False) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof)\n\nmodel.eval()\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=False) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof)",
            "code"
        ],
        [
            "2.2  Run and benchmark inference on (configurable) DEVICE with and without BT fastpath (native MHA only)",
            "markdown"
        ],
        [
            "We check the BT sparsity setting:",
            "markdown"
        ],
        [
            "model.encoder.transformer.layers.enable_nested_tensor",
            "code"
        ],
        [
            "We disable the BT sparsity:",
            "markdown"
        ],
        [
            "model.encoder.transformer.layers.enable_nested_tensor=False",
            "code"
        ],
        [
            "We run the model on DEVICE, and collect profile information for native MHA execution on DEVICE:",
            "markdown"
        ],
        [
            "The first run uses traditional (\u201cslow path\u201d) execution.",
            "markdown"
        ],
        [
            "The second run enables BT fastpath execution by putting the model in inference mode using <cite>model.eval()</cite>\nand disables gradient collection with <cite>torch.no_grad()</cite>.",
            "markdown"
        ],
        [
            "When executing on a GPU, you should see a significant speedup, in particular for the small input batch setting:",
            "markdown"
        ],
        [
            "model.to(DEVICE)\nmodel_input = model_input.to(DEVICE)\n\nprint(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof)\n\nmodel.eval()\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof)",
            "code"
        ],
        [
            "2.3 Run and benchmark inference on (configurable) DEVICE with and without BT fastpath (native MHA + sparsity)",
            "markdown"
        ],
        [
            "We enable sparsity support:",
            "markdown"
        ],
        [
            "model.encoder.transformer.layers.enable_nested_tensor = True",
            "code"
        ],
        [
            "We run the model on DEVICE, and collect profile information for native MHA and sparsity support execution on DEVICE:",
            "markdown"
        ],
        [
            "The first run uses traditional (\u201cslow path\u201d) execution.",
            "markdown"
        ],
        [
            "The second run enables BT fastpath execution by putting the model in inference mode using <cite>model.eval()</cite> and disables gradient collection with <cite>torch.no_grad()</cite>.",
            "markdown"
        ],
        [
            "When executing on a GPU, you should see a significant speedup, in particular for the large input batch setting which includes sparsity:",
            "markdown"
        ],
        [
            "model.to(DEVICE)\nmodel_input = model_input.to(DEVICE)\n\nprint(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof)\n\nmodel.eval()\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof)",
            "code"
        ]
    ],
    "torch->Text->Fast Transformer Inference with Better Transformer->Summary": [
        [
            "In this tutorial, we have introduced fast transformer inference with\nBetter Transformer fastpath execution in torchtext using PyTorch core\nBetter Transformer support for Transformer Encoder models.  We have\ndemonstrated the use of Better Transformer with models trained prior to\nthe availability of BT fastpath execution.  We have demonstrated and\nbenchmarked the use of both BT fastpath execution modes, native MHA execution\nand BT sparsity acceleration.",
            "markdown"
        ]
    ],
    "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "We will be building and training a basic character-level RNN to classify\nwords. This tutorial, along with the following two, show how to do\npreprocess data for NLP modeling \u201cfrom scratch\u201d, in particular not using\nmany of the convenience functions of <cite>torchtext</cite>, so you can see how\npreprocessing for NLP modeling works at a low level.",
            "markdown"
        ],
        [
            "A character-level RNN reads words as a series of characters -\noutputting a prediction and \u201chidden state\u201d at each step, feeding its\nprevious hidden state into each next step. We take the final prediction\nto be the output, i.e. which class the word belongs to.",
            "markdown"
        ],
        [
            "Specifically, we\u2019ll train on a few thousand surnames from 18 languages\nof origin, and predict which language a name is from based on the\nspelling:",
            "markdown"
        ],
        [
            "$ python predict.py Hinton\n(-0.47) Scottish\n(-1.52) English\n(-3.57) Irish\n\n$ python predict.py Schmidhuber\n(-0.19) German\n(-2.48) Czech\n(-2.68) Dutch",
            "code"
        ],
        [
            "<strong>Recommended Reading:</strong>",
            "markdown"
        ],
        [
            "I assume you have at least installed PyTorch, know Python, and\nunderstand Tensors:",
            "markdown"
        ],
        [
            " For installation instructions",
            "markdown"
        ],
        [
            " to get started with PyTorch in general",
            "markdown"
        ],
        [
            " for a wide and deep overview",
            "markdown"
        ],
        [
            " if you are former Lua Torch user",
            "markdown"
        ],
        [
            "It would also be useful to know about RNNs and how they work:",
            "markdown"
        ],
        [
            "shows a bunch of real life examples",
            "markdown"
        ],
        [
            "is about LSTMs specifically but also informative about RNNs in\ngeneral",
            "markdown"
        ]
    ],
    "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Preparing the Data": [
        [
            "Note",
            "markdown"
        ],
        [
            "Download the data from\n\nand extract it to the current directory.",
            "markdown"
        ],
        [
            "Included in the data/names directory are 18 text files named as\n\u201c[Language].txt\u201d. Each file contains a bunch of names, one name per\nline, mostly romanized (but we still need to convert from Unicode to\nASCII).",
            "markdown"
        ],
        [
            "We\u2019ll end up with a dictionary of lists of names per language,\n{language: [names ...]}. The generic variables \u201ccategory\u201d and \u201cline\u201d\n(for language and name in our case) are used for later extensibility.",
            "markdown"
        ],
        [
            "from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport glob\nimport os\n\ndef findFiles(path): return glob.glob(path)\n\nprint(findFiles('data/names/*.txt'))\n\nimport unicodedata\nimport string\n\nall_letters = string.ascii_letters + \" .,;'\"\nn_letters = len(all_letters)\n\n# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in all_letters\n    )\n\nprint(unicodeToAscii('\u015alus\u00e0rski'))\n\n# Build the category_lines dictionary, a list of names per language\ncategory_lines = {}\nall_categories = []\n\n# Read a file and split into lines\ndef readLines(filename):\n    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n    return [unicodeToAscii(line) for line in lines]\n\nfor filename in findFiles('data/names/*.txt'):\n    category = os.path.splitext(os.path.basename(filename))[0]\n    all_categories.append(category)\n    lines = readLines(filename)\n    category_lines[category] = lines\n\nn_categories = len(all_categories)",
            "code"
        ],
        [
            "['data/names/Czech.txt', 'data/names/German.txt', 'data/names/Portuguese.txt', 'data/names/Russian.txt', 'data/names/Irish.txt', 'data/names/French.txt', 'data/names/Korean.txt', 'data/names/Arabic.txt', 'data/names/Vietnamese.txt', 'data/names/Italian.txt', 'data/names/Japanese.txt', 'data/names/English.txt', 'data/names/Polish.txt', 'data/names/Scottish.txt', 'data/names/Chinese.txt', 'data/names/Dutch.txt', 'data/names/Greek.txt', 'data/names/Spanish.txt']\nSlusarski",
            "code"
        ],
        [
            "Now we have category_lines, a dictionary mapping each category\n(language) to a list of lines (names). We also kept track of\nall_categories (just a list of languages) and n_categories for\nlater reference.",
            "markdown"
        ],
        [
            "print(category_lines['Italian'][:5])",
            "code"
        ],
        [
            "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']",
            "code"
        ]
    ],
    "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Preparing the Data->Turning Names into Tensors": [
        [
            "Now that we have all the names organized, we need to turn them into\nTensors to make any use of them.",
            "markdown"
        ],
        [
            "To represent a single letter, we use a \u201cone-hot vector\u201d of size\n&lt;1 x n_letters&gt;. A one-hot vector is filled with 0s except for a 1\nat index of the current letter, e.g. \"b\" = &lt;0 1 0 0 0 ...&gt;.",
            "markdown"
        ],
        [
            "To make a word we join a bunch of those into a 2D matrix\n&lt;line_length x 1 x n_letters&gt;.",
            "markdown"
        ],
        [
            "That extra 1 dimension is because PyTorch assumes everything is in\nbatches - we\u2019re just using a batch size of 1 here.",
            "markdown"
        ],
        [
            "import torch\n\n# Find letter index from all_letters, e.g. \"a\" = 0\ndef letterToIndex(letter):\n    return all_letters.find(letter)\n\n# Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensor\ndef letterToTensor(letter):\n    tensor = (1, n_letters)\n    tensor[0][letterToIndex(letter)] = 1\n    return tensor\n\n# Turn a line into a &lt;line_length x 1 x n_letters&gt;,\n# or an array of one-hot letter vectors\ndef lineToTensor(line):\n    tensor = (len(line), 1, n_letters)\n    for li, letter in enumerate(line):\n        tensor[li][0][letterToIndex(letter)] = 1\n    return tensor\n\nprint(letterToTensor('J'))\n\nprint(lineToTensor('Jones').size())",
            "code"
        ],
        [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0.]])\ntorch.Size([5, 1, 57])",
            "code"
        ]
    ],
    "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Creating the Network": [
        [
            "Before autograd, creating a recurrent neural network in Torch involved\ncloning the parameters of a layer over several timesteps. The layers\nheld hidden state and gradients which are now entirely handled by the\ngraph itself. This means you can implement a RNN in a very \u201cpure\u201d way,\nas regular feed-forward layers.",
            "markdown"
        ],
        [
            "This RNN module (mostly copied from )\nis just 2 linear layers which operate on an input and hidden state, with\na LogSoftmax layer after the output.\n\n<img alt=\"\" src=\"https://i.imgur.com/Z2xbySO.png\"/>",
            "markdown"
        ],
        [
            "import torch.nn as nn\n\nclass RNN():\n    def __init__(self, input_size, hidden_size, output_size):\n        super(, self).__init__()\n\n        self.hidden_size = hidden_size\n\n        self.i2h = (input_size + hidden_size, hidden_size)\n        self.i2o = (input_size + hidden_size, output_size)\n        self.softmax = (dim=1)\n\n    def forward(self, input, ):\n        combined = ((input, ), 1)\n         = self.i2h(combined)\n         = self.i2o(combined)\n         = self.softmax()\n        return , \n\n    def initHidden(self):\n        return (1, self.hidden_size)\n\nn_hidden = 128\nrnn = (n_letters, n_hidden, n_categories)",
            "code"
        ],
        [
            "To run a step of this network we need to pass an input (in our case, the\nTensor for the current letter) and a previous hidden state (which we\ninitialize as zeros at first). We\u2019ll get back the output (probability of\neach language) and a next hidden state (which we keep for the next\nstep).",
            "markdown"
        ],
        [
            "input = letterToTensor('A')\n = (1, n_hidden)\n\n,  = rnn(input, )",
            "code"
        ],
        [
            "For the sake of efficiency we don\u2019t want to be creating a new Tensor for\nevery step, so we will use lineToTensor instead of\nletterToTensor and use slices. This could be further optimized by\npre-computing batches of Tensors.",
            "markdown"
        ],
        [
            "input = lineToTensor('Albert')\n = (1, n_hidden)\n\n,  = rnn(input[0], )\nprint()",
            "code"
        ],
        [
            "tensor([[-2.9694, -2.9119, -2.9363, -2.7759, -2.8583, -2.8755, -2.9201, -2.9122,\n         -2.8318, -2.8369, -2.9570, -2.9842, -2.9026, -2.8321, -2.9084, -2.8505,\n         -2.8649, -2.9244]], grad_fn=&lt;LogSoftmaxBackward0&gt;)",
            "code"
        ],
        [
            "As you can see the output is a &lt;1 x n_categories&gt; Tensor, where\nevery item is the likelihood of that category (higher is more likely).",
            "markdown"
        ]
    ],
    "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Training->Preparing for Training": [
        [
            "Before going into training we should make a few helper functions. The\nfirst is to interpret the output of the network, which we know to be a\nlikelihood of each category. We can use Tensor.topk to get the index\nof the greatest value:",
            "markdown"
        ],
        [
            "def categoryFromOutput():\n    top_n, top_i = .topk(1)\n    category_i = top_i[0].item()\n    return all_categories[category_i], category_i\n\nprint(categoryFromOutput())",
            "code"
        ],
        [
            "('Russian', 3)",
            "code"
        ],
        [
            "We will also want a quick way to get a training example (a name and its\nlanguage):",
            "markdown"
        ],
        [
            "import random\n\ndef randomChoice(l):\n    return l[random.randint(0, len(l) - 1)]\n\ndef randomTrainingExample():\n    category = randomChoice(all_categories)\n    line = randomChoice(category_lines[category])\n     = ([all_categories.index(category)], dtype=)\n     = lineToTensor(line)\n    return category, line, , \n\nfor i in range(10):\n    category, line, ,  = randomTrainingExample()\n    print('category =', category, '/ line =', line)",
            "code"
        ],
        [
            "category = Irish / line = O'Mahony\ncategory = Dutch / line = Schoorl\ncategory = Dutch / line = Snyders\ncategory = Czech / line = Soukup\ncategory = Japanese / line = Takara\ncategory = Dutch / line = Richard\ncategory = Scottish / line = Walker\ncategory = Italian / line = Fonda\ncategory = Vietnamese / line = Dinh\ncategory = Dutch / line = Lauwers",
            "code"
        ]
    ],
    "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Training->Training the Network": [
        [
            "Now all it takes to train this network is show it a bunch of examples,\nhave it make guesses, and tell it if it\u2019s wrong.",
            "markdown"
        ],
        [
            "For the loss function nn.NLLLoss is appropriate, since the last\nlayer of the RNN is nn.LogSoftmax.",
            "markdown"
        ],
        [
            " = ()",
            "code"
        ],
        [
            "Each loop of training will:",
            "markdown"
        ],
        [
            "Create input and target tensors",
            "markdown"
        ],
        [
            "Create a zeroed initial hidden state",
            "markdown"
        ],
        [
            "Read each letter in and",
            "markdown"
        ],
        [
            "Keep hidden state for next letter",
            "markdown"
        ],
        [
            "Compare final output to target",
            "markdown"
        ],
        [
            "Back-propagate",
            "markdown"
        ],
        [
            "Return the output and loss",
            "markdown"
        ],
        [
            "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n\ndef train(, ):\n     = rnn.initHidden()\n\n    ()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    loss = (, )\n    loss.backward()\n\n    # Add parameters' gradients to their values, multiplied by learning rate\n    for p in ():\n        p.data.add_(p.grad.data, alpha=-learning_rate)\n\n    return , loss.item()",
            "code"
        ],
        [
            "Now we just have to run that with a bunch of examples. Since the\ntrain function returns both the output and loss we can print its\nguesses and also keep track of loss for plotting. Since there are 1000s\nof examples we print only every print_every examples, and take an\naverage of the loss.",
            "markdown"
        ],
        [
            "import time\nimport math\n\nn_iters = 100000\nprint_every = 5000\nplot_every = 1000\n\n\n\n# Keep track of losses for plotting\ncurrent_loss = 0\nall_losses = []\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\nstart = time.time()\n\nfor iter in range(1, n_iters + 1):\n    category, line, ,  = randomTrainingExample()\n    , loss = train(, )\n    current_loss += loss\n\n    # Print iter number, loss, name and guess\n    if iter % print_every == 0:\n        guess, guess_i = categoryFromOutput()\n        correct = '\u2713' if guess == category else '\u2717 (%s)' % category\n        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n\n    # Add current loss avg to list of losses\n    if iter % plot_every == 0:\n        all_losses.append(current_loss / plot_every)\n        current_loss = 0",
            "code"
        ],
        [
            "5000 5% (0m 5s) 1.9595 Giordano / Italian \u2713\n10000 10% (0m 11s) 2.1746 Lilly / English \u2713\n15000 15% (0m 18s) 2.2633 Mushanaokoji / Italian \u2717 (Japanese)\n20000 20% (0m 23s) 1.8027 Campbell / Scottish \u2713\n25000 25% (0m 29s) 3.9679 Paulis / Greek \u2717 (German)\n30000 30% (0m 35s) 1.0005 Filipek / Polish \u2713\n35000 35% (0m 41s) 1.6657 Kumiega / Czech \u2717 (Polish)\n40000 40% (0m 48s) 0.2472 Vamvakidis / Greek \u2713\n45000 45% (0m 54s) 1.9403 Tykal / Scottish \u2717 (Czech)\n50000 50% (0m 59s) 3.0146 Meeuwe / French \u2717 (Dutch)\n55000 55% (1m 5s) 0.6135 Sung / Korean \u2713\n60000 60% (1m 11s) 3.2133 Kennedy / English \u2717 (Scottish)\n65000 65% (1m 17s) 0.3967 Fearghal / Irish \u2713\n70000 70% (1m 23s) 2.2125 Scott / English \u2717 (Scottish)\n75000 75% (1m 29s) 0.1403 Kowalski / Polish \u2713\n80000 80% (1m 35s) 1.1492 Silveira / Portuguese \u2713\n85000 85% (1m 41s) 0.0385 Paradjanov / Russian \u2713\n90000 90% (1m 47s) 0.5063 Cameron / Scottish \u2713\n95000 95% (1m 53s) 0.5789 Malone / Irish \u2713\n100000 100% (1m 59s) 1.7323 Baba / Japanese \u2717 (Arabic)",
            "code"
        ]
    ],
    "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Training->Plotting the Results": [
        [
            "Plotting the historical loss from all_losses shows the network\nlearning:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nplt.figure()\nplt.plot(all_losses)\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_001.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_001.png\"/>",
            "code"
        ],
        [
            "[&lt;matplotlib.lines.Line2D object at 0x7f739bf9b640&gt;]",
            "code"
        ]
    ],
    "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results": [
        [
            "To see how well the network performs on different categories, we will\ncreate a confusion matrix, indicating for every actual language (rows)\nwhich language the network guesses (columns). To calculate the confusion\nmatrix a bunch of samples are run through the network with\nevaluate(), which is the same as train() minus the backprop.",
            "markdown"
        ],
        [
            "# Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "code"
        ],
        [
            "/var/lib/jenkins/workspace/intermediate_source/char_rnn_classification_tutorial.py:445: UserWarning:\n\nFixedFormatter should only be used together with FixedLocator\n\n/var/lib/jenkins/workspace/intermediate_source/char_rnn_classification_tutorial.py:446: UserWarning:\n\nFixedFormatter should only be used together with FixedLocator",
            "code"
        ],
        [
            "You can pick out bright spots off the main axis that show which\nlanguages it guesses incorrectly, e.g. Chinese for Korean, and Spanish\nfor Italian. It seems to do very well with Greek, and very poorly with\nEnglish (perhaps because of overlap with other languages).",
            "markdown"
        ]
    ],
    "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results->Running on User Input": [
        [
            "def predict(input_line, n_predictions=3):\n    print('\\n&gt; %s' % input_line)\n    with ():\n         = evaluate(lineToTensor(input_line))\n\n        # Get top N categories\n        topv, topi = .topk(n_predictions, 1, True)\n        predictions = []\n\n        for i in range(n_predictions):\n            value = topv[0][i].item()\n            category_index = topi[0][i].item()\n            print('(%.2f) %s' % (value, all_categories[category_index]))\n            predictions.append([value, all_categories[category_index]])\n\npredict('Dovesky')\npredict('Jackson')\npredict('Satoshi')",
            "code"
        ],
        [
            "&gt; Dovesky\n(-0.99) Polish\n(-1.33) Russian\n(-1.45) Czech\n\n&gt; Jackson\n(-0.15) Scottish\n(-2.29) English\n(-4.74) Russian\n\n&gt; Satoshi\n(-1.12) Japanese\n(-1.47) Italian\n(-2.14) Arabic",
            "code"
        ],
        [
            "The final versions of the scripts \nsplit the above code into a few files:",
            "markdown"
        ],
        [
            "data.py (loads files)",
            "markdown"
        ],
        [
            "model.py (defines the RNN)",
            "markdown"
        ],
        [
            "train.py (runs training)",
            "markdown"
        ],
        [
            "predict.py (runs predict() with command line arguments)",
            "markdown"
        ],
        [
            "server.py (serve prediction as a JSON API with bottle.py)",
            "markdown"
        ],
        [
            "Run train.py to train and save the network.",
            "markdown"
        ],
        [
            "Run predict.py with a name to view predictions:",
            "markdown"
        ],
        [
            "$ python predict.py Hazaki\n(-0.42) Japanese\n(-1.39) Polish\n(-3.51) Czech",
            "code"
        ],
        [
            "Run server.py and visit  to get JSON\noutput of predictions.",
            "markdown"
        ]
    ],
    "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Exercises": [
        [
            "Try with a different dataset of line -&gt; category, for example:",
            "markdown"
        ],
        [
            "Any word -&gt; language",
            "markdown"
        ],
        [
            "First name -&gt; gender",
            "markdown"
        ],
        [
            "Character name -&gt; writer",
            "markdown"
        ],
        [
            "Page title -&gt; blog or subreddit",
            "markdown"
        ],
        [
            "Get better results with a bigger and/or better shaped network",
            "markdown"
        ],
        [
            "Add more linear layers",
            "markdown"
        ],
        [
            "Try the nn.LSTM and nn.GRU layers",
            "markdown"
        ],
        [
            "Combine multiple of these RNNs as a higher level network",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 2 minutes  6.017 seconds)",
            "markdown"
        ]
    ],
    "torch->Text->NLP From Scratch: Generating Names with a Character-Level RNN": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "This is our second of three tutorials on \u201cNLP From Scratch\u201d.\nIn the <cite>first tutorial &lt;/intermediate/char_rnn_classification_tutorial&gt;</cite>\nwe used a RNN to classify names into their language of origin. This time\nwe\u2019ll turn around and generate names from languages.",
            "markdown"
        ],
        [
            "&gt; python sample.py Russian RUS\nRovakov\nUantov\nShavakov\n\n&gt; python sample.py German GER\nGerren\nEreng\nRosher\n\n&gt; python sample.py Spanish SPA\nSalla\nParer\nAllan\n\n&gt; python sample.py Chinese CHI\nChan\nHang\nIun",
            "code"
        ],
        [
            "We are still hand-crafting a small RNN with a few linear layers. The big\ndifference is instead of predicting a category after reading in all the\nletters of a name, we input a category and output one letter at a time.\nRecurrently predicting characters to form language (this could also be\ndone with words or other higher order constructs) is often referred to\nas a \u201clanguage model\u201d.",
            "markdown"
        ],
        [
            "<strong>Recommended Reading:</strong>",
            "markdown"
        ],
        [
            "I assume you have at least installed PyTorch, know Python, and\nunderstand Tensors:",
            "markdown"
        ],
        [
            " For installation instructions",
            "markdown"
        ],
        [
            " to get started with PyTorch in general",
            "markdown"
        ],
        [
            " for a wide and deep overview",
            "markdown"
        ],
        [
            " if you are former Lua Torch user",
            "markdown"
        ],
        [
            "It would also be useful to know about RNNs and how they work:",
            "markdown"
        ],
        [
            "shows a bunch of real life examples",
            "markdown"
        ],
        [
            "is about LSTMs specifically but also informative about RNNs in\ngeneral",
            "markdown"
        ],
        [
            "I also suggest the previous tutorial, ",
            "markdown"
        ]
    ],
    "torch->Text->NLP From Scratch: Generating Names with a Character-Level RNN->Preparing the Data": [
        [
            "Note",
            "markdown"
        ],
        [
            "Download the data from\n\nand extract it to the current directory.",
            "markdown"
        ],
        [
            "See the last tutorial for more detail of this process. In short, there\nare a bunch of plain text files data/names/[Language].txt with a\nname per line. We split lines into an array, convert Unicode to ASCII,\nand end up with a dictionary {language: [names ...]}.",
            "markdown"
        ],
        [
            "from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport glob\nimport os\nimport unicodedata\nimport string\n\nall_letters = string.ascii_letters + \" .,;'-\"\nn_letters = len(all_letters) + 1 # Plus EOS marker\n\ndef findFiles(path): return glob.glob(path)\n\n# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in all_letters\n    )\n\n# Read a file and split into lines\ndef readLines(filename):\n    with open(filename, encoding='utf-8') as some_file:\n        return [unicodeToAscii(line.strip()) for line in some_file]\n\n# Build the category_lines dictionary, a list of lines per category\ncategory_lines = {}\nall_categories = []\nfor filename in findFiles('data/names/*.txt'):\n    category = os.path.splitext(os.path.basename(filename))[0]\n    all_categories.append(category)\n    lines = readLines(filename)\n    category_lines[category] = lines\n\nn_categories = len(all_categories)\n\nif n_categories == 0:\n    raise RuntimeError('Data not found. Make sure that you downloaded data '\n        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n        'the current directory.')\n\nprint('# categories:', n_categories, all_categories)\nprint(unicodeToAscii(\"O'N\u00e9\u00e0l\"))",
            "code"
        ],
        [
            "# categories: 18 ['Czech', 'German', 'Portuguese', 'Russian', 'Irish', 'French', 'Korean', 'Arabic', 'Vietnamese', 'Italian', 'Japanese', 'English', 'Polish', 'Scottish', 'Chinese', 'Dutch', 'Greek', 'Spanish']\nO'Neal",
            "code"
        ]
    ],
    "torch->Text->NLP From Scratch: Generating Names with a Character-Level RNN->Creating the Network": [
        [
            "This network extends \nwith an extra argument for the category tensor, which is concatenated\nalong with the others. The category tensor is a one-hot vector just like\nthe letter input.",
            "markdown"
        ],
        [
            "We will interpret the output as the probability of the next letter. When\nsampling, the most likely output letter is used as the next input\nletter.",
            "markdown"
        ],
        [
            "I added a second linear layer o2o (after combining hidden and\noutput) to give it more muscle to work with. There\u2019s also a dropout\nlayer, which  with a given probability\n(here 0.1) and is usually used to fuzz inputs to prevent overfitting.\nHere we\u2019re using it towards the end of the network to purposely add some\nchaos and increase sampling variety.\n\n<img alt=\"\" src=\"https://i.imgur.com/jzVrf7f.png\"/>",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\n\nclass RNN():\n    def __init__(self, input_size, hidden_size, output_size):\n        super(, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.i2h = (n_categories + input_size + hidden_size, hidden_size)\n        self.i2o = (n_categories + input_size + hidden_size, output_size)\n        self.o2o = (hidden_size + output_size, output_size)\n        self.dropout = (0.1)\n        self.softmax = (dim=1)\n\n    def forward(self, category, input, hidden):\n        input_combined = ((category, input, hidden), 1)\n        hidden = self.i2h(input_combined)\n         = self.i2o(input_combined)\n        output_combined = ((hidden, ), 1)\n         = self.o2o(output_combined)\n         = self.dropout()\n         = self.softmax()\n        return , hidden\n\n    def initHidden(self):\n        return (1, self.hidden_size)",
            "code"
        ]
    ],
    "torch->Text->NLP From Scratch: Generating Names with a Character-Level RNN->Training->Preparing for Training": [
        [
            "First of all, helper functions to get random pairs of (category, line):",
            "markdown"
        ],
        [
            "import random\n\n# Random item from a list\ndef randomChoice(l):\n    return l[random.randint(0, len(l) - 1)]\n\n# Get a random category and random line from that category\ndef randomTrainingPair():\n    category = randomChoice(all_categories)\n    line = randomChoice(category_lines[category])\n    return category, line",
            "code"
        ],
        [
            "For each timestep (that is, for each letter in a training word) the\ninputs of the network will be\n(category, current letter, hidden state) and the outputs will be\n(next letter, next hidden state). So for each training set, we\u2019ll\nneed the category, a set of input letters, and a set of output/target\nletters.",
            "markdown"
        ],
        [
            "Since we are predicting the next letter from the current letter for each\ntimestep, the letter pairs are groups of consecutive letters from the\nline - e.g. for \"ABCD&lt;EOS&gt;\" we would create (\u201cA\u201d, \u201cB\u201d), (\u201cB\u201d, \u201cC\u201d),\n(\u201cC\u201d, \u201cD\u201d), (\u201cD\u201d, \u201cEOS\u201d).\n\n<img alt=\"\" src=\"https://i.imgur.com/JH58tXY.png\"/>",
            "markdown"
        ],
        [
            "The category tensor is a  of size\n&lt;1 x n_categories&gt;. When training we feed it to the network at every\ntimestep - this is a design choice, it could have been included as part\nof initial hidden state or some other strategy.",
            "markdown"
        ],
        [
            "# One-hot vector for category\ndef categoryTensor(category):\n    li = all_categories.index(category)\n    tensor = (1, n_categories)\n    tensor[0][li] = 1\n    return tensor\n\n# One-hot matrix of first to last letters (not including EOS) for input\ndef inputTensor(line):\n    tensor = (len(line), 1, n_letters)\n    for li in range(len(line)):\n        letter = line[li]\n        tensor[li][0][all_letters.find(letter)] = 1\n    return tensor\n\n# LongTensor of second letter to end (EOS) for target\ndef targetTensor(line):\n    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n    letter_indexes.append(n_letters - 1) # EOS\n    return torch.LongTensor(letter_indexes)",
            "code"
        ],
        [
            "For convenience during training we\u2019ll make a randomTrainingExample\nfunction that fetches a random (category, line) pair and turns them into\nthe required (category, input, target) tensors.",
            "markdown"
        ],
        [
            "# Make category, input, and target tensors from a random category, line pair\ndef randomTrainingExample():\n    category, line = randomTrainingPair()\n    category_tensor = categoryTensor(category)\n    input_line_tensor = inputTensor(line)\n    target_line_tensor = targetTensor(line)\n    return category_tensor, input_line_tensor, target_line_tensor",
            "code"
        ]
    ],
    "torch->Text->NLP From Scratch: Generating Names with a Character-Level RNN->Training->Training the Network": [
        [
            "In contrast to classification, where only the last output is used, we\nare making a prediction at every step, so we are calculating loss at\nevery step.",
            "markdown"
        ],
        [
            "The magic of autograd allows you to simply sum these losses at each step\nand call backward at the end.",
            "markdown"
        ],
        [
            " = ()\n\nlearning_rate = 0.0005\n\ndef train(category_tensor, input_line_tensor, target_line_tensor):\n    target_line_tensor.unsqueeze_(-1)\n    hidden = rnn.initHidden()\n\n    ()\n\n    loss = 0\n\n    for i in range(input_line_tensor.size(0)):\n        , hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n        l = (, target_line_tensor[i])\n        loss += l\n\n    loss.backward()\n\n    for p in ():\n        p.data.add_(p.grad.data, alpha=-learning_rate)\n\n    return , loss.item() / input_line_tensor.size(0)",
            "code"
        ],
        [
            "To keep track of how long training takes I am adding a\ntimeSince(timestamp) function which returns a human readable string:",
            "markdown"
        ],
        [
            "import time\nimport math\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)",
            "code"
        ],
        [
            "Training is business as usual - call train a bunch of times and wait a\nfew minutes, printing the current time and loss every print_every\nexamples, and keeping store of an average loss per plot_every examples\nin all_losses for plotting later.",
            "markdown"
        ],
        [
            "rnn = (n_letters, 128, n_letters)\n\nn_iters = 100000\nprint_every = 5000\nplot_every = 500\nall_losses = []\ntotal_loss = 0 # Reset every plot_every iters\n\nstart = time.time()\n\nfor iter in range(1, n_iters + 1):\n    , loss = train(*randomTrainingExample())\n    total_loss += loss\n\n    if iter % print_every == 0:\n        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n\n    if iter % plot_every == 0:\n        all_losses.append(total_loss / plot_every)\n        total_loss = 0",
            "code"
        ],
        [
            "0m 14s (5000 5%) 2.8251\n0m 28s (10000 10%) 2.9402\n0m 42s (15000 15%) 2.8376\n0m 56s (20000 20%) 3.3196\n1m 9s (25000 25%) 3.0596\n1m 23s (30000 30%) 2.5502\n1m 37s (35000 35%) 2.5327\n1m 51s (40000 40%) 2.5966\n2m 5s (45000 45%) 1.8684\n2m 18s (50000 50%) 3.2985\n2m 32s (55000 55%) 2.1817\n2m 46s (60000 60%) 2.1271\n2m 59s (65000 65%) 2.9178\n3m 13s (70000 70%) 1.9024\n3m 27s (75000 75%) 2.7952\n3m 41s (80000 80%) 2.1264\n3m 54s (85000 85%) 1.9758\n4m 8s (90000 90%) 2.5004\n4m 21s (95000 95%) 2.0448\n4m 35s (100000 100%) 2.5346",
            "code"
        ]
    ],
    "torch->Text->NLP From Scratch: Generating Names with a Character-Level RNN->Training->Plotting the Losses": [
        [
            "Plotting the historical loss from all_losses shows the network\nlearning:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nplt.figure()\nplt.plot(all_losses)\n\n\n<img alt=\"char rnn generation tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_generation_tutorial_001.png\" srcset=\"../_images/sphx_glr_char_rnn_generation_tutorial_001.png\"/>",
            "code"
        ],
        [
            "[&lt;matplotlib.lines.Line2D object at 0x7fb3d4641b70&gt;]",
            "code"
        ]
    ],
    "torch->Text->NLP From Scratch: Generating Names with a Character-Level RNN->Sampling the Network": [
        [
            "To sample we give the network a letter and ask what the next one is,\nfeed that in as the next letter, and repeat until the EOS token.",
            "markdown"
        ],
        [
            "Create tensors for input category, starting letter, and empty hidden\nstate",
            "markdown"
        ],
        [
            "Create a string output_name with the starting letter",
            "markdown"
        ],
        [
            "Up to a maximum output length,",
            "markdown"
        ],
        [
            "Feed the current letter to the network",
            "markdown"
        ],
        [
            "Get the next letter from highest output, and next hidden state",
            "markdown"
        ],
        [
            "If the letter is EOS, stop here",
            "markdown"
        ],
        [
            "If a regular letter, add to output_name and continue",
            "markdown"
        ],
        [
            "Return the final name",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Rather than having to give it a starting letter, another\nstrategy would have been to include a \u201cstart of string\u201d token in\ntraining and have the network choose its own starting letter.",
            "markdown"
        ],
        [
            "max_length = 20\n\n# Sample from a category and starting letter\ndef sample(category, start_letter='A'):\n    with ():  # no need to track history in sampling\n        category_tensor = categoryTensor(category)\n        input = inputTensor(start_letter)\n        hidden = rnn.initHidden()\n\n        output_name = start_letter\n\n        for i in range(max_length):\n            , hidden = rnn(category_tensor, input[0], hidden)\n            topv, topi = .topk(1)\n            topi = topi[0][0]\n            if topi == n_letters - 1:\n                break\n            else:\n                letter = all_letters[topi]\n                output_name += letter\n            input = inputTensor(letter)\n\n        return output_name\n\n# Get multiple samples from one category and multiple starting letters\ndef samples(category, start_letters='ABC'):\n    for start_letter in start_letters:\n        print(sample(category, start_letter))\n\nsamples('Russian', 'RUS')\n\nsamples('German', 'GER')\n\nsamples('Spanish', 'SPA')\n\nsamples('Chinese', 'CHI')",
            "code"
        ],
        [
            "Rovanov\nUarinov\nShakinov\nGaner\nEres\nRoure\nSangera\nParan\nAllan\nChing\nHang\nIun",
            "code"
        ]
    ],
    "torch->Text->NLP From Scratch: Generating Names with a Character-Level RNN->Exercises": [
        [
            "Try with a different dataset of category -&gt; line, for example:",
            "markdown"
        ],
        [
            "Fictional series -&gt; Character name",
            "markdown"
        ],
        [
            "Part of speech -&gt; Word",
            "markdown"
        ],
        [
            "Country -&gt; City",
            "markdown"
        ],
        [
            "Use a \u201cstart of sentence\u201d token so that sampling can be done without\nchoosing a start letter",
            "markdown"
        ],
        [
            "Get better results with a bigger and/or better shaped network",
            "markdown"
        ],
        [
            "Try the nn.LSTM and nn.GRU layers",
            "markdown"
        ],
        [
            "Combine multiple of these RNNs as a higher level network",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 4 minutes  35.803 seconds)",
            "markdown"
        ]
    ],
    "torch->Text->Text classification with the torchtext library": [
        [
            "In this tutorial, we will show how to use the torchtext library to build the dataset for the text classification analysis. Users will have the flexibility to\n<blockquote>",
            "markdown"
        ],
        [
            "Access to the raw data as an iterator",
            "markdown"
        ],
        [
            "Build data processing pipeline to convert the raw text strings into torch.Tensor that can be used to train the model",
            "markdown"
        ],
        [
            "Shuffle and iterate the data with \n\n</blockquote>",
            "markdown"
        ]
    ],
    "torch->Text->Text classification with the torchtext library->Access to the raw dataset iterators": [
        [
            "The torchtext library provides a few raw dataset iterators, which yield the raw text strings. For example, the AG_NEWS dataset iterators yield the raw data as a tuple of label and text.",
            "markdown"
        ],
        [
            "To access torchtext datasets, please install torchdata following instructions at .",
            "markdown"
        ],
        [
            "import torch\nfrom torchtext.datasets import \ntrain_iter = iter((split='train'))",
            "code"
        ],
        [
            "next(train_iter)\n&gt;&gt;&gt; (3, \"Fears for T N pension after talks Unions representing workers at Turner\nNewall say they are 'disappointed' after talks with stricken parent firm Federal\nMogul.\")\n\nnext(train_iter)\n&gt;&gt;&gt; (4, \"The Race is On: Second Private Team Sets Launch Date for Human\nSpaceflight (SPACE.com) SPACE.com - TORONTO, Canada -- A second\\\\team of\nrocketeers competing for the  #36;10 million Ansari X Prize, a contest\nfor\\\\privately funded suborbital space flight, has officially announced\nthe first\\\\launch date for its manned rocket.\")\n\nnext(train_iter)\n&gt;&gt;&gt; (4, 'Ky. Company Wins Grant to Study Peptides (AP) AP - A company founded\nby a chemistry researcher at the University of Louisville won a grant to develop\na method of producing better peptides, which are short chains of amino acids, the\nbuilding blocks of proteins.')",
            "code"
        ]
    ],
    "torch->Text->Text classification with the torchtext library->Prepare data processing pipelines": [
        [
            "We have revisited the very basic components of the torchtext library, including vocab, word vectors, tokenizer. Those are the basic data processing building blocks for raw text string.",
            "markdown"
        ],
        [
            "Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Here we use built in\nfactory function <cite>build_vocab_from_iterator</cite> which accepts iterator that yield list or iterator of tokens. Users can also pass any special symbols to be added to the\nvocabulary.",
            "markdown"
        ],
        [
            "from torchtext.data.utils import \nfrom torchtext.vocab import \n\ntokenizer = ('basic_english')\ntrain_iter = (split='train')\n\ndef yield_tokens(data_iter):\n    for _, text in data_iter:\n        yield tokenizer(text)\n\n = (yield_tokens(train_iter), specials=[\"&lt;unk&gt;\"])\n([\"&lt;unk&gt;\"])",
            "code"
        ],
        [
            "The vocabulary block converts a list of tokens into integers.",
            "markdown"
        ],
        [
            "(['here', 'is', 'an', 'example'])\n&gt;&gt;&gt; [475, 21, 30, 5297]",
            "code"
        ],
        [
            "Prepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to process the raw data strings from the dataset iterators.",
            "markdown"
        ],
        [
            "text_pipeline = lambda x: (tokenizer(x))\nlabel_pipeline = lambda x: int(x) - 1",
            "code"
        ],
        [
            "The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. The label pipeline converts the label into integers. For example,",
            "markdown"
        ],
        [
            "text_pipeline('here is the an example')\n&gt;&gt;&gt; [475, 21, 2, 30, 5297]\nlabel_pipeline('10')\n&gt;&gt;&gt; 9",
            "code"
        ]
    ],
    "torch->Text->Text classification with the torchtext library->Generate data batch and iterator": [
        [
            "is recommended for PyTorch users (a tutorial is ).\nIt works with a map-style dataset that implements the getitem() and len() protocols, and represents a map from indices/keys to data samples. It also works with an iterable dataset with the shuffle argument of False.",
            "markdown"
        ],
        [
            "Before sending to the model, collate_fn function works on a batch of samples generated from DataLoader. The input to collate_fn is a batch of data with the batch size in DataLoader, and collate_fn processes them according to the data processing pipelines declared previously. Pay attention here and make sure that collate_fn is declared as a top level def. This ensures that the function is available in each worker.",
            "markdown"
        ],
        [
            "In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of nn.EmbeddingBag. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries.",
            "markdown"
        ],
        [
            "from torch.utils.data import \n = (\"cuda\" if () else \"cpu\")\n\ndef collate_batch(batch):\n    label_list, text_list, offsets = [], [], [0]\n    for (_label, _text) in batch:\n         label_list.append(label_pipeline(_label))\n         processed_text = (text_pipeline(_text), dtype=)\n         text_list.append(processed_text)\n         offsets.append(processed_text.size(0))\n    label_list = (label_list, dtype=)\n    offsets = (offsets[:-1]).cumsum(dim=0)\n    text_list = (text_list)\n    return label_list.to(), text_list.to(), offsets.to()\n\ntrain_iter = (split='train')\n = (train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)",
            "code"
        ]
    ],
    "torch->Text->Text classification with the torchtext library->Define the model": [
        [
            "The model is composed of the  layer plus a linear layer for the classification purpose. nn.EmbeddingBag with the default mode of \u201cmean\u201d computes the mean value of a \u201cbag\u201d of embeddings. Although the text entries here have different lengths, nn.EmbeddingBag module requires no padding here since the text lengths are saved in offsets.",
            "markdown"
        ],
        [
            "Additionally, since nn.EmbeddingBag accumulates the average across\nthe embeddings on the fly, nn.EmbeddingBag can enhance the\nperformance and memory efficiency to process a sequence of tensors.\n<img alt=\"../_images/text_sentiment_ngrams_model.png\" src=\"../_images/text_sentiment_ngrams_model.png\"/>",
            "markdown"
        ],
        [
            "from torch import nn\n\nclass TextClassificationModel():\n\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super(, self).__init__()\n        self.embedding = (vocab_size, embed_dim, sparse=False)\n        self.fc = (embed_dim, num_class)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.5\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        self.fc.bias.data.zero_()\n\n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return self.fc(embedded)",
            "code"
        ]
    ],
    "torch->Text->Text classification with the torchtext library->Initiate an instance": [
        [
            "The AG_NEWS dataset has four labels and therefore the number of classes is four.",
            "markdown"
        ],
        [
            "1 : World\n2 : Sports\n3 : Business\n4 : Sci/Tec",
            "code"
        ],
        [
            "We build a model with the embedding dimension of 64. The vocab size is equal to the length of the vocabulary instance. The number of classes is equal to the number of labels,",
            "markdown"
        ],
        [
            "train_iter = (split='train')\nnum_class = len(set([label for (label, text) in train_iter]))\nvocab_size = len()\nemsize = 64\nmodel = (vocab_size, emsize, num_class).to()",
            "code"
        ]
    ],
    "torch->Text->Text classification with the torchtext library->Define functions to train the model and evaluate results.": [
        [
            "import time\n\ndef train():\n    ()\n    total_acc, total_count = 0, 0\n    log_interval = 500\n    start_time = time.time()\n\n    for idx, (label, text, offsets) in enumerate():\n        ()\n        predicted_label = model(text, offsets)\n        loss = (predicted_label, label)\n        loss.backward()\n        ((), 0.1)\n        .step()\n        total_acc += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n        if idx % log_interval == 0 and idx &gt; 0:\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches '\n                  '| accuracy {:8.3f}'.format(epoch, idx, len(),\n                                              total_acc/total_count))\n            total_acc, total_count = 0, 0\n            start_time = time.time()\n\ndef evaluate():\n    ()\n    total_acc, total_count = 0, 0\n\n    with ():\n        for idx, (label, text, offsets) in enumerate():\n            predicted_label = model(text, offsets)\n            loss = (predicted_label, label)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n    return total_acc/total_count",
            "code"
        ]
    ],
    "torch->Text->Text classification with the torchtext library->Split the dataset and run the model": [
        [
            "Since the original AG_NEWS has no valid dataset, we split the training\ndataset into train/valid sets with a split ratio of 0.95 (train) and\n0.05 (valid). Here we use\n\nfunction in PyTorch core library.",
            "markdown"
        ],
        [
            "criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.\nIt is useful when training a classification problem with C classes.\n\nimplements stochastic gradient descent method as the optimizer. The initial\nlearning rate is set to 5.0.\n\nis used here to adjust the learning rate through epochs.",
            "markdown"
        ],
        [
            "from torch.utils.data.dataset import \nfrom torchtext.data.functional import \n# Hyperparameters\nEPOCHS = 10 # epoch\nLR = 5  # learning rate\nBATCH_SIZE = 64 # batch size for training\n\n = ()\n = ((), lr=LR)\n = (, 1.0, gamma=0.1)\ntotal_accu = None\ntrain_iter, test_iter = ()\ntrain_dataset = (train_iter)\ntest_dataset = (test_iter)\nnum_train = int(len(train_dataset) * 0.95)\n,  = \\\n    (train_dataset, [num_train, len(train_dataset) - num_train])\n\n = (, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=collate_batch)\n = (, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=collate_batch)\n = (test_dataset, batch_size=BATCH_SIZE,\n                             shuffle=True, collate_fn=collate_batch)\n\nfor epoch in range(1, EPOCHS + 1):\n    epoch_start_time = time.time()\n    train()\n    accu_val = evaluate()\n    if total_accu is not None and total_accu &gt; accu_val:\n      .step()\n    else:\n       total_accu = accu_val\n    print('-' * 59)\n    print('| end of epoch {:3d} | time: {:5.2f}s | '\n          'valid accuracy {:8.3f} '.format(epoch,\n                                           time.time() - epoch_start_time,\n                                           accu_val))\n    print('-' * 59)",
            "code"
        ],
        [
            "| epoch   1 |   500/ 1782 batches | accuracy    0.689\n| epoch   1 |  1000/ 1782 batches | accuracy    0.856\n| epoch   1 |  1500/ 1782 batches | accuracy    0.877\n-----------------------------------------------------------\n| end of epoch   1 | time: 12.46s | valid accuracy    0.884\n-----------------------------------------------------------\n| epoch   2 |   500/ 1782 batches | accuracy    0.898\n| epoch   2 |  1000/ 1782 batches | accuracy    0.900\n| epoch   2 |  1500/ 1782 batches | accuracy    0.903\n-----------------------------------------------------------\n| end of epoch   2 | time: 10.27s | valid accuracy    0.892\n-----------------------------------------------------------\n| epoch   3 |   500/ 1782 batches | accuracy    0.914\n| epoch   3 |  1000/ 1782 batches | accuracy    0.914\n| epoch   3 |  1500/ 1782 batches | accuracy    0.914\n-----------------------------------------------------------\n| end of epoch   3 | time: 10.33s | valid accuracy    0.891\n-----------------------------------------------------------\n| epoch   4 |   500/ 1782 batches | accuracy    0.929\n| epoch   4 |  1000/ 1782 batches | accuracy    0.930\n| epoch   4 |  1500/ 1782 batches | accuracy    0.930\n-----------------------------------------------------------\n| end of epoch   4 | time: 10.28s | valid accuracy    0.908\n-----------------------------------------------------------\n| epoch   5 |   500/ 1782 batches | accuracy    0.932\n| epoch   5 |  1000/ 1782 batches | accuracy    0.932\n| epoch   5 |  1500/ 1782 batches | accuracy    0.930\n-----------------------------------------------------------\n| end of epoch   5 | time: 10.26s | valid accuracy    0.910\n-----------------------------------------------------------\n| epoch   6 |   500/ 1782 batches | accuracy    0.934\n| epoch   6 |  1000/ 1782 batches | accuracy    0.934\n| epoch   6 |  1500/ 1782 batches | accuracy    0.929\n-----------------------------------------------------------\n| end of epoch   6 | time: 10.20s | valid accuracy    0.910\n-----------------------------------------------------------\n| epoch   7 |   500/ 1782 batches | accuracy    0.934\n| epoch   7 |  1000/ 1782 batches | accuracy    0.933\n| epoch   7 |  1500/ 1782 batches | accuracy    0.936\n-----------------------------------------------------------\n| end of epoch   7 | time: 10.25s | valid accuracy    0.908\n-----------------------------------------------------------\n| epoch   8 |   500/ 1782 batches | accuracy    0.935\n| epoch   8 |  1000/ 1782 batches | accuracy    0.934\n| epoch   8 |  1500/ 1782 batches | accuracy    0.934\n-----------------------------------------------------------\n| end of epoch   8 | time: 10.24s | valid accuracy    0.909\n-----------------------------------------------------------\n| epoch   9 |   500/ 1782 batches | accuracy    0.935\n| epoch   9 |  1000/ 1782 batches | accuracy    0.936\n| epoch   9 |  1500/ 1782 batches | accuracy    0.933\n-----------------------------------------------------------\n| end of epoch   9 | time: 10.25s | valid accuracy    0.909\n-----------------------------------------------------------\n| epoch  10 |   500/ 1782 batches | accuracy    0.933\n| epoch  10 |  1000/ 1782 batches | accuracy    0.936\n| epoch  10 |  1500/ 1782 batches | accuracy    0.935\n-----------------------------------------------------------\n| end of epoch  10 | time: 10.23s | valid accuracy    0.909\n-----------------------------------------------------------",
            "code"
        ]
    ],
    "torch->Text->Text classification with the torchtext library->Evaluate the model with test dataset": [
        [
            "Checking the results of the test dataset\u2026",
            "markdown"
        ],
        [
            "print('Checking the results of test dataset.')\naccu_test = evaluate()\nprint('test accuracy {:8.3f}'.format(accu_test))",
            "code"
        ],
        [
            "Checking the results of test dataset.\ntest accuracy    0.903",
            "code"
        ]
    ],
    "torch->Text->Text classification with the torchtext library->Test on a random news": [
        [
            "Use the best model so far and test a golf news.",
            "markdown"
        ],
        [
            "ag_news_label = {1: \"World\",\n                 2: \"Sports\",\n                 3: \"Business\",\n                 4: \"Sci/Tec\"}\n\ndef predict(text, text_pipeline):\n    with ():\n        text = (text_pipeline(text))\n        output = model(text, ([0]))\n        return output.argmax(1).item() + 1\n\nex_text_str = \"MEMPHIS, Tenn. \u2013 Four days ago, Jon Rahm was \\\n    enduring the season\u2019s worst weather conditions on Sunday at The \\\n    Open on his way to a closing 75 at Royal Portrush, which \\\n    considering the wind and the rain was a respectable showing. \\\n    Thursday\u2019s first round at the WGC-FedEx St. Jude Invitational \\\n    was another story. With temperatures in the mid-80s and hardly any \\\n    wind, the Spaniard was 13 strokes better in a flawless round. \\\n    Thanks to his best putting performance on the PGA Tour, Rahm \\\n    finished with an 8-under 62 for a three-stroke lead, which \\\n    was even more impressive considering he\u2019d never played the \\\n    front nine at TPC Southwind.\"\n\nmodel = (\"cpu\")\n\nprint(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])",
            "code"
        ],
        [
            "This is a Sports news",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 2 minutes  1.494 seconds)",
            "markdown"
        ]
    ],
    "torch->Text->Language Translation with nn.Transformer and torchtext": [
        [
            "How to train a translation model from scratch using Transformer.",
            "markdown"
        ],
        [
            "Use torchtext library to access   dataset to train a German to English translation model.\n\n</dd>\n</dl>",
            "markdown"
        ]
    ],
    "torch->Text->Language Translation with nn.Transformer and torchtext->Data Sourcing and Processing": [
        [
            " has utilities for creating datasets that can be easily\niterated through for the purposes of creating a language translation\nmodel. In this example, we show how to use torchtext\u2019s inbuilt datasets,\ntokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use\n\nthat yields a pair of source-target raw sentences.",
            "markdown"
        ],
        [
            "To access torchtext datasets, please install torchdata following instructions at .",
            "markdown"
        ],
        [
            "from torchtext.data.utils import \nfrom torchtext.vocab import \nfrom torchtext.datasets import multi30k, \nfrom typing import Iterable, List\n\n\n# We need to modify the URLs for the dataset since the links to the original dataset are broken\n# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\nmulti30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\nmulti30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n\nSRC_LANGUAGE = 'de'\nTGT_LANGUAGE = 'en'\n\n# Place-holders\ntoken_transform = {}\nvocab_transform = {}\n\n\n# Create source and target language tokenizer. Make sure to install the dependencies.\n# pip install -U torchdata\n# pip install -U spacy\n# python -m spacy download en_core_web_sm\n# python -m spacy download de_core_news_sm\ntoken_transform[SRC_LANGUAGE] = ('spacy', language='de_core_news_sm')\ntoken_transform[TGT_LANGUAGE] = ('spacy', language='en_core_web_sm')\n\n\n# helper function to yield list of tokens\ndef yield_tokens(data_iter: Iterable, language: str) -&gt; List[str]:\n    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n\n    for data_sample in data_iter:\n        yield token_transform[language](data_sample[language_index[language]])\n\n# Define special symbols and indices\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n# Make sure the tokens are in order of their indices to properly insert them in vocab\nspecial_symbols = ['&lt;unk&gt;', '&lt;pad&gt;', '&lt;bos&gt;', '&lt;eos&gt;']\n\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n    # Training data Iterator\n    train_iter = (split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n    # Create torchtext's Vocab object\n    vocab_transform[ln] = (yield_tokens(train_iter, ln),\n                                                    min_freq=1,\n                                                    specials=special_symbols,\n                                                    special_first=True)\n\n# Set UNK_IDX as the default index. This index is returned when the token is not found.\n# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n  vocab_transform[ln].set_default_index(UNK_IDX)",
            "code"
        ]
    ],
    "torch->Text->Language Translation with nn.Transformer and torchtext->Seq2Seq Network using Transformer": [
        [
            "Transformer is a Seq2Seq model introduced in \npaper for solving machine translation tasks.\nBelow, we will create a Seq2Seq network that uses Transformer. The network\nconsists of three parts. First part is the embedding layer. This layer converts tensor of input indices\ninto corresponding tensor of input embeddings. These embedding are further augmented with positional\nencodings to provide position information of input tokens to the model. The second part is the\nactual  model.\nFinally, the output of the Transformer model is passed through linear layer\nthat gives un-normalized probabilities for each token in the target language.",
            "markdown"
        ],
        [
            "from torch import \nimport torch\nimport torch.nn as nn\nfrom torch.nn import \nimport math\nDEVICE = ('cuda' if () else 'cpu')\n\n# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\nclass PositionalEncoding():\n    def __init__(self,\n                 emb_size: int,\n                 dropout: float,\n                 maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        den = (- (0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = (0, maxlen).reshape(maxlen, 1)\n        pos_embedding = ((maxlen, emb_size))\n        pos_embedding[:, 0::2] = (pos * den)\n        pos_embedding[:, 1::2] = (pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = (dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: ):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n\n# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\nclass TokenEmbedding():\n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = (vocab_size, emb_size)\n        self.emb_size = emb_size\n\n    def forward(self, tokens: ):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\n# Seq2Seq Network\nclass Seq2SeqTransformer():\n    def __init__(self,\n                 num_encoder_layers: int,\n                 num_decoder_layers: int,\n                 emb_size: int,\n                 nhead: int,\n                 src_vocab_size: int,\n                 tgt_vocab_size: int,\n                 dim_feedforward: int = 512,\n                 dropout: float = 0.1):\n        super(Seq2SeqTransformer, self).__init__()\n        self.transformer = (d_model=emb_size,\n                                       nhead=nhead,\n                                       num_encoder_layers=num_encoder_layers,\n                                       num_decoder_layers=num_decoder_layers,\n                                       dim_feedforward=dim_feedforward,\n                                       dropout=dropout)\n        self.generator = (emb_size, tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(\n            emb_size, dropout=dropout)\n\n    def forward(self,\n                src: ,\n                trg: ,\n                src_mask: ,\n                tgt_mask: ,\n                src_padding_mask: ,\n                tgt_padding_mask: ,\n                memory_key_padding_mask: ):\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        return self.generator(outs)\n\n    def encode(self, src: , src_mask: ):\n        return self.transformer.encoder(self.positional_encoding(\n                            self.src_tok_emb(src)), src_mask)\n\n    def decode(self, tgt: , memory: , tgt_mask: ):\n        return self.transformer.decoder(self.positional_encoding(\n                          self.tgt_tok_emb(tgt)), memory,\n                          tgt_mask)",
            "code"
        ],
        [
            "During training, we need a subsequent word mask that will prevent the model from looking into\nthe future words when making predictions. We will also need masks to hide\nsource and target padding tokens. Below, let\u2019s define a function that will take care of both.",
            "markdown"
        ],
        [
            "def generate_square_subsequent_mask(sz):\n    mask = ((((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\ndef create_mask(src, tgt):\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = ((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask",
            "code"
        ],
        [
            "Let\u2019s now define the parameters of our model and instantiate the same. Below, we also\ndefine our loss function which is the cross-entropy loss and the optmizer used for training.",
            "markdown"
        ],
        [
            "(0)\n\nSRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\nTGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\nEMB_SIZE = 512\nNHEAD = 8\nFFN_HID_DIM = 512\nBATCH_SIZE = 128\nNUM_ENCODER_LAYERS = 3\nNUM_DECODER_LAYERS = 3\n\ntransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n\nfor p in transformer.parameters():\n    if p.dim() &gt; 1:\n        (p)\n\ntransformer = transformer.to(DEVICE)\n\nloss_fn = (ignore_index=PAD_IDX)\n\noptimizer = (transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)",
            "code"
        ]
    ],
    "torch->Text->Language Translation with nn.Transformer and torchtext->Collation": [
        [
            "As seen in the Data Sourcing and Processing section, our data iterator yields a pair of raw strings.\nWe need to convert these string pairs into the batched tensors that can be processed by our Seq2Seq network\ndefined previously. Below we define our collate function that converts a batch of raw strings into batch tensors that\ncan be fed directly into our model.",
            "markdown"
        ],
        [
            "from torch.nn.utils.rnn import \n\n# helper function to club together sequential operations\ndef sequential_transforms(*transforms):\n    def func(txt_input):\n        for transform in transforms:\n            txt_input = transform(txt_input)\n        return txt_input\n    return func\n\n# function to add BOS/EOS and create tensor for input sequence indices\ndef tensor_transform(token_ids: List[int]):\n    return ((([BOS_IDX]),\n                      (token_ids),\n                      ([EOS_IDX])))\n\n# src and tgt language text transforms to convert raw strings into tensors indices\ntext_transform = {}\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n                                               vocab_transform[ln], #Numericalization\n                                               tensor_transform) # Add BOS/EOS and create tensor\n\n\n# function to collate data samples into batch tensors\ndef collate_fn(batch):\n    src_batch, tgt_batch = [], []\n    for src_sample, tgt_sample in batch:\n        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n\n    src_batch = (src_batch, padding_value=PAD_IDX)\n    tgt_batch = (tgt_batch, padding_value=PAD_IDX)\n    return src_batch, tgt_batch",
            "code"
        ],
        [
            "Let\u2019s define training and evaluation loop that will be called for each\nepoch.",
            "markdown"
        ],
        [
            "from torch.utils.data import \n\ndef train_epoch(model, optimizer):\n    model.train()\n    losses = 0\n    train_iter = (split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n    train_dataloader = (train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n\n    for src, tgt in train_dataloader:\n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n\n        tgt_input = tgt[:-1, :]\n\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n\n        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n\n        optimizer.zero_grad()\n\n        tgt_out = tgt[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        loss.backward()\n\n        optimizer.step()\n        losses += loss.item()\n\n    return losses / len(list(train_dataloader))\n\n\ndef evaluate(model):\n    model.eval()\n    losses = 0\n\n    val_iter = (split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n    val_dataloader = (val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n\n    for src, tgt in val_dataloader:\n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n\n        tgt_input = tgt[:-1, :]\n\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n\n        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n\n        tgt_out = tgt[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        losses += loss.item()\n\n    return losses / len(list(val_dataloader))",
            "code"
        ],
        [
            "Now we have all the ingredients to train our model. Let\u2019s do it!",
            "markdown"
        ],
        [
            "from timeit import default_timer as timer\nNUM_EPOCHS = 18\n\nfor epoch in range(1, NUM_EPOCHS+1):\n    start_time = timer()\n    train_loss = train_epoch(transformer, optimizer)\n    end_time = timer()\n    val_loss = evaluate(transformer)\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n\n\n# function to generate output sequence using greedy algorithm\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n\n    memory = model.encode(src, src_mask)\n    ys = (1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    for i in range(max_len-1):\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        _, next_word = (prob, dim=1)\n        next_word = next_word.item()\n\n        ys = ([ys,\n                        (1, 1).type_as(src.data).fill_(next_word)], dim=0)\n        if next_word == EOS_IDX:\n            break\n    return ys\n\n\n# actual function to translate input sentence into target language\ndef translate(model: , src_sentence: str):\n    model.eval()\n    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = ((num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(\n        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"&lt;bos&gt;\", \"\").replace(\"&lt;eos&gt;\", \"\")",
            "code"
        ],
        [
            "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))",
            "code"
        ]
    ],
    "torch->Text->Language Translation with nn.Transformer and torchtext->References": [
        [
            "Attention is all you need paper.",
            "markdown"
        ],
        [
            "The annotated transformer. ",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (DQN) Tutorial": [
        [
            "</dd>\n</dl>",
            "markdown"
        ],
        [
            "This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) agent\non the CartPole-v1 task from .",
            "markdown"
        ],
        [
            "<strong>Task</strong>",
            "markdown"
        ],
        [
            "The agent has to decide between two actions - moving the cart left or\nright - so that the pole attached to it stays upright. You can find more\ninformation about the environment and other more challenging environments at\n.\n\n<img alt=\"cartpole\" src=\"../_images/cartpole.gif\"/>",
            "markdown"
        ],
        [
            "cartpole",
            "markdown"
        ],
        [
            "As the agent observes the current state of the environment and chooses\nan action, the environment <em>transitions</em> to a new state, and also\nreturns a reward that indicates the consequences of the action. In this\ntask, rewards are +1 for every incremental timestep and the environment\nterminates if the pole falls over too far or the cart moves more than 2.4\nunits away from center. This means better performing scenarios will run\nfor longer duration, accumulating larger return.",
            "markdown"
        ],
        [
            "The CartPole task is designed so that the inputs to the agent are 4 real\nvalues representing the environment state (position, velocity, etc.).\nWe take these 4 inputs without any scaling and pass them through a\nsmall fully-connected network with 2 outputs, one for each action.\nThe network is trained to predict the expected value for each action,\ngiven the input state. The action with the highest expected value is\nthen chosen.",
            "markdown"
        ],
        [
            "<strong>Packages</strong>",
            "markdown"
        ],
        [
            "First, let\u2019s import needed packages. Firstly, we need\n for the environment,\ninstalled by using <cite>pip</cite>. This is a fork of the original OpenAI\nGym project and maintained by the same team since Gym v0.19.\nIf you are running this in Google colab, run:",
            "markdown"
        ],
        [
            "%%bash\npip3 install gymnasium[classic_control]",
            "code"
        ],
        [
            "We\u2019ll also use the following from PyTorch:",
            "markdown"
        ],
        [
            "neural networks (torch.nn)",
            "markdown"
        ],
        [
            "optimization (torch.optim)",
            "markdown"
        ],
        [
            "automatic differentiation (torch.autograd)",
            "markdown"
        ],
        [
            "import gymnasium as gym\nimport math\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple, deque\nfrom itertools import count\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nenv = gym.make(\"CartPole-v1\")\n\n# set up matplotlib\nis_ipython = 'inline' in matplotlib.get_backend()\nif is_ipython:\n    from IPython import display\n\nplt.ion()\n\n# if gpu is to be used\n = (\"cuda\" if () else \"cpu\")",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (DQN) Tutorial->Replay Memory": [
        [
            "We\u2019ll be using experience replay memory for training our DQN. It stores\nthe transitions that the agent observes, allowing us to reuse this data\nlater. By sampling from it randomly, the transitions that build up a\nbatch are decorrelated. It has been shown that this greatly stabilizes\nand improves the DQN training procedure.",
            "markdown"
        ],
        [
            "For this, we\u2019re going to need two classses:",
            "markdown"
        ],
        [
            "Transition - a named tuple representing a single transition in\nour environment. It essentially maps (state, action) pairs\nto their (next_state, reward) result, with the state being the\nscreen difference image as described later on.",
            "markdown"
        ],
        [
            "ReplayMemory - a cyclic buffer of bounded size that holds the\ntransitions observed recently. It also implements a .sample()\nmethod for selecting a random batch of transitions for training.",
            "markdown"
        ],
        [
            "Transition = namedtuple('Transition',\n                        ('state', 'action', 'next_state', 'reward'))\n\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.memory = deque([], maxlen=capacity)\n\n    def push(self, *args):\n        \"\"\"Save a transition\"\"\"\n        self.memory.append(Transition(*args))\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)",
            "code"
        ],
        [
            "Now, let\u2019s define our model. But first, let\u2019s quickly recap what a DQN is.",
            "markdown"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (DQN) Tutorial->DQN algorithm": [
        [
            "Our environment is deterministic, so all equations presented here are\nalso formulated deterministically for the sake of simplicity. In the\nreinforcement learning literature, they would also contain expectations\nover stochastic transitions in the environment.",
            "markdown"
        ],
        [
            "Our aim will be to train a policy that tries to maximize the discounted,\ncumulative reward\n\\(R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t\\), where\n\\(R_{t_0}\\) is also known as the <em>return</em>. The discount,\n\\(\\gamma\\), should be a constant between \\(0\\) and \\(1\\)\nthat ensures the sum converges. A lower \\(\\gamma\\) makes\nrewards from the uncertain far future less important for our agent\nthan the ones in the near future that it can be fairly confident\nabout. It also encourages agents to collect reward closer in time\nthan equivalent rewards that are temporally far away in the future.",
            "markdown"
        ],
        [
            "The main idea behind Q-learning is that if we had a function\n\\(Q^*: State \\times Action \\rightarrow \\mathbb{R}\\), that could tell\nus what our return would be, if we were to take an action in a given\nstate, then we could easily construct a policy that maximizes our\nrewards:\n\n\\[\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\n\n\\]",
            "markdown"
        ],
        [
            "However, we don\u2019t know everything about the world, so we don\u2019t have\naccess to \\(Q^*\\). But, since neural networks are universal function\napproximators, we can simply create one and train it to resemble\n\\(Q^*\\).",
            "markdown"
        ],
        [
            "For our training update rule, we\u2019ll use a fact that every \\(Q\\)\nfunction for some policy obeys the Bellman equation:\n\n\\[Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\n\n\\]",
            "markdown"
        ],
        [
            "The difference between the two sides of the equality is known as the\ntemporal difference error, \\(\\delta\\):\n\n\\[\\delta = Q(s, a) - (r + \\gamma \\max_a' Q(s', a))\n\n\\]",
            "markdown"
        ],
        [
            "To minimise this error, we will use the . The Huber loss acts\nlike the mean squared error when the error is small, but like the mean\nabsolute error when the error is large - this makes it more robust to\noutliers when the estimates of \\(Q\\) are very noisy. We calculate\nthis over a batch of transitions, \\(B\\), sampled from the replay\nmemory:\n\n\\[\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\]\n\n\\[\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n  \\frac{1}{2}{\\delta^2}  &amp; \\text{for } |\\delta| \\le 1, \\\\\n  |\\delta| - \\frac{1}{2} &amp; \\text{otherwise.}\n\\end{cases}\\]",
            "markdown"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (DQN) Tutorial->DQN algorithm->Q-network": [
        [
            "Our model will be a convolutional neural network that takes in the\ndifference between the current and previous screen patches. It has two\noutputs, representing \\(Q(s, \\mathrm{left})\\) and\n\\(Q(s, \\mathrm{right})\\) (where \\(s\\) is the input to the\nnetwork). In effect, the network is trying to predict the <em>expected return</em> of\ntaking each action given the current input.",
            "markdown"
        ],
        [
            "class DQN():\n\n    def __init__(self, n_observations, n_actions):\n        super(, self).__init__()\n        self.layer1 = (n_observations, 128)\n        self.layer2 = (128, 128)\n        self.layer3 = (128, n_actions)\n\n    # Called with either one element to determine next action, or a batch\n    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n    def forward(self, x):\n        x = (self.layer1(x))\n        x = (self.layer2(x))\n        return self.layer3(x)",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (DQN) Tutorial->Training->Hyperparameters and utilities": [
        [
            "This cell instantiates our model and its optimizer, and defines some\nutilities:",
            "markdown"
        ],
        [
            "select_action - will select an action accordingly to an epsilon\ngreedy policy. Simply put, we\u2019ll sometimes use our model for choosing\nthe action, and sometimes we\u2019ll just sample one uniformly. The\nprobability of choosing a random action will start at EPS_START\nand will decay exponentially towards EPS_END. EPS_DECAY\ncontrols the rate of the decay.",
            "markdown"
        ],
        [
            "plot_durations - a helper for plotting the durations of episodes,\nalong with an average over the last 100 episodes (the measure used in\nthe official evaluations). The plot will be underneath the cell\ncontaining the main training loop, and will update after every\nepisode.",
            "markdown"
        ],
        [
            "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n# GAMMA is the discount factor as mentioned in the previous section\n# EPS_START is the starting value of epsilon\n# EPS_END is the final value of epsilon\n# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n# TAU is the update rate of the target network\n# LR is the learning rate of the AdamW optimizer\nBATCH_SIZE = 128\nGAMMA = 0.99\nEPS_START = 0.9\nEPS_END = 0.05\nEPS_DECAY = 1000\nTAU = 0.005\nLR = 1e-4\n\n# Get number of actions from gym action space\nn_actions = env.action_space.n\n# Get the number of state observations\n, info = env.reset()\nn_observations = len()\n\npolicy_net = (n_observations, n_actions).to()\ntarget_net = (n_observations, n_actions).to()\n(())\n\n = ((), lr=LR, amsgrad=True)\nmemory = ReplayMemory(10000)\n\n\nsteps_done = 0\n\n\ndef select_action():\n    global steps_done\n    sample = random.random()\n    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n        math.exp(-1. * steps_done / EPS_DECAY)\n    steps_done += 1\n    if sample &gt; eps_threshold:\n        with ():\n            # t.max(1) will return the largest column value of each row.\n            # second column on max result is index of where max element was\n            # found, so we pick action with the larger expected reward.\n            return policy_net().max(1)[1].view(1, 1)\n    else:\n        return ([[env.action_space.sample()]], =, dtype=)\n\n\nepisode_durations = []\n\n\ndef plot_durations(show_result=False):\n    plt.figure(1)\n    durations_t = (episode_durations, dtype=)\n    if show_result:\n        plt.title('Result')\n    else:\n        plt.clf()\n        plt.title('Training...')\n    plt.xlabel('Episode')\n    plt.ylabel('Duration')\n    plt.plot(durations_t.numpy())\n    # Take 100 episode averages and plot them too\n    if len(durations_t) &gt;= 100:\n        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n        means = (((99), means))\n        plt.plot(means.numpy())\n\n    plt.pause(0.001)  # pause a bit so that plots are updated\n    if is_ipython:\n        if not show_result:\n            display.display(plt.gcf())\n            display.clear_output(wait=True)\n        else:\n            display.display(plt.gcf())",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (DQN) Tutorial->Training->Training loop": [
        [
            "Finally, the code for training our model.",
            "markdown"
        ],
        [
            "Here, you can find an optimize_model function that performs a\nsingle step of the optimization. It first samples a batch, concatenates\nall the tensors into a single one, computes \\(Q(s_t, a_t)\\) and\n\\(V(s_{t+1}) = \\max_a Q(s_{t+1}, a)\\), and combines them into our\nloss. By definition we set \\(V(s) = 0\\) if \\(s\\) is a terminal\nstate. We also use a target network to compute \\(V(s_{t+1})\\) for\nadded stability. The target network is updated at every step with a\n controlled by\nthe hyperparameter TAU, which was previously defined.",
            "markdown"
        ],
        [
            "def optimize_model():\n    if len(memory) &lt; BATCH_SIZE:\n        return\n    transitions = memory.sample(BATCH_SIZE)\n    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n    # detailed explanation). This converts batch-array of Transitions\n    # to Transition of batch-arrays.\n    batch = Transition(*zip(*transitions))\n\n    # Compute a mask of non-final states and concatenate the batch elements\n    # (a final state would've been the one after which simulation ended)\n    non_final_mask = (tuple(map(lambda s: s is not None,\n                                          batch.)), =, dtype=)\n    non_final_next_states = ([s for s in batch.\n                                                if s is not None])\n    state_batch = (batch.)\n    action_batch = (batch.)\n    reward_batch = (batch.)\n\n    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n    # columns of actions taken. These are the actions which would've been taken\n    # for each batch state according to policy_net\n    state_action_values = policy_net(state_batch).gather(1, action_batch)\n\n    # Compute V(s_{t+1}) for all next states.\n    # Expected values of actions for non_final_next_states are computed based\n    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n    # This is merged based on the mask, such that we'll have either the expected\n    # state value or 0 in case the state was final.\n    next_state_values = (BATCH_SIZE, =)\n    with ():\n        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n    # Compute the expected Q values\n    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n\n    # Compute Huber loss\n    criterion = ()\n    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n\n    # Optimize the model\n    ()\n    loss.backward()\n    # In-place gradient clipping\n    ((), 100)\n    ()",
            "code"
        ],
        [
            "Below, you can find the main training loop. At the beginning we reset\nthe environment and obtain the initial state Tensor. Then, we sample\nan action, execute it, observe the next state and the reward (always\n1), and optimize our model once. When the episode ends (our model\nfails), we restart the loop.",
            "markdown"
        ],
        [
            "Below, <cite>num_episodes</cite> is set to 600 if a GPU is available, otherwise 50\nepisodes are scheduled so training does not take too long. However, 50\nepisodes is insufficient for to observe good performance on cartpole.\nYou should see the model constantly achieve 500 steps within 600 training\nepisodes. Training RL agents can be a noisy process, so restarting training\ncan produce better results if convergence is not observed.",
            "markdown"
        ],
        [
            "if ():\n    num_episodes = 600\nelse:\n    num_episodes = 50\n\nfor i_episode in range(num_episodes):\n    # Initialize the environment and get it's state\n    , info = env.reset()\n     = (, dtype=, =).unsqueeze(0)\n    for t in count():\n         = select_action()\n        observation, , terminated, truncated, _ = env.step(.item())\n         = ([], =)\n        done = terminated or truncated\n\n        if terminated:\n             = None\n        else:\n             = (observation, dtype=, =).unsqueeze(0)\n\n        # Store the transition in memory\n        memory.push(, , , )\n\n        # Move to the next state\n         = \n\n        # Perform one step of the optimization (on the policy network)\n        optimize_model()\n\n        # Soft update of the target network's weights\n        # \u03b8\u2032 \u2190 \u03c4 \u03b8 + (1 \u2212\u03c4 )\u03b8\u2032\n        target_net_state_dict = ()\n        policy_net_state_dict = ()\n        for key in policy_net_state_dict:\n            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n        (target_net_state_dict)\n\n        if done:\n            episode_durations.append(t + 1)\n            plot_durations()\n            break\n\nprint('Complete')\nplot_durations(show_result=True)\nplt.ioff()\nplt.show()\n\n\n<img alt=\"Result\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_reinforcement_q_learning_001.png\" srcset=\"../_images/sphx_glr_reinforcement_q_learning_001.png\"/>",
            "code"
        ],
        [
            "Complete",
            "code"
        ],
        [
            "Here is the diagram that illustrates the overall resulting data flow.\n\n<img alt=\"../_images/reinforcement_learning_diagram.jpg\" src=\"../_images/reinforcement_learning_diagram.jpg\"/>",
            "markdown"
        ],
        [
            "Actions are chosen either randomly or based on a policy, getting the next\nstep sample from the gym environment. We record the results in the\nreplay memory and also run optimization step on every iteration.\nOptimization picks a random batch from the replay memory to do training of the\nnew policy. The \u201colder\u201d target_net is also used in optimization to compute the\nexpected Q values. A soft update of its weights are performed at every step.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 9 minutes  58.307 seconds)",
            "markdown"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "This tutorial demonstrates how to use PyTorch and torchrl to train a parametric policy\nnetwork to solve the Inverted Pendulum task from the .\n\n<img alt=\"Inverted pendulum\" src=\"../_images/invpendulum.gif\"/>",
            "markdown"
        ],
        [
            "Inverted pendulum",
            "markdown"
        ],
        [
            "Key learnings:",
            "markdown"
        ],
        [
            "How to create an environment in TorchRL, transform its outputs, and collect data from this env;",
            "markdown"
        ],
        [
            "How to make your classes talk to each other using ;",
            "markdown"
        ],
        [
            "The basics of building your training loop with TorchRL:",
            "markdown"
        ],
        [
            "How to compute the advantage signal for policy gradient methods;",
            "markdown"
        ],
        [
            "How to create a stochastic policy using a probabilistic neural network;",
            "markdown"
        ],
        [
            "How to create a dynamic replay buffer and sample from it without repetition.",
            "markdown"
        ],
        [
            "We will cover six crucial components of TorchRL:",
            "markdown"
        ],
        [
            "If you are running this in Google Colab, make sure you install the following dependencies:",
            "markdown"
        ],
        [
            "!pip3 install torchrl\n!pip3 install gym[mujoco]\n!pip3 install tqdm",
            "code"
        ],
        [
            "Proximal Policy Optimization (PPO) is a policy-gradient algorithm where a\nbatch of data is being collected and directly consumed to train the policy to maximise\nthe expected return given some proximality constraints. You can think of it\nas a sophisticated version of ,\nthe foundational policy-optimization algorithm. For more information, see the\n paper.",
            "markdown"
        ],
        [
            "PPO is usually regarded as a fast and efficient method for online, on-policy\nreinforcement algorithm. TorchRL provides a loss-module that does all the work\nfor you, so that you can rely on this implementation and focus on solving your\nproblem rather than re-inventing the wheel every time you want to train a policy.",
            "markdown"
        ],
        [
            "For completeness, here is a brief overview of what the loss computes, even though\nthis is taken care of by our ClipPPOLoss module\u2014the algorithm works as follows:\n1. we will sample a batch of data by playing the\npolicy in the environment for a given number of steps.\n2. Then, we will perform a given number of optimization steps with random sub-samples of this batch using\na clipped version of the REINFORCE loss.\n3. The clipping will put a pessimistic bound on our loss: lower return estimates will\nbe favored compared to higher ones.\nThe precise formula of the loss is:\n\n\\[L(s,a,\\theta_k,\\theta) = \\min\\left(\n\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}  A^{\\pi_{\\theta_k}}(s,a), \\;\\;\ng(\\epsilon, A^{\\pi_{\\theta_k}}(s,a))\n\\right),\\]",
            "markdown"
        ],
        [
            "There are two components in that loss: in the first part of the minimum operator,\nwe simply compute an importance-weighted version of the REINFORCE loss (for example, a\nREINFORCE loss that we have corrected for the fact that the current policy\nconfiguration lags the one that was used for the data collection).\nThe second part of that minimum operator is a similar loss where we have clipped\nthe ratios when they exceeded or were below a given pair of thresholds.",
            "markdown"
        ],
        [
            "This loss ensures that whether the advantage is positive or negative, policy\nupdates that would produce significant shifts from the previous configuration\nare being discouraged.",
            "markdown"
        ],
        [
            "This tutorial is structured as follows:",
            "markdown"
        ],
        [
            "First, we will define a set of hyperparameters we will be using for training.",
            "markdown"
        ],
        [
            "Next, we will focus on creating our environment, or simulator, using TorchRL\u2019s\nwrappers and transforms.",
            "markdown"
        ],
        [
            "Next, we will design the policy network and the value model,\nwhich is indispensable to the loss function. These modules will be used\nto configure our loss module.",
            "markdown"
        ],
        [
            "Next, we will create the replay buffer and data loader.",
            "markdown"
        ],
        [
            "Finally, we will run our training loop and analyze the results.",
            "markdown"
        ],
        [
            "Throughout this tutorial, we\u2019ll be using the tensordict library.\n is the lingua franca of TorchRL: it helps us abstract\nwhat a module reads and writes and care less about the specific data\ndescription and more about the algorithm itself.",
            "markdown"
        ],
        [
            "from collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom tensordict.nn import \nfrom tensordict.nn.distributions import \nfrom torch import nn\nfrom torchrl.collectors import \nfrom torchrl.data.replay_buffers import \nfrom torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\nfrom torchrl.data.replay_buffers.storages import \nfrom torchrl.envs import (\n    ,\n    ,\n    ,\n    ,\n    ,\n)\nfrom torchrl.envs.libs.gym import \nfrom torchrl.envs.utils import , \nfrom torchrl.modules import , , \nfrom torchrl.objectives import \nfrom torchrl.objectives.value import \nfrom tqdm import tqdm",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:259: UserWarning:\n\nGot multiple backends for torchrl.envs.libs.gym._get_gym_envs. Using the last queried (&lt;module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'&gt; with version 0.27.0).\n\n/opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:259: UserWarning:\n\nGot multiple backends for torchrl.envs.libs.gym._build_gym_env. Using the last queried (&lt;module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'&gt; with version 0.27.0).\n\n/opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:259: UserWarning:\n\nGot multiple backends for torchrl.envs.libs.gym._set_seed_initial. Using the last queried (&lt;module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'&gt; with version 0.27.0).\n\n/opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:259: UserWarning:\n\nGot multiple backends for torchrl.envs.libs.gym._set_gym_args. Using the last queried (&lt;module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'&gt; with version 0.27.0).\n\n/opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:259: UserWarning:\n\nGot multiple backends for torchrl.envs.libs.gym._set_gym_default. Using the last queried (&lt;module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'&gt; with version 0.27.0).",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Define Hyperparameters": [
        [
            "We set the hyperparameters for our algorithm. Depending on the resources\navailable, one may choose to execute the policy on GPU or on another\ndevice.\nThe frame_skip will control how for how many frames is a single\naction being executed. The rest of the arguments that count frames\nmust be corrected for this value (since one environment step will\nactually return frame_skip frames).",
            "markdown"
        ],
        [
            "device = \"cpu\" if not torch.has_cuda else \"cuda:0\"\nnum_cells = 256  # number of cells in each layer\nlr = 3e-4\nmax_grad_norm = 1.0",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Define Hyperparameters->Data collection parameters": [
        [
            "When collecting data, we will be able to choose how big each batch will be\nby defining a frames_per_batch parameter. We will also define how many\nframes (such as the number of interactions with the simulator) we will allow ourselves to\nuse. In general, the goal of an RL algorithm is to learn to solve the task\nas fast as it can in terms of environment interactions: the lower the total_frames\nthe better.\nWe also define a frame_skip: in some contexts, repeating the same action\nmultiple times over the course of a trajectory may be beneficial as it makes\nthe behavior more consistent and less erratic. However, \u201cskipping\u201d\ntoo many frames will hamper training by reducing the reactivity of the actor\nto observation changes.",
            "markdown"
        ],
        [
            "When using frame_skip it is good practice to\ncorrect the other frame counts by the number of frames we are grouping\ntogether. If we configure a total count of X frames for training but\nuse a frame_skip of Y, we will be actually collecting XY frames in total\nwhich exceeds our predefined budget.",
            "markdown"
        ],
        [
            "frame_skip = 1\nframes_per_batch = 1000 // frame_skip\n# For a complete training, bring the number of frames up to 1M\ntotal_frames = 50_000 // frame_skip",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Define Hyperparameters->PPO parameters": [
        [
            "At each data collection (or batch collection) we will run the optimization\nover a certain number of <em>epochs</em>, each time consuming the entire data we just\nacquired in a nested training loop. Here, the sub_batch_size is different from the\nframes_per_batch here above: recall that we are working with a \u201cbatch of data\u201d\ncoming from our collector, which size is defined by frames_per_batch, and that\nwe will further split in smaller sub-batches during the inner training loop.\nThe size of these sub-batches is controlled by sub_batch_size.",
            "markdown"
        ],
        [
            "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\nnum_epochs = 10  # optimisation steps per batch of data collected\nclip_epsilon = (\n    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n)\ngamma = 0.99\nlmbda = 0.95\nentropy_eps = 1e-4",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Define an environment": [
        [
            "In RL, an <em>environment</em> is usually the way we refer to a simulator or a\ncontrol system. Various libraries provide simulation environments for reinforcement\nlearning, including Gymnasium (previously OpenAI Gym), DeepMind control suite, and\nmany others.\nAs a generalistic library, TorchRL\u2019s goal is to provide an interchangeable interface\nto a large panel of RL simulators, allowing you to easily swap one environment\nwith another. For example, creating a wrapped gym environment can be achieved with few characters:",
            "markdown"
        ],
        [
            " = (\"InvertedDoublePendulum-v4\", device=device, frame_skip=frame_skip)",
            "code"
        ],
        [
            "There are a few things to notice in this code: first, we created\nthe environment by calling the GymEnv wrapper. If extra keyword arguments\nare passed, they will be transmitted to the gym.make method, hence covering\nthe most common env construction commands.\nAlternatively, one could also directly create a gym environment using gym.make(env_name, **kwargs)\nand wrap it in a <cite>GymWrapper</cite> class.",
            "markdown"
        ],
        [
            "Also the device argument: for gym, this only controls the device where\ninput action and observered states will be stored, but the execution will always\nbe done on CPU. The reason for this is simply that gym does not support on-device\nexecution, unless specified otherwise. For other libraries, we have control over\nthe execution device and, as much as we can, we try to stay consistent in terms of\nstoring and execution backends.",
            "markdown"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Define an environment->Transforms": [
        [
            "We will append some transforms to our environments to prepare the data for\nthe policy. In Gym, this is usually achieved via wrappers. TorchRL takes a different\napproach, more similar to other pytorch domain libraries, through the use of transforms.\nTo add transforms to an environment, one should simply wrap it in a TransformedEnv\ninstance, and append the sequence of transforms to it. The transformed env will inherit\nthe device and meta-data of the wrapped env, and transform these depending on the sequence\nof transforms it contains.",
            "markdown"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Define an environment->Normalization": [
        [
            "The first to encode is a normalization transform.\nAs a rule of thumbs, it is preferable to have data that loosely\nmatch a unit Gaussian distribution: to obtain this, we will\nrun a certain number of random steps in the environment and compute\nthe summary statistics of these observations.",
            "markdown"
        ],
        [
            "We\u2019ll append two other transforms: the DoubleToFloat transform will\nconvert double entries to single-precision numbers, ready to be read by the\npolicy. The StepCounter transform will be used to count the steps before\nthe environment is terminated. We will use this measure as a supplementary measure\nof performance.",
            "markdown"
        ],
        [
            "As we will see later, many of the TorchRL\u2019s classes rely on \nto communicate. You could think of it as a python dictionary with some extra\ntensor features. In practice, this means that many modules we will be working\nwith need to be told what key to read (in_keys) and what key to write\n(out_keys) in the tensordict they will receive. Usually, if out_keys\nis omitted, it is assumed that the in_keys entries will be updated\nin-place. For our transforms, the only entry we are interested in is referred\nto as \"observation\" and our transform layers will be told to modify this\nentry and this entry only:",
            "markdown"
        ],
        [
            " = (\n    ,\n    (\n        # normalize observations\n        (in_keys=[\"observation\"]),\n        (in_keys=[\"observation\"]),\n        (),\n    ),\n)",
            "code"
        ],
        [
            "As you may have noticed, we have created a normalization layer but we did not\nset its normalization parameters. To do this, ObservationNorm can\nautomatically gather the summary statistics of our environment:",
            "markdown"
        ],
        [
            ".transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)",
            "code"
        ],
        [
            "The ObservationNorm transform has now been populated with a\nlocation and a scale that will be used to normalize the data.",
            "markdown"
        ],
        [
            "Let us do a little sanity check for the shape of our summary stats:",
            "markdown"
        ],
        [
            "print(\"normalization constant shape:\", .transform[0].loc.shape)",
            "code"
        ],
        [
            "normalization constant shape: torch.Size([11])",
            "code"
        ],
        [
            "An environment is not only defined by its simulator and transforms, but also\nby a series of metadata that describe what can be expected during its\nexecution.\nFor efficiency purposes, TorchRL is quite stringent when it comes to\nenvironment specs, but you can easily check that your environment specs are\nadequate.\nIn our example, the GymWrapper and GymEnv that inherits\nfrom it already take care of setting the proper specs for your env so\nyou should not have to care about this.",
            "markdown"
        ],
        [
            "Nevertheless, let\u2019s see a concrete example using our transformed\nenvironment by looking at its specs.\nThere are three specs to look at: observation_spec which defines what\nis to be expected when executing an action in the environment,\nreward_spec which indicates the reward domain and finally the\ninput_spec (which contains the action_spec) and which represents\neverything an environment requires to execute a single step.",
            "markdown"
        ],
        [
            "print(\"observation_spec:\", )\nprint(\"reward_spec:\", )\nprint(\"input_spec:\", )\nprint(\"action_spec (as defined by input_spec):\", )",
            "code"
        ],
        [
            "observation_spec: CompositeSpec(\n    observation: UnboundedContinuousTensorSpec(\n         shape=torch.Size([11]), space=None, device=cuda:0, dtype=torch.float32, domain=continuous),\n    step_count: UnboundedDiscreteTensorSpec(\n         shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, contiguous=True)), device=cuda:0, dtype=torch.int64, domain=continuous), device=cuda:0, shape=torch.Size([]))\nreward_spec: UnboundedContinuousTensorSpec(\n     shape=torch.Size([1]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)), device=cuda:0, dtype=torch.float32, domain=continuous)\ninput_spec: CompositeSpec(\n    action: BoundedTensorSpec(\n         shape=torch.Size([1]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)), device=cuda:0, dtype=torch.float32, domain=continuous),\n    step_count: UnboundedDiscreteTensorSpec(\n         shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, contiguous=True)), device=cuda:0, dtype=torch.int64, domain=continuous), device=cuda:0, shape=torch.Size([]))\naction_spec (as defined by input_spec): BoundedTensorSpec(\n     shape=torch.Size([1]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)), device=cuda:0, dtype=torch.float32, domain=continuous)",
            "code"
        ],
        [
            "the check_env_specs() function runs a small rollout and compares its output against the environemnt\nspecs. If no error is raised, we can be confident that the specs are properly defined:",
            "markdown"
        ],
        [
            "()",
            "code"
        ],
        [
            "check_env_specs succeeded!",
            "code"
        ],
        [
            "For fun, let\u2019s see what a simple random rollout looks like. You can\ncall <cite>env.rollout(n_steps)</cite> and get an overview of what the environment inputs\nand outputs look like. Actions will automatically be drawn from the action spec\ndomain, so you don\u2019t need to care about designing a random sampler.",
            "markdown"
        ],
        [
            "Typically, at each step, an RL environment receives an\naction as input, and outputs an observation, a reward and a done state. The\nobservation may be composite, meaning that it could be composed of more than one\ntensor. This is not a problem for TorchRL, since the whole set of observations\nis automatically packed in the output . After executing a rollout\n(ie a sequence of environment steps and random action generations) over a given\nnumber of steps, we will retrieve a  instance with a shape\nthat matches this trajectory length:",
            "markdown"
        ],
        [
            " = (3)\nprint(\"rollout of three steps:\", )\nprint(\"Shape of the rollout TensorDict:\", )",
            "code"
        ],
        [
            "rollout of three steps: TensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n        next: TensorDict(\n            fields={\n                done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n                observation: Tensor(shape=torch.Size([3, 11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n                reward: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n                step_count: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n            batch_size=torch.Size([3]),\n            device=cuda:0,\n            is_shared=True),\n        observation: Tensor(shape=torch.Size([3, 11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        reward: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        step_count: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n    batch_size=torch.Size([3]),\n    device=cuda:0,\n    is_shared=True)\nShape of the rollout TensorDict: torch.Size([3])",
            "code"
        ],
        [
            "Our rollout data has a shape of torch.Size([3])`, which matches the number of steps\nwe ran it for. The ``\"next\" entry points to the data coming after the current step.\nIn most cases, the \"next\"\" data at time <cite>t</cite> matches the data at t+1, but this\nmay not be the case if we are using some specific transformations (e.g. mutli-step).",
            "markdown"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Policy": [
        [
            "PPO utilizes a stochastic policy to handle exploration. This means that our\nneural network will have to output the parameters of a distribution, rather\nthan a single value corresponding to the action taken.",
            "markdown"
        ],
        [
            "As the data is continuous, we use a Tanh-Normal distribution to respect the\naction space boundaries. TorchRL provides such distribution, and the only\nthing we need to care about is to build a neural network that outputs the\nright number of parameters for the policy to work with (a location, or mean,\nand a scale):\n\n\\[f_{\\theta}(\\text{observation}) = \\mu_{\\theta}(\\text{observation}), \\sigma^{+}_{\\theta}(\\text{observation})\\]",
            "markdown"
        ],
        [
            "The only extra-difficulty that is brought up here is to split our output in two\nequal parts and map the second to a scrictly positive space.",
            "markdown"
        ],
        [
            "We design the policy in three steps:",
            "markdown"
        ],
        [
            "Define a neural network D_obs -&gt; 2 * D_action. Indeed, our loc (mu) and scale (sigma) both have dimension D_action;",
            "markdown"
        ],
        [
            "Append a NormalParamExtractor to extract a location and a scale (ie splits the input in two equal parts\n\n<blockquote>",
            "markdown"
        ],
        [
            "and applies a positive transformation to the scale parameter);\n</blockquote>",
            "markdown"
        ],
        [
            "Create a probabilistic TensorDictModule that can create this distribution and sample from it.",
            "markdown"
        ],
        [
            " = (\n    (num_cells, device=device),\n    (),\n    (num_cells, device=device),\n    (),\n    (num_cells, device=device),\n    (),\n    (2 * [-1], device=device),\n    (),\n)",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning:\n\nLazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.",
            "code"
        ],
        [
            "To enable the policy to \u201ctalk\u201d with the environment through the tensordict\ndata carrier, we wrap the nn.Module in a TensorDictModule. This\nclass will simply ready the in_keys it is provided with and write the\noutputs in-place at the registered out_keys.",
            "markdown"
        ],
        [
            " = (\n    , in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n)",
            "code"
        ],
        [
            "We now need to build a distribution out of the location and scale of our\nnormal distribution. To do so, we instruct the ProbabilisticActor\nclass to build a TanhNormal out of the location and scale\nparameters. We also provide the minimum and maximum values of this\ndistribution, which we gather from the environment specs.",
            "markdown"
        ],
        [
            "The name of the in_keys (and hence the name of the out_keys from\nthe TensorDictModule above) cannot be set to any value one may\nlike, as the TanhNormal distribution constructor will expect the\nloc and scale keyword arguments. That being said,\nProbabilisticActor also accepts Dict[str, str] typed in_keys\nwhere the key-value pair indicates what in_key string should be used for\nevery keyword argument that is to be used.",
            "markdown"
        ],
        [
            " = (\n    module=,\n    spec=,\n    in_keys=[\"loc\", \"scale\"],\n    distribution_class=,\n    distribution_kwargs={\n        \"min\": ,\n        \"max\": ,\n    },\n    return_log_prob=True,\n    # we'll need the log-prob for the numerator of the importance weights\n)",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Value network": [
        [
            "The value network is a crucial component of the PPO algorithm, even though it\nwon\u2019t be used at inference time. This module will read the observations and\nreturn an estimation of the discounted return for the following trajectory.\nThis allows us to amortize learning by relying on the some utility estimation\nthat is learnt on-the-fly during training. Our value network share the same\nstructure as the policy, but for simplicity we assign it its own set of\nparameters.",
            "markdown"
        ],
        [
            " = (\n    (num_cells, device=device),\n    (),\n    (num_cells, device=device),\n    (),\n    (num_cells, device=device),\n    (),\n    (1, device=device),\n)\n\n = (\n    module=,\n    in_keys=[\"observation\"],\n)",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning:\n\nLazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.",
            "code"
        ],
        [
            "let\u2019s try our policy and value modules. As we said earlier, the usage of\nTensorDictModule makes it possible to directly read the output\nof the environment to run these modules, as they know what information to read\nand where to write it:",
            "markdown"
        ],
        [
            "print(\"Running policy:\", (()))\nprint(\"Running value:\", (()))",
            "code"
        ],
        [
            "Running policy: TensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n        loc: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        observation: Tensor(shape=torch.Size([11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        reward: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        sample_log_prob: Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        scale: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        step_count: Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n    batch_size=torch.Size([]),\n    device=cuda:0,\n    is_shared=True)\nRunning value: TensorDict(\n    fields={\n        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n        observation: Tensor(shape=torch.Size([11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        reward: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        state_value: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        step_count: Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n    batch_size=torch.Size([]),\n    device=cuda:0,\n    is_shared=True)",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Data collector": [
        [
            "TorchRL provides a set of DataCollector classes. Briefly, these\nclasses execute three operations: reset an environment, compute an action\ngiven the latest observation, execute a step in the environment, and repeat\nthe last two steps until the environment reaches a stop signal (or \"done\"\nstate).",
            "markdown"
        ],
        [
            "They allow you to control how many frames to collect at each iteration\n(through the frames_per_batch parameter),\nwhen to reset the environment (through the max_frames_per_traj argument),\non which device the policy should be executed, etc. They are also\ndesigned to work efficiently with batched and multiprocessed environments.",
            "markdown"
        ],
        [
            "The simplest data collector is the SyncDataCollector: it is an\niterator that you can use to get batches of data of a given length, and\nthat will stop once a total number of frames (total_frames) have been\ncollected.\nOther data collectors (MultiSyncDataCollector and\nMultiaSyncDataCollector) will execute the same operations in synchronous\nand asynchronous manner over a set of multiprocessed workers.",
            "markdown"
        ],
        [
            "As for the policy and environment before, the data collector will return\n instances with a total number of elements that will\nmatch frames_per_batch. Using  to pass data to the\ntraining loop allows you to write dataloading pipelines\nthat are 100% oblivious to the actual specificities of the rollout content.",
            "markdown"
        ],
        [
            " = (\n    ,\n    ,\n    frames_per_batch=frames_per_batch,\n    total_frames=total_frames,\n    split_trajs=False,\n    device=device,\n)",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Replay buffer": [
        [
            "Replay buffers are a common building piece of off-policy RL algorithms.\nIn on-policy contexts, a replay buffer is refilled every time a batch of\ndata is collected, and its data is repeatedly consumed for a certain number\nof epochs.",
            "markdown"
        ],
        [
            "TorchRL\u2019s replay buffers are built using a common container\nReplayBuffer which takes as argument the components of the buffer:\na storage, a writer, a sampler and possibly some transforms. Only the\nstorage (which indicates the replay buffer capacity) is mandatory. We\nalso specify a sampler without repetition to avoid sampling multiple times\nthe same item in one epoch.\nUsing a replay buffer for PPO is not mandatory and we could simply\nsample the sub-batches from the collected batch, but using these classes\nmake it easy for us to build the inner training loop in a reproducible way.",
            "markdown"
        ],
        [
            " = (\n    storage=(frames_per_batch),\n    sampler=SamplerWithoutReplacement(),\n)",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torchrl/data/replay_buffers/replay_buffers.py:151: UserWarning:\n\nConstructing replay buffer without specifying behaviour is no longer recommended, and will be deprecated in the future.",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Loss function": [
        [
            "The PPO loss can be directly imported from torchrl for convenience using the\nClipPPOLoss class. This is the easiest way of utilizing PPO:\nit hides away the mathematical operations of PPO and the control flow that\ngoes with it.",
            "markdown"
        ],
        [
            "PPO requires some \u201cadvantage estimation\u201d to be computed. In short, an advantage\nis a value that reflects an expectancy over the return value while dealing with\nthe bias / variance tradeoff.\nTo compute the advantage, one just needs to (1) build the advantage module, which\nutilizes our value operator, and (2) pass each batch of data through it before each\nepoch.\nThe GAE module will update the input tensordict with new \"advantage\" and\n\"value_target\" entries.\nThe \"value_target\" is a gradient-free tensor that represents the empirical\nvalue that the value network should represent with the input observation.\nBoth of these will be used by ClipPPOLoss to\nreturn the policy and value losses.",
            "markdown"
        ],
        [
            " = (\n    gamma=gamma, lmbda=lmbda, value_network=, average_gae=True\n)\n\n = (\n    actor=,\n    critic=,\n    advantage_key=\"advantage\",\n    clip_epsilon=clip_epsilon,\n    entropy_bonus=bool(entropy_eps),\n    entropy_coef=entropy_eps,\n    # these keys match by default but we set this for completeness\n    value_target_key=.value_target_key,\n    critic_coef=1.0,\n    gamma=0.99,\n    loss_critic_type=\"smooth_l1\",\n)\n\n = ((), lr)\n = (\n    , total_frames // frames_per_batch, 0.0\n)",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Training loop": [
        [
            "We now have all the pieces needed to code our training loop.\nThe steps include:",
            "markdown"
        ],
        [
            "Collect data",
            "markdown"
        ],
        [
            "Compute advantage",
            "markdown"
        ],
        [
            "Loop over the collected to compute loss values",
            "markdown"
        ],
        [
            "Back propagate",
            "markdown"
        ],
        [
            "Optimize",
            "markdown"
        ],
        [
            "Repeat",
            "markdown"
        ],
        [
            "Repeat",
            "markdown"
        ],
        [
            "Repeat",
            "markdown"
        ],
        [
            "logs = defaultdict(list)\npbar = tqdm(total=total_frames * frame_skip)\neval_str = \"\"\n\n# We iterate over the collector until it reaches the total number of frames it was\n# designed to collect:\nfor i,  in enumerate():\n    # we now have a batch of data to work with. Let's learn something from it.\n    for _ in range(num_epochs):\n        # We'll need an \"advantage\" signal to make PPO work.\n        # We re-compute it at each epoch as its value depends on the value\n        # network which is updated in the inner loop.\n        ()\n         = (-1)\n        (())\n        for _ in range(frames_per_batch // sub_batch_size):\n            , *_ = (sub_batch_size)\n             = ((device))\n             = (\n                [\"loss_objective\"]\n                + [\"loss_critic\"]\n                + [\"loss_entropy\"]\n            )\n\n            # Optimization: backward, grad clipping and optim step\n            ()\n            # this is not strictly mandatory but it's good practice to keep\n            # your gradient norm bounded\n            ((), max_grad_norm)\n            .step()\n            ()\n\n    logs[\"reward\"].append([\"next\", \"reward\"].mean().item())\n    pbar.update(() * frame_skip)\n    cum_reward_str = (\n        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n    )\n    logs[\"step_count\"].append([\"step_count\"].max().item())\n    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n    logs[\"lr\"].append(.param_groups[0][\"lr\"])\n    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n    if i % 10 == 0:\n        # We evaluate the policy once every 10 batches of data.\n        # Evaluation is rather simple: execute the policy without exploration\n        # (take the expected value of the action distribution) for a given\n        # number of steps (1000, which is our env horizon).\n        # The ``rollout`` method of the env can take a policy as argument:\n        # it will then execute this policy at each step.\n        with (\"mean\"), ():\n            # execute a rollout with the trained policy\n            eval_rollout = (1000, )\n            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n            logs[\"eval reward (sum)\"].append(\n                eval_rollout[\"next\", \"reward\"].sum().item()\n            )\n            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n            eval_str = (\n                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n                f\"eval step-count: {logs['eval step_count'][-1]}\"\n            )\n            del eval_rollout\n    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n\n    # We're also using a learning rate scheduler. Like the gradient clipping,\n    # this is a nice-to-have but nothing necessary for PPO to work.\n    .step()",
            "code"
        ],
        [
            "  0%|          | 0/50000 [00:00&lt;?, ?it/s]/opt/conda/lib/python3.10/site-packages/torchrl/data/replay_buffers/replay_buffers.py:274: UserWarning:\n\nbatch_size argument in sample has been deprecated. Set the batch_size when constructing the replay buffer instead.\n\n\n  2%|2         | 1000/50000 [00:07&lt;05:53, 138.65it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.0758 (init= 9.0758), step count (max): 13, lr policy:  0.0003:   2%|2         | 1000/50000 [00:07&lt;05:53, 138.65it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.0758 (init= 9.0758), step count (max): 13, lr policy:  0.0003:   4%|4         | 2000/50000 [00:14&lt;05:44, 139.16it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1163 (init= 9.0758), step count (max): 13, lr policy:  0.0003:   4%|4         | 2000/50000 [00:14&lt;05:44, 139.16it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1163 (init= 9.0758), step count (max): 13, lr policy:  0.0003:   6%|6         | 3000/50000 [00:21&lt;05:38, 138.87it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1412 (init= 9.0758), step count (max): 14, lr policy:  0.0003:   6%|6         | 3000/50000 [00:21&lt;05:38, 138.87it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1412 (init= 9.0758), step count (max): 14, lr policy:  0.0003:   8%|8         | 4000/50000 [00:28&lt;05:27, 140.51it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1640 (init= 9.0758), step count (max): 19, lr policy:  0.0003:   8%|8         | 4000/50000 [00:28&lt;05:27, 140.51it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1640 (init= 9.0758), step count (max): 19, lr policy:  0.0003:  10%|#         | 5000/50000 [00:35&lt;05:20, 140.27it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1717 (init= 9.0758), step count (max): 17, lr policy:  0.0003:  10%|#         | 5000/50000 [00:35&lt;05:20, 140.27it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1717 (init= 9.0758), step count (max): 17, lr policy:  0.0003:  12%|#2        | 6000/50000 [00:42&lt;05:14, 139.73it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1535 (init= 9.0758), step count (max): 23, lr policy:  0.0003:  12%|#2        | 6000/50000 [00:42&lt;05:14, 139.73it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1535 (init= 9.0758), step count (max): 23, lr policy:  0.0003:  14%|#4        | 7000/50000 [00:49&lt;05:04, 141.24it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1632 (init= 9.0758), step count (max): 21, lr policy:  0.0003:  14%|#4        | 7000/50000 [00:49&lt;05:04, 141.24it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1632 (init= 9.0758), step count (max): 21, lr policy:  0.0003:  16%|#6        | 8000/50000 [00:56&lt;04:54, 142.65it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1584 (init= 9.0758), step count (max): 18, lr policy:  0.0003:  16%|#6        | 8000/50000 [00:56&lt;04:54, 142.65it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1584 (init= 9.0758), step count (max): 18, lr policy:  0.0003:  18%|#8        | 9000/50000 [01:03&lt;04:48, 142.29it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1305 (init= 9.0758), step count (max): 24, lr policy:  0.0003:  18%|#8        | 9000/50000 [01:03&lt;04:48, 142.29it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1305 (init= 9.0758), step count (max): 24, lr policy:  0.0003:  20%|##        | 10000/50000 [01:10&lt;04:39, 142.86it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1164 (init= 9.0758), step count (max): 17, lr policy:  0.0003:  20%|##        | 10000/50000 [01:10&lt;04:39, 142.86it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1164 (init= 9.0758), step count (max): 17, lr policy:  0.0003:  22%|##2       | 11000/50000 [01:17&lt;04:32, 142.94it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1362 (init= 9.0758), step count (max): 23, lr policy:  0.0003:  22%|##2       | 11000/50000 [01:17&lt;04:32, 142.94it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1362 (init= 9.0758), step count (max): 23, lr policy:  0.0003:  24%|##4       | 12000/50000 [01:24&lt;04:26, 142.55it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1655 (init= 9.0758), step count (max): 23, lr policy:  0.0003:  24%|##4       | 12000/50000 [01:24&lt;04:26, 142.55it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1655 (init= 9.0758), step count (max): 23, lr policy:  0.0003:  26%|##6       | 13000/50000 [01:31&lt;04:19, 142.33it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1693 (init= 9.0758), step count (max): 24, lr policy:  0.0003:  26%|##6       | 13000/50000 [01:31&lt;04:19, 142.33it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1693 (init= 9.0758), step count (max): 24, lr policy:  0.0003:  28%|##8       | 14000/50000 [01:39&lt;04:18, 139.26it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1620 (init= 9.0758), step count (max): 27, lr policy:  0.0003:  28%|##8       | 14000/50000 [01:39&lt;04:18, 139.26it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1620 (init= 9.0758), step count (max): 27, lr policy:  0.0003:  30%|###       | 15000/50000 [01:46&lt;04:10, 139.46it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1641 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  30%|###       | 15000/50000 [01:46&lt;04:10, 139.46it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1641 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  32%|###2      | 16000/50000 [01:53&lt;04:02, 140.35it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1836 (init= 9.0758), step count (max): 19, lr policy:  0.0002:  32%|###2      | 16000/50000 [01:53&lt;04:02, 140.35it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1836 (init= 9.0758), step count (max): 19, lr policy:  0.0002:  34%|###4      | 17000/50000 [02:00&lt;03:54, 140.51it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1994 (init= 9.0758), step count (max): 19, lr policy:  0.0002:  34%|###4      | 17000/50000 [02:00&lt;03:54, 140.51it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1994 (init= 9.0758), step count (max): 19, lr policy:  0.0002:  36%|###6      | 18000/50000 [02:07&lt;03:47, 140.39it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1935 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  36%|###6      | 18000/50000 [02:07&lt;03:47, 140.39it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1935 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  38%|###8      | 19000/50000 [02:14&lt;03:41, 139.88it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1964 (init= 9.0758), step count (max): 23, lr policy:  0.0002:  38%|###8      | 19000/50000 [02:14&lt;03:41, 139.88it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1964 (init= 9.0758), step count (max): 23, lr policy:  0.0002:  40%|####      | 20000/50000 [02:22&lt;03:33, 140.32it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1813 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  40%|####      | 20000/50000 [02:22&lt;03:33, 140.32it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1813 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  42%|####2     | 21000/50000 [02:29&lt;03:27, 140.03it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1820 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  42%|####2     | 21000/50000 [02:29&lt;03:27, 140.03it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1820 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  44%|####4     | 22000/50000 [02:36&lt;03:23, 137.86it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1896 (init= 9.0758), step count (max): 24, lr policy:  0.0002:  44%|####4     | 22000/50000 [02:36&lt;03:23, 137.86it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1896 (init= 9.0758), step count (max): 24, lr policy:  0.0002:  46%|####6     | 23000/50000 [02:43&lt;03:13, 139.33it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1771 (init= 9.0758), step count (max): 23, lr policy:  0.0002:  46%|####6     | 23000/50000 [02:43&lt;03:13, 139.33it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1771 (init= 9.0758), step count (max): 23, lr policy:  0.0002:  48%|####8     | 24000/50000 [02:50&lt;03:06, 139.53it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1971 (init= 9.0758), step count (max): 27, lr policy:  0.0002:  48%|####8     | 24000/50000 [02:50&lt;03:06, 139.53it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1971 (init= 9.0758), step count (max): 27, lr policy:  0.0002:  50%|#####     | 25000/50000 [02:57&lt;02:58, 139.98it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1933 (init= 9.0758), step count (max): 23, lr policy:  0.0002:  50%|#####     | 25000/50000 [02:57&lt;02:58, 139.98it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1933 (init= 9.0758), step count (max): 23, lr policy:  0.0002:  52%|#####2    | 26000/50000 [03:04&lt;02:50, 141.12it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1828 (init= 9.0758), step count (max): 20, lr policy:  0.0001:  52%|#####2    | 26000/50000 [03:04&lt;02:50, 141.12it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1828 (init= 9.0758), step count (max): 20, lr policy:  0.0001:  54%|#####4    | 27000/50000 [03:11&lt;02:42, 141.33it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1912 (init= 9.0758), step count (max): 22, lr policy:  0.0001:  54%|#####4    | 27000/50000 [03:11&lt;02:42, 141.33it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1912 (init= 9.0758), step count (max): 22, lr policy:  0.0001:  56%|#####6    | 28000/50000 [03:19&lt;02:35, 141.18it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.2035 (init= 9.0758), step count (max): 34, lr policy:  0.0001:  56%|#####6    | 28000/50000 [03:19&lt;02:35, 141.18it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.2035 (init= 9.0758), step count (max): 34, lr policy:  0.0001:  58%|#####8    | 29000/50000 [03:26&lt;02:28, 141.16it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.2096 (init= 9.0758), step count (max): 24, lr policy:  0.0001:  58%|#####8    | 29000/50000 [03:26&lt;02:28, 141.16it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.2096 (init= 9.0758), step count (max): 24, lr policy:  0.0001:  60%|######    | 30000/50000 [03:33&lt;02:22, 140.15it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1983 (init= 9.0758), step count (max): 22, lr policy:  0.0001:  60%|######    | 30000/50000 [03:33&lt;02:22, 140.15it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1983 (init= 9.0758), step count (max): 22, lr policy:  0.0001:  62%|######2   | 31000/50000 [03:40&lt;02:16, 139.09it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.1963 (init= 9.0758), step count (max): 24, lr policy:  0.0001:  62%|######2   | 31000/50000 [03:40&lt;02:16, 139.09it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.1963 (init= 9.0758), step count (max): 24, lr policy:  0.0001:  64%|######4   | 32000/50000 [03:47&lt;02:08, 139.72it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2000 (init= 9.0758), step count (max): 22, lr policy:  0.0001:  64%|######4   | 32000/50000 [03:47&lt;02:08, 139.72it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2000 (init= 9.0758), step count (max): 22, lr policy:  0.0001:  66%|######6   | 33000/50000 [03:54&lt;02:01, 140.47it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.1990 (init= 9.0758), step count (max): 23, lr policy:  0.0001:  66%|######6   | 33000/50000 [03:54&lt;02:01, 140.47it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.1990 (init= 9.0758), step count (max): 23, lr policy:  0.0001:  68%|######8   | 34000/50000 [04:01&lt;01:53, 141.00it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2098 (init= 9.0758), step count (max): 24, lr policy:  0.0001:  68%|######8   | 34000/50000 [04:01&lt;01:53, 141.00it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2098 (init= 9.0758), step count (max): 24, lr policy:  0.0001:  70%|#######   | 35000/50000 [04:09&lt;01:47, 139.98it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2150 (init= 9.0758), step count (max): 23, lr policy:  0.0001:  70%|#######   | 35000/50000 [04:09&lt;01:47, 139.98it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2150 (init= 9.0758), step count (max): 23, lr policy:  0.0001:  72%|#######2  | 36000/50000 [04:16&lt;01:39, 141.13it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2132 (init= 9.0758), step count (max): 28, lr policy:  0.0001:  72%|#######2  | 36000/50000 [04:16&lt;01:39, 141.13it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2132 (init= 9.0758), step count (max): 28, lr policy:  0.0001:  74%|#######4  | 37000/50000 [04:23&lt;01:31, 142.07it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2048 (init= 9.0758), step count (max): 34, lr policy:  0.0001:  74%|#######4  | 37000/50000 [04:23&lt;01:31, 142.07it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2048 (init= 9.0758), step count (max): 34, lr policy:  0.0001:  76%|#######6  | 38000/50000 [04:30&lt;01:24, 141.98it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2144 (init= 9.0758), step count (max): 23, lr policy:  0.0000:  76%|#######6  | 38000/50000 [04:30&lt;01:24, 141.98it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2144 (init= 9.0758), step count (max): 23, lr policy:  0.0000:  78%|#######8  | 39000/50000 [04:37&lt;01:18, 140.30it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2123 (init= 9.0758), step count (max): 26, lr policy:  0.0000:  78%|#######8  | 39000/50000 [04:37&lt;01:18, 140.30it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2123 (init= 9.0758), step count (max): 26, lr policy:  0.0000:  80%|########  | 40000/50000 [04:44&lt;01:11, 140.39it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2194 (init= 9.0758), step count (max): 38, lr policy:  0.0000:  80%|########  | 40000/50000 [04:44&lt;01:11, 140.39it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2194 (init= 9.0758), step count (max): 38, lr policy:  0.0000:  82%|########2 | 41000/50000 [04:51&lt;01:04, 140.17it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2183 (init= 9.0758), step count (max): 34, lr policy:  0.0000:  82%|########2 | 41000/50000 [04:51&lt;01:04, 140.17it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2183 (init= 9.0758), step count (max): 34, lr policy:  0.0000:  84%|########4 | 42000/50000 [04:58&lt;00:56, 140.77it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2236 (init= 9.0758), step count (max): 27, lr policy:  0.0000:  84%|########4 | 42000/50000 [04:58&lt;00:56, 140.77it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2236 (init= 9.0758), step count (max): 27, lr policy:  0.0000:  86%|########6 | 43000/50000 [05:05&lt;00:49, 142.01it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2157 (init= 9.0758), step count (max): 26, lr policy:  0.0000:  86%|########6 | 43000/50000 [05:05&lt;00:49, 142.01it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2157 (init= 9.0758), step count (max): 26, lr policy:  0.0000:  88%|########8 | 44000/50000 [05:12&lt;00:42, 141.30it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2210 (init= 9.0758), step count (max): 24, lr policy:  0.0000:  88%|########8 | 44000/50000 [05:12&lt;00:42, 141.30it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2210 (init= 9.0758), step count (max): 24, lr policy:  0.0000:  90%|######### | 45000/50000 [05:19&lt;00:35, 141.66it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2116 (init= 9.0758), step count (max): 23, lr policy:  0.0000:  90%|######### | 45000/50000 [05:19&lt;00:35, 141.66it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2116 (init= 9.0758), step count (max): 23, lr policy:  0.0000:  92%|#########2| 46000/50000 [05:26&lt;00:28, 142.03it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2216 (init= 9.0758), step count (max): 22, lr policy:  0.0000:  92%|#########2| 46000/50000 [05:26&lt;00:28, 142.03it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2216 (init= 9.0758), step count (max): 22, lr policy:  0.0000:  94%|#########3| 47000/50000 [05:33&lt;00:21, 141.37it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2181 (init= 9.0758), step count (max): 27, lr policy:  0.0000:  94%|#########3| 47000/50000 [05:33&lt;00:21, 141.37it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2181 (init= 9.0758), step count (max): 27, lr policy:  0.0000:  96%|#########6| 48000/50000 [05:41&lt;00:14, 140.31it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2188 (init= 9.0758), step count (max): 25, lr policy:  0.0000:  96%|#########6| 48000/50000 [05:41&lt;00:14, 140.31it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2188 (init= 9.0758), step count (max): 25, lr policy:  0.0000:  98%|#########8| 49000/50000 [05:48&lt;00:07, 140.94it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2166 (init= 9.0758), step count (max): 28, lr policy:  0.0000:  98%|#########8| 49000/50000 [05:48&lt;00:07, 140.94it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2166 (init= 9.0758), step count (max): 28, lr policy:  0.0000: 100%|##########| 50000/50000 [05:55&lt;00:00, 141.35it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2224 (init= 9.0758), step count (max): 28, lr policy:  0.0000: 100%|##########| 50000/50000 [05:55&lt;00:00, 141.35it/s]",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Results": [
        [
            "Before the 1M step cap is reached, the algorithm should have reached a max\nstep count of 1000 steps, which is the maximum number of steps before the\ntrajectory is truncated.",
            "markdown"
        ],
        [
            "plt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1)\nplt.plot(logs[\"reward\"])\nplt.title(\"training rewards (average)\")\nplt.subplot(2, 2, 2)\nplt.plot(logs[\"step_count\"])\nplt.title(\"Max step count (training)\")\nplt.subplot(2, 2, 3)\nplt.plot(logs[\"eval reward (sum)\"])\nplt.title(\"Return (test)\")\nplt.subplot(2, 2, 4)\nplt.plot(logs[\"eval step_count\"])\nplt.title(\"Max step count (test)\")\nplt.show()\n\n\n<img alt=\"training rewards (average), Max step count (training), Return (test), Max step count (test)\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_reinforcement_ppo_001.png\" srcset=\"../_images/sphx_glr_reinforcement_ppo_001.png\"/>",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Conclusion and next steps": [
        [
            "In this tutorial, we have learned:",
            "markdown"
        ],
        [
            "How to create and customize an environment with torchrl;",
            "markdown"
        ],
        [
            "How to write a model and a loss function;",
            "markdown"
        ],
        [
            "How to set up a typical training loop.",
            "markdown"
        ],
        [
            "If you want to experiment with this tutorial a bit more, you can apply the following modifications:",
            "markdown"
        ],
        [
            "From an efficiency perspective,\nwe could run several simulations in parallel to speed up data collection.\nCheck  for further information.",
            "markdown"
        ],
        [
            "From a logging perspective, one could add a torchrl.record.VideoRecorder transform to\nthe environment after asking for rendering to get a visual rendering of the\ninverted pendulum in action. Check torchrl.record to\nknow more.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 5 minutes  59.034 seconds)",
            "markdown"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent": [
        [
            "Authors: , , , .",
            "markdown"
        ],
        [
            "This tutorial walks you through the fundamentals of Deep Reinforcement\nLearning. At the end, you will implement an AI-powered Mario (using\n) that\ncan play the game by itself.",
            "markdown"
        ],
        [
            "Although no prior knowledge of RL is necessary for this tutorial, you\ncan familiarize yourself with these RL\n,\nand have this handy\n\nas your companion. The full code is available\n.\n\n<img alt=\"mario\" src=\"../_images/mario.gif\"/>",
            "markdown"
        ],
        [
            "%%bash\npip install gym-super-mario-bros==7.4.0",
            "code"
        ],
        [
            "import torch\nfrom torch import nn\nfrom torchvision import transforms as T\nfrom PIL import Image\nimport numpy as np\nfrom pathlib import Path\nfrom collections import deque\nimport random, datetime, os, copy\n\n# Gym is an OpenAI toolkit for RL\nimport gym\nfrom gym.spaces import Box\nfrom gym.wrappers import FrameStack\n\n# NES Emulator for OpenAI Gym\nfrom nes_py.wrappers import JoypadSpace\n\n# Super Mario environment for OpenAI Gym\nimport gym_super_mario_bros",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->RL Definitions": [
        [
            "<strong>Environment</strong> The world that an agent interacts with and learns from.",
            "markdown"
        ],
        [
            "<strong>Action</strong> \\(a\\) : How the Agent responds to the Environment. The\nset of all possible Actions is called <em>action-space</em>.",
            "markdown"
        ],
        [
            "<strong>State</strong> \\(s\\) : The current characteristic of the Environment. The\nset of all possible States the Environment can be in is called\n<em>state-space</em>.",
            "markdown"
        ],
        [
            "<strong>Reward</strong> \\(r\\) : Reward is the key feedback from Environment to\nAgent. It is what drives the Agent to learn and to change its future\naction. An aggregation of rewards over multiple time steps is called\n<strong>Return</strong>.",
            "markdown"
        ],
        [
            "<strong>Optimal Action-Value function</strong> \\(Q^*(s,a)\\) : Gives the expected\nreturn if you start in state \\(s\\), take an arbitrary action\n\\(a\\), and then for each future time step take the action that\nmaximizes returns. \\(Q\\) can be said to stand for the \u201cquality\u201d of\nthe action in a state. We try to approximate this function.",
            "markdown"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Environment->Initialize Environment": [
        [
            "In Mario, the environment consists of tubes, mushrooms and other\ncomponents.",
            "markdown"
        ],
        [
            "When Mario makes an action, the environment responds with the changed\n(next) state, reward and other info.",
            "markdown"
        ],
        [
            "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\nif gym.__version__ &lt; '0.26':\n    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\nelse:\n    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\n\n# Limit the action-space to\n#   0. walk right\n#   1. jump right\nenv = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n\nenv.reset()\nnext_state, reward, done, trunc, info = env.step(action=0)\nprint(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning:\n\nWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\n\n/opt/conda/lib/python3.10/site-packages/gym/envs/registration.py:627: UserWarning:\n\nWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\n\n(240, 256, 3),\n 0.0,\n False,\n {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Environment->Preprocess Environment": [
        [
            "Environment data is returned to the agent in next_state. As you saw\nabove, each state is represented by a [3, 240, 256] size array.\nOften that is more information than our agent needs; for instance,\nMario\u2019s actions do not depend on the color of the pipes or the sky!",
            "markdown"
        ],
        [
            "We use <strong>Wrappers</strong> to preprocess environment data before sending it to\nthe agent.",
            "markdown"
        ],
        [
            "GrayScaleObservation is a common wrapper to transform an RGB image\nto grayscale; doing so reduces the size of the state representation\nwithout losing useful information. Now the size of each state:\n[1, 240, 256]",
            "markdown"
        ],
        [
            "ResizeObservation downsamples each observation into a square image.\nNew size: [1, 84, 84]",
            "markdown"
        ],
        [
            "SkipFrame is a custom wrapper that inherits from gym.Wrapper and\nimplements the step() function. Because consecutive frames don\u2019t\nvary much, we can skip n-intermediate frames without losing much\ninformation. The n-th frame aggregates rewards accumulated over each\nskipped frame.",
            "markdown"
        ],
        [
            "FrameStack is a wrapper that allows us to squash consecutive frames\nof the environment into a single observation point to feed to our\nlearning model. This way, we can identify if Mario was landing or\njumping based on the direction of his movement in the previous several\nframes.",
            "markdown"
        ],
        [
            "class SkipFrame(gym.Wrapper):\n    def __init__(self, env, skip):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        super().__init__(env)\n        self._skip = skip\n\n    def step(self, action):\n        \"\"\"Repeat action, and sum reward\"\"\"\n        total_reward = 0.0\n        for i in range(self._skip):\n            # Accumulate reward and repeat the same action\n            obs, reward, done, trunk, info = self.env.step(action)\n            total_reward += reward\n            if done:\n                break\n        return obs, total_reward, done, trunk, info\n\n\nclass GrayScaleObservation(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        obs_shape = self.observation_space.shape[:2]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def permute_orientation(self, observation):\n        # permute [H, W, C] array to [C, H, W] tensor\n        observation = np.transpose(observation, (2, 0, 1))\n        observation = (observation.copy(), dtype=)\n        return observation\n\n    def observation(self, observation):\n        observation = self.permute_orientation(observation)\n        transform = ()\n        observation = transform(observation)\n        return observation\n\n\nclass ResizeObservation(gym.ObservationWrapper):\n    def __init__(self, env, shape):\n        super().__init__(env)\n        if isinstance(shape, int):\n            self.shape = (shape, shape)\n        else:\n            self.shape = tuple(shape)\n\n        obs_shape = self.shape + self.observation_space.shape[2:]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def observation(self, observation):\n        transforms = (\n            [(self.shape), (0, 255)]\n        )\n        observation = transforms(observation).squeeze(0)\n        return observation\n\n\n# Apply Wrappers to environment\nenv = SkipFrame(env, skip=4)\nenv = GrayScaleObservation(env)\nenv = ResizeObservation(env, shape=84)\nif gym.__version__ &lt; '0.26':\n    env = FrameStack(env, num_stack=4, new_step_api=True)\nelse:\n    env = FrameStack(env, num_stack=4)",
            "code"
        ],
        [
            "After applying the above wrappers to the environment, the final wrapped\nstate consists of 4 gray-scaled consecutive frames stacked together, as\nshown above in the image on the left. Each time Mario makes an action,\nthe environment responds with a state of this structure. The structure\nis represented by a 3-D array of size [4, 84, 84].\n\n<img alt=\"picture\" src=\"../_images/mario_env.png\"/>",
            "markdown"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Agent": [
        [
            "We create a class Mario to represent our agent in the game. Mario\nshould be able to:",
            "markdown"
        ],
        [
            "<strong>Act</strong> according to the optimal action policy based on the current\nstate (of the environment).",
            "markdown"
        ],
        [
            "<strong>Remember</strong> experiences. Experience = (current state, current\naction, reward, next state). Mario <em>caches</em> and later <em>recalls</em> his\nexperiences to update his action policy.",
            "markdown"
        ],
        [
            "<strong>Learn</strong> a better action policy over time",
            "markdown"
        ],
        [
            "class Mario:\n    def __init__():\n        pass\n\n    def act(self, state):\n        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n        pass\n\n    def cache(self, experience):\n        \"\"\"Add the experience to memory\"\"\"\n        pass\n\n    def recall(self):\n        \"\"\"Sample experiences from memory\"\"\"\n        pass\n\n    def learn(self):\n        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n        pass",
            "code"
        ],
        [
            "In the following sections, we will populate Mario\u2019s parameters and\ndefine his functions.",
            "markdown"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Agent->Act": [
        [
            "For any given state, an agent can choose to do the most optimal action\n(<strong>exploit</strong>) or a random action (<strong>explore</strong>).",
            "markdown"
        ],
        [
            "Mario randomly explores with a chance of self.exploration_rate; when\nhe chooses to exploit, he relies on MarioNet (implemented in\nLearn section) to provide the most optimal action.",
            "markdown"
        ],
        [
            "class Mario:\n    def __init__(self, state_dim, action_dim, save_dir):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.save_dir = save_dir\n\n        self.device = \"cuda\" if () else \"cpu\"\n\n        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n        self.net = (self.state_dim, self.action_dim).float()\n        self.net = self.net.to(device=self.device)\n\n        self.exploration_rate = 1\n        self.exploration_rate_decay = 0.99999975\n        self.exploration_rate_min = 0.1\n        self.curr_step = 0\n\n        self.save_every = 5e5  # no. of experiences between saving Mario Net\n\n    def act(self, state):\n        \"\"\"\n    Given a state, choose an epsilon-greedy action and update value of step.\n\n    Inputs:\n    state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n    Outputs:\n    action_idx (int): An integer representing which action Mario will perform\n    \"\"\"\n        # EXPLORE\n        if np.random.rand() &lt; self.exploration_rate:\n            action_idx = np.random.randint(self.action_dim)\n\n        # EXPLOIT\n        else:\n            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n            state = (state, device=self.device).unsqueeze(0)\n            action_values = self.net(state, model=\"online\")\n            action_idx = (action_values, axis=1).item()\n\n        # decrease exploration_rate\n        self.exploration_rate *= self.exploration_rate_decay\n        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n\n        # increment step\n        self.curr_step += 1\n        return action_idx",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Agent->Cache and Recall": [
        [
            "These two functions serve as Mario\u2019s \u201cmemory\u201d process.",
            "markdown"
        ],
        [
            "cache(): Each time Mario performs an action, he stores the\nexperience to his memory. His experience includes the current\n<em>state</em>, <em>action</em> performed, <em>reward</em> from the action, the <em>next state</em>,\nand whether the game is <em>done</em>.",
            "markdown"
        ],
        [
            "recall(): Mario randomly samples a batch of experiences from his\nmemory, and uses that to learn the game.",
            "markdown"
        ],
        [
            "class Mario(Mario):  # subclassing for continuity\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.memory = deque(maxlen=100000)\n        self.batch_size = 32\n\n    def cache(self, state, next_state, action, reward, done):\n        \"\"\"\n        Store the experience to self.memory (replay buffer)\n\n        Inputs:\n        state (LazyFrame),\n        next_state (LazyFrame),\n        action (int),\n        reward (float),\n        done(bool))\n        \"\"\"\n        def first_if_tuple(x):\n            return x[0] if isinstance(x, tuple) else x\n        state = first_if_tuple(state).__array__()\n        next_state = first_if_tuple(next_state).__array__()\n\n        state = (state, device=self.device)\n        next_state = (next_state, device=self.device)\n        action = ([action], device=self.device)\n        reward = ([reward], device=self.device)\n        done = ([done], device=self.device)\n\n        self.memory.append((state, next_state, action, reward, done,))\n\n    def recall(self):\n        \"\"\"\n        Retrieve a batch of experiences from memory\n        \"\"\"\n        batch = random.sample(self.memory, self.batch_size)\n        state, next_state, action, reward, done = map(, zip(*batch))\n        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Agent->Learn": [
        [
            "Mario uses the \nunder the hood. DDQN uses two ConvNets - \\(Q_{online}\\) and\n\\(Q_{target}\\) - that independently approximate the optimal\naction-value function.",
            "markdown"
        ],
        [
            "In our implementation, we share feature generator features across\n\\(Q_{online}\\) and \\(Q_{target}\\), but maintain separate FC\nclassifiers for each. \\(\\theta_{target}\\) (the parameters of\n\\(Q_{target}\\)) is frozen to prevent updation by backprop. Instead,\nit is periodically synced with \\(\\theta_{online}\\) (more on this\nlater).",
            "markdown"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Agent->Learn->Neural Network": [
        [
            "class MarioNet():\n    \"\"\"mini cnn structure\n  input -&gt; (conv2d + relu) x 3 -&gt; flatten -&gt; (dense + relu) x 2 -&gt; output\n  \"\"\"\n\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        c, h, w = input_dim\n\n        if h != 84:\n            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n        if w != 84:\n            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n\n        self.online = (\n            (in_channels=c, out_channels=32, kernel_size=8, stride=4),\n            (),\n            (in_channels=32, out_channels=64, kernel_size=4, stride=2),\n            (),\n            (in_channels=64, out_channels=64, kernel_size=3, stride=1),\n            (),\n            (),\n            (3136, 512),\n            (),\n            (512, output_dim),\n        )\n\n        self.target = copy.deepcopy(self.online)\n\n        # Q_target parameters are frozen.\n        for p in self.target.parameters():\n            p.requires_grad = False\n\n    def forward(self, input, model):\n        if model == \"online\":\n            return self.online(input)\n        elif model == \"target\":\n            return self.target(input)",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Agent->Learn->TD Estimate & TD Target": [
        [
            "Two values are involved in learning:",
            "markdown"
        ],
        [
            "<strong>TD Estimate</strong> - the predicted optimal \\(Q^*\\) for a given state\n\\(s\\)\n\n\\[{TD}_e = Q_{online}^*(s,a)\\]",
            "markdown"
        ],
        [
            "<strong>TD Target</strong> - aggregation of current reward and the estimated\n\\(Q^*\\) in the next state \\(s'\\)\n\n\\[a' = argmax_{a} Q_{online}(s', a)\\]\n\n\\[{TD}_t = r + \\gamma Q_{target}^*(s',a')\\]",
            "markdown"
        ],
        [
            "Because we don\u2019t know what next action \\(a'\\) will be, we use the\naction \\(a'\\) maximizes \\(Q_{online}\\) in the next state\n\\(s'\\).",
            "markdown"
        ],
        [
            "Notice we use the\n\ndecorator on td_target() to disable gradient calculations here\n(because we don\u2019t need to backpropagate on \\(\\theta_{target}\\)).",
            "markdown"
        ],
        [
            "class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.gamma = 0.9\n\n    def td_estimate(self, state, action):\n        current_Q = self.net(state, model=\"online\")[\n            np.arange(0, self.batch_size), action\n        ]  # Q_online(s,a)\n        return current_Q\n\n    @torch.no_grad()\n    def td_target(self, reward, next_state, done):\n        next_state_Q = self.net(next_state, model=\"online\")\n        best_action = (next_state_Q, axis=1)\n        next_Q = self.net(next_state, model=\"target\")[\n            np.arange(0, self.batch_size), best_action\n        ]\n        return (reward + (1 - done.float()) * self.gamma * next_Q).float()",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Agent->Learn->Updating the model": [
        [
            "As Mario samples inputs from his replay buffer, we compute \\(TD_t\\)\nand \\(TD_e\\) and backpropagate this loss down \\(Q_{online}\\) to\nupdate its parameters \\(\\theta_{online}\\) (\\(\\alpha\\) is the\nlearning rate lr passed to the optimizer)\n\n\\[\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)\\]",
            "markdown"
        ],
        [
            "\\(\\theta_{target}\\) does not update through backpropagation.\nInstead, we periodically copy \\(\\theta_{online}\\) to\n\\(\\theta_{target}\\)\n\n\\[\\theta_{target} \\leftarrow \\theta_{online}\\]",
            "markdown"
        ],
        [
            "class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.optimizer = (self.net.parameters(), lr=0.00025)\n        self.loss_fn = ()\n\n    def update_Q_online(self, td_estimate, td_target):\n        loss = self.loss_fn(td_estimate, td_target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    def sync_Q_target(self):\n        self.net.target.load_state_dict(self.net.online.state_dict())",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Agent->Learn->Save checkpoint": [
        [
            "class Mario(Mario):\n    def save(self):\n        save_path = (\n            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n        )\n        (\n            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n            save_path,\n        )\n        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Agent->Learn->Putting it all together": [
        [
            "class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.burnin = 1e4  # min. experiences before training\n        self.learn_every = 3  # no. of experiences between updates to Q_online\n        self.sync_every = 1e4  # no. of experiences between Q_target &amp; Q_online sync\n\n    def learn(self):\n        if self.curr_step % self.sync_every == 0:\n            self.sync_Q_target()\n\n        if self.curr_step % self.save_every == 0:\n            self.save()\n\n        if self.curr_step &lt; self.burnin:\n            return None, None\n\n        if self.curr_step % self.learn_every != 0:\n            return None, None\n\n        # Sample from memory\n        state, next_state, action, reward, done = self.recall()\n\n        # Get TD Estimate\n        td_est = self.td_estimate(state, action)\n\n        # Get TD Target\n        td_tgt = self.td_target(reward, next_state, done)\n\n        # Backpropagate loss through Q_online\n        loss = self.update_Q_online(td_est, td_tgt)\n\n        return (td_est.mean().item(), loss)",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Agent->Logging": [
        [
            "import numpy as np\nimport time, datetime\nimport matplotlib.pyplot as plt\n\n\nclass MetricLogger:\n    def __init__(self, save_dir):\n        self.save_log = save_dir / \"log\"\n        with open(self.save_log, \"w\") as f:\n            f.write(\n                f\"{'Episode':&gt;8}{'Step':&gt;8}{'Epsilon':&gt;10}{'MeanReward':&gt;15}\"\n                f\"{'MeanLength':&gt;15}{'MeanLoss':&gt;15}{'MeanQValue':&gt;15}\"\n                f\"{'TimeDelta':&gt;15}{'Time':&gt;20}\\n\"\n            )\n        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n\n        # History metrics\n        self.ep_rewards = []\n        self.ep_lengths = []\n        self.ep_avg_losses = []\n        self.ep_avg_qs = []\n\n        # Moving averages, added for every call to record()\n        self.moving_avg_ep_rewards = []\n        self.moving_avg_ep_lengths = []\n        self.moving_avg_ep_avg_losses = []\n        self.moving_avg_ep_avg_qs = []\n\n        # Current episode metric\n        self.init_episode()\n\n        # Timing\n        self.record_time = time.time()\n\n    def log_step(self, reward, loss, q):\n        self.curr_ep_reward += reward\n        self.curr_ep_length += 1\n        if loss:\n            self.curr_ep_loss += loss\n            self.curr_ep_q += q\n            self.curr_ep_loss_length += 1\n\n    def log_episode(self):\n        \"Mark end of episode\"\n        self.ep_rewards.append(self.curr_ep_reward)\n        self.ep_lengths.append(self.curr_ep_length)\n        if self.curr_ep_loss_length == 0:\n            ep_avg_loss = 0\n            ep_avg_q = 0\n        else:\n            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n        self.ep_avg_losses.append(ep_avg_loss)\n        self.ep_avg_qs.append(ep_avg_q)\n\n        self.init_episode()\n\n    def init_episode(self):\n        self.curr_ep_reward = 0.0\n        self.curr_ep_length = 0\n        self.curr_ep_loss = 0.0\n        self.curr_ep_q = 0.0\n        self.curr_ep_loss_length = 0\n\n    def record(self, episode, epsilon, step):\n        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n        self.moving_avg_ep_rewards.append(mean_ep_reward)\n        self.moving_avg_ep_lengths.append(mean_ep_length)\n        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n\n        last_record_time = self.record_time\n        self.record_time = time.time()\n        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n\n        print(\n            f\"Episode {episode} - \"\n            f\"Step {step} - \"\n            f\"Epsilon {epsilon} - \"\n            f\"Mean Reward {mean_ep_reward} - \"\n            f\"Mean Length {mean_ep_length} - \"\n            f\"Mean Loss {mean_ep_loss} - \"\n            f\"Mean Q Value {mean_ep_q} - \"\n            f\"Time Delta {time_since_last_record} - \"\n            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n        )\n\n        with open(self.save_log, \"a\") as f:\n            f.write(\n                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n                f\"{time_since_last_record:15.3f}\"\n                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):&gt;20}\\n\"\n            )\n\n        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n            plt.savefig(getattr(self, f\"{metric}_plot\"))\n            plt.clf()",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Let\u2019s play!": [
        [
            "In this example we run the training loop for 10 episodes, but for Mario to truly learn the ways of\nhis world, we suggest running the loop for at least 40,000 episodes!",
            "markdown"
        ],
        [
            "use_cuda = ()\nprint(f\"Using CUDA: {use_cuda}\")\nprint()\n\nsave_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\nsave_dir.mkdir(parents=True)\n\nmario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n\nlogger = MetricLogger(save_dir)\n\nepisodes = 10\nfor e in range(episodes):\n\n    state = env.reset()\n\n    # Play the game!\n    while True:\n\n        # Run agent on the state\n        action = mario.act(state)\n\n        # Agent performs action\n        next_state, reward, done, trunc, info = env.step(action)\n\n        # Remember\n        mario.cache(state, next_state, action, reward, done)\n\n        # Learn\n        q, loss = mario.learn()\n\n        # Logging\n        logger.log_step(reward, loss, q)\n\n        # Update state\n        state = next_state\n\n        # Check if end of game\n        if done or info[\"flag_get\"]:\n            break\n\n    logger.log_episode()\n\n    if e % 20 == 0:\n        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n\n\n<img alt=\"mario rl tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_mario_rl_tutorial_001.png\" srcset=\"../_images/sphx_glr_mario_rl_tutorial_001.png\"/>",
            "code"
        ],
        [
            "Using CUDA: True\n\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning:\n\nThe default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n\nEpisode 0 - Step 40 - Epsilon 0.9999900000487484 - Mean Reward 231.0 - Mean Length 40.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.401 - Time 2023-03-17T21:26:25\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning:\n\nThe default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).",
            "code"
        ]
    ],
    "torch->Reinforcement Learning->Train a Mario-playing RL Agent->Conclusion": [
        [
            "In this tutorial, we saw how we can use PyTorch to train a game-playing AI. You can use the same methods\nto train an AI to play any of the games at the . Hope you enjoyed this tutorial, feel free to reach us at\n!",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  31.286 seconds)",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "In this tutorial, we will deploy a PyTorch model using Flask and expose a\nREST API for model inference. In particular, we will deploy a pretrained\nDenseNet 121 model which detects the image.",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "All the code used here is released under MIT license and is available on .",
            "markdown"
        ],
        [
            "This represents the first in a series of tutorials on deploying PyTorch models\nin production. Using Flask in this way is by far the easiest way to start\nserving your PyTorch models, but it will not work for a use case\nwith high performance requirements. For that:\n<blockquote>",
            "markdown"
        ],
        [
            "If you\u2019re already familiar with TorchScript, you can jump straight into our\n tutorial.",
            "markdown"
        ],
        [
            "If you first need a refresher on TorchScript, check out our\n tutorial.\n\n</blockquote>",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->API Definition": [
        [
            "We will first define our API endpoints, the request and response types. Our\nAPI endpoint will be at /predict which takes HTTP POST requests with a\nfile parameter which contains the image. The response will be of JSON\nresponse containing the prediction:",
            "markdown"
        ],
        [
            "{\"class_id\": \"n02124075\", \"class_name\": \"Egyptian_cat\"}",
            "code"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Dependencies": [
        [
            "Install the required dependencies by running the following command:",
            "markdown"
        ],
        [
            "$ pip install Flask==2.0.1 torchvision==0.10.0",
            "code"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Simple Web Server": [
        [
            "Following is a simple webserver, taken from Flask\u2019s documentation",
            "markdown"
        ],
        [
            "from flask import Flask\napp = Flask(__name__)\n\n\n@app.route('/')\ndef hello():\n    return 'Hello World!'",
            "code"
        ],
        [
            "Save the above snippet in a file called app.py and you can now run a\nFlask development server by typing:",
            "markdown"
        ],
        [
            "$ FLASK_ENV=development FLASK_APP=app.py flask run",
            "code"
        ],
        [
            "When you visit http://localhost:5000/ in your web browser, you will be\ngreeted with Hello World! text",
            "markdown"
        ],
        [
            "We will make slight changes to the above snippet, so that it suits our API\ndefinition. First, we will rename the method to predict. We will update\nthe endpoint path to /predict. Since the image files will be sent via\nHTTP POST requests, we will update it so that it also accepts only POST\nrequests:",
            "markdown"
        ],
        [
            "@app.route('/predict', methods=['POST'])\ndef predict():\n    return 'Hello World!'",
            "code"
        ],
        [
            "We will also change the response type, so that it returns a JSON response\ncontaining ImageNet class id and name. The updated app.py file will\nbe now:",
            "markdown"
        ],
        [
            "from flask import Flask, jsonify\napp = Flask(__name__)\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    return jsonify({'class_id': 'IMAGE_NET_XXX', 'class_name': 'Cat'})",
            "code"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Inference": [
        [
            "In the next sections we will focus on writing the inference code. This will\ninvolve two parts, one where we prepare the image so that it can be fed\nto DenseNet and next, we will write the code to get the actual prediction\nfrom the model.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Inference->Preparing the image": [
        [
            "DenseNet model requires the image to be of 3 channel RGB image of size\n224 x 224. We will also normalise the image tensor with the required mean\nand standard deviation values. You can read more about it\n.",
            "markdown"
        ],
        [
            "We will use transforms from torchvision library and build a\ntransform pipeline, which transforms our images as required. You\ncan read more about transforms .",
            "markdown"
        ],
        [
            "import io\n\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\ndef transform_image(image_bytes):\n    my_transforms = ([(255),\n                                        (224),\n                                        (),\n                                        (\n                                            [0.485, 0.456, 0.406],\n                                            [0.229, 0.224, 0.225])])\n    image = Image.open(io.BytesIO(image_bytes))\n    return my_transforms(image).unsqueeze(0)",
            "code"
        ],
        [
            "The above method takes image data in bytes, applies the series of transforms\nand returns a tensor. To test the above method, read an image file in\nbytes mode (first replacing <cite>../_static/img/sample_file.jpeg</cite> with the actual\npath to the file on your computer) and see if you get a tensor back:",
            "markdown"
        ],
        [
            "with open(\"../_static/img/sample_file.jpeg\", 'rb') as f:\n    image_bytes = f.read()\n    tensor = transform_image(image_bytes=image_bytes)\n    print(tensor)",
            "code"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Inference->Prediction": [
        [
            "Now will use a pretrained DenseNet 121 model to predict the image class. We\nwill use one from torchvision library, load the model and get an\ninference. While we\u2019ll be using a pretrained model in this example, you can\nuse this same approach for your own models. See more about loading your\nmodels in this .",
            "markdown"
        ],
        [
            "from torchvision import models\n\n# Make sure to pass `pretrained` as `True` to use the pretrained weights:\nmodel = (pretrained=True)\n# Since we are using our model only for inference, switch to `eval` mode:\nmodel.eval()\n\n\ndef get_prediction(image_bytes):\n    tensor = transform_image(image_bytes=image_bytes)\n    outputs = model.forward(tensor)\n    _, y_hat = outputs.max(1)\n    return y_hat",
            "code"
        ],
        [
            "The tensor y_hat will contain the index of the predicted class id.\nHowever, we need a human readable class name. For that we need a class id\nto name mapping. Download\n\nas imagenet_class_index.json and remember where you saved it (or, if you\nare following the exact steps in this tutorial, save it in\n<cite>tutorials/_static</cite>). This file contains the mapping of ImageNet class id to\nImageNet class name. We will load this JSON file and get the class name of\nthe predicted index.",
            "markdown"
        ],
        [
            "import json\n\nimagenet_class_index = json.load(open('../_static/imagenet_class_index.json'))\n\ndef get_prediction(image_bytes):\n    tensor = transform_image(image_bytes=image_bytes)\n    outputs = model.forward(tensor)\n    _, y_hat = outputs.max(1)\n    predicted_idx = str(y_hat.item())\n    return imagenet_class_index[predicted_idx]",
            "code"
        ],
        [
            "Before using imagenet_class_index dictionary, first we will convert\ntensor value to a string value, since the keys in the\nimagenet_class_index dictionary are strings.\nWe will test our above method:",
            "markdown"
        ],
        [
            "with open(\"../_static/img/sample_file.jpeg\", 'rb') as f:\n    image_bytes = f.read()\n    print(get_prediction(image_bytes=image_bytes))",
            "code"
        ],
        [
            "You should get a response like this:",
            "markdown"
        ],
        [
            "['n02124075', 'Egyptian_cat']",
            "code"
        ],
        [
            "The first item in array is ImageNet class id and second item is the human\nreadable name.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Did you notice that model variable is not part of get_prediction\nmethod? Or why is model a global variable? Loading a model can be an\nexpensive operation in terms of memory and compute. If we loaded the model in the\nget_prediction method, then it would get unnecessarily loaded every\ntime the method is called. Since, we are building a web server, there\ncould be thousands of requests per second, we should not waste time\nredundantly loading the model for every inference. So, we keep the model\nloaded in memory just once. In\nproduction systems, it\u2019s necessary to be efficient about your use of\ncompute to be able to serve requests at scale, so you should generally\nload your model before serving requests.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Integrating the model in our API Server": [
        [
            "In this final part we will add our model to our Flask API server. Since\nour API server is supposed to take an image file, we will update our predict\nmethod to read files from the requests:",
            "markdown"
        ],
        [
            "from flask import request\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if request.method == 'POST':\n        # we will get the file from the request\n        file = request.files['file']\n        # convert that to bytes\n        img_bytes = file.read()\n        class_id, class_name = get_prediction(image_bytes=img_bytes)\n        return jsonify({'class_id': class_id, 'class_name': class_name})",
            "code"
        ],
        [
            "The app.py file is now complete. Following is the full version; replace\nthe paths with the paths where you saved your files and it should run:",
            "markdown"
        ],
        [
            "import io\nimport json\n\nfrom torchvision import models\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom flask import Flask, jsonify, request\n\n\napp = Flask(__name__)\nimagenet_class_index = json.load(open('&lt;PATH/TO/.json/FILE&gt;/imagenet_class_index.json'))\nmodel = (pretrained=True)\nmodel.eval()\n\n\ndef transform_image(image_bytes):\n    my_transforms = ([(255),\n                                        (224),\n                                        (),\n                                        (\n                                            [0.485, 0.456, 0.406],\n                                            [0.229, 0.224, 0.225])])\n    image = Image.open(io.BytesIO(image_bytes))\n    return my_transforms(image).unsqueeze(0)\n\n\ndef get_prediction(image_bytes):\n    tensor = transform_image(image_bytes=image_bytes)\n    outputs = model.forward(tensor)\n    _, y_hat = outputs.max(1)\n    predicted_idx = str(y_hat.item())\n    return imagenet_class_index[predicted_idx]\n\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if request.method == 'POST':\n        file = request.files['file']\n        img_bytes = file.read()\n        class_id, class_name = get_prediction(image_bytes=img_bytes)\n        return jsonify({'class_id': class_id, 'class_name': class_name})\n\n\nif __name__ == '__main__':\n    app.run()",
            "code"
        ],
        [
            "Let\u2019s test our web server! Run:",
            "markdown"
        ],
        [
            "$ FLASK_ENV=development FLASK_APP=app.py flask run",
            "code"
        ],
        [
            "We can use the\n\nlibrary to send a POST request to our app:",
            "markdown"
        ],
        [
            "import requests\n\nresp = requests.post(\"http://localhost:5000/predict\",\n                     files={\"file\": open('&lt;PATH/TO/.jpg/FILE&gt;/cat.jpg','rb')})",
            "code"
        ],
        [
            "Printing <cite>resp.json()</cite> will now show the following:",
            "markdown"
        ],
        [
            "{\"class_id\": \"n02124075\", \"class_name\": \"Egyptian_cat\"}",
            "code"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Next steps": [
        [
            "The server we wrote is quite trivial and may not do everything\nyou need for your production application. So, here are some things you\ncan do to make it better:",
            "markdown"
        ],
        [
            "The endpoint /predict assumes that always there will be a image file\nin the request. This may not hold true for all requests. Our user may\nsend image with a different parameter or send no images at all.",
            "markdown"
        ],
        [
            "The user may send non-image type files too. Since we are not handling\nerrors, this will break our server. Adding an explicit error handing\npath that will throw an exception would allow us to better handle\nthe bad inputs",
            "markdown"
        ],
        [
            "Even though the model can recognize a large number of classes of images,\nit may not be able to recognize all images. Enhance the implementation\nto handle cases when the model does not recognize anything in the image.",
            "markdown"
        ],
        [
            "We run the Flask server in the development mode, which is not suitable for\ndeploying in production. You can check out \nfor deploying a Flask server in production.",
            "markdown"
        ],
        [
            "You can also add a UI by creating a page with a form which takes the image and\ndisplays the prediction. Check out the \nof a similar project and its .",
            "markdown"
        ],
        [
            "In this tutorial, we only showed how to build a service that could return predictions for\na single image at a time. We could modify our service to be able to return predictions for\nmultiple images at once. In addition, the \nlibrary automatically queues requests to your service and samples them into mini-batches\nthat can be fed into your model. You can check out .",
            "markdown"
        ],
        [
            "Finally, we encourage you to check out our other tutorials on deploying PyTorch models\nlinked-to at the top of the page.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Introduction to TorchScript": [
        [
            "<em>James Reed (jamesreed@fb.com), Michael Suo (suo@fb.com)</em>, rev2",
            "markdown"
        ],
        [
            "This tutorial is an introduction to TorchScript, an intermediate\nrepresentation of a PyTorch model (subclass of nn.Module) that\ncan then be run in a high-performance environment such as C++.",
            "markdown"
        ],
        [
            "In this tutorial we will cover:",
            "markdown"
        ],
        [
            "The basics of model authoring in PyTorch, including:",
            "markdown"
        ],
        [
            "Modules",
            "markdown"
        ],
        [
            "Defining forward functions",
            "markdown"
        ],
        [
            "Composing modules into a hierarchy of modules",
            "markdown"
        ],
        [
            "Specific methods for converting PyTorch modules to TorchScript, our\nhigh-performance deployment runtime",
            "markdown"
        ],
        [
            "Tracing an existing module",
            "markdown"
        ],
        [
            "Using scripting to directly compile a module",
            "markdown"
        ],
        [
            "How to compose both approaches",
            "markdown"
        ],
        [
            "Saving and loading TorchScript modules",
            "markdown"
        ],
        [
            "We hope that after you complete this tutorial, you will proceed to go through\n\nwhich will walk you through an example of actually calling a TorchScript\nmodel from C++.",
            "markdown"
        ],
        [
            "import torch  # This is all you need to use both PyTorch and TorchScript!\nprint(torch.__version__)",
            "code"
        ],
        [
            "2.0.0+cu117",
            "code"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Introduction to TorchScript->Basics of PyTorch Model Authoring": [
        [
            "Let\u2019s start out by defining a simple Module. A Module is the\nbasic unit of composition in PyTorch. It contains:",
            "markdown"
        ],
        [
            "A constructor, which prepares the module for invocation",
            "markdown"
        ],
        [
            "A set of Parameters and sub-Modules. These are initialized\nby the constructor and can be used by the module during invocation.",
            "markdown"
        ],
        [
            "A forward function. This is the code that is run when the module\nis invoked.",
            "markdown"
        ],
        [
            "Let\u2019s examine a small example:",
            "markdown"
        ],
        [
            "class MyCell():\n    def __init__(self):\n        super(, self).__init__()\n\n    def forward(self, , ):\n        new_h = ( + )\n        return new_h, new_h\n\nmy_cell = ()\n = (3, 4)\n = (3, 4)\nprint(my_cell(, ))",
            "code"
        ],
        [
            "(tensor([[0.8423, 0.5403, 0.5722, 0.4816],\n        [0.8413, 0.8463, 0.8283, 0.9122],\n        [0.7446, 0.6997, 0.8749, 0.7332]]), tensor([[0.8423, 0.5403, 0.5722, 0.4816],\n        [0.8413, 0.8463, 0.8283, 0.9122],\n        [0.7446, 0.6997, 0.8749, 0.7332]]))",
            "code"
        ],
        [
            "So we\u2019ve:",
            "markdown"
        ],
        [
            "Created a class that subclasses torch.nn.Module.",
            "markdown"
        ],
        [
            "Defined a constructor. The constructor doesn\u2019t do much, just calls\nthe constructor for super.",
            "markdown"
        ],
        [
            "Defined a forward function, which takes two inputs and returns\ntwo outputs. The actual contents of the forward function are not\nreally important, but it\u2019s sort of a fake \u2013that\nis\u2013it\u2019s a function that is applied on a loop.",
            "markdown"
        ],
        [
            "We instantiated the module, and made x and h, which are just 3x4\nmatrices of random values. Then we invoked the cell with\nmy_cell(x, h). This in turn calls our forward function.",
            "markdown"
        ],
        [
            "Let\u2019s do something a little more interesting:",
            "markdown"
        ],
        [
            "class MyCell():\n    def __init__(self):\n        super(, self).__init__()\n        self.linear = (4, 4)\n\n    def forward(self, , ):\n        new_h = (self.linear() + )\n        return new_h, new_h\n\nmy_cell = ()\nprint(my_cell)\nprint(my_cell(, ))",
            "code"
        ],
        [
            "MyCell(\n  (linear): Linear(in_features=4, out_features=4, bias=True)\n)\n(tensor([[0.2829, 0.5454, 0.5981, 0.7620],\n        [0.7984, 0.7317, 0.6353, 0.9266],\n        [0.7826, 0.2063, 0.4779, 0.8727]], grad_fn=&lt;TanhBackward0&gt;), tensor([[0.2829, 0.5454, 0.5981, 0.7620],\n        [0.7984, 0.7317, 0.6353, 0.9266],\n        [0.7826, 0.2063, 0.4779, 0.8727]], grad_fn=&lt;TanhBackward0&gt;))",
            "code"
        ],
        [
            "We\u2019ve redefined our module MyCell, but this time we\u2019ve added a\nself.linear attribute, and we invoke self.linear in the forward\nfunction.",
            "markdown"
        ],
        [
            "What exactly is happening here? torch.nn.Linear is a Module from\nthe PyTorch standard library. Just like MyCell, it can be invoked\nusing the call syntax. We are building a hierarchy of Modules.",
            "markdown"
        ],
        [
            "print on a Module will give a visual representation of the\nModule\u2019s subclass hierarchy. In our example, we can see our\nLinear subclass and its parameters.",
            "markdown"
        ],
        [
            "By composing Modules in this way, we can succinctly and readably\nauthor models with reusable components.",
            "markdown"
        ],
        [
            "You may have noticed grad_fn on the outputs. This is a detail of\nPyTorch\u2019s method of automatic differentiation, called\n.\nIn short, this system allows us to compute derivatives through\npotentially complex programs. The design allows for a massive amount of\nflexibility in model authoring.",
            "markdown"
        ],
        [
            "Now let\u2019s examine said flexibility:",
            "markdown"
        ],
        [
            "class MyDecisionGate():\n    def forward(self, ):\n        if .sum() &gt; 0:\n            return \n        else:\n            return -\n\nclass MyCell():\n    def __init__(self):\n        super(, self).__init__()\n        self.dg = ()\n        self.linear = (4, 4)\n\n    def forward(self, , ):\n        new_h = (self.dg(self.linear()) + )\n        return new_h, new_h\n\nmy_cell = ()\nprint(my_cell)\nprint(my_cell(, ))",
            "code"
        ],
        [
            "MyCell(\n  (dg): MyDecisionGate()\n  (linear): Linear(in_features=4, out_features=4, bias=True)\n)\n(tensor([[ 0.8695,  0.4516,  0.7416, -0.0722],\n        [ 0.9652,  0.8120,  0.9017,  0.3150],\n        [ 0.9577,  0.5761,  0.7777, -0.0841]], grad_fn=&lt;TanhBackward0&gt;), tensor([[ 0.8695,  0.4516,  0.7416, -0.0722],\n        [ 0.9652,  0.8120,  0.9017,  0.3150],\n        [ 0.9577,  0.5761,  0.7777, -0.0841]], grad_fn=&lt;TanhBackward0&gt;))",
            "code"
        ],
        [
            "We\u2019ve once again redefined our MyCell class, but here we\u2019ve defined\nMyDecisionGate. This module utilizes <strong>control flow</strong>. Control flow\nconsists of things like loops and if-statements.",
            "markdown"
        ],
        [
            "Many frameworks take the approach of computing symbolic derivatives\ngiven a full program representation. However, in PyTorch, we use a\ngradient tape. We record operations as they occur, and replay them\nbackwards in computing derivatives. In this way, the framework does not\nhave to explicitly define derivatives for all constructs in the\nlanguage.\n\n<img alt=\"How autograd works\" src=\"https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/dynamic_graph.gif\"/>",
            "markdown"
        ],
        [
            "How autograd works",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Introduction to TorchScript->Basics of TorchScript": [
        [
            "Now let\u2019s take our running example and see how we can apply TorchScript.",
            "markdown"
        ],
        [
            "In short, TorchScript provides tools to capture the definition of your\nmodel, even in light of the flexible and dynamic nature of PyTorch.\nLet\u2019s begin by examining what we call <strong>tracing</strong>.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Introduction to TorchScript->Basics of TorchScript->Tracing Modules": [
        [
            "class MyCell():\n    def __init__(self):\n        super(, self).__init__()\n        self.linear = (4, 4)\n\n    def forward(self, , ):\n        new_h = (self.linear() + )\n        return new_h, new_h\n\nmy_cell = ()\n,  = (3, 4), (3, 4)\ntraced_cell = (my_cell, (, ))\nprint(traced_cell)\ntraced_cell(, )",
            "code"
        ],
        [
            "MyCell(\n  original_name=MyCell\n  (linear): Linear(original_name=Linear)\n)\n\n(tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]], grad_fn=&lt;TanhBackward0&gt;), tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]], grad_fn=&lt;TanhBackward0&gt;))",
            "code"
        ],
        [
            "We\u2019ve rewinded a bit and taken the second version of our MyCell\nclass. As before, we\u2019ve instantiated it, but this time, we\u2019ve called\ntorch.jit.trace, passed in the Module, and passed in <em>example\ninputs</em> the network might see.",
            "markdown"
        ],
        [
            "What exactly has this done? It has invoked the Module, recorded the\noperations that occured when the Module was run, and created an\ninstance of torch.jit.ScriptModule (of which TracedModule is an\ninstance)",
            "markdown"
        ],
        [
            "TorchScript records its definitions in an Intermediate Representation\n(or IR), commonly referred to in Deep learning as a <em>graph</em>. We can\nexamine the graph with the .graph property:",
            "markdown"
        ],
        [
            "print()",
            "code"
        ],
        [
            "graph(%self.1 : __torch__.MyCell,\n      %x : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu),\n      %h : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu)):\n  %linear : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear\"](%self.1)\n  %20 : Tensor = prim::CallMethod[name=\"forward\"](%linear, %x)\n  %11 : int = prim::Constant[value=1]() # /var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:188:0\n  %12 : Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu) = aten::add(%20, %h, %11) # /var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:188:0\n  %13 : Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu) = aten::tanh(%12) # /var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:188:0\n  %14 : (Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu), Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu)) = prim::TupleConstruct(%13, %13)\n  return (%14)",
            "code"
        ],
        [
            "However, this is a very low-level representation and most of the\ninformation contained in the graph is not useful for end users. Instead,\nwe can use the .code property to give a Python-syntax interpretation\nof the code:",
            "markdown"
        ],
        [
            "print()",
            "code"
        ],
        [
            "def forward(self,\n    x: Tensor,\n    h: Tensor) -&gt; Tuple[Tensor, Tensor]:\n  linear = self.linear\n  _0 = torch.tanh(torch.add((linear).forward(x, ), h))\n  return (_0, _0)",
            "code"
        ],
        [
            "So <strong>why</strong> did we do all this? There are several reasons:",
            "markdown"
        ],
        [
            "TorchScript code can be invoked in its own interpreter, which is\nbasically a restricted Python interpreter. This interpreter does not\nacquire the Global Interpreter Lock, and so many requests can be\nprocessed on the same instance simultaneously.",
            "markdown"
        ],
        [
            "This format allows us to save the whole model to disk and load it\ninto another environment, such as in a server written in a language\nother than Python",
            "markdown"
        ],
        [
            "TorchScript gives us a representation in which we can do compiler\noptimizations on the code to provide more efficient execution",
            "markdown"
        ],
        [
            "TorchScript allows us to interface with many backend/device runtimes\nthat require a broader view of the program than individual operators.",
            "markdown"
        ],
        [
            "We can see that invoking traced_cell produces the same results as\nthe Python module:",
            "markdown"
        ],
        [
            "print(my_cell(, ))\nprint(traced_cell(, ))",
            "code"
        ],
        [
            "(tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]], grad_fn=&lt;TanhBackward0&gt;), tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]], grad_fn=&lt;TanhBackward0&gt;))\n(tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]],\n       grad_fn=&lt;DifferentiableGraphBackward&gt;), tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]],\n       grad_fn=&lt;DifferentiableGraphBackward&gt;))",
            "code"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Introduction to TorchScript->Using Scripting to Convert Modules": [
        [
            "There\u2019s a reason we used version two of our module, and not the one with\nthe control-flow-laden submodule. Let\u2019s examine that now:",
            "markdown"
        ],
        [
            "class MyDecisionGate():\n    def forward(self, ):\n        if .sum() &gt; 0:\n            return \n        else:\n            return -\n\nclass MyCell():\n    def __init__(self, dg):\n        super(, self).__init__()\n        self.dg = dg\n        self.linear = (4, 4)\n\n    def forward(self, , ):\n        new_h = (self.dg(self.linear()) + )\n        return new_h, new_h\n\nmy_cell = (())\ntraced_cell = (my_cell, (, ))\n\nprint()\nprint()",
            "code"
        ],
        [
            "/var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:260: TracerWarning:\n\nConverting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n\ndef forward(self,\n    argument_1: Tensor) -&gt; NoneType:\n  return None\n\ndef forward(self,\n    x: Tensor,\n    h: Tensor) -&gt; Tuple[Tensor, Tensor]:\n  dg = self.dg\n  linear = self.linear\n  _0 = (linear).forward(x, )\n  _1 = (dg).forward(_0, )\n  _2 = torch.tanh(torch.add(_0, h))\n  return (_2, _2)",
            "code"
        ],
        [
            "Looking at the .code output, we can see that the if-else branch\nis nowhere to be found! Why? Tracing does exactly what we said it would:\nrun the code, record the operations <em>that happen</em> and construct a\nScriptModule that does exactly that. Unfortunately, things like control\nflow are erased.",
            "markdown"
        ],
        [
            "How can we faithfully represent this module in TorchScript? We provide a\n<strong>script compiler</strong>, which does direct analysis of your Python source\ncode to transform it into TorchScript. Let\u2019s convert MyDecisionGate\nusing the script compiler:",
            "markdown"
        ],
        [
            "scripted_gate = (())\n\nmy_cell = (scripted_gate)\nscripted_cell = (my_cell)\n\nprint()\nprint()",
            "code"
        ],
        [
            "def forward(self,\n    x: Tensor) -&gt; Tensor:\n  if bool(torch.gt(torch.sum(x), 0)):\n    _0 = x\n  else:\n    _0 = torch.neg(x)\n  return _0\n\ndef forward(self,\n    x: Tensor,\n    h: Tensor) -&gt; Tuple[Tensor, Tensor]:\n  dg = self.dg\n  linear = self.linear\n  _0 = torch.add((dg).forward((linear).forward(x, ), ), h)\n  new_h = torch.tanh(_0)\n  return (new_h, new_h)",
            "code"
        ],
        [
            "Hooray! We\u2019ve now faithfully captured the behavior of our program in\nTorchScript. Let\u2019s now try running the program:",
            "markdown"
        ],
        [
            "# New inputs\n,  = (3, 4), (3, 4)\ntraced_cell(, )",
            "code"
        ],
        [
            "(tensor([[0.8368, 0.6018, 0.0473, 0.7395],\n        [0.7879, 0.7106, 0.5274, 0.9518],\n        [0.7242, 0.6873, 0.5701, 0.7467]], grad_fn=&lt;TanhBackward0&gt;), tensor([[0.8368, 0.6018, 0.0473, 0.7395],\n        [0.7879, 0.7106, 0.5274, 0.9518],\n        [0.7242, 0.6873, 0.5701, 0.7467]], grad_fn=&lt;TanhBackward0&gt;))",
            "code"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Introduction to TorchScript->Using Scripting to Convert Modules->Mixing Scripting and Tracing": [
        [
            "Some situations call for using tracing rather than scripting (e.g.\u00a0a\nmodule has many architectural decisions that are made based on constant\nPython values that we would like to not appear in TorchScript). In this\ncase, scripting can be composed with tracing: torch.jit.script will\ninline the code for a traced module, and tracing will inline the code\nfor a scripted module.",
            "markdown"
        ],
        [
            "An example of the first case:",
            "markdown"
        ],
        [
            "class MyRNNLoop():\n    def __init__(self):\n        super(, self).__init__()\n        self.cell = ((scripted_gate), (, ))\n\n    def forward(self, xs):\n        , y = (3, 4), (3, 4)\n        for i in range(xs.size(0)):\n            y,  = self.cell(xs[i], )\n        return y, \n\nrnn_loop = (())\nprint()",
            "code"
        ],
        [
            "def forward(self,\n    xs: Tensor) -&gt; Tuple[Tensor, Tensor]:\n  h = torch.zeros([3, 4])\n  y = torch.zeros([3, 4])\n  y0 = y\n  h0 = h\n  for i in range(torch.size(xs, 0)):\n    cell = self.cell\n    _0 = (cell).forward(torch.select(xs, 0, i), h0, )\n    y1, h1, = _0\n    y0, h0 = y1, h1\n  return (y0, h0)",
            "code"
        ],
        [
            "And an example of the second case:",
            "markdown"
        ],
        [
            "class WrapRNN():\n    def __init__(self):\n        super(, self).__init__()\n        self.loop = (())\n\n    def forward(self, xs):\n        y,  = self.loop(xs)\n        return torch.relu(y)\n\ntraced = ((), ((10, 3, 4)))\nprint()",
            "code"
        ],
        [
            "def forward(self,\n    xs: Tensor) -&gt; Tensor:\n  loop = self.loop\n  _0, y, = (loop).forward(xs, )\n  return torch.relu(y)",
            "code"
        ],
        [
            "This way, scripting and tracing can be used when the situation calls for\neach of them and used together.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Introduction to TorchScript->Saving and Loading models": [
        [
            "We provide APIs to save and load TorchScript modules to/from disk in an\narchive format. This format includes code, parameters, attributes, and\ndebug information, meaning that the archive is a freestanding\nrepresentation of the model that can be loaded in an entirely separate\nprocess. Let\u2019s save and load our wrapped RNN module:",
            "markdown"
        ],
        [
            "('wrapped_rnn.pt')\n\nloaded = ('wrapped_rnn.pt')\n\nprint(loaded)\nprint()",
            "code"
        ],
        [
            "RecursiveScriptModule(\n  original_name=WrapRNN\n  (loop): RecursiveScriptModule(\n    original_name=MyRNNLoop\n    (cell): RecursiveScriptModule(\n      original_name=MyCell\n      (dg): RecursiveScriptModule(original_name=MyDecisionGate)\n      (linear): RecursiveScriptModule(original_name=Linear)\n    )\n  )\n)\ndef forward(self,\n    xs: Tensor) -&gt; Tensor:\n  loop = self.loop\n  _0, y, = (loop).forward(xs, )\n  return torch.relu(y)",
            "code"
        ],
        [
            "As you can see, serialization preserves the module hierarchy and the\ncode we\u2019ve been examining throughout. The model can also be loaded, for\nexample,  for\npython-free execution.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Introduction to TorchScript->Saving and Loading models->Further Reading": [
        [
            "We\u2019ve completed our tutorial! For a more involved demonstration, check\nout the NeurIPS demo for converting machine translation models using\nTorchScript:",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.120 seconds)",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Loading a TorchScript Model in C++": [
        [
            "As its name suggests, the primary interface to PyTorch is the Python\nprogramming language. While Python is a suitable and preferred language for\nmany scenarios requiring dynamism and ease of iteration, there are equally many\nsituations where precisely these properties of Python are unfavorable. One\nenvironment in which the latter often applies is <em>production</em> \u2013 the land of\nlow latencies and strict deployment requirements. For production scenarios, C++\nis very often the language of choice, even if only to bind it into another\nlanguage like Java, Rust or Go. The following paragraphs will outline the path\nPyTorch provides to go from an existing Python model to a serialized\nrepresentation that can be <em>loaded</em> and <em>executed</em> purely from C++, with no\ndependency on Python.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Loading a TorchScript Model in C++->Step 1: Converting Your PyTorch Model to Torch Script": [
        [
            "A PyTorch model\u2019s journey from Python to C++ is enabled by , a representation of a PyTorch\nmodel that can be understood, compiled and serialized by the Torch Script\ncompiler. If you are starting out from an existing PyTorch model written in the\nvanilla \u201ceager\u201d API, you must first convert your model to Torch Script. In the\nmost common cases, discussed below, this requires only little effort. If you\nalready have a Torch Script module, you can skip to the next section of this\ntutorial.",
            "markdown"
        ],
        [
            "There exist two ways of converting a PyTorch model to Torch Script. The first\nis known as <em>tracing</em>, a mechanism in which the structure of the model is\ncaptured by evaluating it once using example inputs, and recording the flow of\nthose inputs through the model. This is suitable for models that make limited\nuse of control flow. The second approach is to add explicit annotations to your\nmodel that inform the Torch Script compiler that it may directly parse and\ncompile your model code, subject to the constraints imposed by the Torch Script\nlanguage.",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "You can find the complete documentation for both of these methods, as well as\nfurther guidance on which to use, in the official .",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Loading a TorchScript Model in C++->Step 1: Converting Your PyTorch Model to Torch Script->Converting to Torch Script via Tracing": [
        [
            "To convert a PyTorch model to Torch Script via tracing, you must pass an\ninstance of your model along with an example input to the torch.jit.trace\nfunction. This will produce a torch.jit.ScriptModule object with the trace\nof your model evaluation embedded in the module\u2019s forward method:",
            "markdown"
        ],
        [
            "import torch\nimport torchvision\n\n# An instance of your model.\nmodel = torchvision.models.resnet18()\n\n# An example input you would normally provide to your model's forward() method.\nexample = torch.rand(1, 3, 224, 224)\n\n# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\ntraced_script_module = torch.jit.trace(model, example)",
            "code"
        ],
        [
            "The traced ScriptModule can now be evaluated identically to a regular\nPyTorch module:",
            "markdown"
        ],
        [
            "In[1]: output = traced_script_module(torch.ones(1, 3, 224, 224))\nIn[2]: output[0, :5]\nOut[2]: tensor([-0.2698, -0.0381,  0.4023, -0.3010, -0.0448], grad_fn=&lt;SliceBackward&gt;)",
            "code"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Loading a TorchScript Model in C++->Step 1: Converting Your PyTorch Model to Torch Script->Converting to Torch Script via Annotation": [
        [
            "Under certain circumstances, such as if your model employs particular forms of\ncontrol flow, you may want to write your model in Torch Script directly and\nannotate your model accordingly. For example, say you have the following\nvanilla Pytorch model:",
            "markdown"
        ],
        [
            "import torch\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, N, M):\n        super(MyModule, self).__init__()\n        self.weight = torch.nn.Parameter(torch.rand(N, M))\n\n    def forward(self, input):\n        if input.sum() &gt; 0:\n          output = self.weight.mv(input)\n        else:\n          output = self.weight + input\n        return output",
            "code"
        ],
        [
            "Because the forward method of this module uses control flow that is\ndependent on the input, it is not suitable for tracing. Instead, we can convert\nit to a ScriptModule.\nIn order to convert the module to the ScriptModule, one needs to\ncompile the module with torch.jit.script as follows:",
            "markdown"
        ],
        [
            "class MyModule(torch.nn.Module):\n    def __init__(self, N, M):\n        super(MyModule, self).__init__()\n        self.weight = torch.nn.Parameter(torch.rand(N, M))\n\n    def forward(self, input):\n        if input.sum() &gt; 0:\n          output = self.weight.mv(input)\n        else:\n          output = self.weight + input\n        return output\n\nmy_module = MyModule(10,20)\nsm = torch.jit.script(my_module)",
            "code"
        ],
        [
            "If you need to exclude some methods in your nn.Module\nbecause they use Python features that TorchScript doesn\u2019t support yet,\nyou could annotate those with @torch.jit.ignore",
            "markdown"
        ],
        [
            "sm is an instance of\nScriptModule that is ready for serialization.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Loading a TorchScript Model in C++->Step 2: Serializing Your Script Module to a File": [
        [
            "Once you have a ScriptModule in your hands, either from tracing or\nannotating a PyTorch model, you are ready to serialize it to a file. Later on,\nyou\u2019ll be able to load the module from this file in C++ and execute it without\nany dependency on Python. Say we want to serialize the ResNet18 model shown\nearlier in the tracing example. To perform this serialization, simply call\n\non the module and pass it a filename:",
            "markdown"
        ],
        [
            "traced_script_module.save(\"traced_resnet_model.pt\")",
            "code"
        ],
        [
            "This will produce a traced_resnet_model.pt file in your working directory.\nIf you also would like to serialize sm, call sm.save(\"my_module_model.pt\")\nWe have now officially left the realm of Python and are ready to cross over to the sphere\nof C++.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Loading a TorchScript Model in C++->Step 3: Loading Your Script Module in C++": [
        [
            "To load your serialized PyTorch model in C++, your application must depend on\nthe PyTorch C++ API \u2013 also known as <em>LibTorch</em>. The LibTorch distribution\nencompasses a collection of shared libraries, header files and CMake build\nconfiguration files. While CMake is not a requirement for depending on\nLibTorch, it is the recommended approach and will be well supported into the\nfuture. For this tutorial, we will be building a minimal C++ application using\nCMake and LibTorch that simply loads and executes a serialized PyTorch model.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Loading a TorchScript Model in C++->Step 3: Loading Your Script Module in C++->A Minimal C++ Application": [
        [
            "Let\u2019s begin by discussing the code to load a module. The following will already\ndo:",
            "markdown"
        ],
        [
            "#include &lt;torch/script.h&gt; // One-stop header.\n\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nint main(int argc, const char* argv[]) {\n  if (argc != 2) {\n    std::cerr &lt;&lt; \"usage: example-app &lt;path-to-exported-script-module&gt;\\n\";\n    return -1;\n  }\n\n\n  torch::jit::script::Module module;\n  try {\n    // Deserialize the ScriptModule from a file using torch::jit::load().\n    module = torch::jit::load(argv[1]);\n  }\n  catch (const c10::Error&amp; e) {\n    std::cerr &lt;&lt; \"error loading the model\\n\";\n    return -1;\n  }\n\n  std::cout &lt;&lt; \"ok\\n\";\n}",
            "code"
        ],
        [
            "The &lt;torch/script.h&gt; header encompasses all relevant includes from the\nLibTorch library necessary to run the example. Our application accepts the file\npath to a serialized PyTorch ScriptModule as its only command line argument\nand then proceeds to deserialize the module using the torch::jit::load()\nfunction, which takes this file path as input. In return we receive a torch::jit::script::Module\nobject. We will examine how to execute it in a moment.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Loading a TorchScript Model in C++->Step 3: Loading Your Script Module in C++->Depending on LibTorch and Building the Application": [
        [
            "Assume we stored the above code into a file called example-app.cpp. A\nminimal CMakeLists.txt to build it could look as simple as:",
            "markdown"
        ],
        [
            "cmake_minimum_required(VERSION 3.0 FATAL_ERROR)\nproject(custom_ops)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(example-app example-app.cpp)\ntarget_link_libraries(example-app \"${TORCH_LIBRARIES}\")\nset_property(TARGET example-app PROPERTY CXX_STANDARD 14)",
            "code"
        ],
        [
            "The last thing we need to build the example application is the LibTorch\ndistribution. You can always grab the latest stable release from the  on the PyTorch website. If you download and unzip\nthe latest archive, you should receive a folder with the following directory\nstructure:",
            "markdown"
        ],
        [
            "libtorch/\n  bin/\n  include/\n  lib/\n  share/",
            "code"
        ],
        [
            "The lib/ folder contains the shared libraries you must link against,",
            "markdown"
        ],
        [
            "The include/ folder contains header files your program will need to include,",
            "markdown"
        ],
        [
            "The share/ folder contains the necessary CMake configuration to enable the simple find_package(Torch) command above.",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "On Windows, debug and release builds are not ABI-compatible. If you plan to\nbuild your project in debug mode, please try the debug version of LibTorch.\nAlso, make sure you specify the correct configuration in the cmake --build .\nline below.",
            "markdown"
        ],
        [
            "The last step is building the application. For this, assume our example\ndirectory is laid out like this:",
            "markdown"
        ],
        [
            "example-app/\n  CMakeLists.txt\n  example-app.cpp",
            "code"
        ],
        [
            "We can now run the following commands to build the application from within the\nexample-app/ folder:",
            "markdown"
        ],
        [
            "mkdir build\ncd build\ncmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\ncmake --build . --config Release",
            "code"
        ],
        [
            "where /path/to/libtorch should be the full path to the unzipped LibTorch\ndistribution. If all goes well, it will look something like this:",
            "markdown"
        ],
        [
            "root@4b5a67132e81:/example-app# mkdir build\nroot@4b5a67132e81:/example-app# cd build\nroot@4b5a67132e81:/example-app/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /example-app/build\nroot@4b5a67132e81:/example-app/build# make\nScanning dependencies of target example-app\n[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o\n[100%] Linking CXX executable example-app\n[100%] Built target example-app",
            "code"
        ],
        [
            "If we supply the path to the traced ResNet18 model traced_resnet_model.pt  we created earlier\nto the resulting example-app binary, we should be rewarded with a friendly\n\u201cok\u201d. Please note, if try to run this example with my_module_model.pt you will get an error saying that\nyour input is of an incompatible shape. my_module_model.pt expects 1D instead of 4D.",
            "markdown"
        ],
        [
            "root@4b5a67132e81:/example-app/build# ./example-app &lt;path_to_model&gt;/traced_resnet_model.pt\nok",
            "code"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Loading a TorchScript Model in C++->Step 4: Executing the Script Module in C++": [
        [
            "Having successfully loaded our serialized ResNet18 in C++, we are now just a\ncouple lines of code away from executing it! Let\u2019s add those lines to our C++\napplication\u2019s main() function:",
            "markdown"
        ],
        [
            "// Create a vector of inputs.\nstd::vector&lt;torch::jit::IValue&gt; inputs;\ninputs.push_back(torch::ones({1, 3, 224, 224}));\n\n// Execute the model and turn its output into a tensor.\nat::Tensor output = module.forward(inputs).toTensor();\nstd::cout &lt;&lt; output.slice(/*dim=*/1, /*start=*/0, /*end=*/5) &lt;&lt; '\\n';",
            "code"
        ],
        [
            "The first two lines set up the inputs to our model. We create a vector of\ntorch::jit::IValue (a type-erased value type script::Module methods\naccept and return) and add a single input. To create the input tensor, we use\ntorch::ones(), the equivalent to torch.ones in the C++ API.  We then\nrun the script::Module\u2019s forward method, passing it the input vector we\ncreated. In return we get a new IValue, which we convert to a tensor by\ncalling toTensor().",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "To learn more about functions like torch::ones and the PyTorch C++ API in\ngeneral, refer to its documentation at . The\nPyTorch C++ API provides near feature parity with the Python API, allowing\nyou to further manipulate and process tensors just like in Python.",
            "markdown"
        ],
        [
            "In the last line, we print the first five entries of the output. Since we\nsupplied the same input to our model in Python earlier in this tutorial, we\nshould ideally see the same output. Let\u2019s try it out by re-compiling our\napplication and running it with the same serialized model:",
            "markdown"
        ],
        [
            "root@4b5a67132e81:/example-app/build# make\nScanning dependencies of target example-app\n[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o\n[100%] Linking CXX executable example-app\n[100%] Built target example-app\nroot@4b5a67132e81:/example-app/build# ./example-app traced_resnet_model.pt\n-0.2698 -0.0381  0.4023 -0.3010 -0.0448\n[ Variable[CPUFloatType]{1,5} ]",
            "code"
        ],
        [
            "For reference, the output in Python previously was:",
            "markdown"
        ],
        [
            "tensor([-0.2698, -0.0381,  0.4023, -0.3010, -0.0448], grad_fn=&lt;SliceBackward&gt;)",
            "code"
        ],
        [
            "Looks like a good match!",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "To move your model to GPU memory, you can write model.to(at::kCUDA);.\nMake sure the inputs to a model are also living in CUDA memory\nby calling tensor.to(at::kCUDA), which will return a new tensor in CUDA\nmemory.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Loading a TorchScript Model in C++->Step 5: Getting Help and Exploring the API": [
        [
            "This tutorial has hopefully equipped you with a general understanding of a\nPyTorch model\u2019s path from Python to C++. With the concepts described in this\ntutorial, you should be able to go from a vanilla, \u201ceager\u201d PyTorch model, to a\ncompiled ScriptModule in Python, to a serialized file on disk and \u2013 to\nclose the loop \u2013 to an executable script::Module in C++.",
            "markdown"
        ],
        [
            "Of course, there are many concepts we did not cover. For example, you may find\nyourself wanting to extend your ScriptModule with a custom operator\nimplemented in C++ or CUDA, and executing this custom operator inside your\nScriptModule loaded in your pure C++ production environment. The good news\nis: this is possible, and well supported! For now, you can explore  folder\nfor examples, and we will follow up with a tutorial shortly. In the time being,\nthe following links may be generally helpful:",
            "markdown"
        ],
        [
            "The Torch Script reference: ",
            "markdown"
        ],
        [
            "The PyTorch C++ API documentation: ",
            "markdown"
        ],
        [
            "The PyTorch Python API documentation: ",
            "markdown"
        ],
        [
            "As always, if you run into any problems or have questions, you can use our\n or  to get in touch.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime": [
        [
            "In this tutorial, we describe how to convert a model defined\nin PyTorch into the ONNX format and then run it with ONNX Runtime.",
            "markdown"
        ],
        [
            "ONNX Runtime is a performance-focused engine for ONNX models,\nwhich inferences efficiently across multiple platforms and hardware\n(Windows, Linux, and Mac and on both CPUs and GPUs).\nONNX Runtime has proved to considerably increase performance over\nmultiple models as explained ",
            "markdown"
        ],
        [
            "For this tutorial, you will need to install \nand .\nYou can get binary builds of ONNX and ONNX Runtime with\npip install onnx onnxruntime.\nNote that ONNX Runtime is compatible with Python versions 3.5 to 3.7.",
            "markdown"
        ],
        [
            "NOTE: This tutorial needs PyTorch master branch which can be installed by following\nthe instructions ",
            "markdown"
        ],
        [
            "# Some standard imports\nimport io\nimport numpy as np\n\nfrom torch import nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx",
            "code"
        ],
        [
            "Super-resolution is a way of increasing the resolution of images, videos\nand is widely used in image processing or video editing. For this\ntutorial, we will use a small super-resolution model.",
            "markdown"
        ],
        [
            "First, let\u2019s create a SuperResolution model in PyTorch.\nThis model uses the efficient sub-pixel convolution layer described in\n\nfor increasing the resolution of an image by an upscale factor.\nThe model expects the Y component of the YCbCr of an image as an input, and\noutputs the upscaled Y component in super resolution.",
            "markdown"
        ],
        [
            "comes directly from PyTorch\u2019s examples without modification:",
            "markdown"
        ],
        [
            "# Super Resolution model definition in PyTorch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\nclass SuperResolutionNet():\n    def __init__(self, upscale_factor, inplace=False):\n        super(SuperResolutionNet, self).__init__()\n\n        self.relu = (inplace=inplace)\n        self.conv1 = (1, 64, (5, 5), (1, 1), (2, 2))\n        self.conv2 = (64, 64, (3, 3), (1, 1), (1, 1))\n        self.conv3 = (64, 32, (3, 3), (1, 1), (1, 1))\n        self.conv4 = (32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n        self.pixel_shuffle = (upscale_factor)\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.pixel_shuffle(self.conv4(x))\n        return x\n\n    def _initialize_weights(self):\n        (self.conv1.weight, ('relu'))\n        (self.conv2.weight, ('relu'))\n        (self.conv3.weight, ('relu'))\n        (self.conv4.weight)\n\n# Create the super-resolution model by using the above model definition.\ntorch_model = SuperResolutionNet(upscale_factor=3)",
            "code"
        ],
        [
            "Ordinarily, you would now train this model; however, for this tutorial,\nwe will instead download some pre-trained weights. Note that this model\nwas not trained fully for good accuracy and is used here for\ndemonstration purposes only.",
            "markdown"
        ],
        [
            "It is important to call torch_model.eval() or torch_model.train(False)\nbefore exporting the model, to turn the model to inference mode.\nThis is required since operators like dropout or batchnorm behave\ndifferently in inference and training mode.",
            "markdown"
        ],
        [
            "# Load pretrained model weights\nmodel_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\nbatch_size = 1    # just a random number\n\n# Initialize model with the pretrained weights\nmap_location = lambda storage, loc: storage\nif torch.cuda.is_available():\n    map_location = None\ntorch_model.load_state_dict((model_url, map_location=map_location))\n\n# set the model to inference mode\ntorch_model.eval()",
            "code"
        ],
        [
            "Exporting a model in PyTorch works via tracing or scripting. This\ntutorial will use as an example a model exported by tracing.\nTo export a model, we call the torch.onnx.export() function.\nThis will execute the model, recording a trace of what operators\nare used to compute the outputs.\nBecause export runs the model, we need to provide an input\ntensor x. The values in this can be random as long as it is the\nright type and size.\nNote that the input size will be fixed in the exported ONNX graph for\nall the input\u2019s dimensions, unless specified as a dynamic axes.\nIn this example we export the model with an input of batch_size 1,\nbut then specify the first dimension as dynamic in the dynamic_axes\nparameter in torch.onnx.export().\nThe exported model will thus accept inputs of size [batch_size, 1, 224, 224]\nwhere batch_size can be variable.",
            "markdown"
        ],
        [
            "To learn more details about PyTorch\u2019s export interface, check out the\n.",
            "markdown"
        ],
        [
            "# Input to the model\nx = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\ntorch_out = torch_model(x)\n\n# Export the model\n(torch_model,               # model being run\n                  x,                         # model input (or a tuple for multiple inputs)\n                  \"super_resolution.onnx\",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=10,          # the ONNX version to export the model to\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\n                  input_names = ['input'],   # the model's input names\n                  output_names = ['output'], # the model's output names\n                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n                                'output' : {0 : 'batch_size'}})",
            "code"
        ],
        [
            "We also computed torch_out, the output after of the model,\nwhich we will use to verify that the model we exported computes\nthe same values when run in ONNX Runtime.",
            "markdown"
        ],
        [
            "But before verifying the model\u2019s output with ONNX Runtime, we will check\nthe ONNX model with ONNX\u2019s API.\nFirst, onnx.load(\"super_resolution.onnx\") will load the saved model and\nwill output a onnx.ModelProto structure (a top-level file/container format for bundling a ML model.\nFor more information .).\nThen, onnx.checker.check_model(onnx_model) will verify the model\u2019s structure\nand confirm that the model has a valid schema.\nThe validity of the ONNX graph is verified by checking the model\u2019s\nversion, the graph\u2019s structure, as well as the nodes and their inputs\nand outputs.",
            "markdown"
        ],
        [
            "import onnx\n\nonnx_model = onnx.load(\"super_resolution.onnx\")\nonnx.checker.check_model(onnx_model)",
            "code"
        ],
        [
            "Now let\u2019s compute the output using ONNX Runtime\u2019s Python APIs.\nThis part can normally be done in a separate process or on another\nmachine, but we will continue in the same process so that we can\nverify that ONNX Runtime and PyTorch are computing the same value\nfor the network.",
            "markdown"
        ],
        [
            "In order to run the model with ONNX Runtime, we need to create an\ninference session for the model with the chosen configuration\nparameters (here we use the default config).\nOnce the session is created, we evaluate the model using the run() api.\nThe output of this call is a list containing the outputs of the model\ncomputed by ONNX Runtime.",
            "markdown"
        ],
        [
            "import onnxruntime\n\nort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\")\n\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\n# compute ONNX Runtime output prediction\nort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\nort_outs = ort_session.run(None, ort_inputs)\n\n# compare ONNX Runtime and PyTorch results\nnp.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n\nprint(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")",
            "code"
        ],
        [
            "We should see that the output of PyTorch and ONNX Runtime runs match\nnumerically with the given precision (rtol=1e-03 and atol=1e-05).\nAs a side-note, if they do not match then there is an issue in the\nONNX exporter, so please contact us in that case.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime->Running the model on an image using ONNX Runtime": [
        [
            "So far we have exported a model from PyTorch and shown how to load it\nand run it in ONNX Runtime with a dummy tensor as an input.",
            "markdown"
        ],
        [
            "For this tutorial, we will use a famous cat image used widely which\nlooks like below\n\n<img alt=\"cat\" src=\"../_images/cat_224x224.jpg\"/>",
            "markdown"
        ],
        [
            "First, let\u2019s load the image, pre-process it using standard PIL\npython library. Note that this preprocessing is the standard practice of\nprocessing data for training/testing neural networks.",
            "markdown"
        ],
        [
            "We first resize the image to fit the size of the model\u2019s input (224x224).\nThen we split the image into its Y, Cb, and Cr components.\nThese components represent a greyscale image (Y), and\nthe blue-difference (Cb) and red-difference (Cr) chroma components.\nThe Y component being more sensitive to the human eye, we are\ninterested in this component which we will be transforming.\nAfter extracting the Y component, we convert it to a tensor which\nwill be the input of our model.",
            "markdown"
        ],
        [
            "from PIL import Image\nimport torchvision.transforms as transforms\n\nimg = Image.open(\"./_static/img/cat.jpg\")\n\nresize = ([224, 224])\nimg = resize(img)\n\nimg_ycbcr = img.convert('YCbCr')\nimg_y, img_cb, img_cr = img_ycbcr.split()\n\nto_tensor = ()\nimg_y = to_tensor(img_y)\nimg_y.unsqueeze_(0)",
            "code"
        ],
        [
            "Now, as a next step, let\u2019s take the tensor representing the\ngreyscale resized cat image and run the super-resolution model in\nONNX Runtime as explained previously.",
            "markdown"
        ],
        [
            "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)}\nort_outs = ort_session.run(None, ort_inputs)\nimg_out_y = ort_outs[0]",
            "code"
        ],
        [
            "At this point, the output of the model is a tensor.\nNow, we\u2019ll process the output of the model to construct back the\nfinal output image from the output tensor, and save the image.\nThe post-processing steps have been adopted from PyTorch\nimplementation of super-resolution model\n.",
            "markdown"
        ],
        [
            "img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode='L')\n\n# get the output image follow post-processing step from PyTorch implementation\nfinal_img = Image.merge(\n    \"YCbCr\", [\n        img_out_y,\n        img_cb.resize(img_out_y.size, Image.BICUBIC),\n        img_cr.resize(img_out_y.size, Image.BICUBIC),\n    ]).convert(\"RGB\")\n\n# Save the image, we will compare this with the output image from mobile device\nfinal_img.save(\"./_static/img/cat_superres_with_ort.jpg\")\n\n\n\n<img alt=\"output\\_cat\" src=\"../_images/cat_superres_with_ort.jpg\"/>",
            "code"
        ],
        [
            "ONNX Runtime being a cross platform engine, you can run it across\nmultiple platforms and on both CPUs and GPUs.",
            "markdown"
        ],
        [
            "ONNX Runtime can also be deployed to the cloud for model inferencing\nusing Azure Machine Learning Services. More information .",
            "markdown"
        ],
        [
            "More information about ONNX Runtime\u2019s performance .",
            "markdown"
        ],
        [
            "For more information about ONNX Runtime .",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "PyTorch has out of the box support for Raspberry Pi 4. This tutorial will guide\nyou on how to setup a Raspberry Pi 4 for running PyTorch and run a MobileNet v2\nclassification model in real time (30 fps+) on the CPU.",
            "markdown"
        ],
        [
            "This was all tested with Raspberry Pi 4 Model B 4GB but should work with the 2GB\nvariant as well as on the 3B with reduced performance.\n<img alt=\"https://user-images.githubusercontent.com/909104/153093710-bc736b6f-69d9-4a50-a3e8-9f2b2c9e04fd.gif\" src=\"https://user-images.githubusercontent.com/909104/153093710-bc736b6f-69d9-4a50-a3e8-9f2b2c9e04fd.gif\"/>",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->Prerequisites": [
        [
            "To follow this tutorial you\u2019ll need a Raspberry Pi 4, a camera for it and all\nthe other standard accessories.",
            "markdown"
        ],
        [
            "Heat sinks and Fan (optional but recommended)",
            "markdown"
        ],
        [
            "5V 3A USB-C Power Supply",
            "markdown"
        ],
        [
            "SD card (at least 8gb)",
            "markdown"
        ],
        [
            "SD card read/writer",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->Raspberry Pi 4 Setup": [
        [
            "PyTorch only provides pip packages for Arm 64bit (aarch64) so you\u2019ll need to install a 64 bit version of the OS on your Raspberry Pi",
            "markdown"
        ],
        [
            "You can download the latest arm64 Raspberry Pi OS from  and install it via rpi-imager.",
            "markdown"
        ],
        [
            "<strong>32-bit Raspberry Pi OS will not work.</strong>\n<img alt=\"https://user-images.githubusercontent.com/909104/152866212-36ce29b1-aba6-4924-8ae6-0a283f1fca14.gif\" src=\"https://user-images.githubusercontent.com/909104/152866212-36ce29b1-aba6-4924-8ae6-0a283f1fca14.gif\"/>",
            "markdown"
        ],
        [
            "Installation will take at least a few minutes depending on your internet speed and sdcard speed. Once it\u2019s done it should look like:\n<img alt=\"https://user-images.githubusercontent.com/909104/152867425-c005cff0-5f3f-47f1-922d-e0bbb541cd25.png\" src=\"https://user-images.githubusercontent.com/909104/152867425-c005cff0-5f3f-47f1-922d-e0bbb541cd25.png\"/>",
            "markdown"
        ],
        [
            "Time to put your sdcard in your Raspberry Pi, connect the camera and boot it up.\n<img alt=\"https://user-images.githubusercontent.com/909104/152869862-c239c980-b089-4bd5-84eb-0a1e5cf22df2.png\" src=\"https://user-images.githubusercontent.com/909104/152869862-c239c980-b089-4bd5-84eb-0a1e5cf22df2.png\"/>",
            "markdown"
        ],
        [
            "Once that boots and you complete the initial setup you\u2019ll need to edit the /boot/config.txt file to enable the camera.",
            "markdown"
        ],
        [
            "# This enables the extended features such as the camera.\nstart_x=1\n\n# This needs to be at least 128M for the camera processing, if it's bigger you can just leave it as is.\ngpu_mem=128\n\n# You need to commment/remove the existing camera_auto_detect line since this causes issues with OpenCV/V4L2 capture.\n#camera_auto_detect=1",
            "code"
        ],
        [
            "And then reboot. After you reboot the video4linux2 device /dev/video0 should exist.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->Installing PyTorch and OpenCV": [
        [
            "PyTorch and all the other libraries we need have ARM 64-bit/aarch64 variants so you can just install them via pip and have it work like any other Linux system.",
            "markdown"
        ],
        [
            "$ pip install torch torchvision torchaudio\n$ pip install opencv-python\n$ pip install numpy --upgrade\n\n\n<img alt=\"https://user-images.githubusercontent.com/909104/152874260-95a7a8bd-0f9b-438a-9c0b-5b67729e233f.png\" src=\"https://user-images.githubusercontent.com/909104/152874260-95a7a8bd-0f9b-438a-9c0b-5b67729e233f.png\"/>",
            "code"
        ],
        [
            "We can now check that everything installed correctly:",
            "markdown"
        ],
        [
            "$ python -c \"import torch; print(torch.__version__)\"\n\n\n<img alt=\"https://user-images.githubusercontent.com/909104/152874271-d7057c2d-80fd-4761-aed4-df6c8b7aa99f.png\" src=\"https://user-images.githubusercontent.com/909104/152874271-d7057c2d-80fd-4761-aed4-df6c8b7aa99f.png\"/>",
            "code"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->Video Capture": [
        [
            "For video capture we\u2019re going to be using OpenCV to stream the video frames\ninstead of the more common picamera. <cite>picamera</cite> isn\u2019t available on 64-bit\nRaspberry Pi OS and it\u2019s much slower than OpenCV. OpenCV directly accesses the\n/dev/video0 device to grab frames.",
            "markdown"
        ],
        [
            "The model we\u2019re using (MobileNetV2) takes in image sizes of 224x224 so we\ncan request that directly from OpenCV at 36fps. We\u2019re targeting 30fps for the\nmodel but we request a slightly higher framerate than that so there\u2019s always\nenough frames.",
            "markdown"
        ],
        [
            "import cv2\nfrom PIL import Image\n\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)\ncap.set(cv2.CAP_PROP_FPS, 36)",
            "code"
        ],
        [
            "OpenCV returns a numpy array in BGR so we need to read and do a bit of\nshuffling to get it into the expected RGB format.",
            "markdown"
        ],
        [
            "ret, image = cap.read()\n# convert opencv output from BGR to RGB\nimage = image[:, :, [2, 1, 0]]",
            "code"
        ],
        [
            "This data reading and processing takes about 3.5 ms.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->Image Preprocessing": [
        [
            "We need to take the frames and transform them into the format the model expects. This is the same processing as you would do on any machine with the standard torchvision transforms.",
            "markdown"
        ],
        [
            "from torchvision import transforms\n\npreprocess = transforms.Compose([\n    # convert the frame to a CHW torch tensor for training\n    transforms.ToTensor(),\n    # normalize the colors to the range that mobilenet_v2/3 expect\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(image)\n# The model can handle multiple images simultaneously so we need to add an\n# empty dimension for the batch.\n# [3, 224, 224] -&gt; [1, 3, 224, 224]\ninput_batch = input_tensor.unsqueeze(0)",
            "code"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->Model Choices": [
        [
            "There\u2019s a number of models you can choose from to use with different performance\ncharacteristics. Not all models provide a qnnpack pretrained variant so for\ntesting purposes you should chose one that does but if you train and quantize\nyour own model you can use any of them.",
            "markdown"
        ],
        [
            "We\u2019re using mobilenet_v2 for this tutorial since it has good performance and\naccuracy.",
            "markdown"
        ],
        [
            "Raspberry Pi 4 Benchmark Results:",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->MobileNetV2: Quantization and JIT": [
        [
            "For optimal performance we want a model that\u2019s quantized and fused. Quantized\nmeans that it does the computation using int8 which is much more performant than\nthe standard float32 math. Fused means that consecutive operations have been\nfused together into a more performant version where possible. Commonly things\nlike activations (ReLU) can be merged into the layer before (Conv2d)\nduring inference.",
            "markdown"
        ],
        [
            "The aarch64 version of pytorch requires using the qnnpack engine.",
            "markdown"
        ],
        [
            "import torch\ntorch.backends.quantized.engine = 'qnnpack'",
            "code"
        ],
        [
            "For this example we\u2019ll use a prequantized and fused version of MobileNetV2 that\u2019s provided out of the box by torchvision.",
            "markdown"
        ],
        [
            "from torchvision import models\nnet = models.quantization.mobilenet_v2(pretrained=True, quantize=True)",
            "code"
        ],
        [
            "We then want to jit the model to reduce Python overhead and fuse any ops. Jit gives us ~30fps instead of ~20fps without it.",
            "markdown"
        ],
        [
            "net = torch.jit.script(net)",
            "code"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->Putting It Together": [
        [
            "We can now put all the pieces together and run it:",
            "markdown"
        ],
        [
            "import time\n\nimport torch\nimport numpy as np\nfrom torchvision import models, transforms\n\nimport cv2\nfrom PIL import Image\n\ntorch.backends.quantized.engine = 'qnnpack'\n\ncap = cv2.VideoCapture(0, cv2.CAP_V4L2)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)\ncap.set(cv2.CAP_PROP_FPS, 36)\n\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nnet = models.quantization.mobilenet_v2(pretrained=True, quantize=True)\n# jit model to take it from ~20fps to ~30fps\nnet = torch.jit.script(net)\n\nstarted = time.time()\nlast_logged = time.time()\nframe_count = 0\n\nwith torch.no_grad():\n    while True:\n        # read frame\n        ret, image = cap.read()\n        if not ret:\n            raise RuntimeError(\"failed to read frame\")\n\n        # convert opencv output from BGR to RGB\n        image = image[:, :, [2, 1, 0]]\n        permuted = image\n\n        # preprocess\n        input_tensor = preprocess(image)\n\n        # create a mini-batch as expected by the model\n        input_batch = input_tensor.unsqueeze(0)\n\n        # run model\n        output = net(input_batch)\n        # do something with output ...\n\n        # log model performance\n        frame_count += 1\n        now = time.time()\n        if now - last_logged &gt; 1:\n            print(f\"{frame_count / (now-last_logged)} fps\")\n            last_logged = now\n            frame_count = 0",
            "code"
        ],
        [
            "Running it shows that we\u2019re hovering at ~30 fps.\n<img alt=\"https://user-images.githubusercontent.com/909104/152892609-7d115705-3ec9-4f8d-beed-a51711503a32.png\" src=\"https://user-images.githubusercontent.com/909104/152892609-7d115705-3ec9-4f8d-beed-a51711503a32.png\"/>",
            "markdown"
        ],
        [
            "This is with all the default settings in Raspberry Pi OS. If you disabled the UI\nand all the other background services that are enabled by default it\u2019s more\nperformant and stable.",
            "markdown"
        ],
        [
            "If we check htop we see that we have almost 100% utilization.\n<img alt=\"https://user-images.githubusercontent.com/909104/152892630-f094b84b-19ba-48f6-8632-1b954abc59c7.png\" src=\"https://user-images.githubusercontent.com/909104/152892630-f094b84b-19ba-48f6-8632-1b954abc59c7.png\"/>",
            "markdown"
        ],
        [
            "To verify that it\u2019s working end to end we can compute the probabilities of the\nclasses and\n\nto print the detections.",
            "markdown"
        ],
        [
            "top = list(enumerate(output[0].softmax(dim=0)))\ntop.sort(key=lambda x: x[1], reverse=True)\nfor idx, val in top[:10]:\n    print(f\"{val.item()*100:.2f}% {classes[idx]}\")",
            "code"
        ],
        [
            "mobilenet_v3_large running in real time:\n<img alt=\"https://user-images.githubusercontent.com/909104/153093710-bc736b6f-69d9-4a50-a3e8-9f2b2c9e04fd.gif\" src=\"https://user-images.githubusercontent.com/909104/153093710-bc736b6f-69d9-4a50-a3e8-9f2b2c9e04fd.gif\"/>",
            "markdown"
        ],
        [
            "Detecting an orange:\n<img alt=\"https://user-images.githubusercontent.com/909104/153092153-d9c08dfe-105b-408a-8e1e-295da8a78c19.jpg\" src=\"https://user-images.githubusercontent.com/909104/153092153-d9c08dfe-105b-408a-8e1e-295da8a78c19.jpg\"/>",
            "markdown"
        ],
        [
            "Detecting a mug:\n<img alt=\"https://user-images.githubusercontent.com/909104/153092155-4b90002f-a0f3-4267-8d70-e713e7b4d5a0.jpg\" src=\"https://user-images.githubusercontent.com/909104/153092155-4b90002f-a0f3-4267-8d70-e713e7b4d5a0.jpg\"/>",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->Troubleshooting: Performance": [
        [
            "PyTorch by default will use all of the cores available. If you have anything\nrunning in the background on the Raspberry Pi it may cause contention with the\nmodel inference causing latency spikes. To alleviate this you can reduce the\nnumber of threads which will reduce the peak latency at a small performance\npenalty.",
            "markdown"
        ],
        [
            "torch.set_num_threads(2)",
            "code"
        ],
        [
            "For shufflenet_v2_x1_5 using 2 threads instead of 4 threads\nincreases best case latency to 72 ms from 60 ms but eliminates the\nlatency spikes of 128 ms.",
            "markdown"
        ]
    ],
    "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->Next Steps": [
        [
            "You can create your own model or fine tune an existing one. If you fine tune on\none of the models from\n\nmost of the work to fuse and quantize has already been done for you so you can\ndirectly deploy with good performance on a Raspberry Pi.",
            "markdown"
        ],
        [
            "See more:",
            "markdown"
        ],
        [
            " for more information on how to quantize and fuse your model.",
            "markdown"
        ],
        [
            "for how to use transfer learning to fine tune a pre-existing model to your dataset.",
            "markdown"
        ]
    ],
    "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "In this tutorial, we are going to use FX, a toolkit for composable function\ntransformations of PyTorch, to do the following:",
            "markdown"
        ],
        [
            "Find patterns of conv/batch norm in the data dependencies.",
            "markdown"
        ],
        [
            "For the patterns found in 1), fold the batch norm statistics into the convolution weights.",
            "markdown"
        ],
        [
            "Note that this optimization only works for models in inference mode (i.e. <cite>mode.eval()</cite>)",
            "markdown"
        ],
        [
            "We will be building the fuser that exists here:",
            "markdown"
        ],
        [
            "First, let\u2019s get some imports out of the way (we will be using all\nof these later in the code).",
            "markdown"
        ],
        [
            "from typing import Type, Dict, Any, Tuple, Iterable\nimport copy\nimport torch.fx as fx\nimport torch\nimport torch.nn as nn",
            "code"
        ],
        [
            "For this tutorial, we are going to create a model consisting of convolutions\nand batch norms. Note that this model has some tricky components - some of\nthe conv/batch norm patterns are hidden within Sequentials and one of the\nBatchNorms is wrapped in another Module.",
            "markdown"
        ],
        [
            "class WrappedBatchNorm():\n    def __init__(self):\n        super().__init__()\n        self.mod = (1)\n    def forward(self, x):\n        return self.mod(x)\n\nclass M():\n    def __init__(self):\n        super().__init__()\n        self.conv1 = (1, 1, 1)\n        self.bn1 = (1)\n        self.conv2 = (1, 1, 1)\n        self.nested = (\n            (1),\n            (1, 1, 1),\n        )\n        self.wrapped = WrappedBatchNorm()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.conv2(x)\n        x = self.nested(x)\n        x = self.wrapped(x)\n        return x\n\nmodel = M()\n\nmodel.eval()",
            "code"
        ]
    ],
    "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm": [
        [
            "One of the primary challenges with trying to automatically fuse convolution\nand batch norm in PyTorch is that PyTorch does not provide an easy way of\naccessing the computational graph. FX resolves this problem by symbolically\ntracing the actual operations called, so that we can track the computations\nthrough the <cite>forward</cite> call, nested within Sequential modules, or wrapped in\nan user-defined module.",
            "markdown"
        ],
        [
            "traced_model = (model)\nprint(traced_model.graph)",
            "code"
        ],
        [
            "This gives us a graph representation of our model. Note that both the modules\nhidden within the sequential as well as the wrapped Module have been inlined\ninto the graph. This is the default level of abstraction, but it can be\nconfigured by the pass writer. More information can be found at the FX\noverview ",
            "markdown"
        ],
        [
            "Unlike some other fusions, fusion of convolution with batch norm does not\nrequire any new operators. Instead, as batch norm during inference\nconsists of a pointwise add and multiply, these operations can be \u201cbaked\u201d\ninto the preceding convolution\u2019s weights. This allows us to remove the batch\nnorm entirely from our model! Read\n for further details. The\ncode here is copied from\n\nclarity purposes.",
            "markdown"
        ],
        [
            "def fuse_conv_bn_eval(conv, bn):\n    \"\"\"\n    Given a conv Module `A` and an batch_norm module `B`, returns a conv\n    module `C` such that C(x) == B(A(x)) in inference mode.\n    \"\"\"\n    assert(not (conv.training or bn.training)), \"Fusion only for eval!\"\n    fused_conv = copy.deepcopy(conv)\n\n    fused_conv.weight, fused_conv.bias = \\\n        fuse_conv_bn_weights(fused_conv.weight, fused_conv.bias,\n                             bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)\n\n    return fused_conv\n\ndef fuse_conv_bn_weights(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):\n    if conv_b is None:\n        conv_b = (bn_rm)\n    if bn_w is None:\n        bn_w = (bn_rm)\n    if bn_b is None:\n        bn_b = (bn_rm)\n    bn_var_rsqrt = (bn_rv + bn_eps)\n\n    conv_w = conv_w * (bn_w * bn_var_rsqrt).reshape([-1] + [1] * (len(conv_w.shape) - 1))\n    conv_b = (conv_b - bn_rm) * bn_var_rsqrt * bn_w + bn_b\n\n    return torch.nn.Parameter(conv_w), torch.nn.Parameter(conv_b)",
            "code"
        ]
    ],
    "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->FX Fusion Pass": [
        [
            "Now that we have our computational graph as well as a method for fusing\nconvolution and batch norm, all that remains is to iterate over the FX graph\nand apply the desired fusions.",
            "markdown"
        ],
        [
            "def _parent_name(target : str) -&gt; Tuple[str, str]:\n    \"\"\"\n    Splits a qualname into parent path and last atom.\n    For example, `foo.bar.baz` -&gt; (`foo.bar`, `baz`)\n    \"\"\"\n    *parent, name = target.rsplit('.', 1)\n    return parent[0] if parent else '', name\n\ndef replace_node_module(node: , modules: Dict[str, Any], new_module: ):\n    assert(isinstance(node.target, str))\n    parent_name, name = _parent_name(node.target)\n    setattr(modules[parent_name], name, new_module)\n\n\ndef fuse(model: ) -&gt; :\n    model = copy.deepcopy(model)\n    # The first step of most FX passes is to symbolically trace our model to\n    # obtain a `GraphModule`. This is a representation of our original model\n    # that is functionally identical to our original model, except that we now\n    # also have a graph representation of our forward pass.\n    fx_model:  = (model)\n    modules = dict(fx_model.named_modules())\n\n    # The primary representation for working with FX are the `Graph` and the\n    # `Node`. Each `GraphModule` has a `Graph` associated with it - this\n    # `Graph` is also what generates `GraphModule.code`.\n    # The `Graph` itself is represented as a list of `Node` objects. Thus, to\n    # iterate through all of the operations in our graph, we iterate over each\n    # `Node` in our `Graph`.\n    for node in fx_model.graph.nodes:\n        # The FX IR contains several types of nodes, which generally represent\n        # call sites to modules, functions, or methods. The type of node is\n        # determined by `Node.op`.\n        if node.op != 'call_module': # If our current node isn't calling a Module then we can ignore it.\n            continue\n        # For call sites, `Node.target` represents the module/function/method\n        # that's being called. Here, we check `Node.target` to see if it's a\n        # batch norm module, and then check `Node.args[0].target` to see if the\n        # input `Node` is a convolution.\n        if type(modules[node.target]) is  and type(modules[node.args[0].target]) is :\n            if len(node.args[0].users) &gt; 1:  # Output of conv is used by other nodes\n                continue\n            conv = modules[node.args[0].target]\n            bn = modules[node.target]\n            fused_conv = fuse_conv_bn_eval(conv, bn)\n            replace_node_module(node.args[0], modules, fused_conv)\n            # As we've folded the batch nor into the conv, we need to replace all uses\n            # of the batch norm with the conv.\n            node.replace_all_uses_with(node.args[0])\n            # Now that all uses of the batch norm have been replaced, we can\n            # safely remove the batch norm.\n            fx_model.graph.erase_node(node)\n    fx_model.graph.lint()\n    # After we've modified our graph, we need to recompile our graph in order\n    # to keep the generated code in sync.\n    fx_model.recompile()\n    return fx_model",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "We make some simplifications here for demonstration purposes, such as only\nmatching 2D convolutions. View\n\nfor a more usable pass.",
            "markdown"
        ]
    ],
    "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Testing out our Fusion Pass": [
        [
            "We can now run this fusion pass on our initial toy model and verify that our\nresults are identical. In addition, we can print out the code for our fused\nmodel and verify that there are no more batch norms.",
            "markdown"
        ],
        [
            "fused_model = fuse(model)\nprint(fused_model.code)\ninp = (5, 1, 1, 1)\n(fused_model(inp), model(inp))",
            "code"
        ]
    ],
    "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Benchmarking our Fusion on ResNet18": [
        [
            "We can test our fusion pass on a larger model like ResNet18 and see how much\nthis pass improves inference performance.",
            "markdown"
        ],
        [
            "import torchvision.models as models\nimport time\n\nrn18 = ()\nrn18.eval()\n\ninp = (10, 3, 224, 224)\noutput = rn18(inp)\n\ndef benchmark(model, iters=20):\n    for _ in range(10):\n        model(inp)\n    begin = time.time()\n    for _ in range(iters):\n        model(inp)\n    return str(time.time()-begin)\n\nfused_rn18 = fuse(rn18)\nprint(\"Unfused time: \", benchmark(rn18))\nprint(\"Fused time: \", benchmark(fused_rn18))",
            "code"
        ],
        [
            "As we previously saw, the output of our FX transformation is\n(Torchscriptable) PyTorch code, we can easily <cite>jit.script</cite> the output to try\nand increase our performance even more. In this way, our FX model\ntransformation composes with Torchscript with no issues.",
            "markdown"
        ],
        [
            "jit_rn18 = (fused_rn18)\nprint(\"jit time: \", benchmark(jit_rn18))\n\n\n############\n# Conclusion\n# ----------\n# As we can see, using FX we can easily write static graph transformations on\n# PyTorch code.\n#\n# Since FX is still in beta, we would be happy to hear any\n# feedback you have about using it. Please feel free to use the\n# PyTorch Forums (https://discuss.pytorch.org/) and the issue tracker\n# (https://github.com/pytorch/pytorch/issues) to provide any feedback\n# you might have.",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "In this tutorial, we are going to use FX to do the following:",
            "markdown"
        ],
        [
            "Capture PyTorch Python code in a way that we can inspect and gather\nstatistics about the structure and execution of the code",
            "markdown"
        ],
        [
            "Build out a small class that will serve as a simple performance \u201cprofiler\u201d,\ncollecting runtime statistics about each part of the model from actual\nruns.",
            "markdown"
        ],
        [
            "For this tutorial, we are going to use the torchvision ResNet18 model\nfor demonstration purposes.",
            "markdown"
        ],
        [
            "import torch\nimport torch.fx\nimport torchvision.models as models\n\nrn18 = ()\n()",
            "code"
        ],
        [
            "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)",
            "code"
        ],
        [
            "Now that we have our model, we want to inspect deeper into its\nperformance. That is, for the following invocation, which parts\nof the model are taking the longest?",
            "markdown"
        ],
        [
            "input = (5, 3, 224, 224)\n = rn18(input)",
            "code"
        ],
        [
            "A common way of answering that question is to go through the program\nsource, add code that collects timestamps at various points in the\nprogram, and compare the difference between those timestamps to see\nhow long the regions between the timestamps take.",
            "markdown"
        ],
        [
            "That technique is certainly applicable to PyTorch code, however it\nwould be nicer if we didn\u2019t have to copy over model code and edit it,\nespecially code we haven\u2019t written (like this torchvision model).\nInstead, we are going to use FX to automate this \u201cinstrumentation\u201d\nprocess without needing to modify any source.",
            "markdown"
        ],
        [
            "First, let\u2019s get some imports out of the way (we will be using all\nof these later in the code).",
            "markdown"
        ],
        [
            "import statistics, tabulate, time\nfrom typing import Any, Dict, List\nfrom torch.fx import ",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "tabulate is an external library that is not a dependency of PyTorch.\nWe will be using it to more easily visualize performance data. Please\nmake sure you\u2019ve installed it from your favorite Python package source.",
            "markdown"
        ]
    ],
    "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing": [
        [
            "Next, we are going to use FX\u2019s symbolic tracing mechanism to capture\nthe definition of our model in a data structure we can manipulate\nand examine.",
            "markdown"
        ],
        [
            "traced_rn18 = (rn18)\nprint()",
            "code"
        ],
        [
            "graph():\n    %x : torch.Tensor [#users=1] = placeholder[target=x]\n    %conv1 : [#users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n    %bn1 : [#users=1] = call_module[target=bn1](args = (%conv1,), kwargs = {})\n    %relu : [#users=1] = call_module[target=relu](args = (%bn1,), kwargs = {})\n    %maxpool : [#users=2] = call_module[target=maxpool](args = (%relu,), kwargs = {})\n    %layer1_0_conv1 : [#users=1] = call_module[target=layer1.0.conv1](args = (%maxpool,), kwargs = {})\n    %layer1_0_bn1 : [#users=1] = call_module[target=layer1.0.bn1](args = (%layer1_0_conv1,), kwargs = {})\n    %layer1_0_relu : [#users=1] = call_module[target=layer1.0.relu](args = (%layer1_0_bn1,), kwargs = {})\n    %layer1_0_conv2 : [#users=1] = call_module[target=layer1.0.conv2](args = (%layer1_0_relu,), kwargs = {})\n    %layer1_0_bn2 : [#users=1] = call_module[target=layer1.0.bn2](args = (%layer1_0_conv2,), kwargs = {})\n    %add : [#users=1] = call_function[target=operator.add](args = (%layer1_0_bn2, %maxpool), kwargs = {})\n    %layer1_0_relu_1 : [#users=2] = call_module[target=layer1.0.relu](args = (%add,), kwargs = {})\n    %layer1_1_conv1 : [#users=1] = call_module[target=layer1.1.conv1](args = (%layer1_0_relu_1,), kwargs = {})\n    %layer1_1_bn1 : [#users=1] = call_module[target=layer1.1.bn1](args = (%layer1_1_conv1,), kwargs = {})\n    %layer1_1_relu : [#users=1] = call_module[target=layer1.1.relu](args = (%layer1_1_bn1,), kwargs = {})\n    %layer1_1_conv2 : [#users=1] = call_module[target=layer1.1.conv2](args = (%layer1_1_relu,), kwargs = {})\n    %layer1_1_bn2 : [#users=1] = call_module[target=layer1.1.bn2](args = (%layer1_1_conv2,), kwargs = {})\n    %add_1 : [#users=1] = call_function[target=operator.add](args = (%layer1_1_bn2, %layer1_0_relu_1), kwargs = {})\n    %layer1_1_relu_1 : [#users=2] = call_module[target=layer1.1.relu](args = (%add_1,), kwargs = {})\n    %layer2_0_conv1 : [#users=1] = call_module[target=layer2.0.conv1](args = (%layer1_1_relu_1,), kwargs = {})\n    %layer2_0_bn1 : [#users=1] = call_module[target=layer2.0.bn1](args = (%layer2_0_conv1,), kwargs = {})\n    %layer2_0_relu : [#users=1] = call_module[target=layer2.0.relu](args = (%layer2_0_bn1,), kwargs = {})\n    %layer2_0_conv2 : [#users=1] = call_module[target=layer2.0.conv2](args = (%layer2_0_relu,), kwargs = {})\n    %layer2_0_bn2 : [#users=1] = call_module[target=layer2.0.bn2](args = (%layer2_0_conv2,), kwargs = {})\n    %layer2_0_downsample_0 : [#users=1] = call_module[target=layer2.0.downsample.0](args = (%layer1_1_relu_1,), kwargs = {})\n    %layer2_0_downsample_1 : [#users=1] = call_module[target=layer2.0.downsample.1](args = (%layer2_0_downsample_0,), kwargs = {})\n    %add_2 : [#users=1] = call_function[target=operator.add](args = (%layer2_0_bn2, %layer2_0_downsample_1), kwargs = {})\n    %layer2_0_relu_1 : [#users=2] = call_module[target=layer2.0.relu](args = (%add_2,), kwargs = {})\n    %layer2_1_conv1 : [#users=1] = call_module[target=layer2.1.conv1](args = (%layer2_0_relu_1,), kwargs = {})\n    %layer2_1_bn1 : [#users=1] = call_module[target=layer2.1.bn1](args = (%layer2_1_conv1,), kwargs = {})\n    %layer2_1_relu : [#users=1] = call_module[target=layer2.1.relu](args = (%layer2_1_bn1,), kwargs = {})\n    %layer2_1_conv2 : [#users=1] = call_module[target=layer2.1.conv2](args = (%layer2_1_relu,), kwargs = {})\n    %layer2_1_bn2 : [#users=1] = call_module[target=layer2.1.bn2](args = (%layer2_1_conv2,), kwargs = {})\n    %add_3 : [#users=1] = call_function[target=operator.add](args = (%layer2_1_bn2, %layer2_0_relu_1), kwargs = {})\n    %layer2_1_relu_1 : [#users=2] = call_module[target=layer2.1.relu](args = (%add_3,), kwargs = {})\n    %layer3_0_conv1 : [#users=1] = call_module[target=layer3.0.conv1](args = (%layer2_1_relu_1,), kwargs = {})\n    %layer3_0_bn1 : [#users=1] = call_module[target=layer3.0.bn1](args = (%layer3_0_conv1,), kwargs = {})\n    %layer3_0_relu : [#users=1] = call_module[target=layer3.0.relu](args = (%layer3_0_bn1,), kwargs = {})\n    %layer3_0_conv2 : [#users=1] = call_module[target=layer3.0.conv2](args = (%layer3_0_relu,), kwargs = {})\n    %layer3_0_bn2 : [#users=1] = call_module[target=layer3.0.bn2](args = (%layer3_0_conv2,), kwargs = {})\n    %layer3_0_downsample_0 : [#users=1] = call_module[target=layer3.0.downsample.0](args = (%layer2_1_relu_1,), kwargs = {})\n    %layer3_0_downsample_1 : [#users=1] = call_module[target=layer3.0.downsample.1](args = (%layer3_0_downsample_0,), kwargs = {})\n    %add_4 : [#users=1] = call_function[target=operator.add](args = (%layer3_0_bn2, %layer3_0_downsample_1), kwargs = {})\n    %layer3_0_relu_1 : [#users=2] = call_module[target=layer3.0.relu](args = (%add_4,), kwargs = {})\n    %layer3_1_conv1 : [#users=1] = call_module[target=layer3.1.conv1](args = (%layer3_0_relu_1,), kwargs = {})\n    %layer3_1_bn1 : [#users=1] = call_module[target=layer3.1.bn1](args = (%layer3_1_conv1,), kwargs = {})\n    %layer3_1_relu : [#users=1] = call_module[target=layer3.1.relu](args = (%layer3_1_bn1,), kwargs = {})\n    %layer3_1_conv2 : [#users=1] = call_module[target=layer3.1.conv2](args = (%layer3_1_relu,), kwargs = {})\n    %layer3_1_bn2 : [#users=1] = call_module[target=layer3.1.bn2](args = (%layer3_1_conv2,), kwargs = {})\n    %add_5 : [#users=1] = call_function[target=operator.add](args = (%layer3_1_bn2, %layer3_0_relu_1), kwargs = {})\n    %layer3_1_relu_1 : [#users=2] = call_module[target=layer3.1.relu](args = (%add_5,), kwargs = {})\n    %layer4_0_conv1 : [#users=1] = call_module[target=layer4.0.conv1](args = (%layer3_1_relu_1,), kwargs = {})\n    %layer4_0_bn1 : [#users=1] = call_module[target=layer4.0.bn1](args = (%layer4_0_conv1,), kwargs = {})\n    %layer4_0_relu : [#users=1] = call_module[target=layer4.0.relu](args = (%layer4_0_bn1,), kwargs = {})\n    %layer4_0_conv2 : [#users=1] = call_module[target=layer4.0.conv2](args = (%layer4_0_relu,), kwargs = {})\n    %layer4_0_bn2 : [#users=1] = call_module[target=layer4.0.bn2](args = (%layer4_0_conv2,), kwargs = {})\n    %layer4_0_downsample_0 : [#users=1] = call_module[target=layer4.0.downsample.0](args = (%layer3_1_relu_1,), kwargs = {})\n    %layer4_0_downsample_1 : [#users=1] = call_module[target=layer4.0.downsample.1](args = (%layer4_0_downsample_0,), kwargs = {})\n    %add_6 : [#users=1] = call_function[target=operator.add](args = (%layer4_0_bn2, %layer4_0_downsample_1), kwargs = {})\n    %layer4_0_relu_1 : [#users=2] = call_module[target=layer4.0.relu](args = (%add_6,), kwargs = {})\n    %layer4_1_conv1 : [#users=1] = call_module[target=layer4.1.conv1](args = (%layer4_0_relu_1,), kwargs = {})\n    %layer4_1_bn1 : [#users=1] = call_module[target=layer4.1.bn1](args = (%layer4_1_conv1,), kwargs = {})\n    %layer4_1_relu : [#users=1] = call_module[target=layer4.1.relu](args = (%layer4_1_bn1,), kwargs = {})\n    %layer4_1_conv2 : [#users=1] = call_module[target=layer4.1.conv2](args = (%layer4_1_relu,), kwargs = {})\n    %layer4_1_bn2 : [#users=1] = call_module[target=layer4.1.bn2](args = (%layer4_1_conv2,), kwargs = {})\n    %add_7 : [#users=1] = call_function[target=operator.add](args = (%layer4_1_bn2, %layer4_0_relu_1), kwargs = {})\n    %layer4_1_relu_1 : [#users=1] = call_module[target=layer4.1.relu](args = (%add_7,), kwargs = {})\n    %avgpool : [#users=1] = call_module[target=avgpool](args = (%layer4_1_relu_1,), kwargs = {})\n    %flatten : [#users=1] = call_function[target=torch.flatten](args = (%avgpool, 1), kwargs = {})\n    %fc : [#users=1] = call_module[target=fc](args = (%flatten,), kwargs = {})\n    return fc",
            "code"
        ],
        [
            "This gives us a Graph representation of the ResNet18 model. A Graph\nconsists of a series of Nodes connected to each other. Each Node\nrepresents a call-site in the Python code (whether to a function,\na module, or a method) and the edges (represented as args and kwargs\non each node) represent the values passed between these call-sites. More\ninformation about the Graph representation and the rest of FX\u2019s APIs ca\nbe found at the FX documentation .",
            "markdown"
        ]
    ],
    "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Creating a Profiling Interpreter": [
        [
            "Next, we are going to create a class that inherits from torch.fx.Interpreter.\nThough the GraphModule that symbolic_trace produces compiles Python code\nthat is run when you call a GraphModule, an alternative way to run a\nGraphModule is by executing each Node in the Graph one by one. That is\nthe functionality that Interpreter provides: It interprets the graph node-\nby-node.",
            "markdown"
        ],
        [
            "By inheriting from Interpreter, we can override various functionality and\ninstall the profiling behavior we want. The goal is to have an object to which\nwe can pass a model, invoke the model 1 or more times, then get statistics about\nhow long the model and each part of the model took during those runs.",
            "markdown"
        ],
        [
            "Let\u2019s define our ProfilingInterpreter class:",
            "markdown"
        ],
        [
            "class ProfilingInterpreter():\n    def __init__(self, mod : ):\n        # Rather than have the user symbolically trace their model,\n        # we're going to do it in the constructor. As a result, the\n        # user can pass in any ``Module`` without having to worry about\n        # symbolic tracing APIs\n        gm = (mod)\n        super().__init__(gm)\n\n        # We are going to store away two things here:\n        #\n        # 1. A list of total runtimes for ``mod``. In other words, we are\n        #    storing away the time ``mod(...)`` took each time this\n        #    interpreter is called.\n        self.total_runtime_sec : List[float] = []\n        # 2. A map from ``Node`` to a list of times (in seconds) that\n        #    node took to run. This can be seen as similar to (1) but\n        #    for specific sub-parts of the model.\n        self.runtimes_sec : Dict[, List[float]] = {}\n\n    ######################################################################\n    # Next, let's override our first method: ``run()``. ``Interpreter``'s ``run``\n    # method is the top-level entrypoint for execution of the model. We will\n    # want to intercept this so that we can record the total runtime of the\n    # model.\n\n    def run(self, *args) -&gt; Any:\n        # Record the time we started running the model\n        t_start = time.time()\n        # Run the model by delegating back into Interpreter.run()\n        return_val = super().run(*args)\n        # Record the time we finished running the model\n        t_end = time.time()\n        # Store the total elapsed time this model execution took in the\n        # ProfilingInterpreter\n        self.total_runtime_sec.append(t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Now, let's override ``run_node``. ``Interpreter`` calls ``run_node`` each\n    # time it executes a single node. We will intercept this so that we\n    # can measure and record the time taken for each individual call in\n    # the model.\n\n    def run_node(self, n : ) -&gt; Any:\n        # Record the time we started running the op\n        t_start = time.time()\n        # Run the op by delegating back into Interpreter.run_node()\n        return_val = super().run_node(n)\n        # Record the time we finished running the op\n        t_end = time.time()\n        # If we don't have an entry for this node in our runtimes_sec\n        # data structure, add one with an empty list value.\n        self.runtimes_sec.setdefault(n, [])\n        # Record the total elapsed time for this single invocation\n        # in the runtimes_sec data structure\n        self.runtimes_sec[n].append(t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Finally, we are going to define a method (one which doesn't override\n    # any ``Interpreter`` method) that provides us a nice, organized view of\n    # the data we have collected.\n\n    def summary(self, should_sort : bool = False) -&gt; str:\n        # Build up a list of summary information for each node\n        node_summaries : List[List[Any]] = []\n        # Calculate the mean runtime for the whole network. Because the\n        # network may have been called multiple times during profiling,\n        # we need to summarize the runtimes. We choose to use the\n        # arithmetic mean for this.\n        mean_total_runtime = statistics.mean(self.total_runtime_sec)\n\n        # For each node, record summary statistics\n        for node, runtimes in self.runtimes_sec.items():\n            # Similarly, compute the mean runtime for ``node``\n            mean_runtime = statistics.mean(runtimes)\n            # For easier understanding, we also compute the percentage\n            # time each node took with respect to the whole network.\n            pct_total = mean_runtime / mean_total_runtime * 100\n            # Record the node's type, name of the node, mean runtime, and\n            # percent runtim\n            node_summaries.append(\n                [node.op, str(node), mean_runtime, pct_total])\n\n        # One of the most important questions to answer when doing performance\n        # profiling is \"Which op(s) took the longest?\". We can make this easy\n        # to see by providing sorting functionality in our summary view\n        if should_sort:\n            node_summaries.sort(key=lambda s: s[2], reverse=True)\n\n        # Use the ``tabulate`` library to create a well-formatted table\n        # presenting our summary information\n        headers : List[str] = [\n            'Op type', 'Op', 'Average runtime (s)', 'Pct total runtime'\n        ]\n        return tabulate.tabulate(node_summaries, headers=headers)",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "We use Python\u2019s time.time function to pull wall clock\ntimestamps and compare them. This is not the most accurate\nway to measure performance, and will only give us a first-\norder approximation. We use this simple technique only for the\npurpose of demonstration in this tutorial.",
            "markdown"
        ]
    ],
    "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Investigating the Performance of ResNet18": [
        [
            "We can now use ProfilingInterpreter to inspect the performance\ncharacteristics of our ResNet18 model;",
            "markdown"
        ],
        [
            "interp = (rn18)\n(input)\nprint(interp.summary(True))",
            "code"
        ],
        [
            "Op type        Op                       Average runtime (s)    Pct total runtime\n-------------  ---------------------  ---------------------  -------------------\ncall_module    maxpool                          0.0186694              9.70712\ncall_module    conv1                            0.0158079              8.21929\ncall_module    layer1_1_conv1                   0.0112045              5.82576\ncall_module    layer4_0_conv2                   0.0105081              5.46366\ncall_module    layer1_1_conv2                   0.0103536              5.38333\ncall_module    layer1_0_conv1                   0.0103078              5.35952\ncall_module    layer1_0_conv2                   0.00978041             5.08531\ncall_module    layer4_1_conv1                   0.008847               4.59999\ncall_module    layer4_1_conv2                   0.00849557             4.41726\ncall_module    layer2_1_conv2                   0.00840592             4.37065\ncall_module    layer3_1_conv2                   0.00786257             4.08813\ncall_module    layer3_1_conv1                   0.00758052             3.94148\ncall_module    layer2_0_conv2                   0.00745034             3.8738\ncall_module    layer3_0_conv2                   0.00727534             3.78281\ncall_module    layer2_0_conv1                   0.00721908             3.75355\ncall_module    layer2_1_conv1                   0.00681758             3.54479\ncall_module    bn1                              0.00668931             3.4781\ncall_module    layer4_0_conv1                   0.00650167             3.38054\ncall_module    layer3_0_conv1                   0.00450587             2.34282\ncall_function  add_1                            0.0017252              0.897014\ncall_module    layer1_1_bn2                     0.00153756             0.799454\ncall_function  add                              0.00151062             0.785445\ncall_module    layer2_0_downsample_0            0.00104856             0.5452\ncall_module    layer3_0_downsample_0            0.000977039            0.508011\ncall_module    relu                             0.000867844            0.451234\ncall_module    layer4_0_downsample_0            0.00076437             0.397433\ncall_function  add_3                            0.00070715             0.367682\ncall_module    layer1_0_bn2                     0.000444412            0.231072\ncall_module    layer1_0_bn1                     0.00043869             0.228097\ncall_module    layer1_1_bn1                     0.000427485            0.22227\ncall_module    fc                               0.000384808            0.20008\ncall_module    layer4_1_bn1                     0.000296116            0.153965\ncall_module    layer2_0_bn1                     0.000295162            0.153469\ncall_module    avgpool                          0.00028038             0.145783\ncall_module    layer2_0_bn2                     0.000272751            0.141817\ncall_module    layer2_0_downsample_1            0.000244379            0.127065\ncall_module    layer1_1_relu_1                  0.00023818             0.123842\ncall_module    layer4_0_bn1                     0.000236511            0.122974\ncall_module    layer2_1_bn2                     0.000233173            0.121238\ncall_module    layer2_1_bn1                     0.000229359            0.119255\ncall_module    layer1_0_relu_1                  0.000216007            0.112313\ncall_module    layer1_0_relu                    0.000213623            0.111073\ncall_module    layer3_0_bn1                     0.000210524            0.109462\ncall_module    layer3_0_downsample_1            0.000199795            0.103883\ncall_module    layer3_1_bn1                     0.00019455             0.101156\ncall_module    layer3_1_bn2                     0.000190496            0.0990484\ncall_module    layer3_0_bn2                     0.000186682            0.097065\ncall_function  add_2                            0.000185966            0.0966931\ncall_module    layer1_1_relu                    0.000185013            0.0961972\ncall_module    layer2_0_relu                    0.000171185            0.0890072\ncall_module    layer4_1_bn2                     0.000170231            0.0885114\ncall_module    layer4_0_downsample_1            0.000168324            0.0875196\ncall_module    layer4_0_bn2                     0.00016284             0.0846684\ncall_function  add_4                            0.000138521            0.072024\ncall_module    layer2_1_relu_1                  0.000127316            0.0661976\ncall_module    layer2_1_relu                    0.000125647            0.0653298\ncall_module    layer2_0_relu_1                  0.000118494            0.0616109\ncall_function  add_6                            0.000116587            0.0606191\ncall_function  add_5                            0.00011611             0.0603712\ncall_module    layer3_0_relu_1                  0.000106573            0.0554126\ncall_module    layer3_1_relu                    9.34601e-05            0.0485945\ncall_module    layer3_1_relu_1                  9.29832e-05            0.0483465\ncall_module    layer4_0_relu_1                  9.01222e-05            0.046859\ncall_module    layer4_0_relu                    8.98838e-05            0.046735\ncall_module    layer3_0_relu                    8.63075e-05            0.0448755\ncall_module    layer4_1_relu                    8.24928e-05            0.0428921\ncall_function  add_7                            7.4625e-05             0.0388012\ncall_module    layer4_1_relu_1                  7.43866e-05            0.0386772\ncall_function  flatten                          6.93798e-05            0.036074\noutput         output                           2.86102e-05            0.0148759\nplaceholder    x                                2.5034e-05             0.0130164",
            "code"
        ],
        [
            "There are two things we should call out here:",
            "markdown"
        ],
        [
            "MaxPool2d takes up the most time. This is a known issue:",
            "markdown"
        ],
        [
            "BatchNorm2d also takes up significant time. We can continue this\nline of thinking and optimize this in the Conv-BN Fusion with FX\n.",
            "markdown"
        ]
    ],
    "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Conclusion": [
        [
            "As we can see, using FX we can easily capture PyTorch programs (even\nones we don\u2019t have the source code for!) in a machine-interpretable\nformat and use that for analysis, such as the performance analysis\nwe\u2019ve done here. FX opens up an exiciting world of possibilities for\nworking with PyTorch programs.",
            "markdown"
        ],
        [
            "Finally, since FX is still in beta, we would be happy to hear any\nfeedback you have about using it. Please feel free to use the\nPyTorch Forums () and the issue tracker\n() to provide any feedback\nyou might have.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.687 seconds)",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->(beta) Channels Last Memory Format in PyTorch": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->(beta) Channels Last Memory Format in PyTorch->What is Channels Last": [
        [
            "Channels last memory format is an alternative way of ordering NCHW tensors in memory preserving dimensions ordering. Channels last tensors ordered in such a way that channels become the densest dimension (aka storing images pixel-per-pixel).",
            "markdown"
        ],
        [
            "For example, classic (contiguous) storage of NCHW tensor (in our case it is two 4x4 images with 3 color channels) look like this:\n\n<img alt=\"classic_memory_format\" src=\"../_images/classic_memory_format.png\"/>",
            "markdown"
        ],
        [
            "Channels last memory format orders data differently:\n\n<img alt=\"channels_last_memory_format\" src=\"../_images/channels_last_memory_format.png\"/>",
            "markdown"
        ],
        [
            "Pytorch supports memory formats (and provides back compatibility with existing models including eager, JIT, and TorchScript) by utilizing  existing strides structure.\nFor example, 10x3x16x16 batch in Channels last format will have strides equal to (768, 1, 48, 3).",
            "markdown"
        ],
        [
            "Channels last memory format is implemented for 4D NCHW Tensors only.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->(beta) Channels Last Memory Format in PyTorch->Memory Format API": [
        [
            "Here is how to convert tensors between contiguous and channels\nlast memory formats.",
            "markdown"
        ],
        [
            "Classic PyTorch contiguous tensor",
            "markdown"
        ],
        [
            "import torch\n\nN, C, H, W = 10, 3, 32, 32\n = (N, C, H, W)\nprint(.stride())  # Ouputs: (3072, 1024, 32, 1)",
            "code"
        ],
        [
            "(3072, 1024, 32, 1)",
            "code"
        ],
        [
            "Conversion operator",
            "markdown"
        ],
        [
            " = .to(memory_format=)\nprint(.shape)  # Outputs: (10, 3, 32, 32) as dimensions order preserved\nprint(.stride())  # Outputs: (3072, 1, 96, 3)",
            "code"
        ],
        [
            "torch.Size([10, 3, 32, 32])\n(3072, 1, 96, 3)",
            "code"
        ],
        [
            "Back to contiguous",
            "markdown"
        ],
        [
            " = .to(memory_format=)\nprint(.stride())  # Outputs: (3072, 1024, 32, 1)",
            "code"
        ],
        [
            "(3072, 1024, 32, 1)",
            "code"
        ],
        [
            "Alternative option",
            "markdown"
        ],
        [
            " = .contiguous(memory_format=)\nprint(.stride())  # Ouputs: (3072, 1, 96, 3)",
            "code"
        ],
        [
            "(3072, 1, 96, 3)",
            "code"
        ],
        [
            "Format checks",
            "markdown"
        ],
        [
            "print(.is_contiguous(memory_format=))  # Ouputs: True",
            "code"
        ],
        [
            "True",
            "code"
        ],
        [
            "There are minor difference between the two APIs to and\ncontiguous. We suggest to stick with to when explicitly\nconverting memory format of tensor.",
            "markdown"
        ],
        [
            "For general cases the two APIs behave the same. However in special\ncases for a 4D tensor with size NCHW when either: C==1 or\nH==1 &amp;&amp; W==1, only to would generate a proper stride to\nrepresent channels last memory format.",
            "markdown"
        ],
        [
            "This is because in either of the two cases above, the memory format\nof a tensor is ambiguous, i.e. a contiguous tensor with size\nN1HW is both contiguous and channels last in memory storage.\nTherefore, they are already considered as is_contiguous\nfor the given memory format and hence contiguous call becomes a\nno-op and would not update the stride. On the contrary, to\nwould restride tensor with a meaningful stride on dimensions whose\nsizes are 1 in order to properly represent the intended memory\nformat",
            "markdown"
        ],
        [
            " = (4, 1, 4, 4)\nprint(.is_contiguous(memory_format=))  # Ouputs: True\nprint(.is_contiguous(memory_format=))  # Ouputs: True",
            "code"
        ],
        [
            "True\nTrue",
            "code"
        ],
        [
            "Same thing applies to explicit permutation API permute. In\nspecial case where ambiguity could occur, permute does not\nguarantee to produce a stride that properly carry the intended\nmemory format. We suggest to use to with explicit memory format\nto avoid unintended behavior.",
            "markdown"
        ],
        [
            "And a side note that in the extreme case, where three non-batch\ndimensions are all equal to 1 (C==1 &amp;&amp; H==1 &amp;&amp; W==1),\ncurrent implementation cannot mark a tensor as channels last memory\nformat.",
            "markdown"
        ],
        [
            "Create as channels last",
            "markdown"
        ],
        [
            " = (N, C, H, W, memory_format=)\nprint(.stride())  # Ouputs: (3072, 1, 96, 3)",
            "code"
        ],
        [
            "(3072, 1, 96, 3)",
            "code"
        ],
        [
            "clone preserves memory format",
            "markdown"
        ],
        [
            " = .clone()\nprint(.stride())  # Ouputs: (3072, 1, 96, 3)",
            "code"
        ],
        [
            "(3072, 1, 96, 3)",
            "code"
        ],
        [
            "to, cuda, float \u2026 preserves memory format",
            "markdown"
        ],
        [
            "if ():\n     = .cuda()\n    print(.stride())  # Ouputs: (3072, 1, 96, 3)",
            "code"
        ],
        [
            "(3072, 1, 96, 3)",
            "code"
        ],
        [
            "empty_like, *_like operators preserves memory format",
            "markdown"
        ],
        [
            " = ()\nprint(.stride())  # Ouputs: (3072, 1, 96, 3)",
            "code"
        ],
        [
            "(3072, 1, 96, 3)",
            "code"
        ],
        [
            "Pointwise operators preserves memory format",
            "markdown"
        ],
        [
            " =  + \nprint(.stride())  # Ouputs: (3072, 1, 96, 3)",
            "code"
        ],
        [
            "(3072, 1, 96, 3)",
            "code"
        ],
        [
            "Conv, Batchnorm modules using cudnn backends support channels last\n(only works for CudNN &gt;= 7.6). Convolution modules, unlike binary\np-wise operator, have channels last as the dominating memory format.\nIFF all inputs are in contiguous memory format, the operator\nproduces output in contiguous memory format. Otherwise, output wil\nbe in channels last memroy format.",
            "markdown"
        ],
        [
            "if () &gt;= 7603:\n     = (8, 4, 3).cuda().half()\n     = (memory_format=)  # Module parameters need to be channels last\n\n    input = (1, 10, (2, 8, 4, 4), dtype=, requires_grad=True)\n    input = input.to(device=\"cuda\", memory_format=, dtype=)\n\n     = (input)\n    print(.is_contiguous(memory_format=))  # Ouputs: True",
            "code"
        ],
        [
            "True",
            "code"
        ],
        [
            "When input tensor reaches a operator without channels last support,\na permutation should automatically apply in the kernel to restore\ncontiguous on input tensor. This introduces overhead and stops the\nchannels last memory format propagation. Nevertheless, it guarantees\ncorrect output.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->(beta) Channels Last Memory Format in PyTorch->Performance Gains": [
        [
            "Channels last memory format optimizations are available on both GPU and CPU.\nOn GPU, the most significant performance gains are observed on NVidia\u2019s\nhardware with Tensor Cores support running on reduced precision\n(torch.float16).\nWe were able to archive over 22% perf gains with channels last\ncomparing to contiguous format, both while utilizing\n\u2018AMP (Automated Mixed Precision)\u2019 training scripts.\nOur scripts uses AMP supplied by NVidia\n.",
            "markdown"
        ],
        [
            "python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2\u00a0 ./data",
            "markdown"
        ],
        [
            "# opt_level = O2\n# keep_batchnorm_fp32 = None &lt;class 'NoneType'&gt;\n# loss_scale = None &lt;class 'NoneType'&gt;\n# CUDNN VERSION: 7603\n# =&gt; creating model 'resnet50'\n# Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n# Defaults for this optimization level are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n# Processing user overrides (additional kwargs that are not None)...\n# After processing overrides, optimization options are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n# Epoch: [0][10/125] Time 0.866 (0.866) Speed 230.949 (230.949) Loss 0.6735125184 (0.6735) Prec@1 61.000 (61.000) Prec@5 100.000 (100.000)\n# Epoch: [0][20/125] Time 0.259 (0.562) Speed 773.481 (355.693) Loss 0.6968704462 (0.6852) Prec@1 55.000 (58.000) Prec@5 100.000 (100.000)\n# Epoch: [0][30/125] Time 0.258 (0.461) Speed 775.089 (433.965) Loss 0.7877287269 (0.7194) Prec@1 51.500 (55.833) Prec@5 100.000 (100.000)\n# Epoch: [0][40/125] Time 0.259 (0.410) Speed 771.710 (487.281) Loss 0.8285319805 (0.7467) Prec@1 48.500 (54.000) Prec@5 100.000 (100.000)\n# Epoch: [0][50/125] Time 0.260 (0.380) Speed 770.090 (525.908) Loss 0.7370464802 (0.7447) Prec@1 56.500 (54.500) Prec@5 100.000 (100.000)\n# Epoch: [0][60/125] Time 0.258 (0.360) Speed 775.623 (555.728) Loss 0.7592862844 (0.7472) Prec@1 51.000 (53.917) Prec@5 100.000 (100.000)\n# Epoch: [0][70/125] Time 0.258 (0.345) Speed 774.746 (579.115) Loss 1.9698858261 (0.9218) Prec@1 49.500 (53.286) Prec@5 100.000 (100.000)\n# Epoch: [0][80/125] Time 0.260 (0.335) Speed 770.324 (597.659) Loss 2.2505953312 (1.0879) Prec@1 50.500 (52.938) Prec@5 100.000 (100.000)",
            "code"
        ],
        [
            "Passing --channels-last true allows running a model in Channels last format with observed 22% perf gain.",
            "markdown"
        ],
        [
            "python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2 --channels-last true ./data",
            "markdown"
        ],
        [
            "# opt_level = O2\n# keep_batchnorm_fp32 = None &lt;class 'NoneType'&gt;\n# loss_scale = None &lt;class 'NoneType'&gt;\n#\n# CUDNN VERSION: 7603\n#\n# =&gt; creating model 'resnet50'\n# Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n#\n# Defaults for this optimization level are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n# Processing user overrides (additional kwargs that are not None)...\n# After processing overrides, optimization options are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n#\n# Epoch: [0][10/125] Time 0.767 (0.767) Speed 260.785 (260.785) Loss 0.7579724789 (0.7580) Prec@1 53.500 (53.500) Prec@5 100.000 (100.000)\n# Epoch: [0][20/125] Time 0.198 (0.482) Speed 1012.135 (414.716) Loss 0.7007197738 (0.7293) Prec@1 49.000 (51.250) Prec@5 100.000 (100.000)\n# Epoch: [0][30/125] Time 0.198 (0.387) Speed 1010.977 (516.198) Loss 0.7113101482 (0.7233) Prec@1 55.500 (52.667) Prec@5 100.000 (100.000)\n# Epoch: [0][40/125] Time 0.197 (0.340) Speed 1013.023 (588.333) Loss 0.8943189979 (0.7661) Prec@1 54.000 (53.000) Prec@5 100.000 (100.000)\n# Epoch: [0][50/125] Time 0.198 (0.312) Speed 1010.541 (641.977) Loss 1.7113249302 (0.9551) Prec@1 51.000 (52.600) Prec@5 100.000 (100.000)\n# Epoch: [0][60/125] Time 0.198 (0.293) Speed 1011.163 (683.574) Loss 5.8537774086 (1.7716) Prec@1 50.500 (52.250) Prec@5 100.000 (100.000)\n# Epoch: [0][70/125] Time 0.198 (0.279) Speed 1011.453 (716.767) Loss 5.7595844269 (2.3413) Prec@1 46.500 (51.429) Prec@5 100.000 (100.000)\n# Epoch: [0][80/125] Time 0.198 (0.269) Speed 1011.827 (743.883) Loss 2.8196096420 (2.4011) Prec@1 47.500 (50.938) Prec@5 100.000 (100.000)",
            "code"
        ],
        [
            "The following list of models has the full support of Channels last and showing 8%-35% perf gains on Volta devices:\nalexnet, mnasnet0_5, mnasnet0_75, mnasnet1_0, mnasnet1_3, mobilenet_v2, resnet101, resnet152, resnet18, resnet34, resnet50, resnext50_32x4d, shufflenet_v2_x0_5, shufflenet_v2_x1_0, shufflenet_v2_x1_5, shufflenet_v2_x2_0, squeezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, vgg19_bn, wide_resnet101_2, wide_resnet50_2",
            "markdown"
        ],
        [
            "The following list of models has the full support of Channels last and showing 26%-76% perf gains on Intel(R) Xeon(R) Ice Lake (or newer) CPUs:\nalexnet, densenet121, densenet161, densenet169, googlenet, inception_v3, mnasnet0_5, mnasnet1_0, resnet101, resnet152, resnet18, resnet34, resnet50, resnext101_32x8d, resnext50_32x4d, shufflenet_v2_x0_5, shufflenet_v2_x1_0, squeezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, vgg19_bn, wide_resnet101_2, wide_resnet50_2",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->(beta) Channels Last Memory Format in PyTorch->Converting existing models": [
        [
            "Channels last support is not limited by existing models, as any\nmodel can be converted to channels last and propagate format through\nthe graph as soon as input (or certain weight) is formatted\ncorrectly.",
            "markdown"
        ],
        [
            "# Need to be done once, after model initialization (or load)\n = (memory_format=)  # Replace with your model\n\n# Need to be done for every input\ninput = input.to(memory_format=)  # Replace with your input\n = (input)",
            "code"
        ],
        [
            "However, not all operators fully converted to support channels last\n(usually returning contiguous output instead). In the example posted\nabove, layers that does not support channels last will stop the\nmemory format propagation. In spite of that, as we have converted the\nmodel to channels last format, that means each convolution layer,\nwhich has its 4 dimensional weight in channels last memory format,\nwill restore channels last memory format and benefit from faster\nkernels.",
            "markdown"
        ],
        [
            "But operators that does not support channels last does introduce\noverhead by permutation. Optionally, you can investigate and identify\noperators in your model that does not support channels last, if you\nwant to improve the performance of converted model.",
            "markdown"
        ],
        [
            "That means you need to verify the list of used operators\nagainst supported operators list ,\nor introduce memory format checks into eager execution mode and run your model.",
            "markdown"
        ],
        [
            "After running the code below, operators will raise an exception if the output of the\noperator doesn\u2019t match the memory format of the input.",
            "markdown"
        ],
        [
            "def contains_cl(args):\n    for t in args:\n        if isinstance(t, ):\n            if t.is_contiguous(memory_format=) and not t.is_contiguous():\n                return True\n        elif isinstance(t, list) or isinstance(t, tuple):\n            if contains_cl(list(t)):\n                return True\n    return False\n\n\ndef print_inputs(args, indent=\"\"):\n    for t in args:\n        if isinstance(t, ):\n            print(indent, t.stride(), t.shape, t.device, t.dtype)\n        elif isinstance(t, list) or isinstance(t, tuple):\n            print(indent, type(t))\n            print_inputs(list(t), indent=indent + \"    \")\n        else:\n            print(indent, t)\n\n\ndef check_wrapper(fn):\n    name = fn.__name__\n\n    def check_cl(*args, **kwargs):\n        was_cl = contains_cl(args)\n        try:\n            result = fn(*args, **kwargs)\n        except Exception as e:\n            print(\"`{}` inputs are:\".format(name))\n            print_inputs(args)\n            print(\"-------------------\")\n            raise e\n        failed = False\n        if was_cl:\n            if isinstance(result, ):\n                if result.dim() == 4 and not result.is_contiguous(memory_format=):\n                    print(\n                        \"`{}` got channels_last input, but output is not channels_last:\".format(name),\n                        result.shape,\n                        result.stride(),\n                        result.device,\n                        result.dtype,\n                    )\n                    failed = True\n        if failed and True:\n            print(\"`{}` inputs are:\".format(name))\n            print_inputs(args)\n            raise Exception(\"Operator `{}` lost channels_last property\".format(name))\n        return result\n\n    return check_cl\n\n\nold_attrs = dict()\n\n\ndef attribute(m):\n    old_attrs[m] = dict()\n    for i in dir(m):\n        e = getattr(m, i)\n        exclude_functions = [\"is_cuda\", \"has_names\", \"numel\", \"stride\", \"Tensor\", \"is_contiguous\", \"__class__\"]\n        if i not in exclude_functions and not i.startswith(\"_\") and \"__call__\" in dir(e):\n            try:\n                old_attrs[m][i] = e\n                setattr(m, i, check_wrapper(e))\n            except Exception as e:\n                print(i)\n                print(e)\n\n\nattribute()\nattribute(torch.nn.functional)\nattribute(torch)",
            "code"
        ],
        [
            "If you found an operator that doesn\u2019t support channels last tensors\nand you want to contribute, feel free to use following developers\nguide .",
            "markdown"
        ],
        [
            "Code below is to recover the attributes of torch.",
            "markdown"
        ],
        [
            "for (m, attrs) in old_attrs.items():\n    for (k, v) in attrs.items():\n        setattr(m, k, v)",
            "code"
        ]
    ],
    "torch->Frontend APIs->(beta) Channels Last Memory Format in PyTorch->Work to do": [
        [
            "There are still many things to do, such as:",
            "markdown"
        ],
        [
            "Resolving ambiguity of N1HW and NC11 Tensors;",
            "markdown"
        ],
        [
            "Testing of Distributed Training support;",
            "markdown"
        ],
        [
            "Improving operators coverage.",
            "markdown"
        ],
        [
            "If you have feedback and/or suggestions for improvement, please let us\nknow by creating .",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  1.886 seconds)",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Forward-mode Automatic Differentiation (Beta)": [
        [
            "This tutorial demonstrates how to use forward-mode AD to compute\ndirectional derivatives (or equivalently, Jacobian-vector products).",
            "markdown"
        ],
        [
            "The tutorial below uses some APIs only available in versions &gt;= 1.11\n(or nightly builds).",
            "markdown"
        ],
        [
            "Also note that forward-mode AD is currently in beta. The API is\nsubject to change and operator coverage is still incomplete.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Forward-mode Automatic Differentiation (Beta)->Basic Usage": [
        [
            "Unlike reverse-mode AD, forward-mode AD computes gradients eagerly\nalongside the forward pass. We can use forward-mode AD to compute a\ndirectional derivative by performing the forward pass as before,\nexcept we first associate our input with another tensor representing\nthe direction of the directional derivative (or equivalently, the v\nin a Jacobian-vector product). When an input, which we call \u201cprimal\u201d, is\nassociated with a \u201cdirection\u201d tensor, which we call \u201ctangent\u201d, the\nresultant new tensor object is called a \u201cdual tensor\u201d for its connection\nto dual numbers[0].",
            "markdown"
        ],
        [
            "As the forward pass is performed, if any input tensors are dual tensors,\nextra computation is performed to propogate this \u201csensitivity\u201d of the\nfunction.",
            "markdown"
        ],
        [
            "import torch\nimport torch.autograd.forward_ad as fwAD\n\n = (10, 10)\n = (10, 10)\n\ndef fn(x, ):\n    return x ** 2 +  ** 2\n\n# All forward AD computation must be performed in the context of\n# a ``dual_level`` context. All dual tensors created in such a context\n# will have their tangents destroyed upon exit. This is to ensure that\n# if the output or intermediate results of this computation are reused\n# in a future forward AD computation, their tangents (which are associated\n# with this computation) won't be confused with tangents from the later\n# computation.\nwith ():\n    # To create a dual tensor we associate a tensor, which we call the\n    # primal with another tensor of the same size, which we call the tangent.\n    # If the layout of the tangent is different from that of the primal,\n    # The values of the tangent are copied into a new tensor with the same\n    # metadata as the primal. Otherwise, the tangent itself is used as-is.\n    #\n    # It is also important to note that the dual tensor created by\n    # ``make_dual`` is a view of the primal.\n     = (, )\n    assert (). is \n\n    # To demonstrate the case where the copy of the tangent happens,\n    # we pass in a tangent with a layout different from that of the primal\n     = (, )\n    assert (). is not \n\n    # Tensors that do not have an associated tangent are automatically\n    # considered to have a zero-filled tangent of the same shape.\n     = (10, 10)\n     = fn(, )\n\n    # Unpacking the dual returns a namedtuple with ``primal`` and ``tangent``\n    # as attributes\n     = ().\n\nassert (). is None",
            "code"
        ]
    ],
    "torch->Frontend APIs->Forward-mode Automatic Differentiation (Beta)->Usage with Modules": [
        [
            "To use nn.Module with forward AD, replace the parameters of your\nmodel with dual tensors before performing the forward pass. At the\ntime of writing, it is not possible to create dual tensor\nnn.Parameter`s. As a workaround, one must register the dual tensor\nas a non-parameter attribute of the module.",
            "markdown"
        ],
        [
            "import torch.nn as nn\n\n = (5, 5)\ninput = (16, 5)\n\nparams = {name:  for name,  in ()}\ntangents = {name: () for name,  in params.items()}\n\nwith ():\n    for name,  in params.items():\n        delattr(, name)\n        setattr(, name, (, tangents[name]))\n\n     = (input)\n     = ().",
            "code"
        ]
    ],
    "torch->Frontend APIs->Forward-mode Automatic Differentiation (Beta)->Using the functional Module API (beta)": [
        [
            "Another way to use nn.Module with forward AD is to utilize\nthe functional Module API (also known as the stateless Module API).",
            "markdown"
        ],
        [
            "from torch.func import \n\n# We need a fresh module because the functional call requires the\n# the model to have parameters registered.\n = (5, 5)\n\ndual_params = {}\nwith ():\n    for name,  in params.items():\n        # Using the same ``tangents`` from the above section\n        dual_params[name] = (, tangents[name])\n     = (, dual_params, input)\n     = ().\n\n# Check our results\nassert (, )",
            "code"
        ]
    ],
    "torch->Frontend APIs->Forward-mode Automatic Differentiation (Beta)->Custom autograd Function": [
        [
            "Custom Functions also support forward-mode AD. To create custom Function\nsupporting forward-mode AD, register the jvp() static method. It is\npossible, but not mandatory for custom Functions to support both forward\nand backward AD. See the\n\nfor more information.",
            "markdown"
        ],
        [
            "class Fn():\n    @staticmethod\n    def forward(ctx, foo):\n        result = (foo)\n        # Tensors stored in ctx can be used in the subsequent forward grad\n        # computation.\n        ctx.result = result\n        return result\n\n    @staticmethod\n    def jvp(ctx, gI):\n        gO = gI * ctx.result\n        # If the tensor stored in ctx will not also be used in the backward pass,\n        # one can manually free it using ``del``\n        del ctx.result\n        return gO\n\nfn = Fn.apply\n\n = (10, 10, dtype=, requires_grad=True)\n = (10, 10)\n\nwith ():\n     = (, )\n     = fn()\n     = ().\n\n# It is important to use ``autograd.gradcheck`` to verify that your\n# custom autograd Function computes the gradients correctly. By default,\n# gradcheck only checks the backward-mode (reverse-mode) AD gradients. Specify\n# ``check_forward_ad=True`` to also check forward grads. If you did not\n# implement the backward formula for your function, you can also tell gradcheck\n# to skip the tests that require backward-mode AD by specifying\n# ``check_backward_ad=False``, ``check_undefined_grad=False``, and\n# ``check_batched_grad=False``.\n(Fn.apply, (,), check_forward_ad=True,\n                         check_backward_ad=False, check_undefined_grad=False,\n                         check_batched_grad=False)",
            "code"
        ],
        [
            "True",
            "code"
        ]
    ],
    "torch->Frontend APIs->Forward-mode Automatic Differentiation (Beta)->Functional API (beta)": [
        [
            "We also offer a higher-level functional API in functorch\nfor computing Jacobian-vector products that you may find simpler to use\ndepending on your use case.",
            "markdown"
        ],
        [
            "The benefit of the functional API is that there isn\u2019t a need to understand\nor use the lower-level dual tensor API and that you can compose it with\nother ;\nthe downside is that it offers you less control.",
            "markdown"
        ],
        [
            "Note that the remainder of this tutorial will require functorch\n() to run. Please find installation\ninstructions at the specified link.",
            "markdown"
        ],
        [
            "import functorch as ft\n\n = (10, 10)\n = (10, 10)\n = (10, 10)\n = (10, 10)\n\ndef fn(x, ):\n    return x ** 2 +  ** 2\n\n# Here is a basic example to compute the JVP of the above function.\n# The jvp(func, primals, tangents) returns func(*primals) as well as the\n# computed jvp. Each primal must be associated with a tangent of the same shape.\n,  = ft.(fn, (, ), (, ))\n\n# functorch.jvp requires every primal to be associated with a tangent.\n# If we only want to associate certain inputs to `fn` with tangents,\n# then we'll need to create a new function that captures inputs without tangents:\n = (10, 10)\n = (10, 10)\n = (10, 10)\n\nimport functools\nnew_fn = functools.partial(fn, =)\n,  = ft.(new_fn, (,), (,))",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torch/_functorch/deprecated.py:74: UserWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, functorch.jvp is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch &gt;= 2.3. Please use torch.func.jvp instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html",
            "code"
        ]
    ],
    "torch->Frontend APIs->Forward-mode Automatic Differentiation (Beta)->Using the functional API with Modules": [
        [
            "To use nn.Module with functorch.jvp to compute Jacobian-vector products\nwith respect to the model parameters, we need to reformulate the\nnn.Module as a function that accepts both the model parameters and inputs\nto the module.",
            "markdown"
        ],
        [
            " = (5, 5)\ninput = (16, 5)\ntangents = tuple([() for  in ()])\n\n# Given a torch.nn.Module, ft.make_functional_with_buffers extracts the state\n# (params and buffers) and returns a functional version of the model that\n# can be invoked like a function.\n# That is, the returned ``func`` can be invoked like\n# ``func(params, buffers, input)``.\n# ft.make_functional_with_buffers is analogous to the nn.Modules stateless API\n# that you saw previously and we're working on consolidating the two.\nfunc, params, buffers = ft.make_functional_with_buffers()\n\n# Because jvp requires every input to be associated with a tangent, we need to\n# create a new function that, when given the parameters, produces the output\ndef func_params_only(params):\n    return func(params, buffers, input)\n\n,  = ft.(func_params_only, (params,), (tangents,))",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torch/_functorch/deprecated.py:101: UserWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, functorch.make_functional_with_buffers is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch &gt;= 2.3. Please use torch.func.functional_call instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n\n/opt/conda/lib/python3.10/site-packages/torch/_functorch/deprecated.py:74: UserWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, functorch.jvp is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch &gt;= 2.3. Please use torch.func.jvp instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html",
            "code"
        ],
        [
            "[0] ",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.161 seconds)",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms": [
        [
            "Computing jacobians or hessians are useful in a number of non-traditional\ndeep learning models. It is difficult (or annoying) to compute these quantities\nefficiently using PyTorch\u2019s regular autodiff APIs\n(Tensor.backward(), torch.autograd.grad). PyTorch\u2019s\n\n\nprovides ways of computing various higher-order autodiff quantities\nefficiently.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This tutorial requires PyTorch 2.0.0 or later.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms->Computing the Jacobian": [
        [
            "import torch\nimport torch.nn.functional as F\nfrom functools import partial\n = (0)",
            "code"
        ],
        [
            "Let\u2019s start with a function that we\u2019d like to compute the jacobian of.\nThis is a simple linear function with non-linear activation.",
            "markdown"
        ],
        [
            "def predict(, , ):\n    return (, , ).tanh()",
            "code"
        ],
        [
            "Let\u2019s add some dummy data: a weight, a bias, and a feature vector x.",
            "markdown"
        ],
        [
            "D = 16\n = (D, D)\n = (D)\n = (D)  # feature vector",
            "code"
        ],
        [
            "Let\u2019s think of predict as a function that maps the input x from \\(R^D \\to R^D\\).\nPyTorch Autograd computes vector-Jacobian products. In order to compute the full\nJacobian of this \\(R^D \\to R^D\\) function, we would have to compute it row-by-row\nby using a different unit vector each time.",
            "markdown"
        ],
        [
            "def compute_jac():\n    jacobian_rows = [(predict(, , ), , vec)[0]\n                     for vec in ]\n    return (jacobian_rows)\n\n = .clone().requires_grad_()\n = (D)\n\n = compute_jac()\n\nprint(.shape)\nprint([0])  # show first row",
            "code"
        ],
        [
            "torch.Size([16, 16])\ntensor([-0.5956, -0.6096, -0.1326, -0.2295,  0.4490,  0.3661, -0.1672, -1.1190,\n         0.1705, -0.6683,  0.1851,  0.1630,  0.0634,  0.6547,  0.5908, -0.1308])",
            "code"
        ],
        [
            "Instead of computing the jacobian row-by-row, we can use PyTorch\u2019s\ntorch.vmap function transform to get rid of the for-loop and vectorize the\ncomputation. We can\u2019t directly apply vmap to torch.autograd.grad;\ninstead, PyTorch provides a torch.func.vjp transform that composes with\ntorch.vmap:",
            "markdown"
        ],
        [
            "from torch.func import , \n\n, vjp_fn = (partial(predict, , ), )\n\n, = (vjp_fn)()\n\n# let's confirm both methods compute the same result\nassert (, )",
            "code"
        ],
        [
            "In a later tutorial a composition of reverse-mode AD and vmap will give us\nper-sample-gradients.\nIn this tutorial, composing reverse-mode AD and vmap gives us Jacobian\ncomputation!\nVarious compositions of vmap and autodiff transforms can give us different\ninteresting quantities.",
            "markdown"
        ],
        [
            "PyTorch provides torch.func.jacrev as a convenience function that performs\nthe vmap-vjp composition to compute jacobians. jacrev accepts an argnums\nargument that says which argument we would like to compute Jacobians with\nrespect to.",
            "markdown"
        ],
        [
            "from torch.func import \n\n = (predict, argnums=2)(, , )\n\n# confirm\nassert (, )",
            "code"
        ],
        [
            "Let\u2019s compare the performance of the two ways to compute the jacobian.\nThe function transform version is much faster (and becomes even faster the\nmore outputs there are).",
            "markdown"
        ],
        [
            "In general, we expect that vectorization via vmap can help eliminate overhead\nand give better utilization of your hardware.",
            "markdown"
        ],
        [
            "vmap does this magic by pushing the outer loop down into the function\u2019s\nprimitive operations in order to obtain better performance.",
            "markdown"
        ],
        [
            "Let\u2019s make a quick function to evaluate performance and deal with\nmicroseconds and milliseconds measurements:",
            "markdown"
        ],
        [
            "def get_perf(first, first_descriptor, second, second_descriptor):\n    \"\"\"takes torch.benchmark objects and compares delta of second vs first.\"\"\"\n    faster = second.times[0]\n    slower = first.times[0]\n    gain = (slower-faster)/slower\n    if gain &lt; 0: gain *=-1\n    final_gain = gain*100\n    print(f\" Performance delta: {final_gain:.4f} percent improvement with {second_descriptor} \")",
            "code"
        ],
        [
            "And then run the performance comparison:",
            "markdown"
        ],
        [
            "from torch.utils.benchmark import \n\n = (stmt=\"compute_jac(xp)\", globals=globals())\n = (stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n\n = (500)\n = (500)\n\nprint()\nprint()",
            "code"
        ],
        [
            "&lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f0438039000&gt;\ncompute_jac(xp)\n  1.35 ms\n  1 measurement, 500 runs , 1 thread\n&lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f04380c7e20&gt;\njacrev(predict, argnums=2)(weight, bias, x)\n  625.00 us\n  1 measurement, 500 runs , 1 thread",
            "code"
        ],
        [
            "Let\u2019s do a relative performance comparison of the above with our get_perf function:",
            "markdown"
        ],
        [
            "get_perf(, \"without vmap\",  , \"vmap\")",
            "code"
        ],
        [
            "Performance delta: 53.6071 percent improvement with vmap",
            "code"
        ],
        [
            "Furthemore, it\u2019s pretty easy to flip the problem around and say we want to\ncompute Jacobians of the parameters to our model (weight, bias) instead of the input",
            "markdown"
        ],
        [
            "# note the change in input via argnums params of 0,1 to map to weight and bias\n,  = (predict, argnums=(0, 1))(, , )",
            "code"
        ]
    ],
    "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms->reverse-mode Jacobian (jacrev) vs forward-mode Jacobian (jacfwd)": [
        [
            "We offer two APIs to compute jacobians: jacrev and jacfwd:",
            "markdown"
        ],
        [
            "jacrev uses reverse-mode AD. As you saw above it is a composition of our\nvjp and vmap transforms.",
            "markdown"
        ],
        [
            "jacfwd uses forward-mode AD. It is implemented as a composition of our\njvp and vmap transforms.",
            "markdown"
        ],
        [
            "jacfwd and jacrev can be substituted for each other but they have different\nperformance characteristics.",
            "markdown"
        ],
        [
            "As a general rule of thumb, if you\u2019re computing the jacobian of an \\(R^N \\to R^M\\)\nfunction, and there are many more outputs than inputs (i.e. \\(M &gt; N\\)) then\njacfwd is preferred, otherwise use jacrev. There are exceptions to this rule,\nbut a non-rigorous argument for this follows:",
            "markdown"
        ],
        [
            "In reverse-mode AD, we are computing the jacobian row-by-row, while in\nforward-mode AD (which computes Jacobian-vector products), we are computing\nit column-by-column. The Jacobian matrix has M rows and N columns, so if it\nis taller or wider one way we may prefer the method that deals with fewer\nrows or columns.",
            "markdown"
        ],
        [
            "from torch.func import , ",
            "code"
        ],
        [
            "First, let\u2019s benchmark with more inputs than outputs:",
            "markdown"
        ],
        [
            "Din = 32\nDout = 2048\n = (Dout, Din)\n\n = (Dout)\n = (Din)\n\n# remember the general rule about taller vs wider... here we have a taller matrix:\nprint(.shape)\n\n = (stmt=\"jacfwd(predict, argnums=2)(weight, bias, x)\", globals=globals())\n = (stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n\n = (500)\n = (500)\n\nprint(f'jacfwd time: {}')\nprint(f'jacrev time: {}')",
            "code"
        ],
        [
            "torch.Size([2048, 32])\njacfwd time: &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f04380d2ad0&gt;\njacfwd(predict, argnums=2)(weight, bias, x)\n  1.14 ms\n  1 measurement, 500 runs , 1 thread\njacrev time: &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f04380c71f0&gt;\njacrev(predict, argnums=2)(weight, bias, x)\n  9.52 ms\n  1 measurement, 500 runs , 1 thread",
            "code"
        ],
        [
            "and then do a relative benchmark:",
            "markdown"
        ],
        [
            "get_perf(, \"jacfwd\", , \"jacrev\", );",
            "code"
        ],
        [
            "Performance delta: 738.8406 percent improvement with jacrev",
            "code"
        ],
        [
            "and now the reverse - more outputs (M) than inputs (N):",
            "markdown"
        ],
        [
            "Din = 2048\nDout = 32\n = (Dout, Din)\n = (Dout)\n = (Din)\n\n = (stmt=\"jacfwd(predict, argnums=2)(weight, bias, x)\", globals=globals())\n = (stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n\n = (500)\n = (500)\n\nprint(f'jacfwd time: {}')\nprint(f'jacrev time: {}')",
            "code"
        ],
        [
            "jacfwd time: &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f043804b6d0&gt;\njacfwd(predict, argnums=2)(weight, bias, x)\n  6.24 ms\n  1 measurement, 500 runs , 1 thread\njacrev time: &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f0438069c30&gt;\njacrev(predict, argnums=2)(weight, bias, x)\n  736.54 us\n  1 measurement, 500 runs , 1 thread",
            "code"
        ],
        [
            "and a relative perf comparison:",
            "markdown"
        ],
        [
            "get_perf(, \"jacrev\", , \"jacfwd\")",
            "code"
        ],
        [
            "Performance delta: 747.2533 percent improvement with jacfwd",
            "code"
        ]
    ],
    "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms->Hessian computation with functorch.hessian": [
        [
            "We offer a convenience API to compute hessians: torch.func.hessiani.\nHessians are the jacobian of the jacobian (or the partial derivative of\nthe partial derivative, aka second order).",
            "markdown"
        ],
        [
            "This suggests that one can just compose functorch\u2019s jacobian transforms to\ncompute the Hessian.\nIndeed, under the hood, hessian(f) is simply jacfwd(jacrev(f)).",
            "markdown"
        ],
        [
            "Note: to boost performance: depending on your model, you may also want to\nuse jacfwd(jacfwd(f)) or jacrev(jacrev(f)) instead to compute hessians\nleveraging the rule of thumb above regarding wider vs taller matrices.",
            "markdown"
        ],
        [
            "from torch.func import \n\n# lets reduce the size in order not to blow out colab. Hessians require\n# significant memory:\nDin = 512\nDout = 32\n = (Dout, Din)\n = (Dout)\n = (Din)\n\n = (predict, argnums=2)(, , )\n = ((predict, argnums=2), argnums=2)(, , )\n = ((predict, argnums=2), argnums=2)(, , )",
            "code"
        ],
        [
            "Let\u2019s verify we have the same result regardless of using hessian api or\nusing jacfwd(jacfwd())",
            "markdown"
        ],
        [
            "(, )",
            "code"
        ],
        [
            "True",
            "code"
        ]
    ],
    "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms->Batch Jacobian and Batch Hessian": [
        [
            "In the above examples we\u2019ve been operating with a single feature vector.\nIn some cases you might want to take the Jacobian of a batch of outputs\nwith respect to a batch of inputs. That is, given a batch of inputs of\nshape (B, N) and a function that goes from \\(R^N \\to R^M\\), we would like\na Jacobian of shape (B, M, N).",
            "markdown"
        ],
        [
            "The easiest way to do this is to use vmap:",
            "markdown"
        ],
        [
            "batch_size = 64\nDin = 31\nDout = 33\n\n = (Dout, Din)\nprint(f\"weight shape = {.shape}\")\n\n = (Dout)\n\n = (batch_size, Din)\n\ncompute_batch_jacobian = ((predict, argnums=2), in_dims=(None, None, 0))\n = compute_batch_jacobian(, , )",
            "code"
        ],
        [
            "weight shape = torch.Size([33, 31])",
            "code"
        ],
        [
            "If you have a function that goes from (B, N) -&gt; (B, M) instead and are\ncertain that each input produces an independent output, then it\u2019s also\nsometimes possible to do this without using vmap by summing the outputs\nand then computing the Jacobian of that function:",
            "markdown"
        ],
        [
            "def predict_with_output_summed(, , ):\n    return predict(, , ).sum(0)\n\n = (predict_with_output_summed, argnums=2)(, , ).movedim(1, 0)\nassert (, )",
            "code"
        ],
        [
            "If you instead have a function that goes from \\(R^N \\to R^M\\) but inputs that\nare batched, you compose vmap with jacrev to compute batched jacobians:",
            "markdown"
        ],
        [
            "Finally, batch hessians can be computed similarly. It\u2019s easiest to think\nabout them by using vmap to batch over hessian computation, but in some\ncases the sum trick also works.",
            "markdown"
        ],
        [
            "compute_batch_hessian = ((predict, argnums=2), in_dims=(None, None, 0))\n\n = compute_batch_hessian(, , )\n.shape",
            "code"
        ],
        [
            "torch.Size([64, 33, 31, 31])",
            "code"
        ]
    ],
    "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms->Computing Hessian-vector products": [
        [
            "The naive way to compute a Hessian-vector product (hvp) is to materialize\nthe full Hessian and perform a dot-product with a vector. We can do better:\nit turns out we don\u2019t need to materialize the full Hessian to do this. We\u2019ll\ngo through two (of many) different strategies to compute Hessian-vector products:\n- composing reverse-mode AD with reverse-mode AD\n- composing reverse-mode AD with forward-mode AD",
            "markdown"
        ],
        [
            "Composing reverse-mode AD with forward-mode AD (as opposed to reverse-mode\nwith reverse-mode) is generally the more memory efficient way to compute a\nhvp because forward-mode AD doesn\u2019t need to construct an Autograd graph and\nsave intermediates for backward:",
            "markdown"
        ],
        [
            "from torch.func import , , \n\ndef hvp(f, primals, tangents):\n  return ((f), primals, tangents)[1]",
            "code"
        ],
        [
            "Here\u2019s some sample usage.",
            "markdown"
        ],
        [
            "def f():\n  return .sin().sum()\n\n = (2048)\n = (2048)\n\n = hvp(f, (,), (,))",
            "code"
        ],
        [
            "If PyTorch forward-AD does not have coverage for your operations, then we can\ninstead compose reverse-mode AD with reverse-mode AD:",
            "markdown"
        ],
        [
            "def hvp_revrev(f, primals, tangents):\n  , vjp_fn = ((f), *primals)\n  return vjp_fn(*tangents)\n\nresult_hvp_revrev = hvp_revrev(f, (,), (,))\nassert (, result_hvp_revrev[0])",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  11.558 seconds)",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Model ensembling": [
        [
            "This tutorial illustrates how to vectorize model ensembling using torch.vmap.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Model ensembling->What is model ensembling?": [
        [
            "Model ensembling combines the predictions from multiple models together.\nTraditionally this is done by running each model on some inputs separately\nand then combining the predictions. However, if you\u2019re running models with\nthe same architecture, then it may be possible to combine them together\nusing torch.vmap. vmap is a function transform that maps functions across\ndimensions of the input tensors. One of its use cases is eliminating\nfor-loops and speeding them up through vectorization.",
            "markdown"
        ],
        [
            "Let\u2019s demonstrate how to do this using an ensemble of simple MLPs.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This tutorial requires PyTorch 2.0.0 or later.",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n(0)\n\n# Here's a simple MLP\nclass SimpleMLP():\n    def __init__(self):\n        super(, self).__init__()\n        self.fc1 = (784, 128)\n        self.fc2 = (128, 128)\n        self.fc3 = (128, 10)\n\n    def forward(self, x):\n        x = x.flatten(1)\n        x = self.fc1(x)\n        x = (x)\n        x = self.fc2(x)\n        x = (x)\n        x = self.fc3(x)\n        return x",
            "code"
        ],
        [
            "Let\u2019s generate a batch of dummy data and pretend that we\u2019re working with\nan MNIST dataset. Thus, the dummy images are 28 by 28, and we have a\nminibatch of size 64. Furthermore, lets say we want to combine the predictions\nfrom 10 different models.",
            "markdown"
        ],
        [
            "device = 'cuda'\nnum_models = 10\n\n = (100, 64, 1, 28, 28, device=device)\n = (10, (6400,), device=device)\n\nmodels = [().to(device) for _ in range(num_models)]",
            "code"
        ],
        [
            "We have a couple of options for generating predictions. Maybe we want to\ngive each model a different randomized minibatch of data. Alternatively,\nmaybe we want to run the same minibatch of data through each model (e.g.\nif we were testing the effect of different model initializations).",
            "markdown"
        ],
        [
            "Option 1: different minibatch for each model",
            "markdown"
        ],
        [
            " = [:num_models]\npredictions_diff_minibatch_loop = [model() for model,  in zip(models, )]",
            "code"
        ],
        [
            "Option 2: Same minibatch",
            "markdown"
        ],
        [
            " = [0]\npredictions2 = [model() for model in models]",
            "code"
        ]
    ],
    "torch->Frontend APIs->Model ensembling->Using vmap to vectorize the ensemble": [
        [
            "Let\u2019s use vmap to speed up the for-loop. We must first prepare the models\nfor use with vmap.",
            "markdown"
        ],
        [
            "First, let\u2019s combine the states of the model together by stacking each\nparameter. For example, model[i].fc1.weight has shape [784, 128]; we are\ngoing to stack the .fc1.weight of each of the 10 models to produce a big\nweight of shape [10, 784, 128].",
            "markdown"
        ],
        [
            "PyTorch offers the torch.func.stack_module_state convenience function to do\nthis.",
            "markdown"
        ],
        [
            "from torch.func import \n\nparams, buffers = (models)",
            "code"
        ],
        [
            "Next, we need to define a function to vmap over. The function should,\ngiven parameters and buffers and inputs, run the model using those\nparameters, buffers, and inputs. We\u2019ll use torch.func.functional_call\nto help out:",
            "markdown"
        ],
        [
            "from torch.func import \nimport copy\n\n# Construct a \"stateless\" version of one of the models. It is \"stateless\" in\n# the sense that the parameters are meta Tensors and do not have storage.\nbase_model = copy.deepcopy(models[0])\nbase_model = ('meta')\n\ndef fmodel(params, buffers, x):\n    return (base_model, (params, buffers), (x,))",
            "code"
        ],
        [
            "Option 1: get predictions using a different minibatch for each model.",
            "markdown"
        ],
        [
            "By default, vmap maps a function across the first dimension of all inputs to\nthe passed-in function. After using stack_module_state, each of\nthe params and buffers have an additional dimension of size \u2018num_models\u2019 at\nthe front, and minibatches has a dimension of size \u2018num_models\u2019.",
            "markdown"
        ],
        [
            "print([p.size(0) for p in params.values()]) # show the leading 'num_models' dimension\n\nassert .shape == (num_models, 64, 1, 28, 28) # verify minibatch has leading dimension of size 'num_models'\n\nfrom torch import \n\n = (fmodel)(params, buffers, )\n\n# verify the vmap predictions match the\nassert (, (predictions_diff_minibatch_loop), atol=1e-3, rtol=1e-5)",
            "code"
        ],
        [
            "[10, 10, 10, 10, 10, 10]",
            "code"
        ],
        [
            "Option 2: get predictions using the same minibatch of data.",
            "markdown"
        ],
        [
            "vmap has an in_dims arg that specifies which dimensions to map over.\nBy using None, we tell vmap we want the same minibatch to apply for all of\nthe 10 models.",
            "markdown"
        ],
        [
            " = (fmodel, in_dims=(0, 0, None))(params, buffers, )\n\nassert (, (predictions2), atol=1e-3, rtol=1e-5)",
            "code"
        ],
        [
            "A quick note: there are limitations around what types of functions can be\ntransformed by vmap. The best functions to transform are ones that are pure\nfunctions: a function where the outputs are only determined by the inputs\nthat have no side effects (e.g. mutation). vmap is unable to handle mutation\nof arbitrary Python data structures, but it is able to handle many in-place\nPyTorch operations.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Model ensembling->Performance": [
        [
            "Curious about performance numbers? Here\u2019s how the numbers look.",
            "markdown"
        ],
        [
            "from torch.utils.benchmark import \n = (\n    stmt=\"[model(minibatch) for model, minibatch in zip(models, minibatches)]\",\n    globals=globals())\n = (\n    stmt=\"vmap(fmodel)(params, buffers, minibatches)\",\n    globals=globals())\nprint(f'Predictions without vmap {(100)}')\nprint(f'Predictions with vmap {(100)}')",
            "code"
        ],
        [
            "Predictions without vmap &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f739b27db70&gt;\n[model(minibatch) for model, minibatch in zip(models, minibatches)]\n  1.48 ms\n  1 measurement, 100 runs , 1 thread\nPredictions with vmap &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f739b27ded0&gt;\nvmap(fmodel)(params, buffers, minibatches)\n  630.78 us\n  1 measurement, 100 runs , 1 thread",
            "code"
        ],
        [
            "There\u2019s a large speedup using vmap!",
            "markdown"
        ],
        [
            "In general, vectorization with vmap should be faster than running a function\nin a for-loop and competitive with manual batching. There are some exceptions\nthough, like if we haven\u2019t implemented the vmap rule for a particular\noperation or if the underlying kernels weren\u2019t optimized for older hardware\n(GPUs). If you see any of these cases, please let us know by opening an issue\non GitHub.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.843 seconds)",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Per-sample-gradients->What is it?": [
        [
            "Per-sample-gradient computation is computing the gradient for each and every\nsample in a batch of data. It is a useful quantity in differential privacy,\nmeta-learning, and optimization research.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This tutorial requires PyTorch 2.0.0 or later.",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n(0)\n\n# Here's a simple CNN and loss function:\n\nclass SimpleCNN():\n    def __init__(self):\n        super(, self).__init__()\n        self.conv1 = (1, 32, 3, 1)\n        self.conv2 = (32, 64, 3, 1)\n        self.fc1 = (9216, 128)\n        self.fc2 = (128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = (x)\n        x = self.conv2(x)\n        x = (x)\n        x = (x, 2)\n        x = (x, 1)\n        x = self.fc1(x)\n        x = (x)\n        x = self.fc2(x)\n        output = (x, dim=1)\n        output = x\n        return output\n\ndef loss_fn(, ):\n    return (, )",
            "code"
        ],
        [
            "Let\u2019s generate a batch of dummy data and pretend that we\u2019re working with an MNIST dataset.\nThe dummy images are 28 by 28 and we use a minibatch of size 64.",
            "markdown"
        ],
        [
            "device = 'cuda'\n\nnum_models = 10\nbatch_size = 64\n = (batch_size, 1, 28, 28, device=device)\n\n = (10, (64,), device=device)",
            "code"
        ],
        [
            "In regular model training, one would forward the minibatch through the model,\nand then call .backward() to compute gradients.  This would generate an\n\u2018average\u2019 gradient of the entire mini-batch:",
            "markdown"
        ],
        [
            "model = ().to(device=device)\n = model()  # move the entire mini-batch through the model\n\n = loss_fn(, )\n()  # back propogate the 'average' gradient of this mini-batch",
            "code"
        ],
        [
            "In contrast to the above approach, per-sample-gradient computation is\nequivalent to:",
            "markdown"
        ],
        [
            "for each individual sample of the data, perform a forward and a backward\npass to get an individual (per-sample) gradient.",
            "markdown"
        ],
        [
            "def compute_grad(sample, target):\n    sample = sample.unsqueeze(0)  # prepend batch dimension for processing\n    target = target.unsqueeze(0)\n\n    prediction = model(sample)\n     = loss_fn(prediction, target)\n\n    return (, list(()))\n\n\ndef compute_sample_grads(, ):\n    \"\"\" manually process each sample with per sample gradient \"\"\"\n    sample_grads = [compute_grad([i], [i]) for i in range(batch_size)]\n    sample_grads = zip(*sample_grads)\n    sample_grads = [(shards) for shards in sample_grads]\n    return sample_grads\n\nper_sample_grads = compute_sample_grads(, )",
            "code"
        ],
        [
            "sample_grads[0] is the per-sample-grad for model.conv1.weight.\nmodel.conv1.weight.shape is [32, 1, 3, 3]; notice how there is one\ngradient, per sample, in the batch for a total of 64.",
            "markdown"
        ],
        [
            "print(per_sample_grads[0].shape)",
            "code"
        ],
        [
            "torch.Size([64, 32, 1, 3, 3])",
            "code"
        ]
    ],
    "torch->Frontend APIs->Per-sample-gradients->Per-sample-grads, the efficient way, using function transforms": [
        [
            "We can compute per-sample-gradients efficiently by using function transforms.",
            "markdown"
        ],
        [
            "The torch.func function transform API transforms over functions.\nOur strategy is to define a function that computes the loss and then apply\ntransforms to construct a function that computes per-sample-gradients.",
            "markdown"
        ],
        [
            "We\u2019ll use the torch.func.functional_call function to treat an nn.Module\nlike a function.",
            "markdown"
        ],
        [
            "First, let\u2019s extract the state from model into two dictionaries,\nparameters and buffers. We\u2019ll be detaching them because we won\u2019t use\nregular PyTorch autograd (e.g. Tensor.backward(), torch.autograd.grad).",
            "markdown"
        ],
        [
            "from torch.func import , , \n\nparams = {k: v.detach() for k, v in ()}\nbuffers = {k: v.detach() for k, v in ()}",
            "code"
        ],
        [
            "Next, let\u2019s define a function to compute the loss of the model given a\nsingle input rather than a batch of inputs. It is important that this\nfunction accepts the parameters, the input, and the target, because we will\nbe transforming over them.",
            "markdown"
        ],
        [
            "Note - because the model was originally written to handle batches, we\u2019ll\nuse torch.unsqueeze to add a batch dimension.",
            "markdown"
        ],
        [
            "def compute_loss(params, buffers, sample, target):\n    batch = sample.unsqueeze(0)\n     = target.unsqueeze(0)\n\n     = (model, (params, buffers), (batch,))\n     = loss_fn(, )\n    return ",
            "code"
        ],
        [
            "Now, let\u2019s use the grad transform to create a new function that computes\nthe gradient with respect to the first argument of compute_loss\n(i.e. the params).",
            "markdown"
        ],
        [
            "ft_compute_grad = (compute_loss)",
            "code"
        ],
        [
            "The ft_compute_grad function computes the gradient for a single\n(sample, target) pair. We can use vmap to get it to compute the gradient\nover an entire batch of samples and targets. Note that\nin_dims=(None, None, 0, 0) because we wish to map ft_compute_grad over\nthe 0th dimension of the data and targets, and use the same params and\nbuffers for each.",
            "markdown"
        ],
        [
            "ft_compute_sample_grad = (ft_compute_grad, in_dims=(None, None, 0, 0))",
            "code"
        ],
        [
            "Finally, let\u2019s used our transformed function to compute per-sample-gradients:",
            "markdown"
        ],
        [
            "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, , )",
            "code"
        ],
        [
            "we can double check that the results using grad and vmap match the\nresults of hand processing each one individually:",
            "markdown"
        ],
        [
            "for ,  in zip(per_sample_grads, ft_per_sample_grads.values()):\n    assert (, , atol=3e-3, rtol=1e-5)",
            "code"
        ],
        [
            "A quick note: there are limitations around what types of functions can be\ntransformed by vmap. The best functions to transform are ones that are pure\nfunctions: a function where the outputs are only determined by the inputs,\nand that have no side effects (e.g. mutation). vmap is unable to handle\nmutation of arbitrary Python data structures, but it is able to handle many\nin-place PyTorch operations.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Per-sample-gradients->Performance comparison": [
        [
            "Curious about how the performance of vmap compares?",
            "markdown"
        ],
        [
            "Currently the best results are obtained on newer GPU\u2019s such as the A100\n(Ampere) where we\u2019ve seen up to 25x speedups on this example, but here are\nsome results on our build machines:",
            "markdown"
        ],
        [
            "def get_perf(first, first_descriptor, second, second_descriptor):\n    \"\"\"takes torch.benchmark objects and compares delta of second vs first.\"\"\"\n    second_res = second.times[0]\n    first_res = first.times[0]\n\n    gain = (first_res-second_res)/first_res\n    if gain &lt; 0: gain *=-1\n    final_gain = gain*100\n\n    print(f\"Performance delta: {final_gain:.4f} percent improvement with {first_descriptor} \")\n\nfrom torch.utils.benchmark import \n\n = (stmt=\"compute_sample_grads(data, targets)\", globals=globals())\n = (stmt=\"ft_compute_sample_grad(params, buffers, data, targets)\",globals=globals())\n = (100)\n = (100)\n\nprint(f'Per-sample-grads without vmap {}')\nprint(f'Per-sample-grads with vmap {}')\n\nget_perf(, \"vmap\", , \"no vmap\")",
            "code"
        ],
        [
            "Per-sample-grads without vmap &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f53a48fc250&gt;\ncompute_sample_grads(data, targets)\n  67.80 ms\n  1 measurement, 100 runs , 1 thread\nPer-sample-grads with vmap &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f53a491a8f0&gt;\nft_compute_sample_grad(params, buffers, data, targets)\n  7.30 ms\n  1 measurement, 100 runs , 1 thread\nPerformance delta: 828.9354 percent improvement with vmap",
            "code"
        ],
        [
            "There are other optimized solutions (like in )\nto computing per-sample-gradients in PyTorch that also perform better than\nthe naive method. But it\u2019s cool that composing vmap and grad give us a\nnice speedup.",
            "markdown"
        ],
        [
            "In general, vectorization with vmap should be faster than running a function\nin a for-loop and competitive with manual batching. There are some exceptions\nthough, like if we haven\u2019t implemented the vmap rule for a particular\noperation or if the underlying kernels weren\u2019t optimized for older hardware\n(GPUs). If you see any of these cases, please let us know by opening an issue\nat on GitHub.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  8.425 seconds)",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend": [
        [
            "The PyTorch C++ frontend is a pure C++ interface to the PyTorch machine learning\nframework. While the primary interface to PyTorch naturally is Python, this\nPython API sits atop a substantial C++ codebase providing foundational data\nstructures and functionality such as tensors and automatic differentiation. The\nC++ frontend exposes a pure C++11 API that extends this underlying C++ codebase\nwith tools required for machine learning training and inference. This includes a\nbuilt-in collection of common components for neural network modeling; an API to\nextend this collection with custom modules; a library of popular optimization\nalgorithms such as stochastic gradient descent; a parallel data loader with an\nAPI to define and load datasets; serialization routines and more.",
            "markdown"
        ],
        [
            "This tutorial will walk you through an end-to-end example of training a model\nwith the C++ frontend. Concretely, we will be training a  \u2013 a kind of generative model \u2013 to\ngenerate images of MNIST digits. While conceptually a simple example, it should\nbe enough to give you a whirlwind overview of the PyTorch C++ frontend and wet\nyour appetite for training more complex models. We will begin with some\nmotivating words for why you would want to use the C++ frontend to begin with,\nand then dive straight into defining and training our model.",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "Watch  for a quick (and humorous)\npresentation on the C++ frontend.",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            " provides a sweeping\noverview of the C++ frontend\u2019s components and design philosophy.",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "Documentation for the PyTorch C++ ecosystem is available at\n. There you can find high level descriptions as\nwell as API-level documentation.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Motivation": [
        [
            "Before we embark on our exciting journey of GANs and MNIST digits, let\u2019s take a\nstep back and discuss why you would want to use the C++ frontend instead of the\nPython one to begin with. We (the PyTorch team) created the C++ frontend to\nenable research in environments in which Python cannot be used, or is simply not\nthe right tool for the job. Examples for such environments include:",
            "markdown"
        ],
        [
            "<strong>Low Latency Systems</strong>: You may want to do reinforcement learning research in\na pure C++ game engine with high frames-per-second and low latency\nrequirements. Using a pure C++ library is a much better fit to such an\nenvironment than a Python library. Python may not be tractable at all because\nof the slowness of the Python interpreter.",
            "markdown"
        ],
        [
            "<strong>Highly Multithreaded Environments</strong>: Due to the Global Interpreter Lock\n(GIL), Python cannot run more than one system thread at a time.\nMultiprocessing is an alternative, but not as scalable and has significant\nshortcomings. C++ has no such constraints and threads are easy to use and\ncreate. Models requiring heavy parallelization, like those used in , can benefit from\nthis.",
            "markdown"
        ],
        [
            "<strong>Existing C++ Codebases</strong>: You may be the owner of an existing C++\napplication doing anything from serving web pages in a backend server to\nrendering 3D graphics in photo editing software, and wish to integrate\nmachine learning methods into your system. The C++ frontend allows you to\nremain in C++ and spare yourself the hassle of binding back and forth between\nPython and C++, while retaining much of the flexibility and intuitiveness of\nthe traditional PyTorch (Python) experience.",
            "markdown"
        ],
        [
            "The C++ frontend is not intended to compete with the Python frontend. It is\nmeant to complement it. We know researchers and engineers alike love PyTorch for\nits simplicity, flexibility and intuitive API. Our goal is to make sure you can\ntake advantage of these core design principles in every possible environment,\nincluding the ones described above. If one of these scenarios describes your use\ncase well, or if you are simply interested or curious, follow along as we\nexplore the C++ frontend in detail in the following paragraphs.",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "The C++ frontend tries to provide an API as close as possible to that of the\nPython frontend. If you are experienced with the Python frontend and ever ask\nyourself \u201chow do I do X with the C++ frontend?\u201d, write your code the way you\nwould in Python, and more often than not the same functions and methods will\nbe available in C++ as in Python (just remember to replace dots with double\ncolons).",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Writing a Basic Application": [
        [
            "Let\u2019s begin by writing a minimal C++ application to verify that we\u2019re on the\nsame page regarding our setup and build environment. First, you will need to\ngrab a copy of the <em>LibTorch</em> distribution \u2013 our ready-built zip archive that\npackages all relevant headers, libraries and CMake build files required to use\nthe C++ frontend. The LibTorch distribution is available for download on the\n for Linux, MacOS\nand Windows. The rest of this tutorial will assume a basic Ubuntu Linux\nenvironment, however you are free to follow along on MacOS or Windows too.",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "The note on  describes the following steps\nin more detail.",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "On Windows, debug and release builds are not ABI-compatible. If you plan to\nbuild your project in debug mode, please try the debug version of LibTorch.\nAlso, make sure you specify the correct configuration in the cmake --build .\nline below.",
            "markdown"
        ],
        [
            "The first step is to download the LibTorch distribution locally, via the link\nretrieved from the PyTorch website. For a vanilla Ubuntu Linux environment, this\nmeans running:",
            "markdown"
        ],
        [
            "# If you need e.g. CUDA 9.0 support, please replace \"cpu\" with \"cu90\" in the URL below.\nwget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip\nunzip libtorch-shared-with-deps-latest.zip",
            "code"
        ],
        [
            "Next, let\u2019s write a tiny C++ file called dcgan.cpp that includes\ntorch/torch.h and for now simply prints out a three by three identity\nmatrix:",
            "markdown"
        ],
        [
            "#include &lt;torch/torch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n  torch::Tensor tensor = torch::eye(3);\n  std::cout &lt;&lt; tensor &lt;&lt; std::endl;\n}",
            "code"
        ],
        [
            "To build this tiny application as well as our full-fledged training script later\non we\u2019ll use this CMakeLists.txt file:",
            "markdown"
        ],
        [
            "cmake_minimum_required(VERSION 3.0 FATAL_ERROR)\nproject(dcgan)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(dcgan dcgan.cpp)\ntarget_link_libraries(dcgan \"${TORCH_LIBRARIES}\")\nset_property(TARGET dcgan PROPERTY CXX_STANDARD 14)",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "While CMake is the recommended build system for LibTorch, it is not a hard\nrequirement. You can also use Visual Studio project files, QMake, plain\nMakefiles or any other build environment you feel comfortable with. However,\nwe do not provide out-of-the-box support for this.",
            "markdown"
        ],
        [
            "Make note of line 4 in the above CMake file: find_package(Torch REQUIRED).\nThis instructs CMake to find the build configuration for the LibTorch library.\nIn order for CMake to know <em>where</em> to find these files, we must set the\nCMAKE_PREFIX_PATH when invoking cmake. Before we do this, let\u2019s agree on\nthe following directory structure for our dcgan application:",
            "markdown"
        ],
        [
            "dcgan/\n  CMakeLists.txt\n  dcgan.cpp",
            "code"
        ],
        [
            "Further, I will refer to the path to the unzipped LibTorch distribution as\n/path/to/libtorch. Note that this <strong>must be an absolute path</strong>. In\nparticular, setting CMAKE_PREFIX_PATH to something like ../../libtorch\nwill break in unexpected ways. Instead, write $PWD/../../libtorch to get the\ncorresponding absolute path. Now, we are ready to build our application:",
            "markdown"
        ],
        [
            "root@fa350df05ecf:/home# mkdir build\nroot@fa350df05ecf:/home# cd build\nroot@fa350df05ecf:/home/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /path/to/libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/build\nroot@fa350df05ecf:/home/build# cmake --build . --config Release\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan",
            "code"
        ],
        [
            "Above, we first created a build folder inside of our dcgan directory,\nentered this folder, ran the cmake command to generate the necessary build\n(Make) files and finally compiled the project successfully by running cmake\n--build . --config Release. We are now all set to execute our minimal binary\nand complete this section on basic project configuration:",
            "markdown"
        ],
        [
            "root@fa350df05ecf:/home/build# ./dcgan\n1  0  0\n0  1  0\n0  0  1\n[ Variable[CPUFloatType]{3,3} ]",
            "code"
        ],
        [
            "Looks like an identity matrix to me!",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Defining the Neural Network Models": [
        [
            "Now that we have our basic environment configured, we can dive into the much\nmore interesting parts of this tutorial. First, we will discuss how to define\nand interact with modules in the C++ frontend. We\u2019ll begin with basic,\nsmall-scale example modules and then implement a full-fledged GAN using the\nextensive library of built-in modules provided by the C++ frontend.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Defining the Neural Network Models->Module API Basics": [
        [
            "In line with the Python interface, neural networks based on the C++ frontend are\ncomposed of reusable building blocks called <em>modules</em>. There is a base module\nclass from which all other modules are derived. In Python, this class is\ntorch.nn.Module and in C++ it is torch::nn::Module. Besides a\nforward() method that implements the algorithm the module encapsulates, a\nmodule usually contains any of three kinds of sub-objects: parameters, buffers\nand submodules.",
            "markdown"
        ],
        [
            "Parameters and buffers store state in form of tensors. Parameters record\ngradients, while buffers do not. Parameters are usually the trainable weights of\nyour neural network. Examples of buffers include means and variances for batch\nnormalization. In order to re-use particular blocks of logic and state, the\nPyTorch API allows modules to be nested. A nested module is termed a\n<em>submodule</em>.",
            "markdown"
        ],
        [
            "Parameters, buffers and submodules must be explicitly registered. Once\nregistered, methods like parameters() or buffers() can be used to\nretrieve a container of all parameters in the entire (nested) module hierarchy.\nSimilarly, methods like to(...), where e.g. to(torch::kCUDA) moves all\nparameters and buffers from CPU to CUDA memory, work on the entire module\nhierarchy.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Defining the Neural Network Models->Module API Basics->Defining a Module and Registering Parameters": [
        [
            "To put these words into code, let\u2019s consider this simple module written in the\nPython interface:",
            "markdown"
        ],
        [
            "import torch\n\nclass Net(torch.nn.Module):\n  def __init__(self, N, M):\n    super(Net, self).__init__()\n    self.W = torch.nn.Parameter(torch.randn(N, M))\n    self.b = torch.nn.Parameter(torch.randn(M))\n\n  def forward(self, input):\n    return torch.addmm(self.b, input, self.W)",
            "code"
        ],
        [
            "In C++, it would look like this:",
            "markdown"
        ],
        [
            "#include &lt;torch/torch.h&gt;\n\nstruct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M) {\n    W = register_parameter(\"W\", torch::randn({N, M}));\n    b = register_parameter(\"b\", torch::randn(M));\n  }\n  torch::Tensor forward(torch::Tensor input) {\n    return torch::addmm(b, input, W);\n  }\n  torch::Tensor W, b;\n};",
            "code"
        ],
        [
            "Just like in Python, we define a class called Net (for simplicity here a\nstruct instead of a class) and derive it from the module base class.\nInside the constructor, we create tensors using torch::randn just like we\nuse torch.randn in Python. One interesting difference is how we register the\nparameters. In Python, we wrap the tensors with the torch.nn.Parameter\nclass, while in C++ we have to pass the tensor through the\nregister_parameter method instead. The reason for this is that the Python\nAPI can detect that an attribute is of type torch.nn.Parameter and\nautomatically registers such tensors. In C++, reflection is very limited, so a\nmore traditional (and less magical) approach is provided.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Defining the Neural Network Models->Module API Basics->Registering Submodules and Traversing the Module Hierarchy": [
        [
            "In the same way we can register parameters, we can also register submodules. In\nPython, submodules are automatically detected and registered when they are\nassigned as an attribute of a module:",
            "markdown"
        ],
        [
            "class Net(torch.nn.Module):\n  def __init__(self, N, M):\n      super(Net, self).__init__()\n      # Registered as a submodule behind the scenes\n      self.linear = torch.nn.Linear(N, M)\n      self.another_bias = torch.nn.Parameter(torch.rand(M))\n\n  def forward(self, input):\n    return self.linear(input) + self.another_bias",
            "code"
        ],
        [
            "This allows, for example, to use the parameters() method to recursively\naccess all parameters in our module hierarchy:",
            "markdown"
        ],
        [
            "&gt;&gt;&gt; net = Net(4, 5)\n&gt;&gt;&gt; print(list(net.parameters()))\n[Parameter containing:\ntensor([0.0808, 0.8613, 0.2017, 0.5206, 0.5353], requires_grad=True), Parameter containing:\ntensor([[-0.3740, -0.0976, -0.4786, -0.4928],\n        [-0.1434,  0.4713,  0.1735, -0.3293],\n        [-0.3467, -0.3858,  0.1980,  0.1986],\n        [-0.1975,  0.4278, -0.1831, -0.2709],\n        [ 0.3730,  0.4307,  0.3236, -0.0629]], requires_grad=True), Parameter containing:\ntensor([ 0.2038,  0.4638, -0.2023,  0.1230, -0.0516], requires_grad=True)]",
            "code"
        ],
        [
            "To register submodules in C++, use the aptly named register_module() method\nto register a module like torch::nn::Linear:",
            "markdown"
        ],
        [
            "struct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M)\n      : linear(register_module(\"linear\", torch::nn::Linear(N, M))) {\n    another_bias = register_parameter(\"b\", torch::randn(M));\n  }\n  torch::Tensor forward(torch::Tensor input) {\n    return linear(input) + another_bias;\n  }\n  torch::nn::Linear linear;\n  torch::Tensor another_bias;\n};",
            "code"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "You can find the full list of available built-in modules like\ntorch::nn::Linear, torch::nn::Dropout or torch::nn::Conv2d in the\ndocumentation of the torch::nn namespace .",
            "markdown"
        ],
        [
            "One subtlety about the above code is why the submodule was created in the\nconstructor\u2019s initializer list, while the parameter was created inside the\nconstructor body. There is a good reason for this, which we\u2019ll touch upon this\nin the section on the C++ frontend\u2019s <em>ownership model</em> further below. The end\nresult, however, is that we can recursively access our module tree\u2019s parameters\njust like in Python. Calling parameters() returns a\nstd::vector&lt;torch::Tensor&gt;, which we can iterate over:",
            "markdown"
        ],
        [
            "int main() {\n  Net net(4, 5);\n  for (const auto&amp; p : net.parameters()) {\n    std::cout &lt;&lt; p &lt;&lt; std::endl;\n  }\n}",
            "code"
        ],
        [
            "which prints:",
            "markdown"
        ],
        [
            "root@fa350df05ecf:/home/build# ./dcgan\n0.0345\n1.4456\n-0.6313\n-0.3585\n-0.4008\n[ Variable[CPUFloatType]{5} ]\n-0.1647  0.2891  0.0527 -0.0354\n0.3084  0.2025  0.0343  0.1824\n-0.4630 -0.2862  0.2500 -0.0420\n0.3679 -0.1482 -0.0460  0.1967\n0.2132 -0.1992  0.4257  0.0739\n[ Variable[CPUFloatType]{5,4} ]\n0.01 *\n3.6861\n-10.1166\n-45.0333\n7.9983\n-20.0705\n[ Variable[CPUFloatType]{5} ]",
            "code"
        ],
        [
            "with three parameters just like in Python. To also see the names of these\nparameters, the C++ API provides a named_parameters() method which returns\nan OrderedDict just like in Python:",
            "markdown"
        ],
        [
            "Net net(4, 5);\nfor (const auto&amp; pair : net.named_parameters()) {\n  std::cout &lt;&lt; pair.key() &lt;&lt; \": \" &lt;&lt; pair.value() &lt;&lt; std::endl;\n}",
            "code"
        ],
        [
            "which we can execute again to see the output:",
            "markdown"
        ],
        [
            "root@fa350df05ecf:/home/build# make &amp;&amp; ./dcgan                                                                                                                                            11:13:48\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan\nb: -0.1863\n-0.8611\n-0.1228\n1.3269\n0.9858\n[ Variable[CPUFloatType]{5} ]\nlinear.weight:  0.0339  0.2484  0.2035 -0.2103\n-0.0715 -0.2975 -0.4350 -0.1878\n-0.3616  0.1050 -0.4982  0.0335\n-0.1605  0.4963  0.4099 -0.2883\n0.1818 -0.3447 -0.1501 -0.0215\n[ Variable[CPUFloatType]{5,4} ]\nlinear.bias: -0.0250\n0.0408\n0.3756\n-0.2149\n-0.3636\n[ Variable[CPUFloatType]{5} ]",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "for torch::nn::Module contains the full list of methods that operate on\nthe module hierarchy.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Defining the Neural Network Models->Module API Basics->Running the Network in Forward Mode": [
        [
            "To execute the network in C++, we simply call the forward() method we\ndefined ourselves:",
            "markdown"
        ],
        [
            "int main() {\n  Net net(4, 5);\n  std::cout &lt;&lt; net.forward(torch::ones({2, 4})) &lt;&lt; std::endl;\n}",
            "code"
        ],
        [
            "which prints something like:",
            "markdown"
        ],
        [
            "root@fa350df05ecf:/home/build# ./dcgan\n0.8559  1.1572  2.1069 -0.1247  0.8060\n0.8559  1.1572  2.1069 -0.1247  0.8060\n[ Variable[CPUFloatType]{2,5} ]",
            "code"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Defining the Neural Network Models->Module API Basics->Module Ownership": [
        [
            "At this point, we know how to define a module in C++, register parameters,\nregister submodules, traverse the module hierarchy via methods like\nparameters() and finally run the module\u2019s forward() method. While there\nare many more methods, classes and topics to devour in the C++ API, I will refer\nyou to  for\nthe full menu. We\u2019ll also touch upon some more concepts as we implement the\nDCGAN model and end-to-end training pipeline in just a second. Before we do so,\nlet me briefly touch upon the <em>ownership model</em> the C++ frontend provides for\nsubclasses of torch::nn::Module.",
            "markdown"
        ],
        [
            "For this discussion, the ownership model refers to the way modules are stored\nand passed around \u2013 which determines who or what <em>owns</em> a particular module\ninstance. In Python, objects are always allocated dynamically (on the heap) and\nhave reference semantics. This is very easy to work with and straightforward to\nunderstand. In fact, in Python, you can largely forget about where objects live\nand how they get referenced, and focus on getting things done.",
            "markdown"
        ],
        [
            "C++, being a lower level language, provides more options in this realm. This\nincreases complexity and heavily influences the design and ergonomics of the C++\nfrontend. In particular, for modules in the C++ frontend, we have the option of\nusing <em>either</em> value semantics <em>or</em> reference semantics. The first case is the\nsimplest and was shown in the examples thus far: module objects are allocated on\nthe stack and when passed to a function, can be either copied, moved (with\nstd::move) or taken by reference or by pointer:",
            "markdown"
        ],
        [
            "struct Net : torch::nn::Module { };\n\nvoid a(Net net) { }\nvoid b(Net&amp; net) { }\nvoid c(Net* net) { }\n\nint main() {\n  Net net;\n  a(net);\n  a(std::move(net));\n  b(net);\n  c(&amp;net);\n}",
            "code"
        ],
        [
            "For the second case \u2013 reference semantics \u2013 we can use std::shared_ptr.\nThe advantage of reference semantics is that, like in Python, it reduces the\ncognitive overhead of thinking about how modules must be passed to functions and\nhow arguments must be declared (assuming you use shared_ptr everywhere).",
            "markdown"
        ],
        [
            "struct Net : torch::nn::Module {};\n\nvoid a(std::shared_ptr&lt;Net&gt; net) { }\n\nint main() {\n  auto net = std::make_shared&lt;Net&gt;();\n  a(net);\n}",
            "code"
        ],
        [
            "In our experience, researchers coming from dynamic languages greatly prefer\nreference semantics over value semantics, even though the latter is more\n\u201cnative\u201d to C++. It is also important to note that torch::nn::Module\u2019s\ndesign, in order to stay close to the ergonomics of the Python API, relies on\nshared ownership. For example, take our earlier (here shortened) definition of\nNet:",
            "markdown"
        ],
        [
            "struct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M)\n    : linear(register_module(\"linear\", torch::nn::Linear(N, M)))\n  { }\n  torch::nn::Linear linear;\n};",
            "code"
        ],
        [
            "In order to use the linear submodule, we want to store it directly in our\nclass. However, we also want the module base class to know about and have access\nto this submodule. For this, it must store a reference to this submodule. At\nthis point, we have already arrived at the need for shared ownership. Both the\ntorch::nn::Module class and concrete Net class require a reference to\nthe submodule. For this reason, the base class stores modules as\nshared_ptrs, and therefore the concrete class must too.",
            "markdown"
        ],
        [
            "But wait! I don\u2019t see any mention of shared_ptr in the above code! Why is\nthat? Well, because std::shared_ptr&lt;MyModule&gt; is a hell of a lot to type. To\nkeep our researchers productive, we came up with an elaborate scheme to hide the\nmention of shared_ptr \u2013 a benefit usually reserved for value semantics \u2013\nwhile retaining reference semantics. To understand how this works, we can take a\nlook at a simplified definition of the torch::nn::Linear module in the core\nlibrary (the full definition is ):",
            "markdown"
        ],
        [
            "struct LinearImpl : torch::nn::Module {\n  LinearImpl(int64_t in, int64_t out);\n\n  Tensor forward(const Tensor&amp; input);\n\n  Tensor weight, bias;\n};\n\nTORCH_MODULE(Linear);",
            "code"
        ],
        [
            "In brief: the module is not called Linear, but LinearImpl. A macro,\nTORCH_MODULE then defines the actual Linear class. This \u201cgenerated\u201d\nclass is effectively a wrapper over a std::shared_ptr&lt;LinearImpl&gt;. It is a\nwrapper instead of a simple typedef so that, among other things, constructors\nstill work as expected, i.e. you can still write torch::nn::Linear(3, 4)\ninstead of std::make_shared&lt;LinearImpl&gt;(3, 4). We call the class created by\nthe macro the module <em>holder</em>. Like with (shared) pointers, you access the\nunderlying object using the arrow operator (like model-&gt;forward(...)). The\nend result is an ownership model that resembles that of the Python API quite\nclosely. Reference semantics become the default, but without the extra typing of\nstd::shared_ptr or std::make_shared. For our Net, using the module\nholder API looks like this:",
            "markdown"
        ],
        [
            "struct NetImpl : torch::nn::Module {};\nTORCH_MODULE(Net);\n\nvoid a(Net net) { }\n\nint main() {\n  Net net;\n  a(net);\n}",
            "code"
        ],
        [
            "There is one subtle issue that deserves mention here. A default constructed\nstd::shared_ptr is \u201cempty\u201d, i.e. contains a null pointer. What is a default\nconstructed Linear or Net? Well, it\u2019s a tricky choice. We could say it\nshould be an empty (null) std::shared_ptr&lt;LinearImpl&gt;. However, recall that\nLinear(3, 4) is the same as std::make_shared&lt;LinearImpl&gt;(3, 4). This\nmeans that if we had decided that Linear linear; should be a null pointer,\nthen there would be no way to construct a module that does not take any\nconstructor arguments, or defaults all of them. For this reason, in the current\nAPI, a default constructed module holder (like Linear()) invokes the\ndefault constructor of the underlying module (LinearImpl()). If the\nunderlying module does not have a default constructor, you get a compiler error.\nTo instead construct the empty holder, you can pass nullptr to the\nconstructor of the holder.",
            "markdown"
        ],
        [
            "In practice, this means you can use submodules either like shown earlier, where\nthe module is registered and constructed in the <em>initializer list</em>:",
            "markdown"
        ],
        [
            "struct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M)\n    : linear(register_module(\"linear\", torch::nn::Linear(N, M)))\n  { }\n  torch::nn::Linear linear;\n};",
            "code"
        ],
        [
            "or you can first construct the holder with a null pointer and then assign to it\nin the constructor (more familiar for Pythonistas):",
            "markdown"
        ],
        [
            "struct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M) {\n    linear = register_module(\"linear\", torch::nn::Linear(N, M));\n  }\n  torch::nn::Linear linear{nullptr}; // construct an empty holder\n};",
            "code"
        ],
        [
            "In conclusion: Which ownership model \u2013 which semantics \u2013 should you use? The\nC++ frontend\u2019s API best supports the ownership model provided by module holders.\nThe only disadvantage of this mechanism is one extra line of boilerplate below\nthe module declaration. That said, the simplest model is still the value\nsemantics model shown in the introduction to C++ modules. For small, simple\nscripts, you may get away with it too. But you\u2019ll find sooner or later that, for\ntechnical reasons, it is not always supported. For example, the serialization\nAPI (torch::save and torch::load) only supports module holders (or plain\nshared_ptr). As such, the module holder API is the recommended way of\ndefining modules with the C++ frontend, and we will use this API in this\ntutorial henceforth.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Defining the Neural Network Models->Defining the DCGAN Modules": [
        [
            "We now have the necessary background and introduction to define the modules for\nthe machine learning task we want to solve in this post. To recap: our task is\nto generate images of digits from the . We want to use a  to solve\nthis task. In particular, we\u2019ll use a  \u2013 one of the first and simplest of its\nkind, but entirely sufficient for this task.",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "You can find the full source code presented in this tutorial .",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Defining the Neural Network Models->Defining the DCGAN Modules->What was a GAN aGAN?": [
        [
            "A GAN consists of two distinct neural network models: a <em>generator</em> and a\n<em>discriminator</em>. The generator receives samples from a noise distribution, and\nits aim is to transform each noise sample into an image that resembles those of\na target distribution \u2013 in our case the MNIST dataset. The discriminator in\nturn receives either <em>real</em> images from the MNIST dataset, or <em>fake</em> images from\nthe generator. It is asked to emit a probability judging how real (closer to\n1) or fake (closer to 0) a particular image is. Feedback from the\ndiscriminator on how real the images produced by the generator are is used to\ntrain the generator. Feedback on how good of an eye for authenticity the\ndiscriminator has is used to optimize the discriminator. In theory, a delicate\nbalance between the generator and discriminator makes them improve in tandem,\nleading to the generator producing images indistinguishable from the target\ndistribution, fooling the discriminator\u2019s (by then) excellent eye into emitting\na probability of 0.5 for both real and fake images. For us, the end result\nis a machine that receives noise as input and generates realistic images of\ndigits as its output.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Defining the Neural Network Models->Defining the DCGAN Modules->The Generator Module": [
        [
            "We begin by defining the generator module, which consists of a series of\ntransposed 2D convolutions, batch normalizations and ReLU activation units.\nWe explicitly pass inputs (in a functional way) between modules in the\nforward() method of a module we define ourselves:",
            "markdown"
        ],
        [
            "struct DCGANGeneratorImpl : nn::Module {\n  DCGANGeneratorImpl(int kNoiseSize)\n      : conv1(nn::ConvTranspose2dOptions(kNoiseSize, 256, 4)\n                  .bias(false)),\n        batch_norm1(256),\n        conv2(nn::ConvTranspose2dOptions(256, 128, 3)\n                  .stride(2)\n                  .padding(1)\n                  .bias(false)),\n        batch_norm2(128),\n        conv3(nn::ConvTranspose2dOptions(128, 64, 4)\n                  .stride(2)\n                  .padding(1)\n                  .bias(false)),\n        batch_norm3(64),\n        conv4(nn::ConvTranspose2dOptions(64, 1, 4)\n                  .stride(2)\n                  .padding(1)\n                  .bias(false))\n {\n   // register_module() is needed if we want to use the parameters() method later on\n   register_module(\"conv1\", conv1);\n   register_module(\"conv2\", conv2);\n   register_module(\"conv3\", conv3);\n   register_module(\"conv4\", conv4);\n   register_module(\"batch_norm1\", batch_norm1);\n   register_module(\"batch_norm2\", batch_norm2);\n   register_module(\"batch_norm3\", batch_norm3);\n }\n\n torch::Tensor forward(torch::Tensor x) {\n   x = torch::relu(batch_norm1(conv1(x)));\n   x = torch::relu(batch_norm2(conv2(x)));\n   x = torch::relu(batch_norm3(conv3(x)));\n   x = torch::tanh(conv4(x));\n   return x;\n }\n\n nn::ConvTranspose2d conv1, conv2, conv3, conv4;\n nn::BatchNorm2d batch_norm1, batch_norm2, batch_norm3;\n};\nTORCH_MODULE(DCGANGenerator);\n\nDCGANGenerator generator(kNoiseSize);",
            "code"
        ],
        [
            "We can now invoke forward() on the DCGANGenerator to map a noise sample to an image.",
            "markdown"
        ],
        [
            "The particular modules chosen, like nn::ConvTranspose2d and nn::BatchNorm2d,\nfollows the structure outlined earlier. The kNoiseSize constant determines\nthe size of the input noise vector and is set to 100. Hyperparameters were,\nof course, found via grad student descent.",
            "markdown"
        ],
        [
            "Attention",
            "markdown"
        ],
        [
            "No grad students were harmed in the discovery of hyperparameters. They were\nfed Soylent regularly.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "A brief word on the way options are passed to built-in modules like Conv2d\nin the C++ frontend: Every module has some required options, like the number\nof features for BatchNorm2d. If you only need to configure the required\noptions, you can pass them directly to the module\u2019s constructor, like\nBatchNorm2d(128) or Dropout(0.5) or Conv2d(8, 4, 2) (for input\nchannel count, output channel count, and kernel size). If, however, you need\nto modify other options, which are normally defaulted, such as bias\nfor Conv2d, you need to construct and pass an <em>options</em> object. Every\nmodule in the C++ frontend has an associated options struct, called\nModuleOptions where Module is the name of the module, like\nLinearOptions for Linear. This is what we do for the Conv2d\nmodules above.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Defining the Neural Network Models->Defining the DCGAN Modules->The Discriminator Module": [
        [
            "The discriminator is similarly a sequence of convolutions, batch normalizations\nand activations. However, the convolutions are now regular ones instead of\ntransposed, and we use a leaky ReLU with an alpha value of 0.2 instead of a\nvanilla ReLU. Also, the final activation becomes a Sigmoid, which squashes\nvalues into a range between 0 and 1. We can then interpret these squashed values\nas the probabilities the discriminator assigns to images being real.",
            "markdown"
        ],
        [
            "To build the discriminator, we will try something different: a <cite>Sequential</cite> module.\nLike in Python, PyTorch here provides two APIs for model definition: a functional one\nwhere inputs are passed through successive functions (e.g. the generator module example),\nand a more object-oriented one where we build a <cite>Sequential</cite> module containing the\nentire model as submodules. Using <cite>Sequential</cite>, the discriminator would look like:",
            "markdown"
        ],
        [
            "nn::Sequential discriminator(\n  // Layer 1\n  nn::Conv2d(\n      nn::Conv2dOptions(1, 64, 4).stride(2).padding(1).bias(false)),\n  nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)),\n  // Layer 2\n  nn::Conv2d(\n      nn::Conv2dOptions(64, 128, 4).stride(2).padding(1).bias(false)),\n  nn::BatchNorm2d(128),\n  nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)),\n  // Layer 3\n  nn::Conv2d(\n      nn::Conv2dOptions(128, 256, 4).stride(2).padding(1).bias(false)),\n  nn::BatchNorm2d(256),\n  nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)),\n  // Layer 4\n  nn::Conv2d(\n      nn::Conv2dOptions(256, 1, 3).stride(1).padding(0).bias(false)),\n  nn::Sigmoid());",
            "code"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "A Sequential module simply performs function composition. The output of\nthe first submodule becomes the input of the second, the output of the third\nbecomes the input of the fourth and so on.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Loading Data": [
        [
            "Now that we have defined the generator and discriminator model, we need some\ndata we can train these models with. The C++ frontend, like the Python one,\ncomes with a powerful parallel data loader. This data loader can read batches of\ndata from a dataset (which you can define yourself) and provides many\nconfiguration knobs.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "While the Python data loader uses multi-processing, the C++ data loader is truly\nmulti-threaded and does not launch any new processes.",
            "markdown"
        ],
        [
            "The data loader is part of the C++ frontend\u2019s data api, contained in the\ntorch::data:: namespace. This API consists of a few different components:",
            "markdown"
        ],
        [
            "The data loader class,",
            "markdown"
        ],
        [
            "An API for defining datasets,",
            "markdown"
        ],
        [
            "An API for defining <em>transforms</em>, which can be applied to datasets,",
            "markdown"
        ],
        [
            "An API for defining <em>samplers</em>, which produce the indices with which datasets are indexed,",
            "markdown"
        ],
        [
            "A library of existing datasets, transforms and samplers.",
            "markdown"
        ],
        [
            "For this tutorial, we can use the MNIST dataset that comes with the C++\nfrontend. Let\u2019s instantiate a torch::data::datasets::MNIST for this, and\napply two transformations: First, we normalize the images so that they are in\nthe range of -1 to +1 (from an original range of 0 to 1).\nSecond, we apply the Stack <em>collation</em>, which takes a batch of tensors and\nstacks them into a single tensor along the first dimension:",
            "markdown"
        ],
        [
            "auto dataset = torch::data::datasets::MNIST(\"./mnist\")\n    .map(torch::data::transforms::Normalize&lt;&gt;(0.5, 0.5))\n    .map(torch::data::transforms::Stack&lt;&gt;());",
            "code"
        ],
        [
            "Note that the MNIST dataset should be located in the ./mnist directory\nrelative to wherever you execute the training binary from. You can use \nto download the MNIST dataset.",
            "markdown"
        ],
        [
            "Next, we create a data loader and pass it this dataset. To make a new data\nloader, we use torch::data::make_data_loader, which returns a\nstd::unique_ptr of the correct type (which depends on the type of the\ndataset, the type of the sampler and some other implementation details):",
            "markdown"
        ],
        [
            "auto data_loader = torch::data::make_data_loader(std::move(dataset));",
            "code"
        ],
        [
            "The data loader does come with a lot of options. You can inspect the full set\n.\nFor example, to speed up the data loading, we can increase the number of\nworkers. The default number is zero, which means the main thread will be used.\nIf we set workers to 2, two threads will be spawned that load data\nconcurrently. We should also increase the batch size from its default of 1\nto something more reasonable, like 64 (the value of kBatchSize). So\nlet\u2019s create a DataLoaderOptions object and set the appropriate properties:",
            "markdown"
        ],
        [
            "auto data_loader = torch::data::make_data_loader(\n    std::move(dataset),\n    torch::data::DataLoaderOptions().batch_size(kBatchSize).workers(2));",
            "code"
        ],
        [
            "We can now write a loop to load batches of data, which we\u2019ll only print to the\nconsole for now:",
            "markdown"
        ],
        [
            "for (torch::data::Example&lt;&gt;&amp; batch : *data_loader) {\n  std::cout &lt;&lt; \"Batch size: \" &lt;&lt; batch.data.size(0) &lt;&lt; \" | Labels: \";\n  for (int64_t i = 0; i &lt; batch.data.size(0); ++i) {\n    std::cout &lt;&lt; batch.target[i].item&lt;int64_t&gt;() &lt;&lt; \" \";\n  }\n  std::cout &lt;&lt; std::endl;\n}",
            "code"
        ],
        [
            "The type returned by the data loader in this case is a torch::data::Example.\nThis type is a simple struct with a data field for the data and a target\nfield for the label. Because we applied the Stack collation earlier, the\ndata loader returns only a single such example. If we had not applied the\ncollation, the data loader would yield std::vector&lt;torch::data::Example&lt;&gt;&gt;\ninstead, with one element per example in the batch.",
            "markdown"
        ],
        [
            "If you rebuild and run this code, you should see something like this:",
            "markdown"
        ],
        [
            "root@fa350df05ecf:/home/build# make\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan\nroot@fa350df05ecf:/home/build# make\n[100%] Built target dcgan\nroot@fa350df05ecf:/home/build# ./dcgan\nBatch size: 64 | Labels: 5 2 6 7 2 1 6 7 0 1 6 2 3 6 9 1 8 4 0 6 5 3 3 0 4 6 6 6 4 0 8 6 0 6 9 2 4 0 2 8 6 3 3 2 9 2 0 1 4 2 3 4 8 2 9 9 3 5 8 0 0 7 9 9\nBatch size: 64 | Labels: 2 2 4 7 1 2 8 8 6 9 0 2 2 9 3 6 1 3 8 0 4 4 8 8 8 9 2 6 4 7 1 5 0 9 7 5 4 3 5 4 1 2 8 0 7 1 9 6 1 6 5 3 4 4 1 2 3 2 3 5 0 1 6 2\nBatch size: 64 | Labels: 4 5 4 2 1 4 8 3 8 3 6 1 5 4 3 6 2 2 5 1 3 1 5 0 8 2 1 5 3 2 4 4 5 9 7 2 8 9 2 0 6 7 4 3 8 3 5 8 8 3 0 5 8 0 8 7 8 5 5 6 1 7 8 0\nBatch size: 64 | Labels: 3 3 7 1 4 1 6 1 0 3 6 4 0 2 5 4 0 4 2 8 1 9 6 5 1 6 3 2 8 9 2 3 8 7 4 5 9 6 0 8 3 0 0 6 4 8 2 5 4 1 8 3 7 8 0 0 8 9 6 7 2 1 4 7\nBatch size: 64 | Labels: 3 0 5 5 9 8 3 9 8 9 5 9 5 0 4 1 2 7 7 2 0 0 5 4 8 7 7 6 1 0 7 9 3 0 6 3 2 6 2 7 6 3 3 4 0 5 8 8 9 1 9 2 1 9 4 4 9 2 4 6 2 9 4 0\nBatch size: 64 | Labels: 9 6 7 5 3 5 9 0 8 6 6 7 8 2 1 9 8 8 1 1 8 2 0 7 1 4 1 6 7 5 1 7 7 4 0 3 2 9 0 6 6 3 4 4 8 1 2 8 6 9 2 0 3 1 2 8 5 6 4 8 5 8 6 2\nBatch size: 64 | Labels: 9 3 0 3 6 5 1 8 6 0 1 9 9 1 6 1 7 7 4 4 4 7 8 8 6 7 8 2 6 0 4 6 8 2 5 3 9 8 4 0 9 9 3 7 0 5 8 2 4 5 6 2 8 2 5 3 7 1 9 1 8 2 2 7\nBatch size: 64 | Labels: 9 1 9 2 7 2 6 0 8 6 8 7 7 4 8 6 1 1 6 8 5 7 9 1 3 2 0 5 1 7 3 1 6 1 0 8 6 0 8 1 0 5 4 9 3 8 5 8 4 8 0 1 2 6 2 4 2 7 7 3 7 4 5 3\nBatch size: 64 | Labels: 8 8 3 1 8 6 4 2 9 5 8 0 2 8 6 6 7 0 9 8 3 8 7 1 6 6 2 7 7 4 5 5 2 1 7 9 5 4 9 1 0 3 1 9 3 9 8 8 5 3 7 5 3 6 8 9 4 2 0 1 2 5 4 7\nBatch size: 64 | Labels: 9 2 7 0 8 4 4 2 7 5 0 0 6 2 0 5 9 5 9 8 8 9 3 5 7 5 4 7 3 0 5 7 6 5 7 1 6 2 8 7 6 3 2 6 5 6 1 2 7 7 0 0 5 9 0 0 9 1 7 8 3 2 9 4\nBatch size: 64 | Labels: 7 6 5 7 7 5 2 2 4 9 9 4 8 7 4 8 9 4 5 7 1 2 6 9 8 5 1 2 3 6 7 8 1 1 3 9 8 7 9 5 0 8 5 1 8 7 2 6 5 1 2 0 9 7 4 0 9 0 4 6 0 0 8 6\n...",
            "code"
        ],
        [
            "Which means we are successfully able to load data from the MNIST dataset.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Writing the Training Loop": [
        [
            "Let\u2019s now finish the algorithmic part of our example and implement the delicate\ndance between the generator and discriminator. First, we\u2019ll create two\noptimizers, one for the generator and one for the discriminator. The optimizers\nwe use implement the  algorithm:",
            "markdown"
        ],
        [
            "torch::optim::Adam generator_optimizer(\n    generator-&gt;parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5));\ntorch::optim::Adam discriminator_optimizer(\n    discriminator-&gt;parameters(), torch::optim::AdamOptions(5e-4).beta1(0.5));",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "As of this writing, the C++ frontend provides optimizers implementing Adagrad,\nAdam, LBFGS, RMSprop and SGD. The  have the\nup-to-date list.",
            "markdown"
        ],
        [
            "Next, we need to update our training loop. We\u2019ll add an outer loop to exhaust\nthe data loader every epoch and then write the GAN training code:",
            "markdown"
        ],
        [
            "for (int64_t epoch = 1; epoch &lt;= kNumberOfEpochs; ++epoch) {\n  int64_t batch_index = 0;\n  for (torch::data::Example&lt;&gt;&amp; batch : *data_loader) {\n    // Train discriminator with real images.\n    discriminator-&gt;zero_grad();\n    torch::Tensor real_images = batch.data;\n    torch::Tensor real_labels = torch::empty(batch.data.size(0)).uniform_(0.8, 1.0);\n    torch::Tensor real_output = discriminator-&gt;forward(real_images);\n    torch::Tensor d_loss_real = torch::binary_cross_entropy(real_output, real_labels);\n    d_loss_real.backward();\n\n    // Train discriminator with fake images.\n    torch::Tensor noise = torch::randn({batch.data.size(0), kNoiseSize, 1, 1});\n    torch::Tensor fake_images = generator-&gt;forward(noise);\n    torch::Tensor fake_labels = torch::zeros(batch.data.size(0));\n    torch::Tensor fake_output = discriminator-&gt;forward(fake_images.detach());\n    torch::Tensor d_loss_fake = torch::binary_cross_entropy(fake_output, fake_labels);\n    d_loss_fake.backward();\n\n    torch::Tensor d_loss = d_loss_real + d_loss_fake;\n    discriminator_optimizer.step();\n\n    // Train generator.\n    generator-&gt;zero_grad();\n    fake_labels.fill_(1);\n    fake_output = discriminator-&gt;forward(fake_images);\n    torch::Tensor g_loss = torch::binary_cross_entropy(fake_output, fake_labels);\n    g_loss.backward();\n    generator_optimizer.step();\n\n    std::printf(\n        \"\\r[%2ld/%2ld][%3ld/%3ld] D_loss: %.4f | G_loss: %.4f\",\n        epoch,\n        kNumberOfEpochs,\n        ++batch_index,\n        batches_per_epoch,\n        d_loss.item&lt;float&gt;(),\n        g_loss.item&lt;float&gt;());\n  }\n}",
            "code"
        ],
        [
            "Above, we first evaluate the discriminator on real images, for which it should\nassign a high probability. For this, we use\ntorch::empty(batch.data.size(0)).uniform_(0.8, 1.0) as the target\nprobabilities.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "We pick random values uniformly distributed between 0.8 and 1.0 instead of 1.0\neverywhere in order to make the discriminator training more robust. This trick\nis called <em>label smoothing</em>.",
            "markdown"
        ],
        [
            "Before evaluating the discriminator, we zero out the gradients of its\nparameters. After computing the loss, we back-propagate it through the network by\ncalling d_loss.backward() to compute new gradients. We repeat this spiel for\nthe fake images. Instead of using images from the dataset, we let the generator\ncreate fake images for this by feeding it a batch of random noise. We then\nforward those fake images to the discriminator. This time, we want the\ndiscriminator to emit low probabilities, ideally all zeros. Once we have\ncomputed the discriminator loss for both the batch of real and the batch of fake\nimages, we can progress the discriminator\u2019s optimizer by one step in order to\nupdate its parameters.",
            "markdown"
        ],
        [
            "To train the generator, we again first zero its gradients, and then re-evaluate\nthe discriminator on the fake images. However, this time we want the\ndiscriminator to assign probabilities very close to one, which would indicate\nthat the generator can produce images that fool the discriminator into thinking\nthey are actually real (from the dataset). For this, we fill the fake_labels\ntensor with all ones. We finally step the generator\u2019s optimizer to also update\nits parameters.",
            "markdown"
        ],
        [
            "We should now be ready to train our model on the CPU. We don\u2019t have any code yet\nto capture state or sample outputs, but we\u2019ll add this in just a moment. For\nnow, let\u2019s just observe that our model is doing <em>something</em> \u2013 we\u2019ll later\nverify based on the generated images whether this something is meaningful.\nRe-building and running should print something like:",
            "markdown"
        ],
        [
            "root@3c0711f20896:/home/build# make &amp;&amp; ./dcgan\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcga\n[ 1/10][100/938] D_loss: 0.6876 | G_loss: 4.1304\n[ 1/10][200/938] D_loss: 0.3776 | G_loss: 4.3101\n[ 1/10][300/938] D_loss: 0.3652 | G_loss: 4.6626\n[ 1/10][400/938] D_loss: 0.8057 | G_loss: 2.2795\n[ 1/10][500/938] D_loss: 0.3531 | G_loss: 4.4452\n[ 1/10][600/938] D_loss: 0.3501 | G_loss: 5.0811\n[ 1/10][700/938] D_loss: 0.3581 | G_loss: 4.5623\n[ 1/10][800/938] D_loss: 0.6423 | G_loss: 1.7385\n[ 1/10][900/938] D_loss: 0.3592 | G_loss: 4.7333\n[ 2/10][100/938] D_loss: 0.4660 | G_loss: 2.5242\n[ 2/10][200/938] D_loss: 0.6364 | G_loss: 2.0886\n[ 2/10][300/938] D_loss: 0.3717 | G_loss: 3.8103\n[ 2/10][400/938] D_loss: 1.0201 | G_loss: 1.3544\n[ 2/10][500/938] D_loss: 0.4522 | G_loss: 2.6545\n...",
            "code"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Moving to the GPU": [
        [
            "While our current script can run just fine on the CPU, we all know convolutions\nare a lot faster on GPU. Let\u2019s quickly discuss how we can move our training onto\nthe GPU. We\u2019ll need to do two things for this: pass a GPU device specification\nto tensors we allocate ourselves, and explicitly copy any other tensors onto the\nGPU via the to() method all tensors and modules in the C++ frontend have.\nThe simplest way to achieve both is to create an instance of torch::Device\nat the top level of our training script, and then pass that device to tensor\nfactory functions like torch::zeros as well as the to() method. We can\nstart by doing this with a CPU device:",
            "markdown"
        ],
        [
            "// Place this somewhere at the top of your training script.\ntorch::Device device(torch::kCPU);",
            "code"
        ],
        [
            "New tensor allocations like",
            "markdown"
        ],
        [
            "torch::Tensor fake_labels = torch::zeros(batch.data.size(0));",
            "code"
        ],
        [
            "should be updated to take the device as the last argument:",
            "markdown"
        ],
        [
            "torch::Tensor fake_labels = torch::zeros(batch.data.size(0), device);",
            "code"
        ],
        [
            "For tensors whose creation is not in our hands, like those coming from the MNIST\ndataset, we must insert explicit to() calls. This means",
            "markdown"
        ],
        [
            "torch::Tensor real_images = batch.data;",
            "code"
        ],
        [
            "becomes",
            "markdown"
        ],
        [
            "torch::Tensor real_images = batch.data.to(device);",
            "code"
        ],
        [
            "and also our model parameters should be moved to the correct device:",
            "markdown"
        ],
        [
            "generator-&gt;to(device);\ndiscriminator-&gt;to(device);",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "If a tensor already lives on the device supplied to to(), the call is a\nno-op. No extra copy is made.",
            "markdown"
        ],
        [
            "At this point, we\u2019ve just made our previous CPU-residing code more explicit.\nHowever, it is now also very easy to change the device to a CUDA device:",
            "markdown"
        ],
        [
            "torch::Device device(torch::kCUDA)",
            "code"
        ],
        [
            "And now all tensors will live on the GPU, calling into fast CUDA kernels for all\noperations, without us having to change any downstream code. If we wanted to\nspecify a particular device index, it could be passed as the second argument to\nthe Device constructor. If we wanted different tensors to live on different\ndevices, we could pass separate device instances (for example one on CUDA device\n0 and the other on CUDA device 1). We can even do this configuration\ndynamically, which is often useful to make our training scripts more portable:",
            "markdown"
        ],
        [
            "torch::Device device = torch::kCPU;\nif (torch::cuda::is_available()) {\n  std::cout &lt;&lt; \"CUDA is available! Training on GPU.\" &lt;&lt; std::endl;\n  device = torch::kCUDA;\n}",
            "code"
        ],
        [
            "or even",
            "markdown"
        ],
        [
            "torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU);",
            "code"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Checkpointing and Recovering the Training State": [
        [
            "The last augmentation we should make to our training script is to periodically\nsave the state of our model parameters, the state of our optimizers as well as a\nfew generated image samples. If our computer were to crash in the middle of the\ntraining procedure, the first two will allow us to restore the training state.\nFor long-lasting training sessions, this is absolutely essential. Fortunately,\nthe C++ frontend provides an API to serialize and deserialize both model and\noptimizer state, as well as individual tensors.",
            "markdown"
        ],
        [
            "The core API for this is torch::save(thing,filename) and\ntorch::load(thing,filename), where thing could be a\ntorch::nn::Module subclass or an optimizer instance like the Adam object\nwe have in our training script. Let\u2019s update our training loop to checkpoint the\nmodel and optimizer state at a certain interval:",
            "markdown"
        ],
        [
            "if (batch_index % kCheckpointEvery == 0) {\n  // Checkpoint the model and optimizer state.\n  torch::save(generator, \"generator-checkpoint.pt\");\n  torch::save(generator_optimizer, \"generator-optimizer-checkpoint.pt\");\n  torch::save(discriminator, \"discriminator-checkpoint.pt\");\n  torch::save(discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\");\n  // Sample the generator and save the images.\n  torch::Tensor samples = generator-&gt;forward(torch::randn({8, kNoiseSize, 1, 1}, device));\n  torch::save((samples + 1.0) / 2.0, torch::str(\"dcgan-sample-\", checkpoint_counter, \".pt\"));\n  std::cout &lt;&lt; \"\\n-&gt; checkpoint \" &lt;&lt; ++checkpoint_counter &lt;&lt; '\\n';\n}",
            "code"
        ],
        [
            "where kCheckpointEvery is an integer set to something like 100 to\ncheckpoint every 100 batches, and checkpoint_counter is a counter bumped\nevery time we make a checkpoint.",
            "markdown"
        ],
        [
            "To restore the training state, you can add lines like these after all models and\noptimizers are created, but before the training loop:",
            "markdown"
        ],
        [
            "torch::optim::Adam generator_optimizer(\n    generator-&gt;parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5));\ntorch::optim::Adam discriminator_optimizer(\n    discriminator-&gt;parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5));\n\nif (kRestoreFromCheckpoint) {\n  torch::load(generator, \"generator-checkpoint.pt\");\n  torch::load(generator_optimizer, \"generator-optimizer-checkpoint.pt\");\n  torch::load(discriminator, \"discriminator-checkpoint.pt\");\n  torch::load(\n      discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\");\n}\n\nint64_t checkpoint_counter = 0;\nfor (int64_t epoch = 1; epoch &lt;= kNumberOfEpochs; ++epoch) {\n  int64_t batch_index = 0;\n  for (torch::data::Example&lt;&gt;&amp; batch : *data_loader) {",
            "code"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Inspecting Generated Images": [
        [
            "Our training script is now complete. We are ready to train our GAN, whether on\nCPU or GPU. To inspect the intermediary output of our training procedure, for\nwhich we added code to periodically save image samples to the\n\"dcgan-sample-xxx.pt\" file, we can write a tiny Python script to load the\ntensors and display them with matplotlib:",
            "markdown"
        ],
        [
            "from __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\n\nimport matplotlib.pyplot as plt\nimport torch\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-i\", \"--sample-file\", required=True)\nparser.add_argument(\"-o\", \"--out-file\", default=\"out.png\")\nparser.add_argument(\"-d\", \"--dimension\", type=int, default=3)\noptions = parser.parse_args()\n\nmodule = torch.jit.load(options.sample_file)\nimages = list(module.parameters())[0]\n\nfor index in range(options.dimension * options.dimension):\n  image = images[index].detach().cpu().reshape(28, 28).mul(255).to(torch.uint8)\n  array = image.numpy()\n  axis = plt.subplot(options.dimension, options.dimension, 1 + index)\n  plt.imshow(array, cmap=\"gray\")\n  axis.get_xaxis().set_visible(False)\n  axis.get_yaxis().set_visible(False)\n\nplt.savefig(options.out_file)\nprint(\"Saved \", options.out_file)",
            "code"
        ],
        [
            "Let\u2019s now train our model for around 30 epochs:",
            "markdown"
        ],
        [
            "root@3c0711f20896:/home/build# make &amp;&amp; ./dcgan                                                                                                                                10:17:57\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan\nCUDA is available! Training on GPU.\n[ 1/30][200/938] D_loss: 0.4953 | G_loss: 4.0195\n-&gt; checkpoint 1\n[ 1/30][400/938] D_loss: 0.3610 | G_loss: 4.8148\n-&gt; checkpoint 2\n[ 1/30][600/938] D_loss: 0.4072 | G_loss: 4.36760\n-&gt; checkpoint 3\n[ 1/30][800/938] D_loss: 0.4444 | G_loss: 4.0250\n-&gt; checkpoint 4\n[ 2/30][200/938] D_loss: 0.3761 | G_loss: 3.8790\n-&gt; checkpoint 5\n[ 2/30][400/938] D_loss: 0.3977 | G_loss: 3.3315\n...\n-&gt; checkpoint 120\n[30/30][938/938] D_loss: 0.3610 | G_loss: 3.8084",
            "code"
        ],
        [
            "And display the images in a plot:",
            "markdown"
        ],
        [
            "root@3c0711f20896:/home/build# python display.py -i dcgan-sample-100.pt\nSaved out.png",
            "code"
        ],
        [
            "Which should look something like this:\n\n<img alt=\"digits\" src=\"../_images/digits.png\"/>",
            "markdown"
        ],
        [
            "Digits! Hooray! Now the ball is in your court: can you improve the model to make\nthe digits look even better?",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Using the PyTorch C++ Frontend->Conclusion": [
        [
            "This tutorial has hopefully given you a digestible digest of the PyTorch C++\nfrontend. A machine learning library like PyTorch by necessity has a very broad\nand extensive API. As such, there are many concepts we did not have time or\nspace to discuss here. However, I encourage you to try out the API, and consult\n and in particular the\n section when\nyou get stuck. Also, remember that you can expect the C++ frontend to follow the\ndesign and semantics of the Python frontend whenever we could make this\npossible, so you can leverage this fact to increase your learning rate.",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "You can find the full source code presented in this tutorial .",
            "markdown"
        ],
        [
            "As always, if you run into any problems or have questions, you can use our\n or  to get in touch.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Dynamic Parallelism in TorchScript": [
        [
            "In this tutorial, we introduce the syntax for doing <em>dynamic inter-op parallelism</em>\nin TorchScript. This parallelism has the following properties:",
            "markdown"
        ],
        [
            "dynamic - The number of parallel tasks created and their workload can depend on the control flow of the program.",
            "markdown"
        ],
        [
            "inter-op - The parallelism is concerned with running TorchScript program fragments in parallel. This is distinct from <em>intra-op parallelism</em>, which is concerned with splitting up individual operators and running subsets of the operator\u2019s work in parallel.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Dynamic Parallelism in TorchScript->Basic Syntax": [
        [
            "The two important APIs for dynamic parallelism are:",
            "markdown"
        ],
        [
            "torch.jit.fork(fn : Callable[..., T], *args, **kwargs) -&gt; torch.jit.Future[T]",
            "markdown"
        ],
        [
            "torch.jit.wait(fut : torch.jit.Future[T]) -&gt; T",
            "markdown"
        ],
        [
            "A good way to demonstrate how these work is by way of an example:",
            "markdown"
        ],
        [
            "import torch\n\ndef foo(x):\n    return torch.neg(x)\n\n@torch.jit.script\ndef example(x):\n    # Call `foo` using parallelism:\n    # First, we \"fork\" off a task. This task will run `foo` with argument `x`\n    future = torch.jit.fork(foo, x)\n\n    # Call `foo` normally\n    x_normal = foo(x)\n\n    # Second, we \"wait\" on the task. Since the task may be running in\n    # parallel, we have to \"wait\" for its result to become available.\n    # Notice that by having lines of code between the \"fork()\" and \"wait()\"\n    # call for a given Future, we can overlap computations so that they\n    # run in parallel.\n    x_parallel = torch.jit.wait(future)\n\n    return x_normal, x_parallel\n\nprint(example(torch.ones(1))) # (-1., -1.)",
            "code"
        ],
        [
            "fork() takes the callable fn and arguments to that callable args\nand kwargs and creates an asynchronous task for the execution of fn.\nfn can be a function, method, or Module instance. fork() returns a\nreference to the value of the result of this execution, called a Future.\nBecause fork returns immediately after creating the async task, fn may\nnot have been executed by the time the line of code after the fork() call\nis executed. Thus, wait() is used to wait for the async task to complete\nand return the value.",
            "markdown"
        ],
        [
            "These constructs can be used to overlap the execution of statements within a\nfunction (shown in the worked example section) or be composed with other language\nconstructs like loops:",
            "markdown"
        ],
        [
            "import torch\nfrom typing import List\n\ndef foo(x):\n    return torch.neg(x)\n\n@torch.jit.script\ndef example(x):\n    futures : List[torch.jit.Future[torch.Tensor]] = []\n    for _ in range(100):\n        futures.append(torch.jit.fork(foo, x))\n\n    results = []\n    for future in futures:\n        results.append(torch.jit.wait(future))\n\n    return torch.sum(torch.stack(results))\n\nprint(example(torch.ones([])))",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "When we initialized an empty list of Futures, we needed to add an explicit\ntype annotation to futures. In TorchScript, empty containers default\nto assuming they contain Tensor values, so we annotate the list constructor\n# as being of type List[torch.jit.Future[torch.Tensor]]",
            "markdown"
        ],
        [
            "This example uses fork() to launch 100 instances of the function foo,\nwaits on the 100 tasks to complete, then sums the results, returning -100.0.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Dynamic Parallelism in TorchScript->Applied Example: Ensemble of Bidirectional LSTMs": [
        [
            "Let\u2019s try to apply parallelism to a more realistic example and see what sort\nof performance we can get out of it. First, let\u2019s define the baseline model: an\nensemble of bidirectional LSTM layers.",
            "markdown"
        ],
        [
            "import torch, time\n\n# In RNN parlance, the dimensions we care about are:\n# # of time-steps (T)\n# Batch size (B)\n# Hidden size/number of \"channels\" (C)\nT, B, C = 50, 50, 1024\n\n# A module that defines a single \"bidirectional LSTM\". This is simply two\n# LSTMs applied to the same sequence, but one in reverse\nclass BidirectionalRecurrentLSTM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cell_f = torch.nn.LSTM(input_size=C, hidden_size=C)\n        self.cell_b = torch.nn.LSTM(input_size=C, hidden_size=C)\n\n    def forward(self, x : torch.Tensor) -&gt; torch.Tensor:\n        # Forward layer\n        output_f, _ = self.cell_f(x)\n\n        # Backward layer. Flip input in the time dimension (dim 0), apply the\n        # layer, then flip the outputs in the time dimension\n        x_rev = torch.flip(x, dims=[0])\n        output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n        output_b_rev = torch.flip(output_b, dims=[0])\n\n        return torch.cat((output_f, output_b_rev), dim=2)\n\n\n# An \"ensemble\" of `BidirectionalRecurrentLSTM` modules. The modules in the\n# ensemble are run one-by-one on the same input then their results are\n# stacked and summed together, returning the combined result.\nclass LSTMEnsemble(torch.nn.Module):\n    def __init__(self, n_models):\n        super().__init__()\n        self.n_models = n_models\n        self.models = torch.nn.ModuleList([\n            BidirectionalRecurrentLSTM() for _ in range(self.n_models)])\n\n    def forward(self, x : torch.Tensor) -&gt; torch.Tensor:\n        results = []\n        for model in self.models:\n            results.append(model(x))\n        return torch.stack(results).sum(dim=0)\n\n# For a head-to-head comparison to what we're going to do with fork/wait, let's\n# instantiate the model and compile it with TorchScript\nens = torch.jit.script(LSTMEnsemble(n_models=4))\n\n# Normally you would pull this input out of an embedding table, but for the\n# purpose of this demo let's just use random data.\nx = torch.rand(T, B, C)\n\n# Let's run the model once to warm up things like the memory allocator\nens(x)\n\nx = torch.rand(T, B, C)\n\n# Let's see how fast it runs!\ns = time.time()\nens(x)\nprint('Inference took', time.time() - s, ' seconds')",
            "code"
        ],
        [
            "On my machine, this network runs in 2.05 seconds. We can do a lot better!",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Dynamic Parallelism in TorchScript->Parallelizing Forward and Backward Layers": [
        [
            "A very simple thing we can do is parallelize the forward and backward layers\nwithin BidirectionalRecurrentLSTM. For this, the structure of the computation\nis static, so we don\u2019t actually even need any loops. Let\u2019s rewrite the forward\nmethod of BidirectionalRecurrentLSTM like so:",
            "markdown"
        ],
        [
            "def forward(self, x : torch.Tensor) -&gt; torch.Tensor:\n    # Forward layer - fork() so this can run in parallel to the backward\n    # layer\n    future_f = torch.jit.fork(self.cell_f, x)\n\n    # Backward layer. Flip input in the time dimension (dim 0), apply the\n    # layer, then flip the outputs in the time dimension\n    x_rev = torch.flip(x, dims=[0])\n    output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n    output_b_rev = torch.flip(output_b, dims=[0])\n\n    # Retrieve the output from the forward layer. Note this needs to happen\n    # *after* the stuff we want to parallelize with\n    output_f, _ = torch.jit.wait(future_f)\n\n    return torch.cat((output_f, output_b_rev), dim=2)",
            "code"
        ],
        [
            "In this example, forward() delegates execution of cell_f to another thread,\nwhile it continues to execute cell_b. This causes the execution of both the\ncells to be overlapped with each other.",
            "markdown"
        ],
        [
            "Running the script again with this simple modification yields a runtime of\n1.71 seconds for an improvement of 17%!",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Dynamic Parallelism in TorchScript->Aside: Visualizing Parallelism": [
        [
            "We\u2019re not done optimizing our model but it\u2019s worth introducing the tooling we\nhave for visualizing performance. One important tool is the .",
            "markdown"
        ],
        [
            "Let\u2019s use the profiler along with the Chrome trace export functionality to\nvisualize the performance of our parallelized model:",
            "markdown"
        ],
        [
            "with torch.autograd.profiler.profile() as prof:\n    ens(x)\nprof.export_chrome_trace('parallel.json')",
            "code"
        ],
        [
            "This snippet of code will write out a file named parallel.json. If you\nnavigate Google Chrome to chrome://tracing, click the Load button, and\nload in that JSON file, you should see a timeline like the following:\n<img alt=\"https://i.imgur.com/rm5hdG9.png\" src=\"https://i.imgur.com/rm5hdG9.png\"/>",
            "markdown"
        ],
        [
            "The horizontal axis of the timeline represents time and the vertical axis\nrepresents threads of execution. As we can see, we are running two lstm\ninstances at a time. This is the result of our hard work parallelizing the\nbidirectional layers!",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Dynamic Parallelism in TorchScript->Parallelizing Models in the Ensemble": [
        [
            "You may have noticed that there is a further parallelization opportunity in our\ncode: we can also run the models contained in LSTMEnsemble in parallel with\neach other. The way to do that is simple enough, this is how we should change\nthe forward method of LSTMEnsemble:",
            "markdown"
        ],
        [
            "def forward(self, x : torch.Tensor) -&gt; torch.Tensor:\n    # Launch tasks for each model\n    futures : List[torch.jit.Future[torch.Tensor]] = []\n    for model in self.models:\n        futures.append(torch.jit.fork(model, x))\n\n    # Collect the results from the launched tasks\n    results : List[torch.Tensor] = []\n    for future in futures:\n        results.append(torch.jit.wait(future))\n\n    return torch.stack(results).sum(dim=0)",
            "code"
        ],
        [
            "Or, if you value brevity, we can use list comprehensions:",
            "markdown"
        ],
        [
            "def forward(self, x : torch.Tensor) -&gt; torch.Tensor:\n    futures = [torch.jit.fork(model, x) for model in self.models]\n    results = [torch.jit.wait(fut) for fut in futures]\n    return torch.stack(results).sum(dim=0)",
            "code"
        ],
        [
            "Like described in the intro, we\u2019ve used loops to fork off tasks for each of the\nmodels in our ensemble. We\u2019ve then used another loop to wait for all of the\ntasks to be completed. This provides even more overlap of computation.",
            "markdown"
        ],
        [
            "With this small update, the script runs in 1.4 seconds, for a total speedup\nof 32%! Pretty good for two lines of code.",
            "markdown"
        ],
        [
            "We can also use the Chrome tracer again to see where\u2019s going on:\n<img alt=\"https://i.imgur.com/kA0gyQm.png\" src=\"https://i.imgur.com/kA0gyQm.png\"/>",
            "markdown"
        ],
        [
            "We can now see that all LSTM instances are being run fully in parallel.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Dynamic Parallelism in TorchScript->Conclusion": [
        [
            "In this tutorial, we learned about fork() and wait(), the basic APIs\nfor doing dynamic, inter-op parallelism in TorchScript. We saw a few typical\nusage patterns for using these functions to parallelize the execution of\nfunctions, methods, or Modules in TorchScript code. Finally, we worked through\nan example of optimizing a model using this technique and explored the performance\nmeasurement and visualization tooling available in PyTorch.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Autograd in C++ Frontend": [
        [
            "The autograd package is crucial for building highly flexible and dynamic neural\nnetworks in PyTorch. Most of the autograd APIs in PyTorch Python frontend are also available\nin C++ frontend, allowing easy translation of autograd code from Python to C++.",
            "markdown"
        ],
        [
            "In this tutorial explore several examples of doing autograd in PyTorch C++ frontend.\nNote that this tutorial assumes that you already have a basic understanding of\nautograd in Python frontend. If that\u2019s not the case, please first read\n.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Autograd in C++ Frontend->Basic autograd operations": [
        [
            "(Adapted from )",
            "markdown"
        ],
        [
            "Create a tensor and set torch::requires_grad() to track computation with it",
            "markdown"
        ],
        [
            "auto x = torch::ones({2, 2}, torch::requires_grad());\nstd::cout &lt;&lt; x &lt;&lt; std::endl;",
            "code"
        ],
        [
            "Out:",
            "markdown"
        ],
        [
            "1 1\n1 1\n[ CPUFloatType{2,2} ]",
            "code"
        ],
        [
            "Do a tensor operation:",
            "markdown"
        ],
        [
            "auto y = x + 2;\nstd::cout &lt;&lt; y &lt;&lt; std::endl;",
            "code"
        ],
        [
            "Out:",
            "markdown"
        ],
        [
            " 3  3\n 3  3\n[ CPUFloatType{2,2} ]",
            "code"
        ],
        [
            "y was created as a result of an operation, so it has a grad_fn.",
            "markdown"
        ],
        [
            "std::cout &lt;&lt; y.grad_fn()-&gt;name() &lt;&lt; std::endl;",
            "code"
        ],
        [
            "Out:",
            "markdown"
        ],
        [
            "AddBackward1",
            "code"
        ],
        [
            "Do more operations on y",
            "markdown"
        ],
        [
            "auto z = y * y * 3;\nauto out = z.mean();\n\nstd::cout &lt;&lt; z &lt;&lt; std::endl;\nstd::cout &lt;&lt; z.grad_fn()-&gt;name() &lt;&lt; std::endl;\nstd::cout &lt;&lt; out &lt;&lt; std::endl;\nstd::cout &lt;&lt; out.grad_fn()-&gt;name() &lt;&lt; std::endl;",
            "code"
        ],
        [
            "Out:",
            "markdown"
        ],
        [
            " 27  27\n 27  27\n[ CPUFloatType{2,2} ]\nMulBackward1\n27\n[ CPUFloatType{} ]\nMeanBackward0",
            "code"
        ],
        [
            ".requires_grad_( ... ) changes an existing tensor\u2019s requires_grad flag in-place.",
            "markdown"
        ],
        [
            "auto a = torch::randn({2, 2});\na = ((a * 3) / (a - 1));\nstd::cout &lt;&lt; a.requires_grad() &lt;&lt; std::endl;\n\na.requires_grad_(true);\nstd::cout &lt;&lt; a.requires_grad() &lt;&lt; std::endl;\n\nauto b = (a * a).sum();\nstd::cout &lt;&lt; b.grad_fn()-&gt;name() &lt;&lt; std::endl;",
            "code"
        ],
        [
            "Out:",
            "markdown"
        ],
        [
            "false\ntrue\nSumBackward0",
            "code"
        ],
        [
            "Let\u2019s backprop now. Because out contains a single scalar, out.backward()\nis equivalent to out.backward(torch::tensor(1.)).",
            "markdown"
        ],
        [
            "out.backward();",
            "code"
        ],
        [
            "Print gradients d(out)/dx",
            "markdown"
        ],
        [
            "std::cout &lt;&lt; x.grad() &lt;&lt; std::endl;",
            "code"
        ],
        [
            "Out:",
            "markdown"
        ],
        [
            " 4.5000  4.5000\n 4.5000  4.5000\n[ CPUFloatType{2,2} ]",
            "code"
        ],
        [
            "You should have got a matrix of 4.5. For explanations on how we arrive at this value,\nplease see .",
            "markdown"
        ],
        [
            "Now let\u2019s take a look at an example of vector-Jacobian product:",
            "markdown"
        ],
        [
            "x = torch::randn(3, torch::requires_grad());\n\ny = x * 2;\nwhile (y.norm().item&lt;double&gt;() &lt; 1000) {\n  y = y * 2;\n}\n\nstd::cout &lt;&lt; y &lt;&lt; std::endl;\nstd::cout &lt;&lt; y.grad_fn()-&gt;name() &lt;&lt; std::endl;",
            "code"
        ],
        [
            "Out:",
            "markdown"
        ],
        [
            "-1021.4020\n  314.6695\n -613.4944\n[ CPUFloatType{3} ]\nMulBackward1",
            "code"
        ],
        [
            "If we want the vector-Jacobian product, pass the vector to backward as argument:",
            "markdown"
        ],
        [
            "auto v = torch::tensor({0.1, 1.0, 0.0001}, torch::kFloat);\ny.backward(v);\n\nstd::cout &lt;&lt; x.grad() &lt;&lt; std::endl;",
            "code"
        ],
        [
            "Out:",
            "markdown"
        ],
        [
            "  102.4000\n 1024.0000\n    0.1024\n[ CPUFloatType{3} ]",
            "code"
        ],
        [
            "You can also stop autograd from tracking history on tensors that require gradients\neither by putting torch::NoGradGuard in a code block",
            "markdown"
        ],
        [
            "std::cout &lt;&lt; x.requires_grad() &lt;&lt; std::endl;\nstd::cout &lt;&lt; x.pow(2).requires_grad() &lt;&lt; std::endl;\n\n{\n  torch::NoGradGuard no_grad;\n  std::cout &lt;&lt; x.pow(2).requires_grad() &lt;&lt; std::endl;\n}",
            "code"
        ],
        [
            "Out:",
            "markdown"
        ],
        [
            "true\ntrue\nfalse",
            "code"
        ],
        [
            "Or by using .detach() to get a new tensor with the same content but that does\nnot require gradients:",
            "markdown"
        ],
        [
            "std::cout &lt;&lt; x.requires_grad() &lt;&lt; std::endl;\ny = x.detach();\nstd::cout &lt;&lt; y.requires_grad() &lt;&lt; std::endl;\nstd::cout &lt;&lt; x.eq(y).all().item&lt;bool&gt;() &lt;&lt; std::endl;",
            "code"
        ],
        [
            "Out:",
            "markdown"
        ],
        [
            "true\nfalse\ntrue",
            "code"
        ],
        [
            "For more information on C++ tensor autograd APIs such as grad / requires_grad /\nis_leaf / backward / detach / detach_ / register_hook / retain_grad,\nplease see .",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Autograd in C++ Frontend->Computing higher-order gradients in C++": [
        [
            "One of the applications of higher-order gradients is calculating gradient penalty.\nLet\u2019s see an example of it using torch::autograd::grad:",
            "markdown"
        ],
        [
            "#include &lt;torch/torch.h&gt;\n\nauto model = torch::nn::Linear(4, 3);\n\nauto input = torch::randn({3, 4}).requires_grad_(true);\nauto output = model(input);\n\n// Calculate loss\nauto target = torch::randn({3, 3});\nauto loss = torch::nn::MSELoss()(output, target);\n\n// Use norm of gradients as penalty\nauto grad_output = torch::ones_like(output);\nauto gradient = torch::autograd::grad({output}, {input}, /*grad_outputs=*/{grad_output}, /*create_graph=*/true)[0];\nauto gradient_penalty = torch::pow((gradient.norm(2, /*dim=*/1) - 1), 2).mean();\n\n// Add gradient penalty to loss\nauto combined_loss = loss + gradient_penalty;\ncombined_loss.backward();\n\nstd::cout &lt;&lt; input.grad() &lt;&lt; std::endl;",
            "code"
        ],
        [
            "Out:",
            "markdown"
        ],
        [
            "-0.1042 -0.0638  0.0103  0.0723\n-0.2543 -0.1222  0.0071  0.0814\n-0.1683 -0.1052  0.0355  0.1024\n[ CPUFloatType{3,4} ]",
            "code"
        ],
        [
            "Please see the documentation for torch::autograd::backward\n()\nand torch::autograd::grad\n()\nfor more information on how to use them.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Autograd in C++ Frontend->Using custom autograd function in C++": [
        [
            "(Adapted from )",
            "markdown"
        ],
        [
            "Adding a new elementary operation to torch::autograd requires implementing a new torch::autograd::Function\nsubclass for each operation. torch::autograd::Function s are what torch::autograd\nuses to compute the results and gradients, and encode the operation history. Every\nnew function requires you to implement 2 methods: forward and backward, and\nplease see \nfor the detailed requirements.",
            "markdown"
        ],
        [
            "Below you can find code for a Linear function from torch::nn:",
            "markdown"
        ],
        [
            "#include &lt;torch/torch.h&gt;\n\nusing namespace torch::autograd;\n\n// Inherit from Function\nclass LinearFunction : public Function&lt;LinearFunction&gt; {\n public:\n  // Note that both forward and backward are static functions\n\n  // bias is an optional argument\n  static torch::Tensor forward(\n      AutogradContext *ctx, torch::Tensor input, torch::Tensor weight, torch::Tensor bias = torch::Tensor()) {\n    ctx-&gt;save_for_backward({input, weight, bias});\n    auto output = input.mm(weight.t());\n    if (bias.defined()) {\n      output += bias.unsqueeze(0).expand_as(output);\n    }\n    return output;\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    auto saved = ctx-&gt;get_saved_variables();\n    auto input = saved[0];\n    auto weight = saved[1];\n    auto bias = saved[2];\n\n    auto grad_output = grad_outputs[0];\n    auto grad_input = grad_output.mm(weight);\n    auto grad_weight = grad_output.t().mm(input);\n    auto grad_bias = torch::Tensor();\n    if (bias.defined()) {\n      grad_bias = grad_output.sum(0);\n    }\n\n    return {grad_input, grad_weight, grad_bias};\n  }\n};",
            "code"
        ],
        [
            "Then, we can use the LinearFunction in the following way:",
            "markdown"
        ],
        [
            "auto x = torch::randn({2, 3}).requires_grad_();\nauto weight = torch::randn({4, 3}).requires_grad_();\nauto y = LinearFunction::apply(x, weight);\ny.sum().backward();\n\nstd::cout &lt;&lt; x.grad() &lt;&lt; std::endl;\nstd::cout &lt;&lt; weight.grad() &lt;&lt; std::endl;",
            "code"
        ],
        [
            "Out:",
            "markdown"
        ],
        [
            " 0.5314  1.2807  1.4864\n 0.5314  1.2807  1.4864\n[ CPUFloatType{2,3} ]\n 3.7608  0.9101  0.0073\n 3.7608  0.9101  0.0073\n 3.7608  0.9101  0.0073\n 3.7608  0.9101  0.0073\n[ CPUFloatType{4,3} ]",
            "code"
        ],
        [
            "Here, we give an additional example of a function that is parametrized by non-tensor arguments:",
            "markdown"
        ],
        [
            "#include &lt;torch/torch.h&gt;\n\nusing namespace torch::autograd;\n\nclass MulConstant : public Function&lt;MulConstant&gt; {\n public:\n  static torch::Tensor forward(AutogradContext *ctx, torch::Tensor tensor, double constant) {\n    // ctx is a context object that can be used to stash information\n    // for backward computation\n    ctx-&gt;saved_data[\"constant\"] = constant;\n    return tensor * constant;\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    // We return as many input gradients as there were arguments.\n    // Gradients of non-tensor arguments to forward must be `torch::Tensor()`.\n    return {grad_outputs[0] * ctx-&gt;saved_data[\"constant\"].toDouble(), torch::Tensor()};\n  }\n};",
            "code"
        ],
        [
            "Then, we can use the MulConstant in the following way:",
            "markdown"
        ],
        [
            "auto x = torch::randn({2}).requires_grad_();\nauto y = MulConstant::apply(x, 5.5);\ny.sum().backward();\n\nstd::cout &lt;&lt; x.grad() &lt;&lt; std::endl;",
            "code"
        ],
        [
            "Out:",
            "markdown"
        ],
        [
            " 5.5000\n 5.5000\n[ CPUFloatType{2} ]",
            "code"
        ],
        [
            "For more information on torch::autograd::Function, please see\n.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Autograd in C++ Frontend->Translating autograd code from Python to C++": [
        [
            "On a high level, the easiest way to use autograd in C++ is to have working\nautograd code in Python first, and then translate your autograd code from Python to\nC++ using the following table:",
            "markdown"
        ],
        [
            "After translation, most of your Python autograd code should just work in C++.\nIf that\u2019s not the case, please file a bug report at \nand we will fix it as soon as possible.",
            "markdown"
        ]
    ],
    "torch->Frontend APIs->Autograd in C++ Frontend->Conclusion": [
        [
            "You should now have a good overview of PyTorch\u2019s C++ autograd API.\nYou can find the code examples displayed in this note . As always, if you run into any\nproblems or have questions, you can use our \nor  to get in touch.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Double Backward with Custom Functions": [
        [
            "It is sometimes useful to run backwards twice through backward graph, for\nexample to compute higher-order gradients. It takes an understanding of\nautograd and some care to support double backwards, however. Functions\nthat support performing backward a single time are not necessarily\nequipped to support double backward. In this tutorial we show how to\nwrite a custom autograd function that supports double backward, and\npoint out some things to look out for.",
            "markdown"
        ],
        [
            "When writing a custom autograd function to backward through twice,\nit is important to know when operations performed in a custom function\nare recorded by autograd, when they aren\u2019t, and most importantly, how\n<cite>save_for_backward</cite> works with all of this.",
            "markdown"
        ],
        [
            "Custom functions implicitly affects grad mode in two ways:",
            "markdown"
        ],
        [
            "During forward, autograd does not record any the graph for any\noperations performed within the forward function. When forward\ncompletes, the backward function of the custom function\nbecomes the <cite>grad_fn</cite> of each of the forward\u2019s outputs",
            "markdown"
        ],
        [
            "During backward, autograd records the computation graph used to\ncompute the backward pass if create_graph is specified",
            "markdown"
        ],
        [
            "Next, to understand how <cite>save_for_backward</cite> interacts with the above,\nwe can explore a couple examples:",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Double Backward with Custom Functions->Saving the Inputs": [
        [
            "Consider this simple squaring function. It saves an input tensor\nfor backward. Double backward works automatically when autograd\nis able to record operations in the backward pass, so there is usually\nnothing to worry about when we save an input for backward as\nthe input should have grad_fn if it is a function of any tensor\nthat requires grad. This allows the gradients to be properly propagated.",
            "markdown"
        ],
        [
            "import torch\n\nclass Square(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        # Because we are saving one of the inputs use `save_for_backward`\n        # Save non-tensors and non-inputs/non-outputs directly on ctx\n        ctx.save_for_backward(x)\n        return x**2\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        # A function support double backward automatically if autograd\n        # is able to record the computations performed in backward\n        x, = ctx.saved_tensors\n        return grad_out * 2 * x\n\n# Use double precision because finite differencing method magnifies errors\nx = torch.rand(3, 3, requires_grad=True, dtype=torch.double)\ntorch.autograd.gradcheck(Square.apply, x)\n# Use gradcheck to verify second-order derivatives\ntorch.autograd.gradgradcheck(Square.apply, x)",
            "code"
        ],
        [
            "We can use torchviz to visualize the graph to see why this works",
            "markdown"
        ],
        [
            "import torchviz\n\nx = torch.tensor(1., requires_grad=True).clone()\nout = Square.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), {\"grad_x\": grad_x, \"x\": x, \"out\": out})",
            "code"
        ],
        [
            "We can see that the gradient wrt to x, is itself a function of x (dout/dx = 2x)\nAnd the graph of this function has been properly constructed",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Double Backward with Custom Functions->Saving the Outputs": [
        [
            "A slight variation on the previous example is to save an output\ninstead of input. The mechanics are similar because outputs are also\nassociated with a grad_fn.",
            "markdown"
        ],
        [
            "class Exp(torch.autograd.Function):\n    # Simple case where everything goes well\n    @staticmethod\n    def forward(ctx, x):\n        # This time we save the output\n        result = torch.exp(x)\n        # Note that we should use `save_for_backward` here when\n        # the tensor saved is an ouptut (or an input).\n        ctx.save_for_backward(result)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        result, = ctx.saved_tensors\n        return result * grad_out\n\nx = torch.tensor(1., requires_grad=True, dtype=torch.double).clone()\n# Validate our gradients using gradcheck\ntorch.autograd.gradcheck(Exp.apply, x)\ntorch.autograd.gradgradcheck(Exp.apply, x)",
            "code"
        ],
        [
            "Use torchviz to visualize the graph:",
            "markdown"
        ],
        [
            "out = Exp.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), {\"grad_x\": grad_x, \"x\": x, \"out\": out})",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Double Backward with Custom Functions->Saving Intermediate Results": [
        [
            "A more tricky case is when we need to save an intermediate result.\nWe demonstrate this case by implementing:\n\n\\[sinh(x) := \\frac{e^x - e^{-x}}{2}\n\n\\]",
            "markdown"
        ],
        [
            "Since the derivative of sinh is cosh, it might be useful to reuse\n<cite>exp(x)</cite> and <cite>exp(-x)</cite>, the two intermediate results in forward\nin the backward computation.",
            "markdown"
        ],
        [
            "Intermediate results should not be directly saved and used in backward though.\nBecause forward is performed in no-grad mode, if an intermediate result\nof the forward pass is used to compute gradients in the backward pass\nthe backward graph of the gradients would not include the operations\nthat computed the intermediate result. This leads to incorrect gradients.",
            "markdown"
        ],
        [
            "class Sinh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        expx = torch.exp(x)\n        expnegx = torch.exp(-x)\n        ctx.save_for_backward(expx, expnegx)\n        # In order to be able to save the intermediate results, a trick is to\n        # include them as our outputs, so that the backward graph is constructed\n        return (expx - expnegx) / 2, expx, expnegx\n\n    @staticmethod\n    def backward(ctx, grad_out, _grad_out_exp, _grad_out_negexp):\n        expx, expnegx = ctx.saved_tensors\n        grad_input = grad_out * (expx + expnegx) / 2\n        # We cannot skip accumulating these even though we won't use the outputs\n        # directly. They will be used later in the second backward.\n        grad_input += _grad_out_exp * expx\n        grad_input -= _grad_out_negexp * expnegx\n        return grad_input\n\ndef sinh(x):\n    # Create a wrapper that only returns the first output\n    return Sinh.apply(x)[0]\n\nx = torch.rand(3, 3, requires_grad=True, dtype=torch.double)\ntorch.autograd.gradcheck(sinh, x)\ntorch.autograd.gradgradcheck(sinh, x)",
            "code"
        ],
        [
            "Use torchviz to visualize the graph:",
            "markdown"
        ],
        [
            "out = sinh(x)\ngrad_x, = torch.autograd.grad(out.sum(), x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Double Backward with Custom Functions->Saving Intermediate Results: What not to do": [
        [
            "Now we show what happens when we don\u2019t also return our intermediate\nresults as outputs: <cite>grad_x</cite> would not even have a  backward graph\nbecause it is purely a function <cite>exp</cite> and <cite>expnegx</cite>, which don\u2019t\nrequire grad.",
            "markdown"
        ],
        [
            "class SinhBad(torch.autograd.Function):\n    # This is an example of what NOT to do!\n    @staticmethod\n    def forward(ctx, x):\n        expx = torch.exp(x)\n        expnegx = torch.exp(-x)\n        ctx.expx = expx\n        ctx.expnegx = expnegx\n        return (expx - expnegx) / 2\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        expx = ctx.expx\n        expnegx = ctx.expnegx\n        grad_input = grad_out * (expx + expnegx) / 2\n        return grad_input",
            "code"
        ],
        [
            "Use torchviz to visualize the graph. Notice that <cite>grad_x</cite> is not\npart of the graph!",
            "markdown"
        ],
        [
            "out = SinhBad.apply(x)\ngrad_x, = torch.autograd.grad(out.sum(), x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Double Backward with Custom Functions->When Backward is not Tracked": [
        [
            "Finally, let\u2019s consider an example when it may not be possible for\nautograd to track gradients for a functions backward at all.\nWe can imagine cube_backward to be a function that may require a\nnon-PyTorch library like SciPy or NumPy, or written as a\nC++ extension. The workaround demonstrated here is to create another\ncustom function CubeBackward where you also manually specify the\nbackward of cube_backward!",
            "markdown"
        ],
        [
            "def cube_forward(x):\n    return x**3\n\ndef cube_backward(grad_out, x):\n    return grad_out * 3 * x**2\n\ndef cube_backward_backward(grad_out, sav_grad_out, x):\n    return grad_out * sav_grad_out * 6 * x\n\ndef cube_backward_backward_grad_out(grad_out, x):\n    return grad_out * 3 * x**2\n\nclass Cube(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return cube_forward(x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, = ctx.saved_tensors\n        return CubeBackward.apply(grad_out, x)\n\nclass CubeBackward(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, grad_out, x):\n        ctx.save_for_backward(x, grad_out)\n        return cube_backward(grad_out, x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, sav_grad_out = ctx.saved_tensors\n        dx = cube_backward_backward(grad_out, sav_grad_out, x)\n        dgrad_out = cube_backward_backward_grad_out(grad_out, x)\n        return dgrad_out, dx\n\nx = torch.tensor(2., requires_grad=True, dtype=torch.double)\n\ntorch.autograd.gradcheck(Cube.apply, x)\ntorch.autograd.gradgradcheck(Cube.apply, x)",
            "code"
        ],
        [
            "Use torchviz to visualize the graph:",
            "markdown"
        ],
        [
            "out = Cube.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})",
            "code"
        ],
        [
            "To conclude, whether double backward works for your custom function\nsimply depends on whether the backward pass can be tracked by autograd.\nWith the first two examples we show situations where double backward\nworks out of the box. With the third and fourth examples, we demonstrate\ntechniques that enable a backward function to be tracked, when they\notherwise would not be.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Fusing Convolution and Batch Norm using Custom Function": [
        [
            "Fusing adjacent convolution and batch norm layers together is typically an\ninference-time optimization to improve run-time. It is usually achieved\nby eliminating the batch norm layer entirely and updating the weight\nand bias of the preceding convolution [0]. However, this technique is not\napplicable for training models.",
            "markdown"
        ],
        [
            "In this tutorial, we will show a different technique to fuse the two layers\nthat can be applied during training. Rather than improved runtime, the\nobjective of this optimization is to reduce memory usage.",
            "markdown"
        ],
        [
            "The idea behind this optimization is to see that both convolution and\nbatch norm (as well as many other ops) need to save a copy of their input\nduring forward for the backward pass. For large\nbatch sizes, these saved inputs are responsible for most of your memory usage,\nso being able to avoid allocating another input tensor for every\nconvolution batch norm pair can be a significant reduction.",
            "markdown"
        ],
        [
            "In this tutorial, we avoid this extra allocation by combining convolution\nand batch norm into a single layer (as a custom function). In the forward\nof this combined layer, we perform normal convolution and batch norm as-is,\nwith the only difference being that we will only save the inputs to the convolution.\nTo obtain the input of batch norm, which is necessary to backward through\nit, we recompute convolution forward again during the backward pass.",
            "markdown"
        ],
        [
            "It is important to note that the usage of this optimization is situational.\nThough (by avoiding one buffer saved) we always reduce the memory allocated at\nthe end of the forward pass, there are cases when the <em>peak</em> memory allocated\nmay not actually be reduced. See the final section for more details.",
            "markdown"
        ],
        [
            "For simplicity, in this tutorial we hardcode <cite>bias=False</cite>, <cite>stride=1</cite>, <cite>padding=0</cite>, <cite>dilation=1</cite>,\nand <cite>groups=1</cite> for Conv2D. For BatchNorm2D, we hardcode <cite>eps=1e-3</cite>, <cite>momentum=0.1</cite>,\n<cite>affine=False</cite>, and <cite>track_running_statistics=False</cite>. Another small difference\nis that we add epsilon in the denomator outside of the square root in the computation\nof batch norm.",
            "markdown"
        ],
        [
            "[0] ",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Fusing Convolution and Batch Norm using Custom Function->Backward Formula Implementation for Convolution": [
        [
            "Implementing a custom function requires us to implement the backward\nourselves. In this case, we need both the backward formulas for Conv2D\nand BatchNorm2D. Eventually we\u2019d chain them together in our unified\nbackward function, but below we first implement them as their own\ncustom functions so we can validate their correctness individually",
            "markdown"
        ],
        [
            "import torch\nfrom torch.autograd.function import once_differentiable\nimport torch.nn.functional as F\n\ndef convolution_backward(grad_out, , ):\n    grad_input = (.transpose(0, 1), grad_out.transpose(0, 1)).transpose(0, 1)\n    grad_X = (grad_out, )\n    return grad_X, grad_input\n\nclass Conv2D():\n    @staticmethod\n    def forward(ctx, , ):\n        ctx.save_for_backward(, )\n        return (, )\n\n    # Use @once_differentiable by default unless we intend to double backward\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_out):\n        ,  = ctx.saved_tensors\n        return convolution_backward(grad_out, , )",
            "code"
        ],
        [
            "When testing with gradcheck, it is important to use double precision",
            "markdown"
        ],
        [
            " = (5, 3, 3, 3, requires_grad=True, dtype=)\n = (10, 3, 7, 7, requires_grad=True, dtype=)\n(Conv2D.apply, (, ))",
            "code"
        ],
        [
            "True",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Fusing Convolution and Batch Norm using Custom Function->Backward Formula Implementation for Batch Norm": [
        [
            "Batch Norm has two modes: training and eval mode. In training mode\nthe sample statistics are a function of the inputs. In eval mode,\nwe use the saved running statistics, which are not a function of the inputs.\nThis makes non-training mode\u2019s backward significantly simpler. Below\nwe implement and test only the training mode case.",
            "markdown"
        ],
        [
            "def unsqueeze_all(t):\n    # Helper function to unsqueeze all the dimensions that we reduce over\n    return t[None, :, None, None]\n\ndef batch_norm_backward(grad_out, , sum, sqrt_var, N, eps):\n    # We use the formula: out = (X - mean(X)) / (sqrt(var(X)) + eps)\n    # in batch norm 2d's forward. To simplify our derivation, we follow the\n    # chain rule and compute the gradients as follows before accumulating\n    # them all into a final grad_input.\n    #  1) 'grad of out wrt var(X)' * 'grad of var(X) wrt X'\n    #  2) 'grad of out wrt mean(X)' * 'grad of mean(X) wrt X'\n    #  3) 'grad of out wrt X in the numerator' * 'grad of X wrt X'\n    # We then rewrite the formulas to use as few extra buffers as possible\n    tmp = (( - unsqueeze_all(sum) / N) * grad_out).sum(dim=(0, 2, 3))\n    tmp *= -1\n    d_denom = tmp / (sqrt_var + eps)**2  # d_denom = -num / denom**2\n    # It is useful to delete tensors when you no longer need them with `del`\n    # For example, we could've done `del tmp` here because we won't use it later\n    # In this case, it's not a big difference because tmp only has size of (C,)\n    # The important thing is avoid allocating NCHW-sized tensors unnecessarily\n    d_var = d_denom / (2 * sqrt_var)  # denom = torch.sqrt(var) + eps\n    # Compute d_mean_dx before allocating the final NCHW-sized grad_input buffer\n    d_mean_dx = grad_out / unsqueeze_all(sqrt_var + eps)\n    d_mean_dx = unsqueeze_all(-d_mean_dx.sum(dim=(0, 2, 3)) / N)\n    # d_mean_dx has already been reassigned to a C-sized buffer so no need to worry\n\n    # (1) unbiased_var(x) = ((X - unsqueeze_all(mean))**2).sum(dim=(0, 2, 3)) / (N - 1)\n    grad_input =  * unsqueeze_all(d_var * N)\n    grad_input += unsqueeze_all(-d_var * sum)\n    grad_input *= 2 / ((N - 1) * N)\n    # (2) mean (see above)\n    grad_input += d_mean_dx\n    # (3) Add 'grad_out / &lt;factor&gt;' without allocating an extra buffer\n    grad_input *= unsqueeze_all(sqrt_var + eps)\n    grad_input += grad_out\n    grad_input /= unsqueeze_all(sqrt_var + eps)  # sqrt_var + eps &gt; 0!\n    return grad_input\n\nclass BatchNorm():\n    @staticmethod\n    def forward(ctx, , eps=1e-3):\n        # Don't save keepdim'd values for backward\n        sum = .sum(dim=(0, 2, 3))\n        var = .var(unbiased=True, dim=(0, 2, 3))\n        N = .numel() / .size(1)\n        sqrt_var = (var)\n        ctx.save_for_backward()\n        ctx.eps = eps\n        ctx.sum = sum\n        ctx.N = N\n        ctx.sqrt_var = sqrt_var\n        mean = sum / N\n        denom = sqrt_var + eps\n        out =  - unsqueeze_all(mean)\n        out /= unsqueeze_all(denom)\n        return out\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_out):\n        , = ctx.saved_tensors\n        return batch_norm_backward(grad_out, , ctx.sum, ctx.sqrt_var, ctx.N, ctx.eps)",
            "code"
        ],
        [
            "Testing with gradcheck",
            "markdown"
        ],
        [
            " = (1, 2, 3, 4, requires_grad=True, dtype=)\n(BatchNorm.apply, (,), fast_mode=False)",
            "code"
        ],
        [
            "True",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Fusing Convolution and Batch Norm using Custom Function->Fusing Convolution and BatchNorm": [
        [
            "Now that the bulk of the work has been done, we can combine\nthem together. Note that in (1) we only save a single buffer\nfor backward, but this also means we recompute convolution forward\nin (5). Also see that in (2), (3), (4), and (6), it\u2019s the same\nexact code as the examples above.",
            "markdown"
        ],
        [
            "class FusedConvBN2DFunction():\n    @staticmethod\n    def forward(ctx, , conv_weight, eps=1e-3):\n        assert .ndim == 4  # N, C, H, W\n        # (1) Only need to save this single buffer for backward!\n        ctx.save_for_backward(, conv_weight)\n\n        # (2) Exact same Conv2D forward from example above\n         = (, conv_weight)\n        # (3) Exact same BatchNorm2D forward from example above\n        sum = .sum(dim=(0, 2, 3))\n        var = .var(unbiased=True, dim=(0, 2, 3))\n        N = .numel() / .size(1)\n        sqrt_var = (var)\n        ctx.eps = eps\n        ctx.sum = sum\n        ctx.N = N\n        ctx.sqrt_var = sqrt_var\n        mean = sum / N\n        denom = sqrt_var + eps\n        # Try to do as many things in-place as possible\n        # Instead of `out = (X - a) / b`, doing `out = X - a; out /= b`\n        # avoids allocating one extra NCHW-sized buffer here\n        out =  - unsqueeze_all(mean)\n        out /= unsqueeze_all(denom)\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        , conv_weight, = ctx.saved_tensors\n        # (4) Batch norm backward\n        # (5) We need to recompute conv\n        X_conv_out = (, conv_weight)\n        grad_out = batch_norm_backward(grad_out, X_conv_out, ctx.sum, ctx.sqrt_var,\n                                       ctx.N, ctx.eps)\n        # (6) Conv2d backward\n        grad_X, grad_input = convolution_backward(grad_out, , conv_weight)\n        return grad_X, grad_input, None, None, None, None, None",
            "code"
        ],
        [
            "The next step is to wrap our functional variant in a stateful\n<cite>nn.Module</cite>",
            "markdown"
        ],
        [
            "import torch.nn as nn\nimport math\n\nclass FusedConvBN():\n    def __init__(self, in_channels, out_channels, kernel_size, exp_avg_factor=0.1,\n                 eps=1e-3, =None, dtype=None):\n        super(, self).__init__()\n        factory_kwargs = {'device': , 'dtype': dtype}\n        # Conv parameters\n        weight_shape = (out_channels, in_channels, kernel_size, kernel_size)\n        self.conv_weight = ((*weight_shape, **factory_kwargs))\n        # Batch norm parameters\n        num_features = out_channels\n        self.num_features = num_features\n        self.eps = eps\n        # Initialize\n        self.reset_parameters()\n\n    def forward(self, ):\n        return FusedConvBN2DFunction.apply(, self.conv_weight, self.eps)\n\n    def reset_parameters(self) -&gt; None:\n        (self.conv_weight, =math.sqrt(5))",
            "code"
        ],
        [
            "Use gradcheck to validate the correctness of our backward formula",
            "markdown"
        ],
        [
            " = (5, 3, 3, 3, requires_grad=True, dtype=)\n = (2, 3, 4, 4, requires_grad=True, dtype=)\n(FusedConvBN2DFunction.apply, (, ))",
            "code"
        ],
        [
            "True",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Fusing Convolution and Batch Norm using Custom Function->Testing out our new Layer": [
        [
            "Use FusedConvBN to train a basic network\nThe code below is after some light modifications to the example here:",
            "markdown"
        ],
        [
            "import torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import \n\n# Record memory allocated at the end of the forward pass\nmemory_allocated = [[],[]]\n\nclass Net():\n    def __init__(self, fused=True):\n        super(, self).__init__()\n        self.fused = fused\n        if fused:\n            self.convbn1 = (1, 32, 3)\n            self.convbn2 = (32, 64, 3)\n        else:\n            self.conv1 = (1, 32, 3, 1, bias=False)\n            self.bn1 = (32, affine=False, track_running_stats=False)\n            self.conv2 = (32, 64, 3, 1, bias=False)\n            self.bn2 = (64, affine=False, track_running_stats=False)\n        self.fc1 = (9216, 128)\n        self.dropout = (0.5)\n        self.fc2 = (128, 10)\n\n    def forward(self, x):\n        if self.fused:\n            x = self.convbn1(x)\n        else:\n            x = self.conv1(x)\n            x = self.bn1(x)\n        (x)\n        if self.fused:\n            x = self.convbn2(x)\n        else:\n            x = self.conv2(x)\n            x = self.bn2(x)\n        (x)\n        x = (x, 2)\n        (x)\n        x = x.flatten(1)\n        x = self.fc1(x)\n        x = self.dropout(x)\n        (x)\n        x = self.fc2(x)\n        output = (x, dim=1)\n        if fused:\n            memory_allocated[0].append(())\n        else:\n            memory_allocated[1].append(())\n        return output\n\ndef train(model, , , , epoch):\n    ()\n    for batch_idx, (data, target) in enumerate():\n        data, target = data.to(), target.to()\n        ()\n        output = model(data)\n        loss = (output, target)\n        loss.backward()\n        .step()\n        if batch_idx % 2 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(),\n                100. * batch_idx / len(), loss.item()))\n\ndef test(model, , ):\n    ()\n    test_loss = 0\n    correct = 0\n    # Use inference mode instead of no_grad, for free improved test-time performance\n    with ():\n        for data, target in :\n            data, target = data.to(), target.to()\n            output = model(data)\n            # sum up batch loss\n            test_loss += (output, target, reduction='sum').item()\n            # get the index of the max log-probability\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len()\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(),\n        100. * correct / len()))\n\nuse_cuda = ()\n = (\"cuda\" if use_cuda else \"cpu\")\ntrain_kwargs = {'batch_size': 2048}\ntest_kwargs = {'batch_size': 2048}\n\nif use_cuda:\n    cuda_kwargs = {'num_workers': 1,\n                   'pin_memory': True,\n                   'shuffle': True}\n    train_kwargs.update(cuda_kwargs)\n    test_kwargs.update(cuda_kwargs)\n\n = ([\n    (),\n    ((0.1307,), (0.3081,))\n])\n = ('../data', train=True, download=True,\n                          =)\n = ('../data', train=False,\n                          =)\n = (, **train_kwargs)\n = (, **test_kwargs)",
            "code"
        ],
        [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/9912422 [00:00&lt;?, ?it/s]\n100%|##########| 9912422/9912422 [00:00&lt;00:00, 100193542.52it/s]\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/28881 [00:00&lt;?, ?it/s]\n100%|##########| 28881/28881 [00:00&lt;00:00, 102831658.59it/s]\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/1648877 [00:00&lt;?, ?it/s]\n100%|##########| 1648877/1648877 [00:00&lt;00:00, 25955584.32it/s]\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/4542 [00:00&lt;?, ?it/s]\n100%|##########| 4542/4542 [00:00&lt;00:00, 23606603.18it/s]\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Fusing Convolution and Batch Norm using Custom Function->A Comparison of Memory Usage": [
        [
            "If cuda is enabled, print out memory usage for both <cite>fused=True</cite> and <cite>fused=False</cite>\nFor an example run on RTX 3070, CuDNN 8.0.5: fused peak memory: 1.56GB,\nunfused peak memory: 2.68GB",
            "markdown"
        ],
        [
            "It is important to note that the <em>peak</em> memory usage for this model may vary depending\nthe specific CuDNN convolution algorithm used. For shallower models, it\nmay be possible for the peak memory allocated of the fused model to exceed\nthat of the unfused model! This is because the memory allocated to compute\ncertain CuDNN convolution algorithms can be high enough to \u201chide\u201d the typical peak\nyou would expect to be near the start of the backward pass.",
            "markdown"
        ],
        [
            "For this reason, we also record and display the memory allocated at the end\nof the forward pass as an approximation, and to demonstrate that we indeed\nallocate one fewer buffer per fused conv-bn pair.",
            "markdown"
        ],
        [
            "from statistics import mean\n\ntorch.backends.cudnn.enabled = True\n\nif use_cuda:\n    peak_memory_allocated = []\n\n    for fused in (True, False):\n        (123456)\n\n        model = (fused=fused).to()\n         = ((), lr=1.0)\n         = (, step_size=1, gamma=0.7)\n\n        for epoch in range(1):\n            train(model, , , , epoch)\n            test(model, , )\n            .step()\n        peak_memory_allocated.append(())\n        ()\n    print(\"CuDNN version:\", ())\n    print()\n    print(\"Peak memory allocated:\")\n    print(f\"fused: {peak_memory_allocated[0]/1024**3:.2f}GB, unfused: {peak_memory_allocated[1]/1024**3:.2f}GB\")\n    print(\"Memory allocated at end of forward pass:\")\n    print(f\"fused: {mean(memory_allocated[0])/1024**3:.2f}GB, unfused: {mean(memory_allocated[1])/1024**3:.2f}GB\")",
            "code"
        ],
        [
            "Train Epoch: 0 [0/60000 (0%)]   Loss: 2.352060\nTrain Epoch: 0 [4096/60000 (7%)]        Loss: 7.321198\nTrain Epoch: 0 [8192/60000 (13%)]       Loss: 4.253123\nTrain Epoch: 0 [12288/60000 (20%)]      Loss: 2.916881\nTrain Epoch: 0 [16384/60000 (27%)]      Loss: 2.643569\nTrain Epoch: 0 [20480/60000 (33%)]      Loss: 1.819675\nTrain Epoch: 0 [24576/60000 (40%)]      Loss: 1.546603\nTrain Epoch: 0 [28672/60000 (47%)]      Loss: 1.501737\nTrain Epoch: 0 [32768/60000 (53%)]      Loss: 1.495718\nTrain Epoch: 0 [36864/60000 (60%)]      Loss: 1.421847\nTrain Epoch: 0 [40960/60000 (67%)]      Loss: 1.260746\nTrain Epoch: 0 [45056/60000 (73%)]      Loss: 1.199898\nTrain Epoch: 0 [49152/60000 (80%)]      Loss: 0.951574\nTrain Epoch: 0 [53248/60000 (87%)]      Loss: 0.846121\nTrain Epoch: 0 [57344/60000 (93%)]      Loss: 0.794893\n\nTest set: Average loss: 0.4699, Accuracy: 8564/10000 (86%)\n\nTrain Epoch: 0 [0/60000 (0%)]   Loss: 2.352356\nTrain Epoch: 0 [4096/60000 (7%)]        Loss: 7.323086\nTrain Epoch: 0 [8192/60000 (13%)]       Loss: 4.011035\nTrain Epoch: 0 [12288/60000 (20%)]      Loss: 2.065851\nTrain Epoch: 0 [16384/60000 (27%)]      Loss: 2.220910\nTrain Epoch: 0 [20480/60000 (33%)]      Loss: 1.881398\nTrain Epoch: 0 [24576/60000 (40%)]      Loss: 1.611524\nTrain Epoch: 0 [28672/60000 (47%)]      Loss: 1.692404\nTrain Epoch: 0 [32768/60000 (53%)]      Loss: 1.603679\nTrain Epoch: 0 [36864/60000 (60%)]      Loss: 1.207830\nTrain Epoch: 0 [40960/60000 (67%)]      Loss: 1.220353\nTrain Epoch: 0 [45056/60000 (73%)]      Loss: 0.990351\nTrain Epoch: 0 [49152/60000 (80%)]      Loss: 0.884814\nTrain Epoch: 0 [53248/60000 (87%)]      Loss: 0.895245\nTrain Epoch: 0 [57344/60000 (93%)]      Loss: 0.779024\n\nTest set: Average loss: 0.4244, Accuracy: 8935/10000 (89%)\n\nCuDNN version: 8500\n\nPeak memory allocated:\nfused: 3.36GB, unfused: 2.68GB\nMemory allocated at end of forward pass:\nfused: 0.59GB, unfused: 0.96GB",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  38.242 seconds)",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "PyTorch provides a plethora of operations related to neural networks, arbitrary\ntensor algebra, data wrangling and other purposes. However, you may still find\nyourself in need of a more customized operation. For example, you might want to\nuse a novel activation function you found in a paper, or implement an operation\nyou developed as part of your research.",
            "markdown"
        ],
        [
            "The easiest way of integrating such a custom operation in PyTorch is to write it\nin Python by extending Function and Module as outlined . This gives you the full\npower of automatic differentiation (spares you from writing derivative\nfunctions) as well as the usual expressiveness of Python. However, there may be\ntimes when your operation is better implemented in C++. For example, your code\nmay need to be <em>really</em> fast because it is called very frequently in your model\nor is very expensive even for few calls. Another plausible reason is that it\ndepends on or interacts with other C or C++ libraries. To address such cases,\nPyTorch provides a very easy way of writing custom <em>C++ extensions</em>.",
            "markdown"
        ],
        [
            "C++ extensions are a mechanism we have developed to allow users (you) to create\nPyTorch operators defined <em>out-of-source</em>, i.e. separate from the PyTorch\nbackend. This approach is <em>different</em> from the way native PyTorch operations are\nimplemented. C++ extensions are intended to spare you much of the boilerplate\nassociated with integrating an operation with PyTorch\u2019s backend while providing\nyou with a high degree of flexibility for your PyTorch-based projects.\nNevertheless, once you have defined your operation as a C++ extension, turning\nit into a native PyTorch function is largely a matter of code organization,\nwhich you can tackle after the fact if you decide to contribute your operation\nupstream.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Motivation and Example": [
        [
            "The rest of this note will walk through a practical example of writing and using\na C++ (and CUDA) extension. If you are being chased or someone will fire you if\nyou don\u2019t get that op done by the end of the day, you can skip this section and\nhead straight to the implementation details in the next section.",
            "markdown"
        ],
        [
            "Let\u2019s say you\u2019ve come up with a new kind of recurrent unit that you found to\nhave superior properties compared to the state of the art. This recurrent unit\nis similar to an LSTM, but differs in that it lacks a <em>forget gate</em> and uses an\n<em>Exponential Linear Unit</em> (ELU) as its internal activation function. Because\nthis unit never forgets, we\u2019ll call it <em>LLTM</em>, or <em>Long-Long-Term-Memory</em> unit.",
            "markdown"
        ],
        [
            "The two ways in which LLTMs differ from vanilla LSTMs are significant enough\nthat we can\u2019t configure PyTorch\u2019s LSTMCell for our purposes, so we\u2019ll have to\ncreate a custom cell. The first and easiest approach for this \u2013 and likely in\nall cases a good first step \u2013 is to implement our desired functionality in\nplain PyTorch with Python. For this, we need to subclass\n and implement the forward pass of the LLTM. This would\nlook something like this:",
            "markdown"
        ],
        [
            "class LLTM(torch.nn.Module):\n    def __init__(self, input_features, state_size):\n        super(LLTM, self).__init__()\n        self.input_features = input_features\n        self.state_size = state_size\n        # 3 * state_size for input gate, output gate and candidate cell gate.\n        # input_features + state_size because we will multiply with [input, h].\n        self.weights = torch.nn.Parameter(\n            torch.empty(3 * state_size, input_features + state_size))\n        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.state_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, +stdv)\n\n    def forward(self, input, state):\n        old_h, old_cell = state\n        X = torch.cat([old_h, input], dim=1)\n\n        # Compute the input, output and candidate cell gates with one MM.\n        gate_weights = F.linear(X, self.weights, self.bias)\n        # Split the combined gate weight matrix into its components.\n        gates = gate_weights.chunk(3, dim=1)\n\n        input_gate = torch.sigmoid(gates[0])\n        output_gate = torch.sigmoid(gates[1])\n        # Here we use an ELU instead of the usual tanh.\n        candidate_cell = F.elu(gates[2])\n\n        # Compute the new cell state.\n        new_cell = old_cell + candidate_cell * input_gate\n        # Compute the new hidden state and output.\n        new_h = torch.tanh(new_cell) * output_gate\n\n        return new_h, new_cell",
            "code"
        ],
        [
            "which we could then use as expected:",
            "markdown"
        ],
        [
            "import torch\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nnew_h, new_C = rnn(X, (h, C))",
            "code"
        ],
        [
            "Naturally, if at all possible and plausible, you should use this approach to\nextend PyTorch. Since PyTorch has highly optimized implementations of its\noperations for CPU <em>and</em> GPU, powered by libraries such as ,  or , PyTorch code like above will often be\nfast enough. However, we can also see why, under certain circumstances, there is\nroom for further performance improvements. The most obvious reason is that\nPyTorch has no knowledge of the <em>algorithm</em> you are implementing. It knows only\nof the individual operations you use to compose your algorithm. As such, PyTorch\nmust execute your operations individually, one after the other. Since each\nindividual call to the implementation (or <em>kernel</em>) of an operation, which may\ninvolve the launch of a CUDA kernel, has a certain amount of overhead, this\noverhead may become significant across many function calls. Furthermore, the\nPython interpreter that is running our code can itself slow down our program.",
            "markdown"
        ],
        [
            "A definite method of speeding things up is therefore to rewrite parts in C++ (or\nCUDA) and <em>fuse</em> particular groups of operations. Fusing means combining the\nimplementations of many functions into a single function, which profits from\nfewer kernel launches as well as other optimizations we can perform with\nincreased visibility of the global flow of data.",
            "markdown"
        ],
        [
            "Let\u2019s see how we can use C++ extensions to implement a <em>fused</em> version of the\nLLTM. We\u2019ll begin by writing it in plain C++, using the  library that powers much of PyTorch\u2019s\nbackend, and see how easily it lets us translate our Python code. We\u2019ll then\nspeed things up even more by moving parts of the model to CUDA kernel to benefit\nfrom the massive parallelism GPUs provide.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a C++ Extension": [
        [
            "C++ extensions come in two flavors: They can be built \u201cahead of time\u201d with\nsetuptools, or \u201cjust in time\u201d via\n. We\u2019ll begin with the first approach and\ndiscuss the latter later.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a C++ Extension->Building with setuptools": [
        [
            "For the \u201cahead of time\u201d flavor, we build our C++ extension by writing a\nsetup.py script that uses setuptools to compile our C++ code. For the LLTM, it\nlooks as simple as this:",
            "markdown"
        ],
        [
            "from setuptools import setup, Extension\nfrom torch.utils import cpp_extension\n\nsetup(name='lltm_cpp',\n      ext_modules=[cpp_extension.CppExtension('lltm_cpp', ['lltm.cpp'])],\n      cmdclass={'build_ext': cpp_extension.BuildExtension})",
            "code"
        ],
        [
            "In this code, CppExtension is a convenience wrapper around\nsetuptools.Extension that passes the correct include paths and sets\nthe language of the extension to C++. The equivalent vanilla setuptools\ncode would simply be:",
            "markdown"
        ],
        [
            "Extension(\n   name='lltm_cpp',\n   sources=['lltm.cpp'],\n   include_dirs=cpp_extension.include_paths(),\n   language='c++')",
            "code"
        ],
        [
            "BuildExtension performs a number of required configuration steps and\nchecks and also manages mixed compilation in the case of mixed C++/CUDA\nextensions. And that\u2019s all we really need to know about building C++ extensions\nfor now! Let\u2019s now take a look at the implementation of our C++ extension,\nwhich goes into lltm.cpp.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a C++ Extension->Writing the C++ Op": [
        [
            "Let\u2019s start implementing the LLTM in C++! One function we\u2019ll need for the\nbackward pass is the derivative of the sigmoid. This is a small enough piece of\ncode to discuss the overall environment that is available to us when writing C++\nextensions:",
            "markdown"
        ],
        [
            "#include &lt;torch/extension.h&gt;\n\n#include &lt;iostream&gt;\n\ntorch::Tensor d_sigmoid(torch::Tensor z) {\n  auto s = torch::sigmoid(z);\n  return (1 - s) * s;\n}",
            "code"
        ],
        [
            "&lt;torch/extension.h&gt; is the one-stop header to include all the necessary PyTorch\nbits to write C++ extensions. It includes:",
            "markdown"
        ],
        [
            "The ATen library, which is our primary API for tensor computation,",
            "markdown"
        ],
        [
            ", which is how we create Python bindings for our C++ code,",
            "markdown"
        ],
        [
            "Headers that manage the details of interaction between ATen and pybind11.",
            "markdown"
        ],
        [
            "The implementation of d_sigmoid() shows how to use the ATen API.\nPyTorch\u2019s tensor and variable interface is generated automatically from the\nATen library, so we can more or less translate our Python implementation 1:1\ninto C++. Our primary datatype for all computations will be\ntorch::Tensor. Its full API can be inspected . Notice\nalso that we can include &lt;iostream&gt; or <em>any other C or C++ header</em> \u2013 we have\nthe full power of C++11 at our disposal.",
            "markdown"
        ],
        [
            "Note that CUDA-11.5 nvcc will hit internal compiler error while parsing torch/extension.h on Windows.\nTo workaround the issue, move python binding logic to pure C++ file.\nExample use:",
            "markdown"
        ],
        [
            "#include &lt;ATen/ATen.h&gt;\nat::Tensor SigmoidAlphaBlendForwardCuda(....)",
            "code"
        ],
        [
            "Instead of:",
            "markdown"
        ],
        [
            "#include &lt;torch/extension.h&gt;\ntorch::Tensor SigmoidAlphaBlendForwardCuda(...)",
            "code"
        ],
        [
            "Currently open issue for nvcc bug .\nComplete workaround code example .",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a C++ Extension->Writing the C++ Op->Forward Pass": [
        [
            "Next we can port our entire forward pass to C++:",
            "markdown"
        ],
        [
            "#include &lt;vector&gt;\n\nstd::vector&lt;at::Tensor&gt; lltm_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell) {\n  auto X = torch::cat({old_h, input}, /*dim=*/1);\n\n  auto gate_weights = torch::addmm(bias, X, weights.transpose(0, 1));\n  auto gates = gate_weights.chunk(3, /*dim=*/1);\n\n  auto input_gate = torch::sigmoid(gates[0]);\n  auto output_gate = torch::sigmoid(gates[1]);\n  auto candidate_cell = torch::elu(gates[2], /*alpha=*/1.0);\n\n  auto new_cell = old_cell + candidate_cell * input_gate;\n  auto new_h = torch::tanh(new_cell) * output_gate;\n\n  return {new_h,\n          new_cell,\n          input_gate,\n          output_gate,\n          candidate_cell,\n          X,\n          gate_weights};\n}",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a C++ Extension->Writing the C++ Op->Backward Pass": [
        [
            "The C++ extension API currently does not provide a way of automatically\ngenerating a backwards function for us. As such, we have to also implement the\nbackward pass of our LLTM, which computes the derivative of the loss with\nrespect to each input of the forward pass. Ultimately, we will plop both the\nforward and backward function into a  to create\na nice Python binding. The backward function is slightly more involved, so\nwe\u2019ll not dig deeper into the code (if you are interested,  is a good read for more\ninformation on this):",
            "markdown"
        ],
        [
            "// tanh'(z) = 1 - tanh^2(z)\ntorch::Tensor d_tanh(torch::Tensor z) {\n  return 1 - z.tanh().pow(2);\n}\n\n// elu'(z) = relu'(z) + { alpha * exp(z) if (alpha * (exp(z) - 1)) &lt; 0, else 0}\ntorch::Tensor d_elu(torch::Tensor z, torch::Scalar alpha = 1.0) {\n  auto e = z.exp();\n  auto mask = (alpha * (e - 1)) &lt; 0;\n  return (z &gt; 0).type_as(z) + mask.type_as(z) * (alpha * e);\n}\n\nstd::vector&lt;torch::Tensor&gt; lltm_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gate_weights,\n    torch::Tensor weights) {\n  auto d_output_gate = torch::tanh(new_cell) * grad_h;\n  auto d_tanh_new_cell = output_gate * grad_h;\n  auto d_new_cell = d_tanh(new_cell) * d_tanh_new_cell + grad_cell;\n\n  auto d_old_cell = d_new_cell;\n  auto d_candidate_cell = input_gate * d_new_cell;\n  auto d_input_gate = candidate_cell * d_new_cell;\n\n  auto gates = gate_weights.chunk(3, /*dim=*/1);\n  d_input_gate *= d_sigmoid(gates[0]);\n  d_output_gate *= d_sigmoid(gates[1]);\n  d_candidate_cell *= d_elu(gates[2]);\n\n  auto d_gates =\n      torch::cat({d_input_gate, d_output_gate, d_candidate_cell}, /*dim=*/1);\n\n  auto d_weights = d_gates.t().mm(X);\n  auto d_bias = d_gates.sum(/*dim=*/0, /*keepdim=*/true);\n\n  auto d_X = d_gates.mm(weights);\n  const auto state_size = grad_h.size(1);\n  auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size);\n  auto d_input = d_X.slice(/*dim=*/1, state_size);\n\n  return {d_old_h, d_input, d_weights, d_bias, d_old_cell};\n}",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a C++ Extension->Binding to Python": [
        [
            "Once you have your operation written in C++ and ATen, you can use pybind11 to\nbind your C++ functions or classes into Python in a very simple manner.\nQuestions or issues you have about this part of PyTorch C++ extensions will\nlargely be addressed by .",
            "markdown"
        ],
        [
            "For our extensions, the necessary binding code spans only four lines:",
            "markdown"
        ],
        [
            "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &amp;lltm_forward, \"LLTM forward\");\n  m.def(\"backward\", &amp;lltm_backward, \"LLTM backward\");\n}",
            "code"
        ],
        [
            "One bit to note here is the macro TORCH_EXTENSION_NAME. The torch extension\nbuild will define it as the name you give your extension in the setup.py\nscript. In this case, the value of TORCH_EXTENSION_NAME would be \u201clltm_cpp\u201d.\nThis is to avoid having to maintain the name of the extension in two places\n(the build script and your C++ code), as a mismatch between the two can lead to\nnasty and hard to track issues.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a C++ Extension->Using Your Extension": [
        [
            "We are now set to import our extension in PyTorch. At this point, your directory\nstructure could look something like this:",
            "markdown"
        ],
        [
            "pytorch/\n  lltm-extension/\n    lltm.cpp\n    setup.py",
            "code"
        ],
        [
            "Now, run python setup.py install to build and install your extension. This\nshould look something like this:",
            "markdown"
        ],
        [
            "running install\nrunning bdist_egg\nrunning egg_info\ncreating lltm_cpp.egg-info\nwriting lltm_cpp.egg-info/PKG-INFO\nwriting dependency_links to lltm_cpp.egg-info/dependency_links.txt\nwriting top-level names to lltm_cpp.egg-info/top_level.txt\nwriting manifest file 'lltm_cpp.egg-info/SOURCES.txt'\nreading manifest file 'lltm_cpp.egg-info/SOURCES.txt'\nwriting manifest file 'lltm_cpp.egg-info/SOURCES.txt'\ninstalling library code to build/bdist.linux-x86_64/egg\nrunning install_lib\nrunning build_ext\nbuilding 'lltm_cpp' extension\ncreating build\ncreating build/temp.linux-x86_64-3.7\ngcc -pthread -B ~/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I~/local/miniconda/lib/python3.7/site-packages/torch/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/TH -I~/local/miniconda/lib/python3.7/site-packages/torch/include/THC -I~/local/miniconda/include/python3.7m -c lltm.cpp -o build/temp.linux-x86_64-3.7/lltm.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=lltm_cpp -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++11\ncc1plus: warning: command line option \u2018-Wstrict-prototypes\u2019 is valid for C/ObjC but not for C++\ncreating build/lib.linux-x86_64-3.7\ng++ -pthread -shared -B ~/local/miniconda/compiler_compat -L~/local/miniconda/lib -Wl,-rpath=~/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/lltm.o -o build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so\ncreating build/bdist.linux-x86_64\ncreating build/bdist.linux-x86_64/egg\ncopying build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so -&gt; build/bdist.linux-x86_64/egg\ncreating stub loader for lltm_cpp.cpython-37m-x86_64-linux-gnu.so\nbyte-compiling build/bdist.linux-x86_64/egg/lltm_cpp.py to lltm_cpp.cpython-37.pyc\ncreating build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm_cpp.egg-info/PKG-INFO -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm_cpp.egg-info/SOURCES.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm_cpp.egg-info/dependency_links.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm_cpp.egg-info/top_level.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\nwriting build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\nzip_safe flag not set; analyzing archive contents...\n__pycache__.lltm_cpp.cpython-37: module references __file__\ncreating 'dist/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\nremoving 'build/bdist.linux-x86_64/egg' (and everything under it)\nProcessing lltm_cpp-0.0.0-py3.7-linux-x86_64.egg\nremoving '~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg' (and everything under it)\ncreating ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg\nExtracting lltm_cpp-0.0.0-py3.7-linux-x86_64.egg to ~/local/miniconda/lib/python3.7/site-packages\nlltm-cpp 0.0.0 is already the active version in easy-install.pth\n\nInstalled ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg\nProcessing dependencies for lltm-cpp==0.0.0\nFinished processing dependencies for lltm-cpp==0.0.0",
            "code"
        ],
        [
            "A small note on compilers: Due to ABI versioning issues, the compiler you use to\nbuild your C++ extension must be <em>ABI-compatible</em> with the compiler PyTorch was\nbuilt with. In practice, this means that you must use GCC version 4.9 and above on Linux.\nFor Ubuntu 16.04 and other more-recent Linux distributions, this should be the\ndefault compiler already. On MacOS, you must use clang (which does not have any ABI versioning issues). In the worst\ncase, you can build PyTorch from source with your compiler and then build the\nextension with that same compiler.",
            "markdown"
        ],
        [
            "Once your extension is built, you can simply import it in Python, using the\nname you specified in your setup.py script. Just be sure to import\ntorch first, as this will resolve some symbols that the dynamic linker must\nsee:",
            "markdown"
        ],
        [
            "In [1]: import torch\nIn [2]: import lltm_cpp\nIn [3]: lltm_cpp.forward\nOut[3]: &lt;function lltm.PyCapsule.forward&gt;",
            "code"
        ],
        [
            "If we call help() on the function or module, we can see that its signature\nmatches our C++ code:",
            "markdown"
        ],
        [
            "In[4] help(lltm_cpp.forward)\nforward(...) method of builtins.PyCapsule instance\n    forward(arg0: torch::Tensor, arg1: torch::Tensor, arg2: torch::Tensor, arg3: torch::Tensor, arg4: torch::Tensor) -&gt; List[torch::Tensor]\n\n    LLTM forward",
            "code"
        ],
        [
            "Since we are now able to call our C++ functions from Python, we can wrap them\nwith  and  to make them first\nclass citizens of PyTorch:",
            "markdown"
        ],
        [
            "import math\nimport torch\n\n# Our module!\nimport lltm_cpp\n\nclass LLTMFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, weights, bias, old_h, old_cell):\n        outputs = lltm_cpp.forward(input, weights, bias, old_h, old_cell)\n        new_h, new_cell = outputs[:2]\n        variables = outputs[1:] + [weights]\n        ctx.save_for_backward(*variables)\n\n        return new_h, new_cell\n\n    @staticmethod\n    def backward(ctx, grad_h, grad_cell):\n        outputs = lltm_cpp.backward(\n            grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_tensors)\n        d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs\n        return d_input, d_weights, d_bias, d_old_h, d_old_cell\n\n\nclass LLTM(torch.nn.Module):\n    def __init__(self, input_features, state_size):\n        super(LLTM, self).__init__()\n        self.input_features = input_features\n        self.state_size = state_size\n        self.weights = torch.nn.Parameter(\n            torch.empty(3 * state_size, input_features + state_size))\n        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.state_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, +stdv)\n\n    def forward(self, input, state):\n        return LLTMFunction.apply(input, self.weights, self.bias, *state)",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a C++ Extension->Using Your Extension->Performance Comparison": [
        [
            "Now that we are able to use and call our C++ code from PyTorch, we can run a\nsmall benchmark to see how much performance we gained from rewriting our op in\nC++. We\u2019ll run the LLTM forwards and backwards a few times and measure the\nduration:",
            "markdown"
        ],
        [
            "import time\n\nimport torch\n\nbatch_size = 16\ninput_features = 32\nstate_size = 128\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nforward = 0\nbackward = 0\nfor _ in range(100000):\n    start = time.time()\n    new_h, new_C = rnn(X, (h, C))\n    forward += time.time() - start\n\n    start = time.time()\n    (new_h.sum() + new_C.sum()).backward()\n    backward += time.time() - start\n\nprint('Forward: {:.3f} s | Backward {:.3f} s'.format(forward, backward))",
            "code"
        ],
        [
            "If we run this code with the original LLTM we wrote in pure Python at the start\nof this post, we get the following numbers (on my machine):",
            "markdown"
        ],
        [
            "Forward: 506.480 us | Backward 444.694 us",
            "code"
        ],
        [
            "and with our new C++ version:",
            "markdown"
        ],
        [
            "Forward: 349.335 us | Backward 443.523 us",
            "code"
        ],
        [
            "We can already see a significant speedup for the forward function (more than\n30%). For the backward function, a speedup is visible, albeit not a major one.\nThe backward pass I wrote above was not particularly optimized and could\ndefinitely be improved. Also, PyTorch\u2019s automatic differentiation engine can\nautomatically parallelize computation graphs, may use a more efficient flow of\noperations overall, and is also implemented in C++, so it\u2019s expected to be\nfast. Nevertheless, this is a good start.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a C++ Extension->Using Your Extension->Performance on GPU Devices": [
        [
            "A wonderful fact about PyTorch\u2019s <em>ATen</em> backend is that it abstracts the\ncomputing device you are running on. This means the same code we wrote for CPU\ncan <em>also</em> run on GPU, and individual operations will correspondingly dispatch\nto GPU-optimized implementations. For certain operations like matrix multiply\n(like mm or addmm), this is a big win. Let\u2019s take a look at how much\nperformance we gain from running our C++ code with CUDA tensors. No changes to\nour implementation are required, we simply need to put our tensors in GPU\nmemory from Python, with either adding device=cuda_device argument at\ncreation time or using .to(cuda_device) after creation:",
            "markdown"
        ],
        [
            "import torch\n\nassert torch.cuda.is_available()\ncuda_device = torch.device(\"cuda\")  # device object representing GPU\n\nbatch_size = 16\ninput_features = 32\nstate_size = 128\n\n# Note the device=cuda_device arguments here\nX = torch.randn(batch_size, input_features, device=cuda_device)\nh = torch.randn(batch_size, state_size, device=cuda_device)\nC = torch.randn(batch_size, state_size, device=cuda_device)\n\nrnn = LLTM(input_features, state_size).to(cuda_device)\n\nforward = 0\nbackward = 0\nfor _ in range(100000):\n    start = time.time()\n    new_h, new_C = rnn(X, (h, C))\n    torch.cuda.synchronize()\n    forward += time.time() - start\n\n    start = time.time()\n    (new_h.sum() + new_C.sum()).backward()\n    torch.cuda.synchronize()\n    backward += time.time() - start\n\nprint('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/1e5, backward * 1e6/1e5))",
            "code"
        ],
        [
            "Once more comparing our plain PyTorch code with our C++ version, now both\nrunning on CUDA devices, we again see performance gains. For Python/PyTorch:",
            "markdown"
        ],
        [
            "Forward: 187.719 us | Backward 410.815 us",
            "code"
        ],
        [
            "And C++/ATen:",
            "markdown"
        ],
        [
            "Forward: 149.802 us | Backward 393.458 us",
            "code"
        ],
        [
            "That\u2019s a great overall speedup compared to non-CUDA code. However, we can pull\neven more performance out of our C++ code by writing custom CUDA kernels, which\nwe\u2019ll dive into soon. Before that, let\u2019s discuss another way of building your C++\nextensions.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a C++ Extension->JIT Compiling Extensions": [
        [
            "Previously, I mentioned there were two ways of building C++ extensions: using\nsetuptools or just in time (JIT). Having covered the former, let\u2019s\nelaborate on the latter. The JIT compilation mechanism provides you with a way\nof compiling and loading your extensions on the fly by calling a simple\nfunction in PyTorch\u2019s API called . For\nthe LLTM, this would look as simple as this:",
            "markdown"
        ],
        [
            "from torch.utils.cpp_extension import load\n\nlltm_cpp = load(name=\"lltm_cpp\", sources=[\"lltm.cpp\"])",
            "code"
        ],
        [
            "Here, we provide the function with the same information as for\nsetuptools. In the background, this will do the following:",
            "markdown"
        ],
        [
            "Create a temporary directory /tmp/torch_extensions/lltm,",
            "markdown"
        ],
        [
            "Emit a  build file into that temporary directory,",
            "markdown"
        ],
        [
            "Compile your source files into a shared library,",
            "markdown"
        ],
        [
            "Import this shared library as a Python module.",
            "markdown"
        ],
        [
            "In fact, if you pass verbose=True to cpp_extension.load(), you will\nbe informed about the process:",
            "markdown"
        ],
        [
            "Using /tmp/torch_extensions as PyTorch extensions root...\nEmitting ninja build file /tmp/torch_extensions/lltm_cpp/build.ninja...\nBuilding extension module lltm_cpp...\nLoading extension module lltm_cpp...",
            "code"
        ],
        [
            "The resulting Python module will be exactly the same as produced by setuptools,\nbut removes the requirement of having to maintain a separate setup.py build\nfile. If your setup is more complicated and you do need the full power of\nsetuptools, you <em>can</em> write your own setup.py \u2013 but in many cases\nthis JIT technique will do just fine. The first time you run through this line,\nit will take some time, as the extension is compiling in the background. Since\nwe use the Ninja build system to build your sources, re-compilation is\nincremental and thus re-loading the extension when you run your Python module a\nsecond time is fast and has low overhead if you didn\u2019t change the extension\u2019s\nsource files.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a Mixed C++/CUDA extension": [
        [
            "To really take our implementation to the next level, we can hand-write parts of\nour forward and backward passes with custom CUDA kernels. For the LLTM, this has\nthe prospect of being particularly effective, as there are a large number of\npointwise operations in sequence, that can all be fused and parallelized in a\nsingle CUDA kernel. Let\u2019s see how we could write such a CUDA kernel and\nintegrate it with PyTorch using this extension mechanism.",
            "markdown"
        ],
        [
            "The general strategy for writing a CUDA extension is to first write a C++ file\nwhich defines the functions that will be called from Python, and binds those\nfunctions to Python with pybind11. Furthermore, this file will also <em>declare</em>\nfunctions that are defined in CUDA (.cu) files. The C++ functions will then\ndo some checks and ultimately forward its calls to the CUDA functions. In the\nCUDA files, we write our actual CUDA kernels. The cpp_extension package\nwill then take care of compiling the C++ sources with a C++ compiler like\ngcc and the CUDA sources with NVIDIA\u2019s nvcc compiler. This ensures that\neach compiler takes care of files it knows best to compile. Ultimately, they\nwill be linked into one shared library that is available to us from Python\ncode.",
            "markdown"
        ],
        [
            "We\u2019ll start with the C++ file, which we\u2019ll call lltm_cuda.cpp, for example:",
            "markdown"
        ],
        [
            "#include &lt;torch/extension.h&gt;\n\n#include &lt;vector&gt;\n\n// CUDA forward declarations\n\nstd::vector&lt;torch::Tensor&gt; lltm_cuda_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell);\n\nstd::vector&lt;torch::Tensor&gt; lltm_cuda_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gate_weights,\n    torch::Tensor weights);\n\n// C++ interface\n\n#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n\nstd::vector&lt;torch::Tensor&gt; lltm_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell) {\n  CHECK_INPUT(input);\n  CHECK_INPUT(weights);\n  CHECK_INPUT(bias);\n  CHECK_INPUT(old_h);\n  CHECK_INPUT(old_cell);\n\n  return lltm_cuda_forward(input, weights, bias, old_h, old_cell);\n}\n\nstd::vector&lt;torch::Tensor&gt; lltm_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gate_weights,\n    torch::Tensor weights) {\n  CHECK_INPUT(grad_h);\n  CHECK_INPUT(grad_cell);\n  CHECK_INPUT(input_gate);\n  CHECK_INPUT(output_gate);\n  CHECK_INPUT(candidate_cell);\n  CHECK_INPUT(X);\n  CHECK_INPUT(gate_weights);\n  CHECK_INPUT(weights);\n\n  return lltm_cuda_backward(\n      grad_h,\n      grad_cell,\n      new_cell,\n      input_gate,\n      output_gate,\n      candidate_cell,\n      X,\n      gate_weights,\n      weights);\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &amp;lltm_forward, \"LLTM forward (CUDA)\");\n  m.def(\"backward\", &amp;lltm_backward, \"LLTM backward (CUDA)\");\n}",
            "code"
        ],
        [
            "As you can see, it is largely boilerplate, checks and forwarding to functions\nthat we\u2019ll define in the CUDA file. We\u2019ll name this file\nlltm_cuda_kernel.cu (note the .cu extension!). NVCC can reasonably\ncompile C++11, thus we still have ATen and the C++ standard library available\nto us (but not torch.h). Note that setuptools cannot handle files\nwith the same name but different extensions, so if you use the setup.py\nmethod instead of the JIT method, you must give your CUDA file a different name\nthan your C++ file (for the JIT method, lltm.cpp and lltm.cu would work\nfine). Let\u2019s take a small peek at what this file will look like:",
            "markdown"
        ],
        [
            "#include &lt;torch/extension.h&gt;\n\n#include &lt;cuda.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\n#include &lt;vector&gt;\n\ntemplate &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t sigmoid(scalar_t z) {\n  return 1.0 / (1.0 + exp(-z));\n}",
            "code"
        ],
        [
            "Here we see the headers I just described, as well as the fact that we are using\nCUDA-specific declarations like __device__ and __forceinline__ and\nfunctions like exp. Let\u2019s continue with a few more helper functions that\nwe\u2019ll need:",
            "markdown"
        ],
        [
            "template &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t d_sigmoid(scalar_t z) {\n  const auto s = sigmoid(z);\n  return (1.0 - s) * s;\n}\n\ntemplate &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t d_tanh(scalar_t z) {\n  const auto t = tanh(z);\n  return 1 - (t * t);\n}\n\ntemplate &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t elu(scalar_t z, scalar_t alpha = 1.0) {\n  return fmax(0.0, z) + fmin(0.0, alpha * (exp(z) - 1.0));\n}\n\ntemplate &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t d_elu(scalar_t z, scalar_t alpha = 1.0) {\n  const auto e = exp(z);\n  const auto d_relu = z &lt; 0.0 ? 0.0 : 1.0;\n  return d_relu + (((alpha * (e - 1.0)) &lt; 0.0) ? (alpha * e) : 0.0);\n}",
            "code"
        ],
        [
            "To now actually implement a function, we\u2019ll again need two things: one function\nthat performs operations we don\u2019t wish to explicitly write by hand and calls\ninto CUDA kernels, and then the actual CUDA kernel for the parts we want to\nspeed up. For the forward pass, the first function should look like this:",
            "markdown"
        ],
        [
            "std::vector&lt;torch::Tensor&gt; lltm_cuda_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell) {\n  auto X = torch::cat({old_h, input}, /*dim=*/1);\n  auto gates = torch::addmm(bias, X, weights.transpose(0, 1));\n\n  const auto batch_size = old_cell.size(0);\n  const auto state_size = old_cell.size(1);\n\n  auto new_h = torch::zeros_like(old_cell);\n  auto new_cell = torch::zeros_like(old_cell);\n  auto input_gate = torch::zeros_like(old_cell);\n  auto output_gate = torch::zeros_like(old_cell);\n  auto candidate_cell = torch::zeros_like(old_cell);\n\n  const int threads = 1024;\n  const dim3 blocks((state_size + threads - 1) / threads, batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(gates.type(), \"lltm_forward_cuda\", ([&amp;] {\n    lltm_cuda_forward_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(\n        gates.data&lt;scalar_t&gt;(),\n        old_cell.data&lt;scalar_t&gt;(),\n        new_h.data&lt;scalar_t&gt;(),\n        new_cell.data&lt;scalar_t&gt;(),\n        input_gate.data&lt;scalar_t&gt;(),\n        output_gate.data&lt;scalar_t&gt;(),\n        candidate_cell.data&lt;scalar_t&gt;(),\n        state_size);\n  }));\n\n  return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates};\n}",
            "code"
        ],
        [
            "The main point of interest here is the AT_DISPATCH_FLOATING_TYPES macro and\nthe kernel launch (indicated by the &lt;&lt;&lt;...&gt;&gt;&gt;). While ATen abstracts away\nthe device and datatype of the tensors we deal with, a tensor will, at runtime,\nstill be backed by memory of a concrete type on a concrete device. As such, we\nneed a way of determining at runtime what type a tensor is and then selectively\ncall functions with the corresponding correct type signature. Done manually,\nthis would (conceptually) look something like this:",
            "markdown"
        ],
        [
            "switch (tensor.type().scalarType()) {\n  case torch::ScalarType::Double:\n    return function&lt;double&gt;(tensor.data&lt;double&gt;());\n  case torch::ScalarType::Float:\n    return function&lt;float&gt;(tensor.data&lt;float&gt;());\n  ...\n}",
            "code"
        ],
        [
            "The purpose of AT_DISPATCH_FLOATING_TYPES is to take care of this dispatch\nfor us. It takes a type (gates.type() in our case), a name (for error\nmessages) and a lambda function. Inside this lambda function, the type alias\nscalar_t is available and is defined as the type that the tensor actually\nis at runtime in that context. As such, if we have a template function (which\nour CUDA kernel will be), we can instantiate it with this scalar_t alias,\nand the correct function will be called. In this case, we also want to retrieve\nthe data pointers of the tensors as pointers of that scalar_t type. If you\nwanted to dispatch over all types and not just floating point types (Float\nand Double), you can use AT_DISPATCH_ALL_TYPES.",
            "markdown"
        ],
        [
            "Note that we perform some operations with plain ATen. These operations will\nstill run on the GPU, but using ATen\u2019s default implementations. This makes\nsense because ATen will use highly optimized routines for things like matrix\nmultiplies (e.g. addmm) or convolutions which would be much harder to\nimplement and improve ourselves.",
            "markdown"
        ],
        [
            "As for the kernel launch itself, we are here specifying that each CUDA block\nwill have 1024 threads, and that the entire GPU grid is split into as many\nblocks of 1 x 1024 threads as are required to fill our matrices with one\nthread per component. For example, if our state size was 2048 and our batch\nsize 4, we\u2019d launch a total of 4 x 2 = 8 blocks with each 1024 threads. If\nyou\u2019ve never heard of CUDA \u201cblocks\u201d or \u201cgrids\u201d before, an  may\nhelp.",
            "markdown"
        ],
        [
            "The actual CUDA kernel is fairly simple (if you\u2019ve ever programmed GPUs before):",
            "markdown"
        ],
        [
            "template &lt;typename scalar_t&gt;\n__global__ void lltm_cuda_forward_kernel(\n    const scalar_t* __restrict__ gates,\n    const scalar_t* __restrict__ old_cell,\n    scalar_t* __restrict__ new_h,\n    scalar_t* __restrict__ new_cell,\n    scalar_t* __restrict__ input_gate,\n    scalar_t* __restrict__ output_gate,\n    scalar_t* __restrict__ candidate_cell,\n    size_t state_size) {\n  const int column = blockIdx.x * blockDim.x + threadIdx.x;\n  const int index = blockIdx.y * state_size + column;\n  const int gates_row = blockIdx.y * (state_size * 3);\n  if (column &lt; state_size) {\n    input_gate[index] = sigmoid(gates[gates_row + column]);\n    output_gate[index] = sigmoid(gates[gates_row + state_size + column]);\n    candidate_cell[index] = elu(gates[gates_row + 2 * state_size + column]);\n    new_cell[index] =\n        old_cell[index] + candidate_cell[index] * input_gate[index];\n    new_h[index] = tanh(new_cell[index]) * output_gate[index];\n  }\n}",
            "code"
        ],
        [
            "What\u2019s primarily interesting here is that we are able to compute all of these\npointwise operations entirely in parallel for each individual component in our\ngate matrices. If you imagine having to do this with a giant for loop over\na million elements in serial, you can see why this would be much faster.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a Mixed C++/CUDA extension->Using accessors": [
        [
            "You can see in the CUDA kernel that we work directly on pointers with the right\ntype. Indeed, working directly with high level type agnostic tensors inside cuda\nkernels would be very inefficient.",
            "markdown"
        ],
        [
            "However, this comes at a cost of ease of use and readability, especially for\nhighly dimensional data. In our example, we know for example that the contiguous\ngates tensor has 3 dimensions:",
            "markdown"
        ],
        [
            "batch, size of batch_size and stride of 3*state_size",
            "markdown"
        ],
        [
            "row, size of 3 and stride of state_size",
            "markdown"
        ],
        [
            "index, size  of state_size and stride of 1",
            "markdown"
        ],
        [
            "How can we access the element gates[n][row][column] inside the kernel then?\nIt turns out that you need the strides to access your element with some simple\narithmetic.",
            "markdown"
        ],
        [
            "gates.data&lt;scalar_t&gt;()[n*3*state_size + row*state_size + column]",
            "code"
        ],
        [
            "In addition to being verbose, this expression needs stride to be explicitly\nknown, and thus passed to the kernel function within its arguments. You can see\nthat in the case of kernel functions accepting multiple tensors with different\nsizes you will end up with a very long list of arguments.",
            "markdown"
        ],
        [
            "Fortunately for us, ATen provides accessors that are created with a single\ndynamic check that a Tensor is the type and number of dimensions.\nAccessors then expose an API for accessing the Tensor elements efficiently\nwithout having to convert to a single pointer:",
            "markdown"
        ],
        [
            "torch::Tensor foo = torch::rand({12, 12});\n\n// assert foo is 2-dimensional and holds floats.\nauto foo_a = foo.accessor&lt;float,2&gt;();\nfloat trace = 0;\n\nfor(int i = 0; i &lt; foo_a.size(0); i++) {\n  // use the accessor foo_a to get tensor data.\n  trace += foo_a[i][i];\n}",
            "code"
        ],
        [
            "Accessor objects have a relatively high level interface, with .size() and\n.stride() methods and multi-dimensional indexing. The .accessor&lt;&gt;\ninterface is designed to access data efficiently on cpu tensor. The equivalent\nfor cuda tensors are packed_accessor64&lt;&gt; and packed_accessor32&lt;&gt;, which\nproduce Packed Accessors with either 64-bit or 32-bit integer indexing.",
            "markdown"
        ],
        [
            "The fundamental difference with Accessor is that a Packed Accessor copies size\nand stride data inside of its structure instead of pointing to it. It allows us\nto pass it to a CUDA kernel function and use its interface inside it.",
            "markdown"
        ],
        [
            "We can design a function that takes Packed Accessors instead of pointers.",
            "markdown"
        ],
        [
            "__global__ void lltm_cuda_forward_kernel(\n    const torch::PackedTensorAccessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt; gates,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; old_cell,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_h,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_cell,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; input_gate,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; output_gate,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; candidate_cell)",
            "code"
        ],
        [
            "Let\u2019s decompose the template used here. the first two arguments scalar_t and\n2 are the same as regular Accessor. The argument\ntorch::RestrictPtrTraits indicates that the __restrict__ keyword must be\nused. Note also that we\u2019ve used the PackedAccessor32 variant which store the\nsizes and strides in an int32_t. This is important as using the 64-bit\nvariant (PackedAccessor64) can make the kernel slower.",
            "markdown"
        ],
        [
            "The function declaration becomes",
            "markdown"
        ],
        [
            "template &lt;typename scalar_t&gt;\n__global__ void lltm_cuda_forward_kernel(\n    const torch::PackedTensorAccessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt; gates,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; old_cell,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_h,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_cell,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; input_gate,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; output_gate,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; candidate_cell) {\n  //batch index\n  const int n = blockIdx.y;\n  // column index\n  const int c = blockIdx.x * blockDim.x + threadIdx.x;\n  if (c &lt; gates.size(2)){\n    input_gate[n][c] = sigmoid(gates[n][0][c]);\n    output_gate[n][c] = sigmoid(gates[n][1][c]);\n    candidate_cell[n][c] = elu(gates[n][2][c]);\n    new_cell[n][c] =\n        old_cell[n][c] + candidate_cell[n][c] * input_gate[n][c];\n    new_h[n][c] = tanh(new_cell[n][c]) * output_gate[n][c];\n  }\n}",
            "code"
        ],
        [
            "The implementation is much more readable! This function is then called by\ncreating Packed Accessors with the .packed_accessor32&lt;&gt; method within the\nhost function.",
            "markdown"
        ],
        [
            "std::vector&lt;torch::Tensor&gt; lltm_cuda_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell) {\n  auto X = torch::cat({old_h, input}, /*dim=*/1);\n  auto gate_weights = torch::addmm(bias, X, weights.transpose(0, 1));\n\n  const auto batch_size = old_cell.size(0);\n  const auto state_size = old_cell.size(1);\n\n  auto gates = gate_weights.reshape({batch_size, 3, state_size});\n  auto new_h = torch::zeros_like(old_cell);\n  auto new_cell = torch::zeros_like(old_cell);\n  auto input_gate = torch::zeros_like(old_cell);\n  auto output_gate = torch::zeros_like(old_cell);\n  auto candidate_cell = torch::zeros_like(old_cell);\n\n  const int threads = 1024;\n  const dim3 blocks((state_size + threads - 1) / threads, batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(gates.type(), \"lltm_forward_cuda\", ([&amp;] {\n    lltm_cuda_forward_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(\n        gates.packed_accessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt;(),\n        old_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        new_h.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        new_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        input_gate.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        output_gate.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        candidate_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;());\n  }));\n\n  return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates};\n}",
            "code"
        ],
        [
            "The backwards pass follows much the same pattern and I won\u2019t elaborate further\non it:",
            "markdown"
        ],
        [
            "template &lt;typename scalar_t&gt;\n__global__ void lltm_cuda_backward_kernel(\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; d_old_cell,\n    torch::PackedTensorAccessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt; d_gates,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; grad_h,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; grad_cell,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_cell,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; input_gate,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; output_gate,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; candidate_cell,\n    const torch::PackedTensorAccessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt; gate_weights) {\n  //batch index\n  const int n = blockIdx.y;\n  // column index\n  const int c = blockIdx.x * blockDim.x + threadIdx.x;\n  if (c &lt; d_gates.size(2)){\n    const auto d_output_gate = tanh(new_cell[n][c]) * grad_h[n][c];\n    const auto d_tanh_new_cell = output_gate[n][c] * grad_h[n][c];\n    const auto d_new_cell =\n        d_tanh(new_cell[n][c]) * d_tanh_new_cell + grad_cell[n][c];\n\n\n    d_old_cell[n][c] = d_new_cell;\n    const auto d_candidate_cell = input_gate[n][c] * d_new_cell;\n    const auto d_input_gate = candidate_cell[n][c] * d_new_cell;\n\n    d_gates[n][0][c] =\n        d_input_gate * d_sigmoid(gate_weights[n][0][c]);\n    d_gates[n][1][c] =\n        d_output_gate * d_sigmoid(gate_weights[n][1][c]);\n    d_gates[n][2][c] =\n        d_candidate_cell * d_elu(gate_weights[n][2][c]);\n  }\n}\n\nstd::vector&lt;torch::Tensor&gt; lltm_cuda_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gates,\n    torch::Tensor weights) {\n  auto d_old_cell = torch::zeros_like(new_cell);\n  auto d_gates = torch::zeros_like(gates);\n\n  const auto batch_size = new_cell.size(0);\n  const auto state_size = new_cell.size(1);\n\n  const int threads = 1024;\n  const dim3 blocks((state_size + threads - 1) / threads, batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(X.type(), \"lltm_backward_cuda\", ([&amp;] {\n    lltm_cuda_backward_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(\n        d_old_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        d_gates.packed_accessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt;(),\n        grad_h.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        grad_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        new_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        input_gate.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        output_gate.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        candidate_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        gates.packed_accessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt;());\n  }));\n\n  auto d_gate_weights = d_gates.reshape({batch_size, 3*state_size});\n  auto d_weights = d_gate_weights.t().mm(X);\n  auto d_bias = d_gate_weights.sum(/*dim=*/0, /*keepdim=*/true);\n\n  auto d_X = d_gate_weights.mm(weights);\n  auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size);\n  auto d_input = d_X.slice(/*dim=*/1, state_size);\n\n  return {d_old_h, d_input, d_weights, d_bias, d_old_cell, d_gates};\n}",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a Mixed C++/CUDA extension->Integrating a C++/CUDA Operation with PyTorch": [
        [
            "Integration of our CUDA-enabled op with PyTorch is again very straightforward.\nIf you want to write a setup.py script, it could look like this:",
            "markdown"
        ],
        [
            "from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='lltm',\n    ext_modules=[\n        CUDAExtension('lltm_cuda', [\n            'lltm_cuda.cpp',\n            'lltm_cuda_kernel.cu',\n        ])\n    ],\n    cmdclass={\n        'build_ext': BuildExtension\n    })",
            "code"
        ],
        [
            "Instead of CppExtension(), we now use CUDAExtension(). We can just\nspecify the .cu file along with the .cpp files \u2013 the library takes\ncare of all the hassle this entails for you. The JIT mechanism is even\nsimpler:",
            "markdown"
        ],
        [
            "from torch.utils.cpp_extension import load\n\nlltm = load(name='lltm', sources=['lltm_cuda.cpp', 'lltm_cuda_kernel.cu'])",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Writing a Mixed C++/CUDA extension->Integrating a C++/CUDA Operation with PyTorch->Performance Comparison": [
        [
            "Our hope was that parallelizing and fusing the pointwise operations of our code\nwith CUDA would improve the performance of our LLTM. Let\u2019s see if that holds\ntrue. We can run the code I listed earlier to run a benchmark. Our fastest\nversion earlier was the CUDA-based C++ code:",
            "markdown"
        ],
        [
            "Forward: 149.802 us | Backward 393.458 us",
            "code"
        ],
        [
            "And now with our custom CUDA kernel:",
            "markdown"
        ],
        [
            "Forward: 129.431 us | Backward 304.641 us",
            "code"
        ],
        [
            "More performance increases!",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Conclusion": [
        [
            "You should now be equipped with a good overview of PyTorch\u2019s C++ extension\nmechanism as well as a motivation for using them. You can find the code\nexamples displayed in this note . If you have questions, please use\n. Also be sure to check our  in case you run into any issues.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators": [
        [
            "The PyTorch 1.0 release introduced a new programming model to PyTorch called\n. TorchScript is a\nsubset of the Python programming language which can be parsed, compiled and\noptimized by the TorchScript compiler. Further, compiled TorchScript models have\nthe option of being serialized into an on-disk file format, which you can\nsubsequently load and run from pure C++ (as well as Python) for inference.",
            "markdown"
        ],
        [
            "TorchScript supports a large subset of operations provided by the torch\npackage, allowing you to express many kinds of complex models purely as a series\nof tensor operations from PyTorch\u2019s \u201cstandard library\u201d. Nevertheless, there may\nbe times where you find yourself in need of extending TorchScript with a custom\nC++ or CUDA function. While we recommend that you only resort to this option if\nyour idea cannot be expressed (efficiently enough) as a simple Python function,\nwe do provide a very friendly and simple interface for defining custom C++ and\nCUDA kernels using , PyTorch\u2019s high\nperformance C++ tensor library. Once bound into TorchScript, you can embed these\ncustom kernels (or \u201cops\u201d) into your TorchScript model and execute them both in\nPython and in their serialized form directly in C++.",
            "markdown"
        ],
        [
            "The following paragraphs give an example of writing a TorchScript custom op to\ncall into , a computer vision library written\nin C++. We will discuss how to work with tensors in C++, how to efficiently\nconvert them to third party tensor formats (in this case, OpenCV Mat), how\nto register your operator with the TorchScript runtime and finally how to\ncompile the operator and use it in Python and C++.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Implementing the Custom Operator in C++": [
        [
            "For this tutorial, we\u2019ll be exposing the \nfunction, which applies a perspective transformation to an image, from OpenCV to\nTorchScript as a custom operator. The first step is to write the implementation\nof our custom operator in C++. Let\u2019s call the file for this implementation\nop.cpp and make it look like this:",
            "markdown"
        ],
        [
            "torch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) {\n  // BEGIN image_mat\n  cv::Mat image_mat(/*rows=*/image.size(0),\n                    /*cols=*/image.size(1),\n                    /*type=*/CV_32FC1,\n                    /*data=*/image.data_ptr&lt;float&gt;());\n  // END image_mat\n\n  // BEGIN warp_mat\n  cv::Mat warp_mat(/*rows=*/warp.size(0),\n                   /*cols=*/warp.size(1),\n                   /*type=*/CV_32FC1,\n                   /*data=*/warp.data_ptr&lt;float&gt;());\n  // END warp_mat\n\n  // BEGIN output_mat\n  cv::Mat output_mat;\n  cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{8, 8});\n  // END output_mat\n\n  // BEGIN output_tensor\n  torch::Tensor output = torch::from_blob(output_mat.ptr&lt;float&gt;(), /*sizes=*/{8, 8});\n  return output.clone();\n  // END output_tensor\n}",
            "code"
        ],
        [
            "The code for this operator is quite short. At the top of the file, we include\nthe OpenCV header file, opencv2/opencv.hpp, alongside the torch/script.h\nheader which exposes all the necessary goodies from PyTorch\u2019s C++ API that we\nneed to write custom TorchScript operators. Our function warp_perspective\ntakes two arguments: an input image and the warp transformation matrix\nwe wish to apply to the image. The type of these inputs is torch::Tensor,\nPyTorch\u2019s tensor type in C++ (which is also the underlying type of all tensors\nin Python). The return type of our warp_perspective function will also be a\ntorch::Tensor.",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "See  for\nmore information about ATen, the library that provides the Tensor class to\nPyTorch. Further,  describes how to\nallocate and initialize new tensor objects in C++ (not required for this\noperator).",
            "markdown"
        ],
        [
            "Attention",
            "markdown"
        ],
        [
            "The TorchScript compiler understands a fixed number of types. Only these types\ncan be used as arguments to your custom operator. Currently these types are:\ntorch::Tensor, torch::Scalar, double, int64_t and\nstd::vector s of these types. Note that <em>only</em> double and <em>not</em>\nfloat, and <em>only</em> int64_t and <em>not</em> other integral types such as\nint, short or long are supported.",
            "markdown"
        ],
        [
            "Inside of our function, the first thing we need to do is convert our PyTorch\ntensors to OpenCV matrices, as OpenCV\u2019s warpPerspective expects cv::Mat\nobjects as inputs. Fortunately, there is a way to do this <strong>without copying\nany</strong> data. In the first few lines,",
            "markdown"
        ],
        [
            "  cv::Mat image_mat(/*rows=*/image.size(0),\n                    /*cols=*/image.size(1),\n                    /*type=*/CV_32FC1,\n                    /*data=*/image.data_ptr&lt;float&gt;());",
            "code"
        ],
        [
            "we are calling \nof the OpenCV Mat class to convert our tensor to a Mat object. We pass\nit the number of rows and columns of the original image tensor, the datatype\n(which we\u2019ll fix as float32 for this example), and finally a raw pointer to\nthe underlying data \u2013 a float*. What is special about this constructor of\nthe Mat class is that it does not copy the input data. Instead, it will\nsimply reference this memory for all operations performed on the Mat. If an\nin-place operation is performed on the image_mat, this will be reflected in\nthe original image tensor (and vice-versa). This allows us to call\nsubsequent OpenCV routines with the library\u2019s native matrix type, even though\nwe\u2019re actually storing the data in a PyTorch tensor. We repeat this procedure to\nconvert the warp PyTorch tensor to the warp_mat OpenCV matrix:",
            "markdown"
        ],
        [
            "  cv::Mat warp_mat(/*rows=*/warp.size(0),\n                   /*cols=*/warp.size(1),\n                   /*type=*/CV_32FC1,\n                   /*data=*/warp.data_ptr&lt;float&gt;());",
            "code"
        ],
        [
            "Next, we are ready to call the OpenCV function we were so eager to use in\nTorchScript: warpPerspective. For this, we pass the OpenCV function the\nimage_mat and warp_mat matrices, as well as an empty output matrix\ncalled output_mat. We also specify the size dsize we want the output\nmatrix (image) to be. It is hardcoded to 8 x 8 for this example:",
            "markdown"
        ],
        [
            "  cv::Mat output_mat;\n  cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{8, 8});",
            "code"
        ],
        [
            "The final step in our custom operator implementation is to convert the\noutput_mat back into a PyTorch tensor, so that we can further use it in\nPyTorch. This is strikingly similar to what we did earlier to convert in the\nother direction. In this case, PyTorch provides a torch::from_blob method. A\n<em>blob</em> in this case is intended to mean some opaque, flat pointer to memory that\nwe want to interpret as a PyTorch tensor. The call to torch::from_blob looks\nlike this:",
            "markdown"
        ],
        [
            "  torch::Tensor output = torch::from_blob(output_mat.ptr&lt;float&gt;(), /*sizes=*/{8, 8});\n  return output.clone();",
            "code"
        ],
        [
            "We use the .ptr&lt;float&gt;() method on the OpenCV Mat class to get a raw\npointer to the underlying data (just like .data_ptr&lt;float&gt;() for the PyTorch\ntensor earlier). We also specify the output shape of the tensor, which we\nhardcoded as 8 x 8. The output of torch::from_blob is then a\ntorch::Tensor, pointing to the memory owned by the OpenCV matrix.",
            "markdown"
        ],
        [
            "Before returning this tensor from our operator implementation, we must call\n.clone() on the tensor to perform a memory copy of the underlying data. The\nreason for this is that torch::from_blob returns a tensor that does not own\nits data. At that point, the data is still owned by the OpenCV matrix. However,\nthis OpenCV matrix will go out of scope and be deallocated at the end of the\nfunction. If we returned the output tensor as-is, it would point to invalid\nmemory by the time we use it outside the function. Calling .clone() returns\na new tensor with a copy of the original data that the new tensor owns itself.\nIt is thus safe to return to the outside world.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Registering the Custom Operator with TorchScript": [
        [
            "Now that have implemented our custom operator in C++, we need to <em>register</em> it\nwith the TorchScript runtime and compiler. This will allow the TorchScript\ncompiler to resolve references to our custom operator in TorchScript code.\nIf you have ever used the pybind11 library, our syntax for registration\nresembles the pybind11 syntax very closely.  To register a single function,\nwe write:",
            "markdown"
        ],
        [
            "TORCH_LIBRARY(my_ops, m) {\n  m.def(\"warp_perspective\", warp_perspective);\n}",
            "code"
        ],
        [
            "somewhere at the top level of our op.cpp file.  The TORCH_LIBRARY macro\ncreates a function that will be called when your program starts.  The name\nof your library (my_ops) is given as the first argument (it should not\nbe in quotes).  The second argument (m) defines a variable of type\ntorch::Library which is the main interface to register your operators.\nThe method Library::def actually creates an operator named warp_perspective,\nexposing it to both Python and TorchScript.  You can define as many operators\nas you like by making multiple calls to def.",
            "markdown"
        ],
        [
            "Behinds the scenes, the def function is actually doing quite a bit of work:\nit is using template metaprogramming to inspect the type signature of your\nfunction and translate it into an operator schema which specifies the operators\ntype within TorchScript\u2019s type system.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Building the Custom Operator": [
        [
            "Now that we have implemented our custom operator in C++ and written its\nregistration code, it is time to build the operator into a (shared) library that\nwe can load into Python for research and experimentation, or into C++ for\ninference in a no-Python environment. There exist multiple ways to build our\noperator, using either pure CMake, or Python alternatives like setuptools.\nFor brevity, the paragraphs below only discuss the CMake approach. The appendix\nof this tutorial dives into other alternatives.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Building the Custom Operator->Environment setup": [
        [
            "We need an installation of PyTorch and OpenCV.  The easiest and most platform\nindependent way to get both is to via Conda:",
            "markdown"
        ],
        [
            "conda install -c pytorch pytorch\nconda install opencv",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Building the Custom Operator->Building with CMake": [
        [
            "To build our custom operator into a shared library using the  build system, we need to write a short CMakeLists.txt\nfile and place it with our previous op.cpp file. For this, let\u2019s agree on a\na directory structure that looks like this:",
            "markdown"
        ],
        [
            "warp-perspective/\n  op.cpp\n  CMakeLists.txt",
            "code"
        ],
        [
            "The contents of our CMakeLists.txt file should then be the following:",
            "markdown"
        ],
        [
            "cmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(warp_perspective)\n\nfind_package(Torch REQUIRED)\nfind_package(OpenCV REQUIRED)\n\n# Define our library target\nadd_library(warp_perspective SHARED op.cpp)\n# Enable C++14\ntarget_compile_features(warp_perspective PRIVATE cxx_std_14)\n# Link against LibTorch\ntarget_link_libraries(warp_perspective \"${TORCH_LIBRARIES}\")\n# Link against OpenCV\ntarget_link_libraries(warp_perspective opencv_core opencv_imgproc)",
            "code"
        ],
        [
            "To now build our operator, we can run the following commands from our\nwarp_perspective folder:",
            "markdown"
        ],
        [
            "$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /warp_perspective/build\n$ make -j\nScanning dependencies of target warp_perspective\n[ 50%] Building CXX object CMakeFiles/warp_perspective.dir/op.cpp.o\n[100%] Linking CXX shared library libwarp_perspective.so\n[100%] Built target warp_perspective",
            "code"
        ],
        [
            "which will place a libwarp_perspective.so shared library file in the\nbuild folder. In the cmake command above, we use the helper\nvariable torch.utils.cmake_prefix_path to conveniently tell us where\nthe cmake files for our PyTorch install are.",
            "markdown"
        ],
        [
            "We will explore how to use and call our operator in detail further below, but to\nget an early sensation of success, we can try running the following code in\nPython:",
            "markdown"
        ],
        [
            "import torch\ntorch.ops.load_library(\"build/libwarp_perspective.so\")\nprint(torch.ops.my_ops.warp_perspective)",
            "code"
        ],
        [
            "If all goes well, this should print something like:",
            "markdown"
        ],
        [
            "&lt;built-in method my_ops::warp_perspective of PyCapsule object at 0x7f618fc6fa50&gt;",
            "code"
        ],
        [
            "which is the Python function we will later use to invoke our custom operator.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Using the TorchScript Custom Operator in Python": [
        [
            "Once our custom operator is built into a shared library  we are ready to use\nthis operator in our TorchScript models in Python. There are two parts to this:\nfirst loading the operator into Python, and second using the operator in\nTorchScript code.",
            "markdown"
        ],
        [
            "You already saw how to import your operator into Python:\ntorch.ops.load_library(). This function takes the path to a shared library\ncontaining custom operators, and loads it into the current process. Loading the\nshared library will also execute the TORCH_LIBRARY block. This will register\nour custom operator with the TorchScript compiler and allow us to use that\noperator in TorchScript code.",
            "markdown"
        ],
        [
            "You can refer to your loaded operator as torch.ops.&lt;namespace&gt;.&lt;function&gt;,\nwhere &lt;namespace&gt; is the namespace part of your operator name, and\n&lt;function&gt; the function name of your operator. For the operator we wrote\nabove, the namespace was my_ops and the function name warp_perspective,\nwhich means our operator is available as torch.ops.my_ops.warp_perspective.\nWhile this function can be used in scripted or traced TorchScript modules, we\ncan also just use it in vanilla eager PyTorch and pass it regular PyTorch\ntensors:",
            "markdown"
        ],
        [
            "import torch\ntorch.ops.load_library(\"build/libwarp_perspective.so\")\nprint(torch.ops.my_ops.warp_perspective(torch.randn(32, 32), torch.rand(3, 3)))",
            "code"
        ],
        [
            "producing:",
            "markdown"
        ],
        [
            "tensor([[0.0000, 0.3218, 0.4611,  ..., 0.4636, 0.4636, 0.4636],\n      [0.3746, 0.0978, 0.5005,  ..., 0.4636, 0.4636, 0.4636],\n      [0.3245, 0.0169, 0.0000,  ..., 0.4458, 0.4458, 0.4458],\n      ...,\n      [0.1862, 0.1862, 0.1692,  ..., 0.0000, 0.0000, 0.0000],\n      [0.1862, 0.1862, 0.1692,  ..., 0.0000, 0.0000, 0.0000],\n      [0.1862, 0.1862, 0.1692,  ..., 0.0000, 0.0000, 0.0000]])",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "What happens behind the scenes is that the first time you access\ntorch.ops.namespace.function in Python, the TorchScript compiler (in C++\nland) will see if a function namespace::function has been registered, and\nif so, return a Python handle to this function that we can subsequently use to\ncall into our C++ operator implementation from Python. This is one noteworthy\ndifference between TorchScript custom operators and C++ extensions: C++\nextensions are bound manually using pybind11, while TorchScript custom ops are\nbound on the fly by PyTorch itself. Pybind11 gives you more flexibility with\nregards to what types and classes you can bind into Python and is thus\nrecommended for purely eager code, but it is not supported for TorchScript\nops.",
            "markdown"
        ],
        [
            "From here on, you can use your custom operator in scripted or traced code just\nas you would other functions from the torch package. In fact, \u201cstandard\nlibrary\u201d functions like torch.matmul go through largely the same\nregistration path as custom operators, which makes custom operators really\nfirst-class citizens when it comes to how and where they can be used in\nTorchScript.  (One difference, however, is that standard library functions\nhave custom written Python argument parsing logic that differs from\ntorch.ops argument parsing.)",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Using the TorchScript Custom Operator in Python->Using the Custom Operator with Tracing": [
        [
            "Let\u2019s start by embedding our operator in a traced function. Recall that for\ntracing, we start with some vanilla Pytorch code:",
            "markdown"
        ],
        [
            "def compute(x, y, z):\n    return x.matmul(y) + torch.relu(z)",
            "code"
        ],
        [
            "and then call torch.jit.trace on it. We further pass torch.jit.trace\nsome example inputs, which it will forward to our implementation to record the\nsequence of operations that occur as the inputs flow through it. The result of\nthis is effectively a \u201cfrozen\u201d version of the eager PyTorch program, which the\nTorchScript compiler can further analyze, optimize and serialize:",
            "markdown"
        ],
        [
            "inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(4, 5)]\ntrace = torch.jit.trace(compute, inputs)\nprint(trace.graph)",
            "code"
        ],
        [
            "Producing:",
            "markdown"
        ],
        [
            "graph(%x : Float(4:8, 8:1),\n      %y : Float(8:5, 5:1),\n      %z : Float(4:5, 5:1)):\n  %3 : Float(4:5, 5:1) = aten::matmul(%x, %y) # test.py:10:0\n  %4 : Float(4:5, 5:1) = aten::relu(%z) # test.py:10:0\n  %5 : int = prim::Constant[value=1]() # test.py:10:0\n  %6 : Float(4:5, 5:1) = aten::add(%3, %4, %5) # test.py:10:0\n  return (%6)",
            "code"
        ],
        [
            "Now, the exciting revelation is that we can simply drop our custom operator into\nour PyTorch trace as if it were torch.relu or any other torch function:",
            "markdown"
        ],
        [
            "def compute(x, y, z):\n    x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n    return x.matmul(y) + torch.relu(z)",
            "code"
        ],
        [
            "and then trace it as before:",
            "markdown"
        ],
        [
            "inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(8, 5)]\ntrace = torch.jit.trace(compute, inputs)\nprint(trace.graph)",
            "code"
        ],
        [
            "Producing:",
            "markdown"
        ],
        [
            "graph(%x.1 : Float(4:8, 8:1),\n      %y : Float(8:5, 5:1),\n      %z : Float(8:5, 5:1)):\n  %3 : int = prim::Constant[value=3]() # test.py:25:0\n  %4 : int = prim::Constant[value=6]() # test.py:25:0\n  %5 : int = prim::Constant[value=0]() # test.py:25:0\n  %6 : Device = prim::Constant[value=\"cpu\"]() # test.py:25:0\n  %7 : bool = prim::Constant[value=0]() # test.py:25:0\n  %8 : Float(3:3, 3:1) = aten::eye(%3, %4, %5, %6, %7) # test.py:25:0\n  %x : Float(8:8, 8:1) = my_ops::warp_perspective(%x.1, %8) # test.py:25:0\n  %10 : Float(8:5, 5:1) = aten::matmul(%x, %y) # test.py:26:0\n  %11 : Float(8:5, 5:1) = aten::relu(%z) # test.py:26:0\n  %12 : int = prim::Constant[value=1]() # test.py:26:0\n  %13 : Float(8:5, 5:1) = aten::add(%10, %11, %12) # test.py:26:0\n  return (%13)",
            "code"
        ],
        [
            "Integrating TorchScript custom ops into traced PyTorch code is as easy as this!",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Using the TorchScript Custom Operator in Python->Using the Custom Operator with Script": [
        [
            "Besides tracing, another way to arrive at a TorchScript representation of a\nPyTorch program is to directly write your code <em>in</em> TorchScript. TorchScript is\nlargely a subset of the Python language, with some restrictions that make it\neasier for the TorchScript compiler to reason about programs. You turn your\nregular PyTorch code into TorchScript by annotating it with\n@torch.jit.script for free functions and @torch.jit.script_method for\nmethods in a class (which must also derive from torch.jit.ScriptModule). See\n for more details on\nTorchScript annotations.",
            "markdown"
        ],
        [
            "One particular reason to use TorchScript instead of tracing is that tracing is\nunable to capture control flow in PyTorch code. As such, let us consider this\nfunction which does use control flow:",
            "markdown"
        ],
        [
            "def compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  return x.matmul(y) + z",
            "code"
        ],
        [
            "To convert this function from vanilla PyTorch to TorchScript, we annotate it\nwith @torch.jit.script:",
            "markdown"
        ],
        [
            "@torch.jit.script\ndef compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  return x.matmul(y) + z",
            "code"
        ],
        [
            "This will just-in-time compile the compute function into a graph\nrepresentation, which we can inspect in the compute.graph property:",
            "markdown"
        ],
        [
            "&gt;&gt;&gt; compute.graph\ngraph(%x : Dynamic\n    %y : Dynamic) {\n  %14 : int = prim::Constant[value=1]()\n  %2 : int = prim::Constant[value=0]()\n  %7 : int = prim::Constant[value=42]()\n  %z.1 : int = prim::Constant[value=5]()\n  %z.2 : int = prim::Constant[value=10]()\n  %4 : Dynamic = aten::select(%x, %2, %2)\n  %6 : Dynamic = aten::select(%4, %2, %2)\n  %8 : Dynamic = aten::eq(%6, %7)\n  %9 : bool = prim::TensorToBool(%8)\n  %z : int = prim::If(%9)\n    block0() {\n      -&gt; (%z.1)\n    }\n    block1() {\n      -&gt; (%z.2)\n    }\n  %13 : Dynamic = aten::matmul(%x, %y)\n  %15 : Dynamic = aten::add(%13, %z, %14)\n  return (%15);\n}",
            "code"
        ],
        [
            "And now, just like before, we can use our custom operator like any other\nfunction inside of our script code:",
            "markdown"
        ],
        [
            "torch.ops.load_library(\"libwarp_perspective.so\")\n\n@torch.jit.script\ndef compute(x, y):\n  if bool(x[0] == 42):\n      z = 5\n  else:\n      z = 10\n  x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n  return x.matmul(y) + z",
            "code"
        ],
        [
            "When the TorchScript compiler sees the reference to\ntorch.ops.my_ops.warp_perspective, it will find the implementation we\nregistered via the TORCH_LIBRARY function in C++, and compile it into its\ngraph representation:",
            "markdown"
        ],
        [
            "&gt;&gt;&gt; compute.graph\ngraph(%x.1 : Dynamic\n    %y : Dynamic) {\n    %20 : int = prim::Constant[value=1]()\n    %16 : int[] = prim::Constant[value=[0, -1]]()\n    %14 : int = prim::Constant[value=6]()\n    %2 : int = prim::Constant[value=0]()\n    %7 : int = prim::Constant[value=42]()\n    %z.1 : int = prim::Constant[value=5]()\n    %z.2 : int = prim::Constant[value=10]()\n    %13 : int = prim::Constant[value=3]()\n    %4 : Dynamic = aten::select(%x.1, %2, %2)\n    %6 : Dynamic = aten::select(%4, %2, %2)\n    %8 : Dynamic = aten::eq(%6, %7)\n    %9 : bool = prim::TensorToBool(%8)\n    %z : int = prim::If(%9)\n      block0() {\n        -&gt; (%z.1)\n      }\n      block1() {\n        -&gt; (%z.2)\n      }\n    %17 : Dynamic = aten::eye(%13, %14, %2, %16)\n    %x : Dynamic = my_ops::warp_perspective(%x.1, %17)\n    %19 : Dynamic = aten::matmul(%x, %y)\n    %21 : Dynamic = aten::add(%19, %z, %20)\n    return (%21);\n  }",
            "code"
        ],
        [
            "Notice in particular the reference to my_ops::warp_perspective at the end of\nthe graph.",
            "markdown"
        ],
        [
            "Attention",
            "markdown"
        ],
        [
            "The TorchScript graph representation is still subject to change. Do not rely\non it looking like this.",
            "markdown"
        ],
        [
            "And that\u2019s really it when it comes to using our custom operator in Python. In\nshort, you import the library containing your operator(s) using\ntorch.ops.load_library, and call your custom op like any other torch\noperator from your traced or scripted TorchScript code.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Using the TorchScript Custom Operator in C++": [
        [
            "One useful feature of TorchScript is the ability to serialize a model into an\non-disk file. This file can be sent over the wire, stored in a file system or,\nmore importantly, be dynamically deserialized and executed without needing to\nkeep the original source code around. This is possible in Python, but also in\nC++. For this, PyTorch provides \nfor deserializing as well as executing TorchScript models. If you haven\u2019t yet,\nplease read , on which the\nnext few paragraphs will build.",
            "markdown"
        ],
        [
            "In short, custom operators can be executed just like regular torch operators\neven when deserialized from a file and run in C++. The only requirement for this\nis to link the custom operator shared library we built earlier with the C++\napplication in which we execute the model. In Python, this worked simply calling\ntorch.ops.load_library. In C++, you need to link the shared library with\nyour main application in whatever build system you are using. The following\nexample will showcase this using CMake.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Technically, you can also dynamically load the shared library into your C++\napplication at runtime in much the same way we did it in Python. On Linux,\n. There exist\nequivalents on other platforms.",
            "markdown"
        ],
        [
            "Building on the C++ execution tutorial linked above, let\u2019s start with a minimal\nC++ application in one file, main.cpp in a different folder from our\ncustom operator, that loads and executes a serialized TorchScript model:",
            "markdown"
        ],
        [
            "#include &lt;torch/script.h&gt; // One-stop header.\n\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\n\nint main(int argc, const char* argv[]) {\n  if (argc != 2) {\n    std::cerr &lt;&lt; \"usage: example-app &lt;path-to-exported-script-module&gt;\\n\";\n    return -1;\n  }\n\n  // Deserialize the ScriptModule from a file using torch::jit::load().\n  torch::jit::script::Module module = torch::jit::load(argv[1]);\n\n  std::vector&lt;torch::jit::IValue&gt; inputs;\n  inputs.push_back(torch::randn({4, 8}));\n  inputs.push_back(torch::randn({8, 5}));\n\n  torch::Tensor output = module.forward(std::move(inputs)).toTensor();\n\n  std::cout &lt;&lt; output &lt;&lt; std::endl;\n}",
            "code"
        ],
        [
            "Along with a small CMakeLists.txt file:",
            "markdown"
        ],
        [
            "cmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(example_app)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(example_app main.cpp)\ntarget_link_libraries(example_app \"${TORCH_LIBRARIES}\")\ntarget_compile_features(example_app PRIVATE cxx_range_for)",
            "code"
        ],
        [
            "At this point, we should be able to build the application:",
            "markdown"
        ],
        [
            "$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /example_app/build\n$ make -j\nScanning dependencies of target example_app\n[ 50%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o\n[100%] Linking CXX executable example_app\n[100%] Built target example_app",
            "code"
        ],
        [
            "And run it without passing a model just yet:",
            "markdown"
        ],
        [
            "$ ./example_app\nusage: example_app &lt;path-to-exported-script-module&gt;",
            "code"
        ],
        [
            "Next, let\u2019s serialize the script function we wrote earlier that uses our custom\noperator:",
            "markdown"
        ],
        [
            "torch.ops.load_library(\"libwarp_perspective.so\")\n\n@torch.jit.script\ndef compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n  return x.matmul(y) + z\n\ncompute.save(\"example.pt\")",
            "code"
        ],
        [
            "The last line will serialize the script function into a file called\n\u201cexample.pt\u201d. If we then pass this serialized model to our C++ application, we\ncan run it straight away:",
            "markdown"
        ],
        [
            "$ ./example_app example.pt\nterminate called after throwing an instance of 'torch::jit::script::ErrorReport'\nwhat():\nSchema not found for node. File a bug report.\nNode: %16 : Dynamic = my_ops::warp_perspective(%0, %19)",
            "code"
        ],
        [
            "Or maybe not. Maybe not just yet. Of course! We haven\u2019t linked the custom\noperator library with our application yet. Let\u2019s do this right now, and to do it\nproperly let\u2019s update our file organization slightly, to look like this:",
            "markdown"
        ],
        [
            "example_app/\n  CMakeLists.txt\n  main.cpp\n  warp_perspective/\n    CMakeLists.txt\n    op.cpp",
            "code"
        ],
        [
            "This will allow us to add the warp_perspective library CMake target as a\nsubdirectory of our application target. The top level CMakeLists.txt in the\nexample_app folder should look like this:",
            "markdown"
        ],
        [
            "cmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(example_app)\n\nfind_package(Torch REQUIRED)\n\nadd_subdirectory(warp_perspective)\n\nadd_executable(example_app main.cpp)\ntarget_link_libraries(example_app \"${TORCH_LIBRARIES}\")\ntarget_link_libraries(example_app -Wl,--no-as-needed warp_perspective)\ntarget_compile_features(example_app PRIVATE cxx_range_for)",
            "code"
        ],
        [
            "This basic CMake configuration looks much like before, except that we add the\nwarp_perspective CMake build as a subdirectory. Once its CMake code runs, we\nlink our example_app application with the warp_perspective shared\nlibrary.",
            "markdown"
        ],
        [
            "Attention",
            "markdown"
        ],
        [
            "There is one crucial detail embedded in the above example: The\n-Wl,--no-as-needed prefix to the warp_perspective link line. This is\nrequired because we will not actually be calling any function from the\nwarp_perspective shared library in our application code. We only need the\nTORCH_LIBRARY function to run. Inconveniently, this\nconfuses the linker and makes it think it can just skip linking against the\nlibrary altogether. On Linux, the -Wl,--no-as-needed flag forces the link\nto happen (NB: this flag is specific to Linux!). There are other workarounds\nfor this. The simplest is to define <em>some function</em> in the operator library\nthat you need to call from the main application. This could be as simple as a\nfunction void init(); declared in some header, which is then defined as\nvoid init() { } in the operator library. Calling this init() function\nin the main application will give the linker the impression that this is a\nlibrary worth linking against. Unfortunately, this is outside of our control,\nand we would rather let you know the reason and the simple workaround for this\nthan handing you some opaque macro to plop in your code.",
            "markdown"
        ],
        [
            "Now, since we find the Torch package at the top level now, the\nCMakeLists.txt file in the  warp_perspective subdirectory can be\nshortened a bit. It should look like this:",
            "markdown"
        ],
        [
            "find_package(OpenCV REQUIRED)\nadd_library(warp_perspective SHARED op.cpp)\ntarget_compile_features(warp_perspective PRIVATE cxx_range_for)\ntarget_link_libraries(warp_perspective PRIVATE \"${TORCH_LIBRARIES}\")\ntarget_link_libraries(warp_perspective PRIVATE opencv_core opencv_photo)",
            "code"
        ],
        [
            "Let\u2019s re-build our example app, which will also link with the custom operator\nlibrary. In the top level example_app directory:",
            "markdown"
        ],
        [
            "$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /warp_perspective/example_app/build\n$ make -j\nScanning dependencies of target warp_perspective\n[ 25%] Building CXX object warp_perspective/CMakeFiles/warp_perspective.dir/op.cpp.o\n[ 50%] Linking CXX shared library libwarp_perspective.so\n[ 50%] Built target warp_perspective\nScanning dependencies of target example_app\n[ 75%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o\n[100%] Linking CXX executable example_app\n[100%] Built target example_app",
            "code"
        ],
        [
            "If we now run the example_app binary and hand it our serialized model, we\nshould arrive at a happy ending:",
            "markdown"
        ],
        [
            "$ ./example_app example.pt\n11.4125   5.8262   9.5345   8.6111  12.3997\n 7.4683  13.5969   9.0850  11.0698   9.4008\n 7.4597  15.0926  12.5727   8.9319   9.0666\n 9.4834  11.1747   9.0162  10.9521   8.6269\n10.0000  10.0000  10.0000  10.0000  10.0000\n10.0000  10.0000  10.0000  10.0000  10.0000\n10.0000  10.0000  10.0000  10.0000  10.0000\n10.0000  10.0000  10.0000  10.0000  10.0000\n[ Variable[CPUFloatType]{8,5} ]",
            "code"
        ],
        [
            "Success! You are now ready to inference away.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Conclusion": [
        [
            "This tutorial walked you throw how to implement a custom TorchScript operator in\nC++, how to build it into a shared library, how to use it in Python to define\nTorchScript models and lastly how to load it into a C++ application for\ninference workloads. You are now ready to extend your TorchScript models with\nC++ operators that interface with third party C++ libraries, write custom high\nperformance CUDA kernels, or implement any other use case that requires the\nlines between Python, TorchScript and C++ to blend smoothly.",
            "markdown"
        ],
        [
            "As always, if you run into any problems or have questions, you can use our\n or  to get in touch. Also, our\n may have helpful information.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Appendix A: More Ways of Building Custom Operators": [
        [
            "The section \u201cBuilding the Custom Operator\u201d explained how to build a custom\noperator into a shared library using CMake. This appendix outlines two further\napproaches for compilation. Both of them use Python as the \u201cdriver\u201d or\n\u201cinterface\u201d to the compilation process. Also, both re-use the  PyTorch\nprovides for , which are the\nvanilla (eager) PyTorch equivalent of TorchScript custom operators that rely on\n for \u201cexplicit\u201d binding of\nfunctions from C++ into Python.",
            "markdown"
        ],
        [
            "The first approach uses C++ extensions\u2019 \nto compile your code in the background of your PyTorch script the first time you\nrun it. The second approach relies on the venerable setuptools package and\ninvolves writing a separate setup.py file. This allows more advanced\nconfiguration as well as integration with other setuptools-based projects.\nWe will explore both approaches in detail below.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Appendix A: More Ways of Building Custom Operators->Building with JIT compilation": [
        [
            "The JIT compilation feature provided by the PyTorch C++ extension toolkit allows\nembedding the compilation of your custom operator directly into your Python\ncode, e.g. at the top of your training script.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "\u201cJIT compilation\u201d here has nothing to do with the JIT compilation taking place\nin the TorchScript compiler to optimize your program. It simply means that\nyour custom operator C++ code will be compiled in a folder under your system\u2019s\n<cite>/tmp</cite> directory the first time you import it, as if you had compiled it\nyourself beforehand.",
            "markdown"
        ],
        [
            "This JIT compilation feature comes in two flavors. In the first, you still keep\nyour operator implementation in a separate file (op.cpp), and then use\ntorch.utils.cpp_extension.load() to compile your extension. Usually, this\nfunction will return the Python module exposing your C++ extension. However,\nsince we are not compiling our custom operator into its own Python module, we\nonly want to compile a plain shared library . Fortunately,\ntorch.utils.cpp_extension.load() has an argument is_python_module which\nwe can set to False to indicate that we are only interested in building a\nshared library and not a Python module. torch.utils.cpp_extension.load()\nwill then compile and also load the shared library into the current process,\njust like torch.ops.load_library did before:",
            "markdown"
        ],
        [
            "import torch.utils.cpp_extension\n\ntorch.utils.cpp_extension.load(\n    name=\"warp_perspective\",\n    sources=[\"op.cpp\"],\n    extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"],\n    is_python_module=False,\n    verbose=True\n)\n\nprint(torch.ops.my_ops.warp_perspective)",
            "code"
        ],
        [
            "This should approximately print:",
            "markdown"
        ],
        [
            "&lt;built-in method my_ops::warp_perspective of PyCapsule object at 0x7f3e0f840b10&gt;",
            "code"
        ],
        [
            "The second flavor of JIT compilation allows you to pass the source code for your\ncustom TorchScript operator as a string. For this, use\ntorch.utils.cpp_extension.load_inline:",
            "markdown"
        ],
        [
            "import torch\nimport torch.utils.cpp_extension\n\nop_source = \"\"\"\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;torch/script.h&gt;\n\ntorch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) {\n  cv::Mat image_mat(/*rows=*/image.size(0),\n                    /*cols=*/image.size(1),\n                    /*type=*/CV_32FC1,\n                    /*data=*/image.data&lt;float&gt;());\n  cv::Mat warp_mat(/*rows=*/warp.size(0),\n                   /*cols=*/warp.size(1),\n                   /*type=*/CV_32FC1,\n                   /*data=*/warp.data&lt;float&gt;());\n\n  cv::Mat output_mat;\n  cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{64, 64});\n\n  torch::Tensor output =\n    torch::from_blob(output_mat.ptr&lt;float&gt;(), /*sizes=*/{64, 64});\n  return output.clone();\n}\n\nTORCH_LIBRARY(my_ops, m) {\n  m.def(\"warp_perspective\", &amp;warp_perspective);\n}\n\"\"\"\n\ntorch.utils.cpp_extension.load_inline(\n    name=\"warp_perspective\",\n    cpp_sources=op_source,\n    extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"],\n    is_python_module=False,\n    verbose=True,\n)\n\nprint(torch.ops.my_ops.warp_perspective)",
            "code"
        ],
        [
            "Naturally, it is best practice to only use\ntorch.utils.cpp_extension.load_inline if your source code is reasonably\nshort.",
            "markdown"
        ],
        [
            "Note that if you\u2019re using this in a Jupyter Notebook, you should not execute\nthe cell with the registration multiple times because each execution registers\na new library and re-registers the custom operator. If you need to re-execute it,\nplease restart the Python kernel of your notebook beforehand.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Appendix A: More Ways of Building Custom Operators->Building with Setuptools": [
        [
            "The second approach to building our custom operator exclusively from Python is\nto use setuptools. This has the advantage that setuptools has a quite\npowerful and extensive interface for building Python modules written in C++.\nHowever, since setuptools is really intended for building Python modules and\nnot plain shared libraries (which do not have the necessary entry points Python\nexpects from a module), this route can be slightly quirky. That said, all you\nneed is a setup.py file in place of the CMakeLists.txt which looks like\nthis:",
            "markdown"
        ],
        [
            "from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\n\nsetup(\n    name=\"warp_perspective\",\n    ext_modules=[\n        CppExtension(\n            \"warp_perspective\",\n            [\"example_app/warp_perspective/op.cpp\"],\n            libraries=[\"opencv_core\", \"opencv_imgproc\"],\n        )\n    ],\n    cmdclass={\"build_ext\": BuildExtension.with_options(no_python_abi_suffix=True)},\n)",
            "code"
        ],
        [
            "Notice that we enabled the no_python_abi_suffix option in the\nBuildExtension at the bottom. This instructs setuptools to omit any\nPython-3 specific ABI suffixes in the name of the produced shared library.\nOtherwise, on Python 3.7 for example, the library may be called\nwarp_perspective.cpython-37m-x86_64-linux-gnu.so where\ncpython-37m-x86_64-linux-gnu is the ABI tag, but we really just want it to\nbe called warp_perspective.so",
            "markdown"
        ],
        [
            "If we now run python setup.py build develop in a terminal from within the\nfolder in which setup.py is situated, we should see something like:",
            "markdown"
        ],
        [
            "$ python setup.py build develop\nrunning build\nrunning build_ext\nbuilding 'warp_perspective' extension\ncreating build\ncreating build/temp.linux-x86_64-3.7\ngcc -pthread -B /root/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/TH -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/THC -I/root/local/miniconda/include/python3.7m -c op.cpp -o build/temp.linux-x86_64-3.7/op.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=warp_perspective -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\ncc1plus: warning: command line option \u2018-Wstrict-prototypes\u2019 is valid for C/ObjC but not for C++\ncreating build/lib.linux-x86_64-3.7\ng++ -pthread -shared -B /root/local/miniconda/compiler_compat -L/root/local/miniconda/lib -Wl,-rpath=/root/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/op.o -lopencv_core -lopencv_imgproc -o build/lib.linux-x86_64-3.7/warp_perspective.so\nrunning develop\nrunning egg_info\ncreating warp_perspective.egg-info\nwriting warp_perspective.egg-info/PKG-INFO\nwriting dependency_links to warp_perspective.egg-info/dependency_links.txt\nwriting top-level names to warp_perspective.egg-info/top_level.txt\nwriting manifest file 'warp_perspective.egg-info/SOURCES.txt'\nreading manifest file 'warp_perspective.egg-info/SOURCES.txt'\nwriting manifest file 'warp_perspective.egg-info/SOURCES.txt'\nrunning build_ext\ncopying build/lib.linux-x86_64-3.7/warp_perspective.so -&gt;\nCreating /root/local/miniconda/lib/python3.7/site-packages/warp-perspective.egg-link (link to .)\nAdding warp-perspective 0.0.0 to easy-install.pth file\n\nInstalled /warp_perspective\nProcessing dependencies for warp-perspective==0.0.0\nFinished processing dependencies for warp-perspective==0.0.0",
            "code"
        ],
        [
            "This will produce a shared library called warp_perspective.so, which we can\npass to torch.ops.load_library as we did earlier to make our operator\nvisible to TorchScript:",
            "markdown"
        ],
        [
            "&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.ops.load_library(\"warp_perspective.so\")\n&gt;&gt;&gt; print(torch.ops.my_ops.warp_perspective)\n&lt;built-in method custom::warp_perspective of PyCapsule object at 0x7ff51c5b7bd0&gt;",
            "code"
        ]
    ],
    "torch->Extending PyTorch->Registering a Dispatched Operator in C++": [
        [
            "The dispatcher is an internal component of PyTorch which is responsible for\nfiguring out what code should actually get run when you call a function like\ntorch::add.  This can be nontrivial, because PyTorch operations need\nto handle a lot of cross-cutting concerns that are \u201clayered\u201d on top of one\nof another.  Here is a sampling of some of the things it handles:",
            "markdown"
        ],
        [
            "Switching between the CPU and CUDA implementations of an operator, depending\non the devices of the input tensors.",
            "markdown"
        ],
        [
            "Switching between the autograd and backend implementations of an operator,\ndepending on whether or not autograd handling is necessary.",
            "markdown"
        ],
        [
            "Applying autocasting when necessary for automatic mixed precision.",
            "markdown"
        ],
        [
            "Applying batching rules when an operator is run under a vmap call.",
            "markdown"
        ],
        [
            "Tracing execution of operations, if you are tracing a model for export.",
            "markdown"
        ],
        [
            "If in your  you find yourself\nmanually writing if statements to handle these cases, the dispatcher APIs can\nhelp organize your code.  (Conversely, if your custom operator is very simple\nand is only for CPU inference, you probably don\u2019t need to use the dispatcher,\njust use the basic API.)",
            "markdown"
        ],
        [
            "In this tutorial, we will describe how to structure a custom operator\nregistration to use the dispatcher to organize various components.  We\u2019ll\nassume that you are familiar with how to\n and how to write\na .",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Registering a Dispatched Operator in C++->Defining schema and backend implementations": [
        [
            "The general principle behind the dispatcher is that it divides the\nimplementation of an operator into multiple kernels, each of which implements\nfunctionality for a specific <em>dispatch key</em>, e.g. CPU, CUDA.  The dispatcher\ndetermines what the highest priority dispatch key is at the time\nyou call an operator (this is done by looking at both the tensor arguments as\nwell as some thread local state), and transfers control to the kernel for that\ndispatch key.  The end effect is that when you call an operator, we first\nexecute the Autograd kernel, and then we redispatch to the backend kernel\ndepending on the device types of the passed in tensors.",
            "markdown"
        ],
        [
            "Let\u2019s take a look at the various parts involved in making this\nhappen.  First, we must define the schema for the operator in question.\nUnlike simple pybind11-style operator registration, we don\u2019t actually\nprovide an implementation of our operator at this point; we just\nprovide a schema string specifying the type signature of the operator\nthat all of our other kernels will abide by:",
            "markdown"
        ],
        [
            "TORCH_LIBRARY(myops, m) {\n  m.def(\"myadd(Tensor self, Tensor other) -&gt; Tensor\");\n}",
            "code"
        ],
        [
            "Next, we need to actually provide some implementations of this operator.\nFor concreteness, here is a really simple implementation of addition on CPU:",
            "markdown"
        ],
        [
            "Tensor myadd_cpu(const Tensor&amp; self_, const Tensor&amp; other_) {\n  TORCH_CHECK(self_.sizes() == other_.sizes());\n  TORCH_INTERNAL_ASSERT(self_.device().type() == DeviceType::CPU);\n  TORCH_INTERNAL_ASSERT(other_.device().type() == DeviceType::CPU);\n  Tensor self = self_.contiguous();\n  Tensor other = other_.contiguous();\n  Tensor result = torch::empty(self.sizes(), self.options());\n  const float* self_ptr = self.data_ptr&lt;float&gt;();\n  const float* other_ptr = other.data_ptr&lt;float&gt;();\n  float* result_ptr = result.data_ptr&lt;float&gt;();\n  for (int64_t i = 0; i &lt; result.numel(); i++) {\n    result_ptr[i] = self_ptr[i] + other_ptr[i];\n  }\n  return result;\n}",
            "code"
        ],
        [
            "We\u2019d like to register this function as an implementation of myops::myadd.\nHowever, the simple way of registering it (def(\"myadd\", myadd_cpu)) would\nregister the kernel to run in all cases, even if the tensor is not a CPU\ntensor!  (Internally, we refer to these as \u201ccatch-all\u201d kernels, since they\ncatch all cases.)  To ensure that myadd_cpu is only run for\nCPU tensors, we can use the TORCH_LIBRARY_IMPL macro:",
            "markdown"
        ],
        [
            "TORCH_LIBRARY_IMPL(myops, CPU, m) {\n  m.impl(\"myadd\", myadd_cpu);\n}",
            "code"
        ],
        [
            "The TORCH_LIBRARY_IMPL lets us register implementations for operators on\na specific dispatch key (in this case, CPU).  Each call to impl\nassociates a CPU kernel with the corresponding operator (which we previously\ndefined in the TORCH_LIBRARY block).  If we also have a CUDA implementation myadd_cuda,\nwe can register it in a separate TORCH_LIBRARY_IMPL block:",
            "markdown"
        ],
        [
            "TORCH_LIBRARY_IMPL(myops, CUDA, m) {\n  m.impl(\"myadd\", myadd_cuda);\n}",
            "code"
        ],
        [
            "These registrations can be split across files or even across library boundaries; so\nfor example, you could have these two TORCH_LIBRARY_IMPL blocks compiled\ninto a separate myops_cpu and myops_cuda dynamic libraries.  Generally,\nspeaking, the structure of your registrations will look like this:",
            "markdown"
        ],
        [
            "A single TORCH_LIBRARY that lists every custom operator in your namespace\nin a centralized place.",
            "markdown"
        ],
        [
            "A TORCH_LIBRARY_IMPL per dispatch key that registers implementations for\nthat key (e.g., CPU or CUDA).  If you like, you can further subdivide\nTORCH_LIBRARY_IMPL blocks into a block per operator. This is convenient\nif you have a separate file per operator implementation, but don\u2019t want to\nexpose the operators in a header; you can just put the registration in the\ncpp file that defines your operator.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Did you know that you can also write TORCH_LIBRARY_IMPL blocks for existing\ncore operators in PyTorch?  This is how XLA support for PyTorch is\nimplemented: the torch_xla library contains a TORCH_LIBRARY_IMPL\nthat provides implementations for all basic operators on the XLA dispatch\nkey.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Registering a Dispatched Operator in C++->For operators that do not need autograd": [
        [
            "Note: This section only applies to versions of PyTorch &gt;= 1.10.",
            "markdown"
        ],
        [
            "In the next section, we will discuss how to add autograd support to an operator.\nBut for the ops that do not need autograd support, the following kernel should be\nregistered improve useability and make your op behave like PyTorch\u2019s built-in\noperators.",
            "markdown"
        ],
        [
            "TORCH_LIBRARY_IMPL(myops, Autograd, m) {\n  m.impl(op, autogradNotImplementedFallback());\n}",
            "code"
        ],
        [
            "The above lines registers an Autograd kernel that appends a dummy\nNotImplemented node on forward (preserving the require_grad-ness of the inputs).\nOn backward, the NotImplemented node raises an error. This can be helpful\nfor debugging in larger models where previously it can be hard to pin-point\nexactly where the requires_grad-ness is lost during the forward pass.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Registering a Dispatched Operator in C++->For operators that do not need autograd->In-place or view ops": [
        [
            "To ensure correctness and best possible performance, if your op mutates an input\nin-place or returns a tensor that aliases with one of the inputs, two additional\nsteps should be taken:",
            "markdown"
        ],
        [
            "Register an ADInplaceOrView kernel in addition to the Autograd kernel\nabove. This kernel handles the necessary bookkeeping to ensure the correctness\nof in-place or view operations. It is important to note that this ADInplaceOrView\nkernel should only be used with autogradNotImplementedFallback.",
            "markdown"
        ],
        [
            "TORCH_LIBRARY_IMPL(myops, Autograd, m) {\n  m.impl(op, autogradNotImplementedFallback());\n}\nTORCH_LIBRARY_IMPL(myops, ADInplaceOrView, m) {\n  m.impl(op, autogradNotImplementedInplaceOrViewFallback());\n}",
            "code"
        ],
        [
            "The Autograd or ADInplaceOrView boxed kernels registered above\nrely on operator schema information in their logi. If your op mutates an input\nin-place or returns a tensor that aliases with one of the inputs it is important to\nensure that your schema properly reflects this. See\n\nfor more information on how to annotate the schema.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Registering a Dispatched Operator in C++->Adding autograd support": [
        [
            "At this point, we have an operator with both CPU and CUDA implementations.  How\ncan we add autograd support to it?  As you might guess, we will register an\nautograd kernel (similar to what\u2019s described in the  tutorial)!\nHowever, there is a twist: unlike the CPU and CUDA kernels, the autograd kernel\nneeds to <em>redispatch</em>: it needs to call back into the dispatcher to get to\nthe inference kernels, e.g. CPU or CUDA implementations.",
            "markdown"
        ],
        [
            "Thus, before we write the autograd kernel, let\u2019s write a <em>dispatching function</em>\nwhich calls into the dispatcher to find the right kernel for your operator.\nThis function constitutes the public C++ API for your operators\u2013in fact, all of\nthe tensor functions in PyTorch\u2019s C++ API all call the dispatcher in the same\nway under the hood.  Here\u2019s what the dispatching function looks like:",
            "markdown"
        ],
        [
            "Tensor myadd(const Tensor&amp; self, const Tensor&amp; other) {\n  static auto op = torch::Dispatcher::singleton()\n    .findSchemaOrThrow(\"myops::myadd\", \"\")\n    .typed&lt;decltype(myadd)&gt;();\n  return op.call(self, other);\n}",
            "code"
        ],
        [
            "Let\u2019s break it down:",
            "markdown"
        ],
        [
            "In the first line, we look up a typed operator handle from the dispatcher\ncorresponding to the operator that we are going to dispatch to.\nfindSchemaOrThrow takes two arguments: the (namespace qualified) name\nof the operator, and the overload name of the operator (typically just\nthe empty string).  typed casts the dynamically typed handle into\na statically typed handle (doing a runtime test to make sure you\u2019ve given\nthe correct C++ type), so that we can do a normal C++ call on it.  We\npass it decltype(myadd) since the type of the dispatching function is\nthe same as the type of the underlying kernels registered to the dispatcher.",
            "markdown"
        ],
        [
            "For performance, this computation is done in a static variable, so that\nwe only need to do the (slow) lookup once.  If you typoed the name of the\noperator you want to call, this lookup will error the first time you call this\nfunction.",
            "markdown"
        ],
        [
            "In the second line, we simply call the operator handle with all of the\narguments passed into the dispatching function.  This will actually invoke\nthe dispatcher and in the end control will be transferred to whatever kernel\nis appropriate for this call.",
            "markdown"
        ],
        [
            "With the dispatch function in hand, we can now write the autograd kernel:",
            "markdown"
        ],
        [
            "class MyAddFunction : public torch::autograd::Function&lt;MyAddFunction&gt; {\n public:\n  static Tensor forward(\n      AutogradContext *ctx, torch::Tensor self, torch::Tensor other) {\n    at::AutoNonVariableTypeMode g;\n    return myadd(self, other);\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    auto grad_output = grad_outputs[0];\n    return {grad_output, grad_output};\n  }\n};\n\nTensor myadd_autograd(const Tensor&amp; self, const Tensor&amp; other) {\n  return MyAddFunction::apply(self, other)[0];\n}",
            "code"
        ],
        [
            "The autograd function is written as normal using torch::autograd::Function,\nexcept that instead of directly writing the implementation in forward(),\nwe:",
            "markdown"
        ],
        [
            "Turn off autograd handling with the at::AutoNonVariableTypeMode RAII\nguard, and then",
            "markdown"
        ],
        [
            "Call the dispatch function myadd to call back into the dispatcher.",
            "markdown"
        ],
        [
            "Without (1), your calls will infinite loop (and stack overflow), because\nmyadd will send you back to this function (as the highest priority dispatch\nkey would still be autograd.) With (1),\nautograd is excluded from the set of dispatch keys under consideration, and\nwe will go to the next handlers, which will either be CPU and CUDA.",
            "markdown"
        ],
        [
            "We can now register this function in the same way we registered the CPU/CUDA\nfunctions:",
            "markdown"
        ],
        [
            "TORCH_LIBRARY_IMPL(myops, Autograd, m) {\n  m.impl(\"myadd\", myadd_autograd);\n}",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "In this example we register the kernel to Autograd, which installs it as the\nautograd kernel for all backends. You can also register optimized kernels for specific\nbackends by using the corresponding backend-specific dispatch key - for example,\nAutogradCPU or AutogradCUDA. To explore these and other dispatch key\noptions in more detail, check out the PythonDispatcher tool provided in\n.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Registering a Dispatched Operator in C++->Going beyond autograd": [
        [
            "In some sense, the dispatcher isn\u2019t doing all that much: all it does is\nimplement a glorified if-statement, along the lines of this:",
            "markdown"
        ],
        [
            "class MyAddFunction : ... {\npublic:\n  static Tensor forward(\n    AutogradContext *ctx, torch::Tensor self, torch::Tensor other) {\n\n    if (self.device().type() == DeviceType::CPU) {\n      return add_cpu(self, other);\n    } else if (self.device().type() == DeviceType::CUDA) {\n      return add_cuda(self, other);\n    } else {\n      TORCH_CHECK(0, \"Unsupported device \", self.device().type());\n    }\n  }\n  ...\n}",
            "code"
        ],
        [
            "So why use the dispatcher?  There are a few reasons:",
            "markdown"
        ],
        [
            "It is decentralized.  You can assemble all of the pieces of an operator\n(CPU, CUDA, Autograd) without having to write a single, centralized\nif statement that refers to all of them.  Importantly, third parties can\nregister extra implementations for other aspects without having to patch the\noriginal definition of an operator.  We\u2019ll talk more about extending the\ndispatcher in .",
            "markdown"
        ],
        [
            "It supports more dispatch keys than CPU, CUDA and Autograd.  You can\nsee a full list of dispatch keys that are currently implemented\nin PyTorch in c10/core/DispatchKey.h.  These dispatch keys\nimplement a variety of optional functionality for operators, and if you\ndecide you want your custom operator to support this functionality,\nall you have to register a kernel for the appropriate key.",
            "markdown"
        ],
        [
            "The dispatcher implements support for boxed fallback functions, which\nare functions that can be implemented once and apply to all operators\nin the system.  Boxed fallbacks can be used to provide default behavior\nfor a dispatch key; if you use the dispatcher to implement your operator,\nyou also opt into the fallbacks for all of these operations.",
            "markdown"
        ],
        [
            "Here are some particular dispatch keys which you may need to define an operator\nfor.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Registering a Dispatched Operator in C++->Going beyond autograd->Autocast": [
        [
            "The Autocast dispatch key implements support for\n.\nAn autocast wrapper kernel typically casts incoming float16 or float32 CUDA tensors\nto some preferred precision before running the op.\nFor example, matmuls and convolutions on floating-point CUDA tensors usually run faster\nand use less memory in float16 without impairing convergence.\nAutocast wrappers only have an effect in\n.",
            "markdown"
        ],
        [
            "Here\u2019s an autocast wrapper for a hypothetical custom matmul, along with its registration:",
            "markdown"
        ],
        [
            "// Autocast-specific helper functions\n#include &lt;ATen/autocast_mode.h&gt;\n\nTensor mymatmul_autocast(const Tensor&amp; self, const Tensor&amp; other) {\n  c10::impl::ExcludeDispatchKeyGuard no_autocast(c10::DispatchKey::Autocast);\n  return mymatmul(at::autocast::cached_cast(at::kHalf, self),\n                  at::autocast::cached_cast(at::kHalf, other));\n}\n\nTORCH_LIBRARY_IMPL(myops, Autocast, m) {\n  m.impl(\"mymatmul\", mymatmul_autocast);\n}",
            "code"
        ],
        [
            "cached_cast(kHalf, tensor) casts tensor to float16 if tensor is CUDA and float32,\notherwise, it leaves tensor unchanged (c.f. the\n for natively autocasted ops).\nThis ensures if the network calls mymatmul on any mixture of float16 and float32 CUDA tensors,\nmymatmul runs in float16.  Meanwhile, calls to mymatmul with non-CUDA, integer-type, or float64\ninputs are unaffected.  Using cached_cast to follow the native eligibility policy in your own autocast wrapper\nis recommended, but not required.  For example, if you wanted to force float16 execution for all input types,\nyou could return mymatmul(self.half(), other.half()); instead of using cached_cast.",
            "markdown"
        ],
        [
            "Notice that, like our autograd kernels, we exclude the Autocast key from\ndispatch before redispatching.",
            "markdown"
        ],
        [
            "By default, if no autocast wrapper is provided,\nwe fallthrough directly to the regular operator implementation (no\nautocasting occurs).  (We didn\u2019t use myadd for this example, since pointwise\naddition doesn\u2019t need autocasting and should just fall through.)",
            "markdown"
        ],
        [
            "When should an autocast wrapper be registered? Unfortunately, there aren\u2019t\ncut-and-dried rules for an op\u2019s preferred precision.  You can\nget a sense for some native ops\u2019 preferred precisions by looking at the\n.\nGeneral guidance:",
            "markdown"
        ],
        [
            "Ops that do reductions should probably execute in float32,",
            "markdown"
        ],
        [
            "Any op that does a convolution or gemm under the hood should\nprobably execute in float16, and",
            "markdown"
        ],
        [
            "Other ops with multiple floating-point tensor inputs should standardize\nthem to a common precision (unless the implementation supports inputs with different precisions).",
            "markdown"
        ],
        [
            "If your custom op falls into the third category, the promote_type template\nhelps figure out the widest floating-point type present among input tensors, which is\nthe safest choice for the execution type:",
            "markdown"
        ],
        [
            "#include &lt;ATen/autocast_mode.h&gt;\n\nTensor my_multiple_input_op_autocast(const Tensor&amp; t0, const Tensor&amp; t1) {\n  c10::impl::ExcludeDispatchKeyGuard no_autocast(c10::DispatchKey::Autocast);\n  // The required at::kHalf argument is an optimistic initial guess.\n  auto exec_type = at::autocast::promote_type(at::kHalf, t0, t1);\n  return my_multiple_input_op(at::autocast::cached_cast(exec_type, t0),\n                              at::autocast::cached_cast(exec_type, t1));\n}",
            "code"
        ],
        [
            "If your custom op is , you only need to write and register\nan autocast wrapper for the same name onto which the autograd wrapper is registered.\nFor example, if you wanted an autocast wrapper for the myadd function shown\nin the autograd section, all you\u2019d need is",
            "markdown"
        ],
        [
            "Tensor myadd_autocast(const Tensor&amp; self, const Tensor&amp; other) {\n  c10::impl::ExcludeDispatchKeyGuard no_autocast(c10::DispatchKey::Autocast);\n  return myadd(at::autocast::cached_cast(&lt;desired dtype&gt;, self),\n               at::autocast::cached_cast(&lt;desired dtype&gt;, other));\n}\n\nTORCH_LIBRARY_IMPL(myops, Autocast, m) {\n  m.impl(\"myadd\", myadd_autocast);\n}",
            "code"
        ],
        [
            "There are no separate gymnastics to make the backward method autocast compatible.\nHowever, the backward method defined in your custom autograd function will run in the same\ndtype as autocast sets for the forward method, so you should choose a &lt;desired dtype&gt;\nsuitable for both your forward and backward methods.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Registering a Dispatched Operator in C++->Going beyond autograd->Batched": [
        [
            "Batched tensors allow you to write your code in a per-example manner, and then\nhave them be automatically batched when run under a vmap invocation.  The\nAPI for writing batching rules is currently under development, but once it is\nstabilized, you can add support for vmap for your operators by registering\na kernel at the Batched dispatch key.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Registering a Dispatched Operator in C++->Going beyond autograd->Tracer": [
        [
            "The Tracer dispatch key implements support for recording invocations of operators\ninto a trace when you run torch.jit.trace.  We intend to provide a\nboxed fallback that will implement tracing for arbitrary operations,\nsee  to track\nprogress.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++": [
        [
            "In this tutorial we will walk through all necessary steps to extend the dispatcher to\nadd a new device living outside pytorch/pytorch repo and maintain it to keep in\nsync with native PyTorch devices.  Here we\u2019ll assume that you\u2019re familiar with how\nto  and how to write a\n.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This tutorial touches a lot of internal components inside PyTorch which are being actively improved,\nplease expect changes to APIs if you decide to follow this tutorial.  We\u2019ll keep this tutorial\nup to date with the latest APIs.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->What\u2019s a new backend?": [
        [
            "Adding a new backend to PyTorch requires a lot of developement and maintainence from backend extenders.\nBefore adding a new backend, let\u2019s first consider a few common use cases and recommended solutions for them:",
            "markdown"
        ],
        [
            "If you have new algorithms for an existing PyTorch operator, send a PR to PyTorch.",
            "markdown"
        ],
        [
            "If you want to propose a new operator, send a feature request/PR to PyTorch.",
            "markdown"
        ],
        [
            "If you want to add support for a new device/hardware like Google TPU and customized chips, which often requires using\nhardware-specific API to write kernels, follow this tutorial and add a out-of-tree backend to PyTorch.",
            "markdown"
        ],
        [
            "If you want to add support for existing operators but with a different Tensor layout/representation\nlike sparse and quantized, which enforces your kernels to be written in a way that\u2019s more efficient\ngiven the layout/representation limitation, follow this tutorial and add a out-of-tree backend to PyTorch.",
            "markdown"
        ],
        [
            "In this tutorial we\u2019ll mainly focus on adding a new out-of-tree device below.  Adding out-of-tree support\nfor a different tensor layout might share many common steps with devices, but we haven\u2019t seen an example of\nsuch integrations yet so it might require addtional work from PyTorch to support it.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->Get a dispatch key for your backend": [
        [
            "PyTorch operators are implemented in C++ and made available in Python frontend through Python bindings.\nThe PyTorch dispatcher divides the implementation of an operator into multiple kernels, each of which is\nassociated with a specific dispatch key.  Supporting a new backend in PyTorch essentially means writing\na kernel for each PyTorch operator in C++ and then registering them to a dispatch key representing your\ncustomized backend in the dispatcher.",
            "markdown"
        ],
        [
            "Dispatch key is your identifier in the dispatcher system. The dispatcher looks at the dispatch keys carried on\ninput tensors and calls the right kernel accordingly.  PyTorch provides three reserved dispatch keys\n(and their corresponding Autograd keys) for prototyping out-of-tree backend extensions:",
            "markdown"
        ],
        [
            "PrivateUse1/AutogradPrivateUse1",
            "markdown"
        ],
        [
            "PrivateUse2/AutogradPrivateUse2",
            "markdown"
        ],
        [
            "PrivateUse3/AutogradPrivateUse3",
            "markdown"
        ],
        [
            "You can choose any of keys above to prototype your customized backend.\nTo create a Tensor on PrivateUse1 backend, you need to set dispatch key in TensorImpl constructor.",
            "markdown"
        ],
        [
            "/* Example TensorImpl constructor */\nTensorImpl(\n    Storage&amp;&amp; storage,\n    DispatchKeySet ks,\n    const caffe2::TypeMeta data_type);\n\n// To create a TensorImpl on PrivateUse1 backend, pass in the following ks to TensorImpl creation.\nDispatchKeySet ks = c10::DispatchKeySet{c10::DispatchKey::PrivateUse1, c10::DispatchKey::AutogradPrivateUse1};",
            "code"
        ],
        [
            "Note that TensorImpl class above assumes your Tensor is backed by a storage like CPU/CUDA. We also\nprovide OpaqueTensorImpl for backends without a storage. And you might need to tweak/override certain\nmethods to fit your customized hardware.\nOne example in pytorch repo is .",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Once the prototype is done and you plan to do regular releases for your backend extension,  please feel free to\nsubmit a PR to pytorch/pytorch to reserve a dedicated dispath key for your backend.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->Get the full list of PyTorch operators": [
        [
            "PyTorch provides a full list of extensible C++ operators in generated file\nbuild/aten/src/ATen/RegistrationDeclarations.h.\nThis file is only available after building PyTorch from source.\nHere\u2019s a snippet of the file:",
            "markdown"
        ],
        [
            "Tensor abs(const Tensor &amp; self); // {\"schema\": \"aten::abs(Tensor self) -&gt; Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor &amp; abs_(Tensor &amp; self); // {\"schema\": \"aten::abs_(Tensor(a!) self) -&gt; Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor &amp; abs_out(Tensor &amp; out, const Tensor &amp; self); // {\"schema\": \"aten::abs.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"False\"}\nTensor absolute(const Tensor &amp; self); // {\"schema\": \"aten::absolute(Tensor self) -&gt; Tensor\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor &amp; absolute_(Tensor &amp; self); // {\"schema\": \"aten::absolute_(Tensor(a!) self) -&gt; Tensor(a!)\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor &amp; absolute_out(Tensor &amp; out, const Tensor &amp; self); // {\"schema\": \"aten::absolute.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor angle(const Tensor &amp; self); // {\"schema\": \"aten::angle(Tensor self) -&gt; Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor &amp; angle_out(Tensor &amp; out, const Tensor &amp; self); // {\"schema\": \"aten::angle.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"False\"}\nTensor sgn(const Tensor &amp; self); // {\"schema\": \"aten::sgn(Tensor self) -&gt; Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}",
            "code"
        ],
        [
            "There\u2019re multiple fields associated with a single operator. Let\u2019s break it down using abs_out as an example:",
            "markdown"
        ],
        [
            "Tensor &amp; abs_out(Tensor &amp; out, const Tensor &amp; self); is the C++ signature of the operator, your C++\nkernel should match this signature exactly.",
            "markdown"
        ],
        [
            "aten::abs.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!) is the unique schema representing the operator,\nwhich also contains aliasing and mutation annotations compared to the C++ signature.  This is the unique identifier\nthe dispatcher uses to find an operator.",
            "markdown"
        ],
        [
            "dispatch and default are boolean fields that provide information about what native PyTorch kernels\ncan do, thus implies whether it\u2019s required for backend extenders to implement the kernel.\nMore details can be found in .",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->Register kernels for the new backend": [
        [
            "To register your kernels to PyTorch dispatcher, you can use the\nTORCH_LIBRARY_IMPL API described in\n:",
            "markdown"
        ],
        [
            "TORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  m.impl(&lt;schema_my_op1&gt;, &amp;my_op1);\n  m.impl(&lt;schema_my_op2&gt;, &amp;my_op2);\n  m.impl(&lt;schema_my_op2_backward&gt;, &amp;my_op2_backward);\n}",
            "code"
        ],
        [
            "Now let\u2019s zoom in and what operator requires a kernel from a customized backend and what\u2019s\ninside the kernels exactly.",
            "markdown"
        ],
        [
            "PyTorch currently has more than 1600 operators and it\u2019s still growing.  It\u2019s unrealistic\nfor backend extensions to keep up with this speed.  Even for native backends like CPU\nor CUDA, it often requires a lot of work to write dedicated kernels for every new op.",
            "markdown"
        ],
        [
            "Fortunately, some native PyTorch kernels are written in a way that they decompose to\ncombination of several known operators. In other words, you only need to implement\na set of known operators (ops that require registration below) instead of all PyTorch operators.",
            "markdown"
        ],
        [
            "PyTorch operators can be classified into two categories:",
            "markdown"
        ],
        [
            "Ops that require registration: PyTorch native implementation for these ops is backend specific\nand thus it\u2019s required to provide a kernel for customized backend.  Otherwise calling such op\non the customized backend will error out.\n<blockquote>",
            "markdown"
        ],
        [
            "In RegistrationDeclarations.h these operators have dispatch set to True <em>and</em> default set to False\nin the metadata found in their accompanying comments.\n\n</blockquote>",
            "markdown"
        ],
        [
            "Registration is optional: backend extenders can skip registering to these ops without sacrificing any support.\nHowever, if a backend extender wants to override the default kernel provided by PyTorch, they can still\nregister their customized kernel to their backend and the dispatcher will use it for your backend only.\nFor example, current implementation of PyTorch\u2019s max_pool2d returns indices as part of forward outputs which\ncreates overhead in torch_xla, so torch_xla registers its own kernel for max_pool2d instead.\n<blockquote>",
            "markdown"
        ],
        [
            "In RegistrationDeclarations.h these operators have dispatch set to False <em>or</em> default set to True\nin the metadata found in their accompanying comments.\n\n</blockquote>",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->Autograd support for the new backend": [
        [
            "Gradient formulas are mostly purely mathematical and thus are general for all backends.\nPyTorch often registers a kernel to alias dispatch key Autograd, which means it can be used by all backends.",
            "markdown"
        ],
        [
            "For these operators you don\u2019t have to worry about their derivative formulas,\nyou can just write forward definitions for operators in RegistrationDeclarations.h and PyTorch handles\nbackward for you automatically.",
            "markdown"
        ],
        [
            "Tensor my_op1(const Tensor&amp; self, const Tensor&amp; other) {\n  // call your backend-specific APIs to implement my_op so that\n  // it matches PyTorch's native behavior\n}\nTORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  m.impl(&lt;schema_my_op1&gt;, &amp;my_op);\n}",
            "code"
        ],
        [
            "In some cases, PyTorch backward kernel implementations are also device specific so that they can squeeze out\nmax performance out of each backend. For those operators you\u2019ll see op_backward showing up in\nRegistrationDeclarations.h as <em>required registration</em> as well.",
            "markdown"
        ],
        [
            "Tensor my_op2_backward(const Tensor&amp; self, const Tensor&amp; other) {\n  // call your backend-specific APIs to implement my_op2_backward so that\n  // it matches PyTorch's native behavior\n}\n\n// Note backward kernel is still registered to PrivateUse1 instead of AutogradPrivateUse1.\n// PyTorch will wrap your backward kernel with proper autograd setup and then link to it in\n// my_op2's AutogradPrivateUse1 kernel.\nTORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  m.impl(&lt;schema_my_op2&gt;, &amp;my_op2);\n  m.impl(&lt;schema_my_op2_backward&gt;, &amp;my_op2_backward);\n}",
            "code"
        ],
        [
            "In a few <em>rare</em> cases, PyTorch\u2019s gradient formula for certain operators may have assumptions that don\u2019t generalize\nfor all backends. In those cases backend extenders can optionally override PyTorch Autograd layer by registering\na kernel from torch::autograd::Function to the corresponding dispatch key (for example, AutogradPrivateUse1 if\nyou\u2019re using PrivateUse1 for your backend):",
            "markdown"
        ],
        [
            "class MyAddFunction : public torch::autograd::Function&lt;MyAddFunction&gt; {\n  public:\n  static Tensor forward(AutogradContext *ctx, torch::Tensor self, torch::Tensor other) {\n    at::AutoNonVariableTypeMode g;\n    return myadd(self, other);\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    auto grad_output = grad_outputs[0];\n    return {grad_output, grad_output};\n  }\n};\n\nTensor myadd_autograd(const Tensor&amp; self, const Tensor&amp; other) {\n  return MyAddFunction::apply(self, other)[0];\n}\n\n// Register the autograd kernel to AutogradPrivateUse1\nTORCH_LIBRARY_IMPL(aten, AutogradPrivateUse1, m) {\n  m.impl(&lt;myadd_schema&gt;, &amp;myadd_autograd);\n}\n\n// Register the inference kernel to PrivateUse1\nTORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  m.impl(&lt;myadd_schema&gt;, &amp;myadd);\n}",
            "code"
        ],
        [
            "With this trick you have full control over both training and inference behavior for my_add operator in your backend.\nHere\u2019s  in the pytorch/xla repository.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->Build an extension": [
        [
            "Out-of-tree backend is supported by adding a C++ extension to PyTorch.\nOnce you have kernels and registrations ready, you can build a C++ extension by\nwriting a setup.py script that uses setuptools to compile C++ code.  Here\u2019s a simplified example from\n:",
            "markdown"
        ],
        [
            "from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\n\nsetup(\n    name='torch_xla',\n    ext_modules=[\n        CppExtension(\n            '_XLAC',\n            torch_xla_sources,\n            include_dirs=include_dirs,\n            extra_compile_args=extra_compile_args,\n            library_dirs=library_dirs,\n            extra_link_args=extra_link_args + \\\n                [make_relative_rpath('torch_xla/lib')],\n        ),\n    ],\n    cmdclass={\n        'build_ext': Build,  # Build is a derived class of BuildExtension\n    }\n    # more configs...\n)",
            "code"
        ],
        [
            "See \nfor more details.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->Custom operator support": [
        [
            "Your new backend should work seamlessly with\n\nwithout writing any new kernels as long as the customized operator is composed of existing\nPyTorch operators (which are already supported by your backend).",
            "markdown"
        ],
        [
            "For  they often come with a\n\nas well as .\nTo support these operators, backend extenders will need to write a C++ kernel for your backend and properly\nregister it to the corresponding namespace in the dispatcher similar to supporting PyTorch native operators.\nAlternatively you could also add a customized API in your extension e.g torch_xla.core.functions.nms for\nthese adhoc requests.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->JIT support": [
        [
            "As we mentioned in , kernels registered through <cite>m.impl()</cite> API\nsupport being called in both unboxed and boxed ways. In other words your customized backend can also work with our\nJIT tracing/scripting frontend just like the in-tree backends like CPU or CUDA do.  You could potentially also write specialized optimization\npasses for your backend on a JIT graph.  But we will not discuss it here since we haven\u2019t finalized the integration point\nin JIT, so the current backend support will focus on the eager frontend for now.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->Testing your backend against native PyTorch backends": [
        [
            "PyTorch lets tests run on multiple device types using its .\nYou can find details about \nand information about .\nOnce added, PyTorch tests using the generic device type testing framework will be run using your device type, too.\nSee  for an example of how tests are instantiated.",
            "markdown"
        ],
        [
            "Running PyTorch\u2019s existing test suites with your device type is important to ensure correctness,\nbut not all PyTorch features are supported by every device type.  The generic device type testing\nframework allows for considerable customization so that device types can select which tests to run,\nwhich dtypes they support, and even which precisions to use when comparing tensors for equality.",
            "markdown"
        ],
        [
            "An example device type that uses the generic device type testing framework and doesn\u2019t ship with\nPyTorch is XLA.  See ,\nwhich contains examples of block listing tests, block listing dtypes, and overriding test precision.",
            "markdown"
        ],
        [
            "The generic device type testing framework is actively developed. To request a feature please file an\nissue on PyTorch\u2019s Github.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->Backward Compatibility": [
        [
            "Currently PyTorch can\u2019t guarantee backward compatibility for registered operators.\nOperators, as well as their schemas, might be added/modified/deleted as needed.  Registered\nkernels must be <em>exactly</em> the same as PyTorch version.  If PyTorch adds more parameters (\neven with defaults) for an operator, your old registration won\u2019t work until it\u2019s updated\nto match PyTorch\u2019s new signature.",
            "markdown"
        ],
        [
            "As a result, we <em>highly recommend</em> out-of-tree backend extenders only sync with major PyTorch\nreleases to minimize interruptions in development.  PyTorch is on a quarterly release cadence.\nBackend extenders should join the <em>#announcement</em> channel at \nto get latest updates on releases.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->Known issues & additional notes": [
        [
            "Not all test suites are device generic yet. Extensible test classes can be found by searching\ninstantiate_device_type_tests in PyTorch codebase, e.g\nTestTorchDeviceType, TestViewOps, TestTensorDeviceOps, TestTypePromotion etc.",
            "markdown"
        ],
        [
            "There\u2019s no extension point in C++ for serializing a python Tensor object on customized backend. Currently\nyou can only extend it by modifying \nor monkey patching in out-of-tree repository.",
            "markdown"
        ],
        [
            "If your backend doesn\u2019t allow direct memory access, you should pay additional attention to supporting\nview ops since they\u2019re supposed to share storage. Changes to view tensor need to propagated to its\nbase tensor and vice versa.",
            "markdown"
        ],
        [
            "There\u2019s no extension point in C++ for Optimizer if your backend doesn\u2019t work with the native PyTorch\nOptimizers, e.g. need to carry the states to be updated in backward like torch-xla. Such use cases\ncurrently can only be done through adding customized API or monkey patching in out-of-tree repository.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->Future Work": [
        [
            "Making every component in PyTorch extensible for an out-of-tree backend seamless\nrequires a lot of changes to PyTorch internals.  Here are a few items that we\u2019re\nactively working on might improve the experience in the future:",
            "markdown"
        ],
        [
            "Improve test coverage of generic testing framework.",
            "markdown"
        ],
        [
            "Improve Math kernel coverage and more comprehensive tests to make sure Math\nkernel bahavior matches other backends like CPU/CUDA.",
            "markdown"
        ],
        [
            "Refactor RegistrationDeclarations.h to carry the minimal information and reuse\nPyTorch\u2019s codegen as much as possible.",
            "markdown"
        ],
        [
            "Support a backend fallback kernel to automatic convert inputs to CPU and convert the\nresult back to the customized backend. This will allow \u201cfull\u201d operator coverage even\nthough you don\u2019t have kernels written for every operator.",
            "markdown"
        ]
    ],
    "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->Stay in touch": [
        [
            "Please use  for questions and discussions. If you have\nany feature requests or bug reports, please .",
            "markdown"
        ],
        [
            "If you\u2019re interested in helping in any of the future work items above (e.g adding more Math\nkernels for PyTorch operators in C++), please reach out to us through Github or Slack!",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Profiling your PyTorch Module": [
        [
            "<strong>Author:</strong> ",
            "markdown"
        ],
        [
            "PyTorch includes a profiler API that is useful to identify the time and\nmemory costs of various PyTorch operations in your code. Profiler can be\neasily integrated in your code, and the results can be printed as a table\nor retured in a JSON trace file.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Profiler supports multithreaded models. Profiler runs in the\nsame thread as the operation but it will also profile child operators\nthat might run in another thread. Concurrently-running profilers will be\nscoped to their own thread to prevent mixing of results.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "PyTorch 1.8 introduces the new API that will replace the older profiler API\nin the future releases. Check the new API at .",
            "markdown"
        ],
        [
            "Head on over to \nfor a quicker walkthrough of Profiler API usage.",
            "markdown"
        ],
        [
            "import torch\nimport numpy as np\nfrom torch import nn\nimport torch.autograd.profiler as profiler",
            "code"
        ]
    ],
    "torch->Model Optimization->Profiling your PyTorch Module->Performance debugging using Profiler": [
        [
            "Profiler can be useful to identify performance bottlenecks in your\nmodels. In this example, we build a custom module that performs two\nsub-tasks:",
            "markdown"
        ],
        [
            "a linear transformation on the input, and",
            "markdown"
        ],
        [
            "use the transformation result to get indices on a mask tensor.",
            "markdown"
        ],
        [
            "We wrap the code for each sub-task in separate labelled context managers using\nprofiler.record_function(\"label\"). In the profiler output, the\naggregate performance metrics of all operations in the sub-task will\nshow up under its corresponding label.",
            "markdown"
        ],
        [
            "Note that using Profiler incurs some overhead, and is best used only for investigating\ncode. Remember to remove it if you are benchmarking runtimes.",
            "markdown"
        ],
        [
            "class MyModule():\n    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n        super(MyModule, self).__init__()\n        self.linear = (in_features, out_features, bias)\n\n    def forward(self, input, mask):\n        with profiler.record_function(\"LINEAR PASS\"):\n            out = self.linear(input)\n\n        with profiler.record_function(\"MASK INDICES\"):\n            threshold = out.sum(axis=1).mean().item()\n            hi_idx = np.argwhere(mask.cpu().numpy() &gt; threshold)\n            hi_idx = (hi_idx).cuda()\n\n        return out, hi_idx",
            "code"
        ]
    ],
    "torch->Model Optimization->Profiling your PyTorch Module->Profile the forward pass": [
        [
            "We initialize random input and mask tensors, and the model.",
            "markdown"
        ],
        [
            "Before we run the profiler, we warm-up CUDA to ensure accurate\nperformance benchmarking. We wrap the forward pass of our module in the\nprofiler.profile context manager. The with_stack=True parameter appends the\nfile and line number of the operation in the trace.",
            "markdown"
        ],
        [
            "Warning",
            "markdown"
        ],
        [
            "with_stack=True incurs an additional overhead, and is better suited for investigating code.\nRemember to remove it if you are benchmarking performance.",
            "markdown"
        ],
        [
            "model = MyModule(500, 10).cuda()\ninput = (128, 500).cuda()\nmask = ((500, 500, 500), dtype=torch.double).cuda()\n\n# warm-up\nmodel(input, mask)\n\nwith (with_stack=True, profile_memory=True) as prof:\n    out, idx = model(input, mask)",
            "code"
        ]
    ],
    "torch->Model Optimization->Profiling your PyTorch Module->Print profiler results": [
        [
            "Finally, we print the profiler results. profiler.key_averages\naggregates the results by operator name, and optionally by input\nshapes and/or stack trace events.\nGrouping by input shapes is useful to identify which tensor shapes\nare utilized by the model.",
            "markdown"
        ],
        [
            "Here, we use group_by_stack_n=5 which aggregates runtimes by the\noperation and its traceback (truncated to the most recent 5 events), and\ndisplay the events in the order they are registered. The table can also\nbe sorted by passing a sort_by argument (refer to the\n for\nvalid sorting keys).",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "When running profiler in a notebook, you might see entries like &lt;ipython-input-18-193a910735e8&gt;(13): forward\ninstead of filenames in the stacktrace. These correspond to &lt;notebook-cell&gt;(line number): calling-function.",
            "markdown"
        ],
        [
            "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n\n\"\"\"\n(Some columns are omitted)\n\n-------------  ------------  ------------  ------------  ---------------------------------\n         Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\n-------------  ------------  ------------  ------------  ---------------------------------\n MASK INDICES        87.88%        5.212s    -953.67 Mb  /mnt/xarfuse/.../torch/au\n                                                         &lt;ipython-input-...&gt;(10): forward\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                         /mnt/xarfuse/.../IPython/\n\n  aten::copy_        12.07%     715.848ms           0 b  &lt;ipython-input-...&gt;(12): forward\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                         /mnt/xarfuse/.../IPython/\n                                                         /mnt/xarfuse/.../IPython/\n\n  LINEAR PASS         0.01%     350.151us         -20 b  /mnt/xarfuse/.../torch/au\n                                                         &lt;ipython-input-...&gt;(7): forward\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                         /mnt/xarfuse/.../IPython/\n\n  aten::addmm         0.00%     293.342us           0 b  /mnt/xarfuse/.../torch/nn\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(8): forward\n                                                         /mnt/xarfuse/.../torch/nn\n\n   aten::mean         0.00%     235.095us           0 b  &lt;ipython-input-...&gt;(11): forward\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                         /mnt/xarfuse/.../IPython/\n                                                         /mnt/xarfuse/.../IPython/\n\n-----------------------------  ------------  ---------- ----------------------------------\nSelf CPU time total: 5.931s\n\n\"\"\"",
            "code"
        ]
    ],
    "torch->Model Optimization->Profiling your PyTorch Module->Improve memory performance": [
        [
            "Note that the most expensive operations - in terms of memory and time -\nare at forward (10) representing the operations within MASK INDICES. Let\u2019s try to\ntackle the memory consumption first. We can see that the .to()\noperation at line 12 consumes 953.67 Mb. This operation copies mask to the CPU.\nmask is initialized with a torch.double datatype. Can we reduce the memory footprint by casting\nit to torch.float instead?",
            "markdown"
        ],
        [
            "model = MyModule(500, 10).cuda()\ninput = (128, 500).cuda()\nmask = ((500, 500, 500), dtype=torch.float).cuda()\n\n# warm-up\nmodel(input, mask)\n\nwith (with_stack=True, profile_memory=True) as prof:\n    out, idx = model(input, mask)\n\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n\n\"\"\"\n(Some columns are omitted)\n\n-----------------  ------------  ------------  ------------  --------------------------------\n             Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\n-----------------  ------------  ------------  ------------  --------------------------------\n     MASK INDICES        93.61%        5.006s    -476.84 Mb  /mnt/xarfuse/.../torch/au\n                                                             &lt;ipython-input-...&gt;(10): forward\n                                                             /mnt/xarfuse/  /torch/nn\n                                                             &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                             /mnt/xarfuse/.../IPython/\n\n      aten::copy_         6.34%     338.759ms           0 b  &lt;ipython-input-...&gt;(12): forward\n                                                             /mnt/xarfuse/.../torch/nn\n                                                             &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                             /mnt/xarfuse/.../IPython/\n                                                             /mnt/xarfuse/.../IPython/\n\n aten::as_strided         0.01%     281.808us           0 b  &lt;ipython-input-...&gt;(11): forward\n                                                             /mnt/xarfuse/.../torch/nn\n                                                             &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                             /mnt/xarfuse/.../IPython/\n                                                             /mnt/xarfuse/.../IPython/\n\n      aten::addmm         0.01%     275.721us           0 b  /mnt/xarfuse/.../torch/nn\n                                                             /mnt/xarfuse/.../torch/nn\n                                                             /mnt/xarfuse/.../torch/nn\n                                                             &lt;ipython-input-...&gt;(8): forward\n                                                             /mnt/xarfuse/.../torch/nn\n\n      aten::_local        0.01%     268.650us           0 b  &lt;ipython-input-...&gt;(11): forward\n      _scalar_dense                                          /mnt/xarfuse/.../torch/nn\n                                                             &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                             /mnt/xarfuse/.../IPython/\n                                                             /mnt/xarfuse/.../IPython/\n\n-----------------  ------------  ------------  ------------  --------------------------------\nSelf CPU time total: 5.347s\n\n\"\"\"",
            "code"
        ],
        [
            "The CPU memory footprint for this operation has halved.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Profiling your PyTorch Module->Improve time performance": [
        [
            "While the time consumed has also reduced a bit, it\u2019s still too high.\nTurns out copying a matrix from CUDA to CPU is pretty expensive!\nThe aten::copy_ operator in forward (12) copies mask to CPU\nso that it can use the NumPy argwhere function. aten::copy_ at forward(13)\ncopies the array back to CUDA as a tensor. We could eliminate both of these if we use a\ntorch function nonzero() here instead.",
            "markdown"
        ],
        [
            "class MyModule():\n    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n        super(MyModule, self).__init__()\n        self.linear = (in_features, out_features, bias)\n\n    def forward(self, input, mask):\n        with profiler.record_function(\"LINEAR PASS\"):\n            out = self.linear(input)\n\n        with profiler.record_function(\"MASK INDICES\"):\n            threshold = out.sum(axis=1).mean()\n            hi_idx = (mask &gt; threshold).nonzero(as_tuple=True)\n\n        return out, hi_idx\n\n\nmodel = MyModule(500, 10).cuda()\ninput = (128, 500).cuda()\nmask = ((500, 500, 500), dtype=torch.float).cuda()\n\n# warm-up\nmodel(input, mask)\n\nwith (with_stack=True, profile_memory=True) as prof:\n    out, idx = model(input, mask)\n\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n\n\"\"\"\n(Some columns are omitted)\n\n--------------  ------------  ------------  ------------  ---------------------------------\n          Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\n--------------  ------------  ------------  ------------  ---------------------------------\n      aten::gt        57.17%     129.089ms           0 b  &lt;ipython-input-...&gt;(12): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          &lt;ipython-input-...&gt;(25): &lt;module&gt;\n                                                          /mnt/xarfuse/.../IPython/\n                                                          /mnt/xarfuse/.../IPython/\n\n aten::nonzero        37.38%      84.402ms           0 b  &lt;ipython-input-...&gt;(12): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          &lt;ipython-input-...&gt;(25): &lt;module&gt;\n                                                          /mnt/xarfuse/.../IPython/\n                                                          /mnt/xarfuse/.../IPython/\n\n   INDEX SCORE         3.32%       7.491ms    -119.21 Mb  /mnt/xarfuse/.../torch/au\n                                                          &lt;ipython-input-...&gt;(10): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          &lt;ipython-input-...&gt;(25): &lt;module&gt;\n                                                          /mnt/xarfuse/.../IPython/\n\naten::as_strided         0.20%    441.587us          0 b  &lt;ipython-input-...&gt;(12): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          &lt;ipython-input-...&gt;(25): &lt;module&gt;\n                                                          /mnt/xarfuse/.../IPython/\n                                                          /mnt/xarfuse/.../IPython/\n\n aten::nonzero\n     _numpy             0.18%     395.602us           0 b  &lt;ipython-input-...&gt;(12): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          &lt;ipython-input-...&gt;(25): &lt;module&gt;\n                                                          /mnt/xarfuse/.../IPython/\n                                                          /mnt/xarfuse/.../IPython/\n--------------  ------------  ------------  ------------  ---------------------------------\nSelf CPU time total: 225.801ms\n\n\"\"\"",
            "code"
        ]
    ],
    "torch->Model Optimization->Profiling your PyTorch Module->Further Reading": [
        [
            "We have seen how Profiler can be used to investigate time and memory bottlenecks in PyTorch models.\nRead more about Profiler here:",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->Model Optimization->PyTorch Profiler With TensorBoard": [
        [
            "This tutorial demonstrates how to use TensorBoard plugin with PyTorch Profiler\nto detect performance bottlenecks of the model.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->PyTorch Profiler With TensorBoard->Introduction": [
        [
            "PyTorch 1.8 includes an updated profiler API capable of\nrecording the CPU side operations as well as the CUDA kernel launches on the GPU side.\nThe profiler can visualize this information\nin TensorBoard Plugin and provide analysis of the performance bottlenecks.",
            "markdown"
        ],
        [
            "In this tutorial, we will use a simple Resnet model to demonstrate how to\nuse TensorBoard plugin to analyze model performance.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->PyTorch Profiler With TensorBoard->Setup": [
        [
            "To install torch and torchvision use the following command:",
            "markdown"
        ],
        [
            "pip install torch torchvision",
            "code"
        ]
    ],
    "torch->Model Optimization->PyTorch Profiler With TensorBoard->Steps": [
        [
            "Prepare the data and model",
            "markdown"
        ],
        [
            "Use profiler to record execution events",
            "markdown"
        ],
        [
            "Run the profiler",
            "markdown"
        ],
        [
            "Use TensorBoard to view results and analyze model performance",
            "markdown"
        ],
        [
            "Improve performance with the help of profiler",
            "markdown"
        ],
        [
            "Analyze performance with other advanced features",
            "markdown"
        ]
    ],
    "torch->Model Optimization->PyTorch Profiler With TensorBoard->Steps->1. Prepare the data and model": [
        [
            "First, import all necessary libraries:",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn\nimport torch.optim\nimport torch.profiler\nimport torch.utils.data\nimport torchvision.datasets\nimport torchvision.models\nimport torchvision.transforms as T",
            "code"
        ],
        [
            "Then prepare the input data. For this tutorial, we use the CIFAR10 dataset.\nTransform it to the desired format and use DataLoader to load each batch.",
            "markdown"
        ],
        [
            "transform = (\n    [(224),\n     (),\n     ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ntrain_set = (root='./data', train=True, download=True, transform=transform)\ntrain_loader = (train_set, batch_size=32, shuffle=True)",
            "code"
        ],
        [
            "Next, create Resnet model, loss function, and optimizer objects.\nTo run on GPU, move model and loss to GPU device.",
            "markdown"
        ],
        [
            "device = (\"cuda:0\")\nmodel = (pretrained=True).cuda(device)\ncriterion = ().cuda(device)\noptimizer = (model.parameters(), lr=0.001, momentum=0.9)\nmodel.train()",
            "code"
        ],
        [
            "Define the training step for each batch of input data.",
            "markdown"
        ],
        [
            "def train(data):\n    inputs, labels = data[0].to(device=device), data[1].to(device=device)\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()",
            "code"
        ]
    ],
    "torch->Model Optimization->PyTorch Profiler With TensorBoard->Steps->2. Use profiler to record execution events": [
        [
            "The profiler is enabled through the context manager and accepts several parameters,\nsome of the most useful are:",
            "markdown"
        ],
        [
            "schedule - callable that takes step (int) as a single parameter\nand returns the profiler action to perform at each step.",
            "markdown"
        ],
        [
            "In this example with wait=1, warmup=1, active=3, repeat=2,\nprofiler will skip the first step/iteration,\nstart warming up on the second,\nrecord the following three iterations,\nafter which the trace will become available and on_trace_ready (when set) is called.\nIn total, the cycle repeats twice. Each cycle is called a \u201cspan\u201d in TensorBoard plugin.",
            "markdown"
        ],
        [
            "During wait steps, the profiler is disabled.\nDuring warmup steps, the profiler starts tracing but the results are discarded.\nThis is for reducing the profiling overhead.\nThe overhead at the beginning of profiling is high and easy to bring skew to the profiling result.\nDuring active steps, the profiler works and records events.",
            "markdown"
        ],
        [
            "on_trace_ready - callable that is called at the end of each cycle;\nIn this example we use torch.profiler.tensorboard_trace_handler to generate result files for TensorBoard.\nAfter profiling, result files will be saved into the ./log/resnet18 directory.\nSpecify this directory as a logdir parameter to analyze profile in TensorBoard.",
            "markdown"
        ],
        [
            "record_shapes - whether to record shapes of the operator inputs.",
            "markdown"
        ],
        [
            "profile_memory - Track tensor memory allocation/deallocation. Note, for old version of pytorch with version\nbefore 1.10, if you suffer long profiling time, please disable it or upgrade to new version.",
            "markdown"
        ],
        [
            "with_stack - Record source information (file and line number) for the ops.\nIf the TensorBoard is launched in VSCode (),\nclicking a stack frame will navigate to the specific code line.",
            "markdown"
        ],
        [
            "with (\n        schedule=(wait=1, warmup=1, active=3, repeat=2),\n        on_trace_ready=('./log/resnet18'),\n        record_shapes=True,\n        profile_memory=True,\n        with_stack=True\n) as prof:\n    for step, batch_data in enumerate(train_loader):\n        if step &gt;= (1 + 1 + 3) * 2:\n            break\n        train(batch_data)\n        prof.step()  # Need to call this at the end of each step to notify profiler of steps' boundary.",
            "code"
        ],
        [
            "Alternatively, the following non-context manager start/stop is supported as well.",
            "markdown"
        ],
        [
            "prof = (\n        schedule=(wait=1, warmup=1, active=3, repeat=2),\n        on_trace_ready=('./log/resnet18'),\n        record_shapes=True,\n        with_stack=True)\nprof.start()\nfor step, batch_data in enumerate(train_loader):\n    if step &gt;= (1 + 1 + 3) * 2:\n        break\n    train(batch_data)\n    prof.step()\nprof.stop()",
            "code"
        ]
    ],
    "torch->Model Optimization->PyTorch Profiler With TensorBoard->Steps->3. Run the profiler": [
        [
            "Run the above code. The profiling result will be saved under ./log/resnet18 directory.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->PyTorch Profiler With TensorBoard->Steps->4. Use TensorBoard to view results and analyze model performance": [
        [
            "Install PyTorch Profiler TensorBoard Plugin.",
            "markdown"
        ],
        [
            "pip install torch_tb_profiler",
            "code"
        ],
        [
            "Launch the TensorBoard.",
            "markdown"
        ],
        [
            "tensorboard --logdir=./log",
            "code"
        ],
        [
            "Open the TensorBoard profile URL in Google Chrome browser or Microsoft Edge browser.",
            "markdown"
        ],
        [
            "http://localhost:6006/#pytorch_profiler",
            "code"
        ],
        [
            "You could see Profiler plugin page as shown below.",
            "markdown"
        ],
        [
            "Overview",
            "markdown"
        ],
        [
            "The overview shows a high-level summary of model performance.",
            "markdown"
        ],
        [
            "The \u201cGPU Summary\u201d panel shows the GPU configuration, GPU usage and Tensor Cores usage.\nIn this example, the GPU Utilization is low.\nThe details of these metrics are .",
            "markdown"
        ],
        [
            "The \u201cStep Time Breakdown\u201d shows distribution of time spent in each step over different categories of execution.\nIn this example, you can see the DataLoader overhead is significant.",
            "markdown"
        ],
        [
            "The bottom \u201cPerformance Recommendation\u201d uses the profiling data\nto automatically highlight likely bottlenecks,\nand gives you actionable optimization suggestions.",
            "markdown"
        ],
        [
            "You can change the view page in left \u201cViews\u201d dropdown list.\n<img alt=\"\" src=\"../_static/img/profiler_views_list.png\"/>",
            "markdown"
        ],
        [
            "Operator view",
            "markdown"
        ],
        [
            "The operator view displays the performance of every PyTorch operator\nthat is executed either on the host or device.",
            "markdown"
        ],
        [
            "The \u201cSelf\u201d duration does not include its child operators\u2019 time.\nThe \u201cTotal\u201d duration includes its child operators\u2019 time.",
            "markdown"
        ],
        [
            "View call stack",
            "markdown"
        ],
        [
            "Click the \u201cView Callstack\u201d of an operator, the operators with same name but different call stacks will be shown.\nThen click a \u201cView Callstack\u201d in this sub-table, the call stack frames will be shown.",
            "markdown"
        ],
        [
            "If the TensorBoard is launched inside VSCode\n(),\nclicking a call stack frame will navigate to the specific code line.",
            "markdown"
        ],
        [
            "Kernel view",
            "markdown"
        ],
        [
            "The GPU kernel view shows all kernels\u2019 time spent on GPU.",
            "markdown"
        ],
        [
            "Tensor Cores Used:\nWhether this kernel uses Tensor Cores.",
            "markdown"
        ],
        [
            "Mean Blocks per SM:\nBlocks per SM = Blocks of this kernel / SM number of this GPU.\nIf this number is less than 1, it indicates the GPU multiprocessors are not fully utilized.\n\u201cMean Blocks per SM\u201d is weighted average of all runs of this kernel name, using each run\u2019s duration as weight.",
            "markdown"
        ],
        [
            "Mean Est. Achieved Occupancy:\nEst. Achieved Occupancy is defined in this column\u2019s tooltip.\nFor most cases such as memory bandwidth bounded kernels, the higher the better.\n\u201cMean Est. Achieved Occupancy\u201d is weighted average of all runs of this kernel name,\nusing each run\u2019s duration as weight.",
            "markdown"
        ],
        [
            "Trace view",
            "markdown"
        ],
        [
            "The trace view shows timeline of profiled operators and GPU kernels.\nYou can select it to see details as below.",
            "markdown"
        ],
        [
            "You can move the graph and zoom in/out with the help of right side toolbar.\nAnd keyboard can also be used to zoom and move around inside the timeline.\nThe \u2018w\u2019 and \u2018s\u2019 keys zoom in centered around the mouse,\nand the \u2018a\u2019 and \u2018d\u2019 keys move the timeline left and right.\nYou can hit these keys multiple times until you see a readable representation.",
            "markdown"
        ],
        [
            "If a backward operator\u2019s \u201cIncoming Flow\u201d field is with value \u201cforward correspond to backward\u201d,\nyou can click the text to get its launching forward operator.",
            "markdown"
        ],
        [
            "In this example, we can see the event prefixed with enumerate(DataLoader) costs a lot of time.\nAnd during most of this period, the GPU is idle.\nBecause this function is loading data and transforming data on host side,\nduring which the GPU resource is wasted.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->PyTorch Profiler With TensorBoard->Steps->5. Improve performance with the help of profiler": [
        [
            "At the bottom of \u201cOverview\u201d page, the suggestion in \u201cPerformance Recommendation\u201d hints the bottleneck is DataLoader.\nThe PyTorch DataLoader uses single process by default.\nUser could enable multi-process data loading by setting the parameter num_workers.\n is more details.",
            "markdown"
        ],
        [
            "In this example, we follow the \u201cPerformance Recommendation\u201d and set num_workers as below,\npass a different name such as ./log/resnet18_4workers to tensorboard_trace_handler, and run it again.",
            "markdown"
        ],
        [
            "train_loader = (train_set, batch_size=32, shuffle=True, num_workers=4)",
            "code"
        ],
        [
            "Then let\u2019s choose the recently profiled run in left \u201cRuns\u201d dropdown list.",
            "markdown"
        ],
        [
            "From the above view, we can find the step time is reduced to about 76ms comparing with previous run\u2019s 132ms,\nand the time reduction of DataLoader mainly contributes.",
            "markdown"
        ],
        [
            "From the above view, we can see that the runtime of enumerate(DataLoader) is reduced,\nand the GPU utilization is increased.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->PyTorch Profiler With TensorBoard->Steps->6. Analyze performance with other advanced features": [
        [
            "Memory view",
            "markdown"
        ],
        [
            "To profile memory, profile_memory must be set to True in arguments of torch.profiler.profile.",
            "markdown"
        ],
        [
            "You can try it by using existing example on Azure",
            "markdown"
        ],
        [
            "pip install azure-storage-blob\ntensorboard --logdir=https://torchtbprofiler.blob.core.windows.net/torchtbprofiler/demo/memory_demo_1_10",
            "code"
        ],
        [
            "The profiler records all memory allocation/release events and allocator\u2019s internal state during profiling.\nThe memory view consists of three components as shown in the following.",
            "markdown"
        ],
        [
            "The components are memory curve graph, memory events table and memory statistics table, from top to bottom, respectively.",
            "markdown"
        ],
        [
            "The memory type could be selected in \u201cDevice\u201d selection box.\nFor example, \u201cGPU0\u201d means the following table only shows each operator\u2019s memory usage on GPU 0, not including CPU or other GPUs.",
            "markdown"
        ],
        [
            "The memory curve shows the trends of memory consumption. The \u201cAllocated\u201d curve shows the total memory that is actually\nin use, e.g., tensors. In PyTorch, caching mechanism is employed in CUDA allocator and some other allocators. The\n\u201cReserved\u201d curve shows the total memory that is reserved by the allocator. You can left click and drag on the graph\nto select events in the desired range:",
            "markdown"
        ],
        [
            "After selection, the three components will be updated for the restricted time range, so that you can gain more\ninformation about it. By repeating this process, you can zoom into a very fine-grained detail. Right click on the graph\nwill reset the graph to the initial state.",
            "markdown"
        ],
        [
            "In the memory events table, the allocation and release events are paired into one entry. The \u201coperator\u201d column shows\nthe immediate ATen operator that is causing the allocation. Notice that in PyTorch, ATen operators commonly use\naten::empty to allocate memory. For example, aten::ones is implemented as aten::empty followed by an\naten::fill_. Solely display the opeartor name as aten::empty is of little help. It will be shown as\naten::ones (aten::empty) in this special case. The \u201cAllocation Time\u201d, \u201cRelease Time\u201d and \u201cDuration\u201d\ncolumns\u2019 data might be missing if the event occurs outside of the time range.",
            "markdown"
        ],
        [
            "In the memory statistics table, the \u201cSize Increase\u201d column sums up all allocation size and minus all the memory\nrelease size, that is, the net increase of memory usage after this operator. The \u201cSelf Size Increase\u201d column is\nsimilar to \u201cSize Increase\u201d, but it does not count children operators\u2019 allocation. With regards to ATen operators\u2019\nimplementation detail, some operators might call other operators, so memory allocations can happen at any level of the\ncall stack. That says, \u201cSelf Size Increase\u201d only count the memory usage increase at current level of call stack.\nFinally, the \u201cAllocation Size\u201d column sums up all allocation without considering the memory release.",
            "markdown"
        ],
        [
            "Distributed view",
            "markdown"
        ],
        [
            "The plugin now supports distributed view on profiling DDP with NCCL/GLOO as backend.",
            "markdown"
        ],
        [
            "You can try it by using existing example on Azure:",
            "markdown"
        ],
        [
            "pip install azure-storage-blob\ntensorboard --logdir=https://torchtbprofiler.blob.core.windows.net/torchtbprofiler/demo/distributed_bert",
            "code"
        ],
        [
            "The \u201cComputation/Communication Overview\u201d shows computation/communication ratio and their overlapping degree.\nFrom this view, User can figure out load balance issue among workers.\nFor example, if the computation + overlapping time of one worker is much larger than others,\nthere may be a problem of load balance or this worker may be a straggler.",
            "markdown"
        ],
        [
            "The \u201cSynchronizing/Communication Overview\u201d shows the efficiency of communication.\n\u201cData Transfer Time\u201d is the time for actual data exchanging.\n\u201cSynchronizing Time\u201d is the time for waiting and synchronizing with other workers.",
            "markdown"
        ],
        [
            "If one worker\u2019s \u201cSynchronizing Time\u201d is much shorter than that of other workers\u2019,\nthis worker may be a straggler which may have more computation workload than other workers\u2019.",
            "markdown"
        ],
        [
            "The \u201cCommunication Operations Stats\u201d summarizes the detailed statistics of all communication ops in each worker.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->PyTorch Profiler With TensorBoard->Learn More": [
        [
            "Take a look at the following documents to continue your learning,\nand feel free to open an issue .",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Hyperparameter tuning with Ray Tune": [
        [
            "Hyperparameter tuning can make the difference between an average model and a highly\naccurate one. Often simple things like choosing a different learning rate or changing\na network layer size can have a dramatic impact on your model performance.",
            "markdown"
        ],
        [
            "Fortunately, there are tools that help with finding the best combination of parameters.\n is an industry standard tool for\ndistributed hyperparameter tuning. Ray Tune includes the latest hyperparameter search\nalgorithms, integrates with TensorBoard and other analysis libraries, and natively\nsupports distributed training through .",
            "markdown"
        ],
        [
            "In this tutorial, we will show you how to integrate Ray Tune into your PyTorch\ntraining workflow. We will extend  for training\na CIFAR10 image classifier.",
            "markdown"
        ],
        [
            "As you will see, we only need to add some slight modifications. In particular, we\nneed to",
            "markdown"
        ],
        [
            "wrap data loading and training in functions,",
            "markdown"
        ],
        [
            "make some network parameters configurable,",
            "markdown"
        ],
        [
            "add checkpointing (optional),",
            "markdown"
        ],
        [
            "and define the search space for the model tuning\n\n\n<br/>",
            "markdown"
        ],
        [
            "To run this tutorial, please make sure the following packages are\ninstalled:",
            "markdown"
        ],
        [
            "ray[tune]: Distributed hyperparameter tuning library",
            "markdown"
        ],
        [
            "torchvision: For the data transformers",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Hyperparameter tuning with Ray Tune->Setup / Imports": [
        [
            "Let\u2019s start with the imports:",
            "markdown"
        ],
        [
            "from functools import partial\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import \nimport torchvision\nimport torchvision.transforms as transforms\nfrom ray import tune\nfrom ray.tune import CLIReporter\nfrom ray.tune.schedulers import ASHAScheduler",
            "code"
        ],
        [
            "Most of the imports are needed for building the PyTorch model. Only the last three\nimports are for Ray Tune.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Hyperparameter tuning with Ray Tune->Data loaders": [
        [
            "We wrap the data loaders in their own function and pass a global data directory.\nThis way we can share a data directory between different trials.",
            "markdown"
        ],
        [
            "def load_data(data_dir=\"./data\"):\n    transform = ([\n        (),\n        ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    trainset = (\n        root=data_dir, train=True, download=True, transform=transform)\n\n    testset = (\n        root=data_dir, train=False, download=True, transform=transform)\n\n    return trainset, testset",
            "code"
        ]
    ],
    "torch->Model Optimization->Hyperparameter tuning with Ray Tune->Configurable neural network": [
        [
            "We can only tune those parameters that are configurable. In this example, we can specify\nthe layer sizes of the fully connected layers:",
            "markdown"
        ],
        [
            "class Net():\n    def __init__(self, l1=120, l2=84):\n        super(, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, l1)\n        self.fc2 = (l1, l2)\n        self.fc3 = (l2, 10)\n\n    def forward(self, x):\n        x = self.pool((self.conv1(x)))\n        x = self.pool((self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = (self.fc1(x))\n        x = (self.fc2(x))\n        x = self.fc3(x)\n        return x",
            "code"
        ]
    ],
    "torch->Model Optimization->Hyperparameter tuning with Ray Tune->The train function": [
        [
            "Now it gets interesting, because we introduce some changes to the example .",
            "markdown"
        ],
        [
            "We wrap the training script in a function train_cifar(config, checkpoint_dir=None, data_dir=None).\nAs you can guess, the config parameter will receive the hyperparameters we would like to\ntrain with. The checkpoint_dir parameter is used to restore checkpoints. The data_dir specifies\nthe directory where we load and store the data, so multiple runs can share the same data source.",
            "markdown"
        ],
        [
            "net = (config[\"l1\"], config[\"l2\"])\n\nif checkpoint_dir:\n    model_state, optimizer_state = (\n        os.path.join(checkpoint_dir, \"checkpoint\"))\n    net.load_state_dict(model_state)\n    optimizer.load_state_dict(optimizer_state)",
            "code"
        ],
        [
            "The learning rate of the optimizer is made configurable, too:",
            "markdown"
        ],
        [
            "optimizer = (net.parameters(), lr=config[\"lr\"], momentum=0.9)",
            "code"
        ],
        [
            "We also split the training data into a training and validation subset. We thus train on\n80% of the data and calculate the validation loss on the remaining 20%. The batch sizes\nwith which we iterate through the training and test sets are configurable as well.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Hyperparameter tuning with Ray Tune->The train function->Adding (multi) GPU support with DataParallel": [
        [
            "Image classification benefits largely from GPUs. Luckily, we can continue to use\nPyTorch\u2019s abstractions in Ray Tune. Thus, we can wrap our model in nn.DataParallel\nto support data parallel training on multiple GPUs:",
            "markdown"
        ],
        [
            "device = \"cpu\"\nif ():\n    device = \"cuda:0\"\n    if () &gt; 1:\n        net = (net)\nnet.to(device)",
            "code"
        ],
        [
            "By using a device variable we make sure that training also works when we have\nno GPUs available. PyTorch requires us to send our data to the GPU memory explicitly,\nlike this:",
            "markdown"
        ],
        [
            "for i, data in enumerate(trainloader, 0):\n    inputs, labels = data\n    inputs, labels = inputs.to(device), labels.to(device)",
            "code"
        ],
        [
            "The code now supports training on CPUs, on a single GPU, and on multiple GPUs. Notably, Ray\nalso supports \nso we can share GPUs among trials, as long as the model still fits on the GPU memory. We\u2019ll come back\nto that later.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Hyperparameter tuning with Ray Tune->The train function->Communicating with Ray Tune": [
        [
            "The most interesting part is the communication with Ray Tune:",
            "markdown"
        ],
        [
            "with tune.checkpoint_dir(epoch) as checkpoint_dir:\n    path = os.path.join(checkpoint_dir, \"checkpoint\")\n    ((net.state_dict(), optimizer.state_dict()), path)\n\ntune.report(loss=(val_loss / val_steps), accuracy=correct / total)",
            "code"
        ],
        [
            "Here we first save a checkpoint and then report some metrics back to Ray Tune. Specifically,\nwe send the validation loss and accuracy back to Ray Tune. Ray Tune can then use these metrics\nto decide which hyperparameter configuration lead to the best results. These metrics\ncan also be used to stop bad performing trials early in order to avoid wasting\nresources on those trials.",
            "markdown"
        ],
        [
            "The checkpoint saving is optional, however, it is necessary if we wanted to use advanced\nschedulers like\n.\nAlso, by saving the checkpoint we can later load the trained models and validate them\non a test set.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Hyperparameter tuning with Ray Tune->The train function->Full training function": [
        [
            "The full code example looks like this:",
            "markdown"
        ],
        [
            "def train_cifar(config, checkpoint_dir=None, data_dir=None):\n    net = (config[\"l1\"], config[\"l2\"])\n\n    device = \"cpu\"\n    if ():\n        device = \"cuda:0\"\n        if () &gt; 1:\n            net = (net)\n    net.to(device)\n\n    criterion = ()\n    optimizer = (net.parameters(), lr=config[\"lr\"], momentum=0.9)\n\n    if checkpoint_dir:\n        model_state, optimizer_state = (\n            os.path.join(checkpoint_dir, \"checkpoint\"))\n        net.load_state_dict(model_state)\n        optimizer.load_state_dict(optimizer_state)\n\n    trainset, testset = load_data(data_dir)\n\n    test_abs = int(len(trainset) * 0.8)\n    train_subset, val_subset = (\n        trainset, [test_abs, len(trainset) - test_abs])\n\n    trainloader = (\n        train_subset,\n        batch_size=int(config[\"batch_size\"]),\n        shuffle=True,\n        num_workers=8)\n    valloader = (\n        val_subset,\n        batch_size=int(config[\"batch_size\"]),\n        shuffle=True,\n        num_workers=8)\n\n    for epoch in range(10):  # loop over the dataset multiple times\n        running_loss = 0.0\n        epoch_steps = 0\n        for i, data in enumerate(trainloader, 0):\n            # get the inputs; data is a list of [inputs, labels]\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            epoch_steps += 1\n            if i % 2000 == 1999:  # print every 2000 mini-batches\n                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n                                                running_loss / epoch_steps))\n                running_loss = 0.0\n\n        # Validation loss\n        val_loss = 0.0\n        val_steps = 0\n        total = 0\n        correct = 0\n        for i, data in enumerate(valloader, 0):\n            with ():\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = net(inputs)\n                _, predicted = (outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n                loss = criterion(outputs, labels)\n                val_loss += loss.cpu().numpy()\n                val_steps += 1\n\n        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n            path = os.path.join(checkpoint_dir, \"checkpoint\")\n            ((net.state_dict(), optimizer.state_dict()), path)\n\n        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n    print(\"Finished Training\")",
            "code"
        ],
        [
            "As you can see, most of the code is adapted directly from the original example.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Hyperparameter tuning with Ray Tune->Test set accuracy": [
        [
            "Commonly the performance of a machine learning model is tested on a hold-out test\nset with data that has not been used for training the model. We also wrap this in a\nfunction:",
            "markdown"
        ],
        [
            "def test_accuracy(net, device=\"cpu\"):\n    trainset, testset = load_data()\n\n    testloader = (\n        testset, batch_size=4, shuffle=False, num_workers=2)\n\n    correct = 0\n    total = 0\n    with ():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = net(images)\n            _, predicted = (outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return correct / total",
            "code"
        ],
        [
            "The function also expects a device parameter, so we can do the\ntest set validation on a GPU.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Hyperparameter tuning with Ray Tune->Configuring the search space": [
        [
            "Lastly, we need to define Ray Tune\u2019s search space. Here is an example:",
            "markdown"
        ],
        [
            "config = {\n    \"l1\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n    \"l2\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n    \"lr\": tune.loguniform(1e-4, 1e-1),\n    \"batch_size\": tune.choice([2, 4, 8, 16])\n}",
            "code"
        ],
        [
            "The tune.sample_from() function makes it possible to define your own sample\nmethods to obtain hyperparameters. In this example, the l1 and l2 parameters\nshould be powers of 2 between 4 and 256, so either 4, 8, 16, 32, 64, 128, or 256.\nThe lr (learning rate) should be uniformly sampled between 0.0001 and 0.1. Lastly,\nthe batch size is a choice between 2, 4, 8, and 16.",
            "markdown"
        ],
        [
            "At each trial, Ray Tune will now randomly sample a combination of parameters from these\nsearch spaces. It will then train a number of models in parallel and find the best\nperforming one among these. We also use the ASHAScheduler which will terminate bad\nperforming trials early.",
            "markdown"
        ],
        [
            "We wrap the train_cifar function with functools.partial to set the constant\ndata_dir parameter. We can also tell Ray Tune what resources should be\navailable for each trial:",
            "markdown"
        ],
        [
            "gpus_per_trial = 2\n# ...\nresult = tune.run(\n    partial(train_cifar, data_dir=data_dir),\n    resources_per_trial={\"cpu\": 8, \"gpu\": gpus_per_trial},\n    config=config,\n    num_samples=num_samples,\n    scheduler=scheduler,\n    progress_reporter=reporter,\n    checkpoint_at_end=True)",
            "code"
        ],
        [
            "You can specify the number of CPUs, which are then available e.g.\nto increase the num_workers of the PyTorch DataLoader instances. The selected\nnumber of GPUs are made visible to PyTorch in each trial. Trials do not have access to\nGPUs that haven\u2019t been requested for them - so you don\u2019t have to care about two trials\nusing the same set of resources.",
            "markdown"
        ],
        [
            "Here we can also specify fractional GPUs, so something like gpus_per_trial=0.5 is\ncompletely valid. The trials will then share GPUs among each other.\nYou just have to make sure that the models still fit in the GPU memory.",
            "markdown"
        ],
        [
            "After training the models, we will find the best performing one and load the trained\nnetwork from the checkpoint file. We then obtain the test set accuracy and report\neverything by printing.",
            "markdown"
        ],
        [
            "The full main function looks like this:",
            "markdown"
        ],
        [
            "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n    data_dir = os.path.abspath(\"./data\")\n    load_data(data_dir)\n    config = {\n        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n        \"lr\": tune.loguniform(1e-4, 1e-1),\n        \"batch_size\": tune.choice([2, 4, 8, 16])\n    }\n    scheduler = ASHAScheduler(\n        metric=\"loss\",\n        mode=\"min\",\n        max_t=max_num_epochs,\n        grace_period=1,\n        reduction_factor=2)\n    reporter = CLIReporter(\n        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n    result = tune.run(\n        partial(train_cifar, data_dir=data_dir),\n        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n        config=config,\n        num_samples=num_samples,\n        scheduler=scheduler,\n        progress_reporter=reporter)\n\n    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n    print(\"Best trial config: {}\".format(best_trial.config))\n    print(\"Best trial final validation loss: {}\".format(\n        best_trial.last_result[\"loss\"]))\n    print(\"Best trial final validation accuracy: {}\".format(\n        best_trial.last_result[\"accuracy\"]))\n\n    best_trained_model = (best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n    device = \"cpu\"\n    if ():\n        device = \"cuda:0\"\n        if gpus_per_trial &gt; 1:\n            best_trained_model = (best_trained_model)\n    best_trained_model.to(device)\n\n    best_checkpoint_dir = best_trial.checkpoint.value\n    model_state, optimizer_state = (os.path.join(\n        best_checkpoint_dir, \"checkpoint\"))\n    best_trained_model.load_state_dict(model_state)\n\n    test_acc = test_accuracy(best_trained_model, device)\n    print(\"Best trial test set accuracy: {}\".format(test_acc))\n\n\nif __name__ == \"__main__\":\n    # You can change the number of GPUs per trial here:\n    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)",
            "code"
        ],
        [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /var/lib/jenkins/workspace/beginner_source/data/cifar-10-python.tar.gz\n\n  0%|          | 0/170498071 [00:00&lt;?, ?it/s]\n  0%|          | 458752/170498071 [00:00&lt;00:40, 4186474.07it/s]\n  4%|4         | 7208960/170498071 [00:00&lt;00:04, 39950224.28it/s]\n 11%|#1        | 18907136/170498071 [00:00&lt;00:02, 74299871.50it/s]\n 18%|#7        | 30605312/170498071 [00:00&lt;00:01, 90840159.11it/s]\n 25%|##4       | 42303488/170498071 [00:00&lt;00:01, 100170270.54it/s]\n 32%|###1      | 54001664/170498071 [00:00&lt;00:01, 105728331.82it/s]\n 39%|###8      | 65699840/170498071 [00:00&lt;00:00, 109304415.25it/s]\n 45%|####5     | 77365248/170498071 [00:00&lt;00:00, 111624781.97it/s]\n 52%|#####2    | 89063424/170498071 [00:00&lt;00:00, 113270080.57it/s]\n 59%|#####9    | 100728832/170498071 [00:01&lt;00:00, 114262468.46it/s]\n 66%|######5   | 112459776/170498071 [00:01&lt;00:00, 115102688.65it/s]\n 73%|#######2  | 124190720/170498071 [00:01&lt;00:00, 115709271.02it/s]\n 80%|#######9  | 135921664/170498071 [00:01&lt;00:00, 116147541.76it/s]\n 87%|########6 | 147619840/170498071 [00:01&lt;00:00, 116321987.25it/s]\n 93%|#########3| 159318016/170498071 [00:01&lt;00:00, 116484832.18it/s]\n100%|##########| 170498071/170498071 [00:01&lt;00:00, 105946246.35it/s]\nExtracting /var/lib/jenkins/workspace/beginner_source/data/cifar-10-python.tar.gz to /var/lib/jenkins/workspace/beginner_source/data\nFiles already downloaded and verified\n2023-03-17 21:22:42,513 WARNING services.py:2002 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67104768 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.46gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n2023-03-17 21:22:45,624 WARNING tune.py:668 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n== Status ==\nCurrent time: 2023-03-17 21:22:45 (running for 00:00:00.30)\nMemory usage on this node: 1.8/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (9 PENDING, 1 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | PENDING  |                 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\n(func pid=1790) Files already downloaded and verified\n(func pid=1790) Files already downloaded and verified\n(func pid=1790) /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n(func pid=1790)   warnings.warn(_create_warning_msg(\n(func pid=1821) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:22:54 (running for 00:00:09.28)\nMemory usage on this node: 2.6/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\n(func pid=1821) Files already downloaded and verified\n(func pid=1821) /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n(func pid=1821)   warnings.warn(_create_warning_msg(\n== Status ==\nCurrent time: 2023-03-17 21:22:59 (running for 00:00:14.31)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\n(func pid=1790) [1,  2000] loss: 2.151\n== Status ==\nCurrent time: 2023-03-17 21:23:04 (running for 00:00:19.34)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:23:09 (running for 00:00:24.36)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\n(func pid=1821) [1,  2000] loss: 1.821\n== Status ==\nCurrent time: 2023-03-17 21:23:15 (running for 00:00:29.38)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\n(func pid=1790) [1,  4000] loss: 1.037\n== Status ==\nCurrent time: 2023-03-17 21:23:20 (running for 00:00:34.39)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.4741\n  date: 2023-03-17_21-23-21\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 1.4619517978668213\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 27.52146553993225\n  time_this_iter_s: 27.52146553993225\n  time_total_s: 27.52146553993225\n  timestamp: 1679088201\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:23:26 (running for 00:00:41.33)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.4619517978668213\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+----------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |         |            |                      |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.46195 |     0.4741 |                    1 |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00000:\n  accuracy: 0.1911\n  date: 2023-03-17_21-23-31\n  done: true\n  experiment_id: bf67837a32e9457489ba78a88a5e9de9\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 2.2055853404998778\n  node_ip: 172.17.0.2\n  pid: 1790\n  should_checkpoint: true\n  time_since_restore: 41.20965528488159\n  time_this_iter_s: 41.20965528488159\n  time_total_s: 41.20965528488159\n  timestamp: 1679088211\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00000\n  warmup_time: 0.003386974334716797\n\n(func pid=1790) Files already downloaded and verified\n(func pid=1790) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:23:36 (running for 00:00:50.53)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.46195 |     0.4741 |                    1 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [2,  2000] loss: 1.404\n== Status ==\nCurrent time: 2023-03-17 21:23:41 (running for 00:00:55.55)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.46195 |     0.4741 |                    1 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  2000] loss: 2.323\n== Status ==\nCurrent time: 2023-03-17 21:23:46 (running for 00:01:00.58)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.46195 |     0.4741 |                    1 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.4988\n  date: 2023-03-17_21-23-46\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 2\n  loss: 1.4047928235054017\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 52.457958936691284\n  time_this_iter_s: 24.936493396759033\n  time_total_s: 52.457958936691284\n  timestamp: 1679088226\n  timesteps_since_restore: 0\n  training_iteration: 2\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:23:51 (running for 00:01:06.28)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.40479 |     0.4988 |                    2 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  4000] loss: 1.156\n== Status ==\nCurrent time: 2023-03-17 21:23:56 (running for 00:01:11.30)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.40479 |     0.4988 |                    2 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:24:01 (running for 00:01:16.32)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.40479 |     0.4988 |                    2 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [3,  2000] loss: 1.301\n== Status ==\nCurrent time: 2023-03-17 21:24:06 (running for 00:01:21.35)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.40479 |     0.4988 |                    2 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  6000] loss: 0.769\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5339\n  date: 2023-03-17_21-24-10\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 3\n  loss: 1.335026089000702\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 76.37172889709473\n  time_this_iter_s: 23.913769960403442\n  time_total_s: 76.37172889709473\n  timestamp: 1679088250\n  timesteps_since_restore: 0\n  training_iteration: 3\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:24:15 (running for 00:01:30.18)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33503 |     0.5339 |                    3 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  8000] loss: 0.575\n== Status ==\nCurrent time: 2023-03-17 21:24:20 (running for 00:01:35.20)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33503 |     0.5339 |                    3 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:24:25 (running for 00:01:40.24)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33503 |     0.5339 |                    3 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [4,  2000] loss: 1.226\n(func pid=1790) [1, 10000] loss: 0.459\n== Status ==\nCurrent time: 2023-03-17 21:24:30 (running for 00:01:45.27)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33503 |     0.5339 |                    3 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5321\n  date: 2023-03-17_21-24-34\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 4\n  loss: 1.337672945547104\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 100.5483820438385\n  time_this_iter_s: 24.176653146743774\n  time_total_s: 100.5483820438385\n  timestamp: 1679088274\n  timesteps_since_restore: 0\n  training_iteration: 4\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:24:39 (running for 00:01:54.36)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33767 |     0.5321 |                    4 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00002:\n  accuracy: 0.1028\n  date: 2023-03-17_21-24-40\n  done: true\n  experiment_id: bf67837a32e9457489ba78a88a5e9de9\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 2.2863188765525817\n  node_ip: 172.17.0.2\n  pid: 1790\n  should_checkpoint: true\n  time_since_restore: 69.34229445457458\n  time_this_iter_s: 69.34229445457458\n  time_total_s: 69.34229445457458\n  timestamp: 1679088280\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00002\n  warmup_time: 0.003386974334716797\n\n(func pid=1790) Files already downloaded and verified\n(func pid=1790) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:24:45 (running for 00:01:59.91)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33767 |     0.5321 |                    4 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:24:50 (running for 00:02:04.96)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33767 |     0.5321 |                    4 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [5,  2000] loss: 1.177\n(func pid=1790) [1,  2000] loss: 2.188\n== Status ==\nCurrent time: 2023-03-17 21:24:55 (running for 00:02:09.99)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33767 |     0.5321 |                    4 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) E0317 21:24:57.316660920    1817 chttp2_transport.cc:1103]   Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5775\n  date: 2023-03-17_21-24-58\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 5\n  loss: 1.2192501729011536\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 123.69131422042847\n  time_this_iter_s: 23.142932176589966\n  time_total_s: 123.69131422042847\n  timestamp: 1679088298\n  timesteps_since_restore: 0\n  training_iteration: 5\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:25:03 (running for 00:02:17.51)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.21925 |     0.5775 |                    5 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  4000] loss: 1.002\n== Status ==\nCurrent time: 2023-03-17 21:25:08 (running for 00:02:22.53)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.21925 |     0.5775 |                    5 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:25:13 (running for 00:02:27.55)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.21925 |     0.5775 |                    5 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  6000] loss: 0.642\n(func pid=1821) [6,  2000] loss: 1.131\n== Status ==\nCurrent time: 2023-03-17 21:25:18 (running for 00:02:32.56)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.21925 |     0.5775 |                    5 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5707\n  date: 2023-03-17_21-25-21\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 6\n  loss: 1.2494660420417785\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 146.70809984207153\n  time_this_iter_s: 23.016785621643066\n  time_total_s: 146.70809984207153\n  timestamp: 1679088321\n  timesteps_since_restore: 0\n  training_iteration: 6\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n(func pid=1790) [1,  8000] loss: 0.456\n== Status ==\nCurrent time: 2023-03-17 21:25:26 (running for 00:02:40.53)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.24947 |     0.5707 |                    6 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:25:31 (running for 00:02:45.55)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.24947 |     0.5707 |                    6 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 10000] loss: 0.352\n== Status ==\nCurrent time: 2023-03-17 21:25:36 (running for 00:02:50.58)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.24947 |     0.5707 |                    6 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [7,  2000] loss: 1.103\n== Status ==\nCurrent time: 2023-03-17 21:25:41 (running for 00:02:55.60)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.24947 |     0.5707 |                    6 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 12000] loss: 0.288\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5704\n  date: 2023-03-17_21-25-44\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 7\n  loss: 1.2636950318336486\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 169.82927751541138\n  time_this_iter_s: 23.121177673339844\n  time_total_s: 169.82927751541138\n  timestamp: 1679088344\n  timesteps_since_restore: 0\n  training_iteration: 7\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:25:49 (running for 00:03:03.64)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.2637  |     0.5704 |                    7 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 14000] loss: 0.242\n== Status ==\nCurrent time: 2023-03-17 21:25:54 (running for 00:03:08.66)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.2637  |     0.5704 |                    7 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:25:59 (running for 00:03:13.68)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.2637  |     0.5704 |                    7 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [8,  2000] loss: 1.088\n(func pid=1790) [1, 16000] loss: 0.210\n== Status ==\nCurrent time: 2023-03-17 21:26:04 (running for 00:03:18.70)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.2637  |     0.5704 |                    7 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5865\n  date: 2023-03-17_21-26-07\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 8\n  loss: 1.2344939463615416\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 192.97265577316284\n  time_this_iter_s: 23.143378257751465\n  time_total_s: 192.97265577316284\n  timestamp: 1679088367\n  timesteps_since_restore: 0\n  training_iteration: 8\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:26:12 (running for 00:03:26.81)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.23449 |     0.5865 |                    8 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 18000] loss: 0.186\n== Status ==\nCurrent time: 2023-03-17 21:26:17 (running for 00:03:31.84)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.23449 |     0.5865 |                    8 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:26:22 (running for 00:03:36.85)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.23449 |     0.5865 |                    8 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [9,  2000] loss: 1.071\n(func pid=1790) [1, 20000] loss: 0.164\n== Status ==\nCurrent time: 2023-03-17 21:26:27 (running for 00:03:41.87)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.23449 |     0.5865 |                    8 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5764\n  date: 2023-03-17_21-26-30\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 9\n  loss: 1.2786366604804993\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 216.41862058639526\n  time_this_iter_s: 23.445964813232422\n  time_total_s: 216.41862058639526\n  timestamp: 1679088390\n  timesteps_since_restore: 0\n  training_iteration: 9\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:26:35 (running for 00:03:50.23)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.27864 |     0.5764 |                    9 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:26:40 (running for 00:03:55.24)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.27864 |     0.5764 |                    9 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00003:\n  accuracy: 0.3986\n  date: 2023-03-17_21-26-43\n  done: false\n  experiment_id: bf67837a32e9457489ba78a88a5e9de9\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 1.62057805493623\n  node_ip: 172.17.0.2\n  pid: 1790\n  should_checkpoint: true\n  time_since_restore: 122.60904693603516\n  time_this_iter_s: 122.60904693603516\n  time_total_s: 122.60904693603516\n  timestamp: 1679088403\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00003\n  warmup_time: 0.003386974334716797\n\n(func pid=1821) [10,  2000] loss: 1.057\n== Status ==\nCurrent time: 2023-03-17 21:26:48 (running for 00:04:02.53)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.27864 |     0.5764 |                    9 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:26:53 (running for 00:04:07.55)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.27864 |     0.5764 |                    9 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2,  2000] loss: 1.614\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5834\n  date: 2023-03-17_21-26-54\n  done: true\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 10\n  loss: 1.2592490710258484\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 239.92289233207703\n  time_this_iter_s: 23.504271745681763\n  time_total_s: 239.92289233207703\n  timestamp: 1679088414\n  timesteps_since_restore: 0\n  training_iteration: 10\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n(func pid=1821) Files already downloaded and verified\n(func pid=1821) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:26:59 (running for 00:04:13.76)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2,  4000] loss: 0.809\n== Status ==\nCurrent time: 2023-03-17 21:27:04 (running for 00:04:18.77)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  2000] loss: 2.404\n== Status ==\nCurrent time: 2023-03-17 21:27:09 (running for 00:04:23.79)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2,  6000] loss: 0.539\n== Status ==\nCurrent time: 2023-03-17 21:27:14 (running for 00:04:28.80)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  4000] loss: 1.209\n(func pid=1790) [2,  8000] loss: 0.405\n== Status ==\nCurrent time: 2023-03-17 21:27:19 (running for 00:04:33.82)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:27:24 (running for 00:04:38.83)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  6000] loss: 0.805\n(func pid=1790) [2, 10000] loss: 0.321\n== Status ==\nCurrent time: 2023-03-17 21:27:29 (running for 00:04:43.85)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:27:34 (running for 00:04:48.87)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2, 12000] loss: 0.265\n(func pid=1821) [1,  8000] loss: 0.601\n== Status ==\nCurrent time: 2023-03-17 21:27:39 (running for 00:04:53.89)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:27:44 (running for 00:04:58.90)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2, 14000] loss: 0.230\n(func pid=1821) [1, 10000] loss: 0.483\n== Status ==\nCurrent time: 2023-03-17 21:27:49 (running for 00:05:03.92)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2, 16000] loss: 0.202\n== Status ==\nCurrent time: 2023-03-17 21:27:54 (running for 00:05:08.94)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1, 12000] loss: 0.400\n== Status ==\nCurrent time: 2023-03-17 21:27:59 (running for 00:05:13.96)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2, 18000] loss: 0.175\n== Status ==\nCurrent time: 2023-03-17 21:28:04 (running for 00:05:18.97)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1, 14000] loss: 0.344\n== Status ==\nCurrent time: 2023-03-17 21:28:09 (running for 00:05:23.99)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2, 20000] loss: 0.157\n== Status ==\nCurrent time: 2023-03-17 21:28:14 (running for 00:05:29.00)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1, 16000] loss: 0.301\n== Status ==\nCurrent time: 2023-03-17 21:28:19 (running for 00:05:34.03)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:28:24 (running for 00:05:39.04)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00003:\n  accuracy: 0.443\n  date: 2023-03-17_21-28-25\n  done: true\n  experiment_id: bf67837a32e9457489ba78a88a5e9de9\n  hostname: 6f9535515a81\n  iterations_since_restore: 2\n  loss: 1.5665831570994109\n  node_ip: 172.17.0.2\n  pid: 1790\n  should_checkpoint: true\n  time_since_restore: 224.4824321269989\n  time_this_iter_s: 101.87338519096375\n  time_total_s: 224.4824321269989\n  timestamp: 1679088505\n  timesteps_since_restore: 0\n  training_iteration: 2\n  trial_id: dbd80_00003\n  warmup_time: 0.003386974334716797\n\n(func pid=1790) Files already downloaded and verified\n(func pid=1790) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:28:30 (running for 00:05:44.42)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (4 PENDING, 2 RUNNING, 4 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1, 18000] loss: 0.267\n== Status ==\nCurrent time: 2023-03-17 21:28:35 (running for 00:05:49.44)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (4 PENDING, 2 RUNNING, 4 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  2000] loss: 2.305\n== Status ==\nCurrent time: 2023-03-17 21:28:40 (running for 00:05:54.47)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (4 PENDING, 2 RUNNING, 4 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1, 20000] loss: 0.242\n== Status ==\nCurrent time: 2023-03-17 21:28:45 (running for 00:05:59.49)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (4 PENDING, 2 RUNNING, 4 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  4000] loss: 1.151\n== Status ==\nCurrent time: 2023-03-17 21:28:50 (running for 00:06:04.51)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (4 PENDING, 2 RUNNING, 4 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:28:55 (running for 00:06:09.53)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (4 PENDING, 2 RUNNING, 4 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00004:\n  accuracy: 0.0955\n  date: 2023-03-17_21-28-57\n  done: true\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 2.463977705168724\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 123.56385898590088\n  time_this_iter_s: 123.56385898590088\n  time_total_s: 123.56385898590088\n  timestamp: 1679088537\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00004\n  warmup_time: 0.004238128662109375\n\n(func pid=1821) Files already downloaded and verified\n(func pid=1821) Files already downloaded and verified\nResult for train_cifar_dbd80_00005:\n  accuracy: 0.1019\n  date: 2023-03-17_21-29-01\n  done: true\n  experiment_id: bf67837a32e9457489ba78a88a5e9de9\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 2.301889562225342\n  node_ip: 172.17.0.2\n  pid: 1790\n  should_checkpoint: true\n  time_since_restore: 36.047141790390015\n  time_this_iter_s: 36.047141790390015\n  time_total_s: 36.047141790390015\n  timestamp: 1679088541\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00005\n  warmup_time: 0.003386974334716797\n\n== Status ==\nCurrent time: 2023-03-17 21:29:01 (running for 00:06:15.48)\nMemory usage on this node: 2.9/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (3 PENDING, 2 RUNNING, 5 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) Files already downloaded and verified\n(func pid=1790) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:29:06 (running for 00:06:20.51)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  2000] loss: 2.319\n== Status ==\nCurrent time: 2023-03-17 21:29:11 (running for 00:06:25.54)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  2000] loss: 2.305\n== Status ==\nCurrent time: 2023-03-17 21:29:16 (running for 00:06:30.55)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  4000] loss: 1.156\n(func pid=1821) [1,  4000] loss: 1.160\n== Status ==\nCurrent time: 2023-03-17 21:29:21 (running for 00:06:35.57)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:29:26 (running for 00:06:40.58)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  6000] loss: 0.768\n(func pid=1821) [1,  6000] loss: 0.774\n== Status ==\nCurrent time: 2023-03-17 21:29:31 (running for 00:06:45.60)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:29:36 (running for 00:06:50.62)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  8000] loss: 0.575\n== Status ==\nCurrent time: 2023-03-17 21:29:41 (running for 00:06:55.64)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  8000] loss: 0.580\n== Status ==\nCurrent time: 2023-03-17 21:29:46 (running for 00:07:00.66)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 10000] loss: 0.459\n== Status ==\nCurrent time: 2023-03-17 21:29:51 (running for 00:07:05.68)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1, 10000] loss: 0.465\n(func pid=1790) [1, 12000] loss: 0.393\n== Status ==\nCurrent time: 2023-03-17 21:29:56 (running for 00:07:10.70)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00006:\n  accuracy: 0.0967\n  date: 2023-03-17_21-30-00\n  done: true\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 2.32388559012413\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 62.30062174797058\n  time_this_iter_s: 62.30062174797058\n  time_total_s: 62.30062174797058\n  timestamp: 1679088600\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00006\n  warmup_time: 0.004238128662109375\n\n(func pid=1821) Files already downloaded and verified\n(func pid=1821) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:30:05 (running for 00:07:19.68)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2863188765525817\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 14000] loss: 0.331\n== Status ==\nCurrent time: 2023-03-17 21:30:10 (running for 00:07:24.71)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2863188765525817\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:30:15 (running for 00:07:29.74)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2863188765525817\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 16000] loss: 0.289\n(func pid=1821) [1,  2000] loss: 2.211\n== Status ==\nCurrent time: 2023-03-17 21:30:20 (running for 00:07:34.77)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2863188765525817\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:30:25 (running for 00:07:39.79)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2863188765525817\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00008:\n  accuracy: 0.299\n  date: 2023-03-17_21-30-25\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 1.9318199354171752\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 25.344778776168823\n  time_this_iter_s: 25.344778776168823\n  time_total_s: 25.344778776168823\n  timestamp: 1679088625\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00008\n  warmup_time: 0.004238128662109375\n\n(func pid=1790) [1, 18000] loss: 0.257\n== Status ==\nCurrent time: 2023-03-17 21:30:30 (running for 00:07:45.02)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.93182 |     0.299  |                    1 |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:30:35 (running for 00:07:50.06)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.93182 |     0.299  |                    1 |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 20000] loss: 0.232\n== Status ==\nCurrent time: 2023-03-17 21:30:40 (running for 00:07:55.08)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.93182 |     0.299  |                    1 |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [2,  2000] loss: 1.797\n== Status ==\nCurrent time: 2023-03-17 21:30:45 (running for 00:08:00.10)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.93182 |     0.299  |                    1 |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00008:\n  accuracy: 0.4054\n  date: 2023-03-17_21-30-49\n  done: true\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 2\n  loss: 1.625212480354309\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 49.30907487869263\n  time_this_iter_s: 23.964296102523804\n  time_total_s: 49.30907487869263\n  timestamp: 1679088649\n  timesteps_since_restore: 0\n  training_iteration: 2\n  trial_id: dbd80_00008\n  warmup_time: 0.004238128662109375\n\n(func pid=1821) Files already downloaded and verified\n(func pid=1821) Files already downloaded and verified\nResult for train_cifar_dbd80_00007:\n  accuracy: 0.0976\n  date: 2023-03-17_21-30-53\n  done: true\n  experiment_id: bf67837a32e9457489ba78a88a5e9de9\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 2.329462006855011\n  node_ip: 172.17.0.2\n  pid: 1790\n  should_checkpoint: true\n  time_since_restore: 112.19222688674927\n  time_this_iter_s: 112.19222688674927\n  time_total_s: 112.19222688674927\n  timestamp: 1679088653\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00007\n  warmup_time: 0.003386974334716797\n\n== Status ==\nCurrent time: 2023-03-17 21:30:53 (running for 00:08:07.71)\nMemory usage on this node: 2.9/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2863188765525817\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:30:58 (running for 00:08:12.74)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2863188765525817\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  2000] loss: 2.274\n== Status ==\nCurrent time: 2023-03-17 21:31:03 (running for 00:08:17.75)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2863188765525817\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:31:08 (running for 00:08:22.78)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2863188765525817\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  4000] loss: 1.043\n== Status ==\nCurrent time: 2023-03-17 21:31:13 (running for 00:08:27.79)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2863188765525817\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00009:\n  accuracy: 0.2988\n  date: 2023-03-17_21-31-18\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 1.888344468307495\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 28.592221975326538\n  time_this_iter_s: 28.592221975326538\n  time_total_s: 28.592221975326538\n  timestamp: 1679088678\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00009\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:31:23 (running for 00:08:37.61)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2459521085262297\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 | 1.88834 |     0.2988 |                    1 |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [2,  2000] loss: 1.755\n== Status ==\nCurrent time: 2023-03-17 21:31:28 (running for 00:08:42.61)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2459521085262297\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 | 1.88834 |     0.2988 |                    1 |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:31:33 (running for 00:08:47.63)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2459521085262297\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 | 1.88834 |     0.2988 |                    1 |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [2,  4000] loss: 0.801\n== Status ==\nCurrent time: 2023-03-17 21:31:38 (running for 00:08:52.64)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2459521085262297\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 | 1.88834 |     0.2988 |                    1 |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:31:43 (running for 00:08:57.66)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2459521085262297\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 | 1.88834 |     0.2988 |                    1 |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00009:\n  accuracy: 0.4218\n  date: 2023-03-17_21-31-44\n  done: true\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 2\n  loss: 1.5962100546360016\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 55.290165424346924\n  time_this_iter_s: 26.697943449020386\n  time_total_s: 55.290165424346924\n  timestamp: 1679088704\n  timesteps_since_restore: 0\n  training_iteration: 2\n  trial_id: dbd80_00009\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:31:44 (running for 00:08:59.30)\nMemory usage on this node: 2.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=10\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5813966058677063 | Iter 1.000: -2.2459521085262297\nResources requested: 0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (10 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n| train_cifar_dbd80_00009 | TERMINATED | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 | 1.59621 |     0.4218 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n2023-03-17 21:31:45,037 INFO tune.py:747 -- Total run time: 539.85 seconds (539.29 seconds for the tuning loop).\nBest trial config: {'l1': 64, 'l2': 64, 'lr': 0.0068986736602156895, 'batch_size': 16}\nBest trial final validation loss: 1.2592490710258484\nBest trial final validation accuracy: 0.5834\nFiles already downloaded and verified\nFiles already downloaded and verified\nBest trial test set accuracy: 0.5803",
            "code"
        ],
        [
            "If you run the code, an example output could look like this:",
            "markdown"
        ],
        [
            "Number of trials: 10 (10 TERMINATED)\n+-----+------+------+-------------+--------------+---------+------------+--------------------+\n| ... |   l1 |   l2 |          lr |   batch_size |    loss |   accuracy | training_iteration |\n|-----+------+------+-------------+--------------+---------+------------+--------------------|\n| ... |   64 |    4 | 0.00011629  |            2 | 1.87273 |     0.244  |                  2 |\n| ... |   32 |   64 | 0.000339763 |            8 | 1.23603 |     0.567  |                  8 |\n| ... |    8 |   16 | 0.00276249  |           16 | 1.1815  |     0.5836 |                 10 |\n| ... |    4 |   64 | 0.000648721 |            4 | 1.31131 |     0.5224 |                  8 |\n| ... |   32 |   16 | 0.000340753 |            8 | 1.26454 |     0.5444 |                  8 |\n| ... |    8 |    4 | 0.000699775 |            8 | 1.99594 |     0.1983 |                  2 |\n| ... |  256 |    8 | 0.0839654   |           16 | 2.3119  |     0.0993 |                  1 |\n| ... |   16 |  128 | 0.0758154   |           16 | 2.33575 |     0.1327 |                  1 |\n| ... |   16 |    8 | 0.0763312   |           16 | 2.31129 |     0.1042 |                  4 |\n| ... |  128 |   16 | 0.000124903 |            4 | 2.26917 |     0.1945 |                  1 |\n+-----+------+------+-------------+--------------+---------+------------+--------------------+\n\n\nBest trial config: {'l1': 8, 'l2': 16, 'lr': 0.00276249, 'batch_size': 16, 'data_dir': '...'}\nBest trial final validation loss: 1.181501\nBest trial final validation accuracy: 0.5836\nBest trial test set accuracy: 0.5806",
            "code"
        ],
        [
            "Most trials have been stopped early in order to avoid wasting resources.\nThe best performing trial achieved a validation accuracy of about 58%, which could\nbe confirmed on the test set.",
            "markdown"
        ],
        [
            "So that\u2019s it! You can now tune the parameters of your PyTorch models.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 9 minutes  25.490 seconds)",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Optimizing Vision Transformer Model for Deployment": [
        [
            ",",
            "markdown"
        ],
        [
            "Vision Transformer models apply the cutting-edge attention-based\ntransformer models, introduced in Natural Language Processing to achieve\nall kinds of the state of the art (SOTA) results, to Computer Vision\ntasks. Facebook Data-efficient Image Transformers \nis a Vision Transformer model trained on ImageNet for image\nclassification.",
            "markdown"
        ],
        [
            "In this tutorial, we will first cover what DeiT is and how to use it,\nthen go through the complete steps of scripting, quantizing, optimizing,\nand using the model in iOS and Android apps. We will also compare the\nperformance of quantized, optimized and non-quantized, non-optimized\nmodels, and show the benefits of applying quantization and optimization\nto the model along the steps.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Optimizing Vision Transformer Model for Deployment->What is DeiT": [
        [
            "Convolutional Neural Networks (CNNs) have been the main models for image\nclassification since deep learning took off in 2012, but CNNs typically\nrequire hundreds of millions of images for training to achieve the\nSOTAresults. DeiT is a vision transformer model that requires a lot less\ndata and computing resources for training to compete with the leading\nCNNs in performing image classification, which is made possible by two\nkey components of of DeiT:",
            "markdown"
        ],
        [
            "Data augmentation that simulates training on a much larger dataset;",
            "markdown"
        ],
        [
            "Native distillation that allows the transformer network to learn from\na CNN\u2019s output.",
            "markdown"
        ],
        [
            "DeiT shows that Transformers can be successfully applied to computer\nvision tasks, with limited access to data and resources. For more\ndetails on DeiT, see the \nand .",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Optimizing Vision Transformer Model for Deployment->Classifying Images with DeiT": [
        [
            "Follow the README at the DeiT repo for detailed information on how to\nclassify images using DeiT, or for a quick test, first install the\nrequired packages:",
            "markdown"
        ],
        [
            "# pip install torch torchvision timm pandas requests",
            "code"
        ],
        [
            "To run in Google Colab, uncomment the following line:",
            "markdown"
        ],
        [
            "# !pip install timm pandas requests",
            "code"
        ],
        [
            "then run the script below:",
            "markdown"
        ],
        [
            "from PIL import Image\nimport torch\nimport timm\nimport requests\nimport torchvision.transforms as transforms\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\nprint(torch.__version__)\n# should be 1.8.0\n\n\nmodel = ('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n()\n\n = ([\n    (256, interpolation=3),\n    (224),\n    (),\n    (IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n])\n\n = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n = ()[None,]\n = model()\n = ()\nprint(.item())",
            "code"
        ],
        [
            "2.0.0+cu117\nDownloading: \"https://github.com/facebookresearch/deit/zipball/main\" to /var/lib/jenkins/.cache/torch/hub/main.zip\nDownloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /var/lib/jenkins/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n\n  0%|          | 0.00/330M [00:00&lt;?, ?B/s]\n  0%|          | 56.0k/330M [00:00&lt;10:58, 526kB/s]\n  0%|          | 256k/330M [00:00&lt;04:20, 1.33MB/s]\n  0%|          | 1.13M/330M [00:00&lt;01:15, 4.57MB/s]\n  1%|1         | 4.59M/330M [00:00&lt;00:21, 16.0MB/s]\n  4%|3         | 12.4M/330M [00:00&lt;00:09, 36.3MB/s]\n  6%|6         | 19.9M/330M [00:00&lt;00:06, 47.0MB/s]\n  8%|8         | 27.5M/330M [00:00&lt;00:05, 55.1MB/s]\n 11%|#         | 35.3M/330M [00:00&lt;00:05, 59.8MB/s]\n 13%|#2        | 42.9M/330M [00:01&lt;00:04, 62.9MB/s]\n 15%|#5        | 50.7M/330M [00:01&lt;00:04, 64.7MB/s]\n 18%|#7        | 58.4M/330M [00:01&lt;00:04, 67.6MB/s]\n 20%|#9        | 65.9M/330M [00:01&lt;00:04, 68.2MB/s]\n 22%|##2       | 73.6M/330M [00:01&lt;00:03, 68.7MB/s]\n 25%|##4       | 81.4M/330M [00:01&lt;00:03, 69.3MB/s]\n 27%|##6       | 89.0M/330M [00:01&lt;00:03, 68.8MB/s]\n 29%|##9       | 96.5M/330M [00:01&lt;00:03, 66.1MB/s]\n 32%|###1      | 104M/330M [00:01&lt;00:03, 66.9MB/s]\n 34%|###3      | 112M/330M [00:02&lt;00:03, 68.4MB/s]\n 36%|###5      | 118M/330M [00:02&lt;00:03, 65.9MB/s]\n 38%|###8      | 126M/330M [00:02&lt;00:03, 66.8MB/s]\n 41%|####      | 134M/330M [00:02&lt;00:03, 68.1MB/s]\n 43%|####2     | 142M/330M [00:02&lt;00:02, 67.2MB/s]\n 45%|####5     | 149M/330M [00:02&lt;00:02, 67.7MB/s]\n 47%|####7     | 156M/330M [00:02&lt;00:02, 65.5MB/s]\n 49%|####9     | 162M/330M [00:02&lt;00:03, 54.0MB/s]\n 51%|#####     | 168M/330M [00:03&lt;00:03, 54.9MB/s]\n 53%|#####3    | 176M/330M [00:03&lt;00:02, 59.3MB/s]\n 55%|#####5    | 183M/330M [00:03&lt;00:02, 62.6MB/s]\n 58%|#####7    | 191M/330M [00:03&lt;00:02, 63.2MB/s]\n 60%|#####9    | 198M/330M [00:03&lt;00:02, 65.3MB/s]\n 62%|######1   | 204M/330M [00:03&lt;00:02, 60.0MB/s]\n 65%|######4   | 213M/330M [00:03&lt;00:01, 66.8MB/s]\n 67%|######6   | 221M/330M [00:03&lt;00:01, 68.0MB/s]\n 69%|######9   | 229M/330M [00:03&lt;00:01, 69.0MB/s]\n 71%|#######1  | 236M/330M [00:04&lt;00:01, 69.2MB/s]\n 74%|#######3  | 244M/330M [00:04&lt;00:01, 67.8MB/s]\n 76%|#######5  | 251M/330M [00:04&lt;00:01, 67.4MB/s]\n 78%|#######8  | 258M/330M [00:04&lt;00:01, 67.6MB/s]\n 81%|########  | 266M/330M [00:04&lt;00:01, 67.4MB/s]\n 83%|########2 | 274M/330M [00:04&lt;00:00, 68.4MB/s]\n 85%|########5 | 281M/330M [00:04&lt;00:00, 68.4MB/s]\n 87%|########7 | 289M/330M [00:04&lt;00:00, 69.6MB/s]\n 90%|########9 | 297M/330M [00:05&lt;00:00, 69.9MB/s]\n 92%|#########2| 304M/330M [00:05&lt;00:00, 69.0MB/s]\n 94%|#########4| 312M/330M [00:05&lt;00:00, 69.6MB/s]\n 97%|#########6| 319M/330M [00:05&lt;00:00, 69.4MB/s]\n 99%|#########9| 327M/330M [00:05&lt;00:00, 70.3MB/s]\n100%|##########| 330M/330M [00:05&lt;00:00, 63.0MB/s]\n269",
            "code"
        ],
        [
            "The output should be 269, which, according to the ImageNet list of class\nindex to , maps to \u2018timber\nwolf, grey wolf, gray wolf, Canis lupus\u2019.",
            "markdown"
        ],
        [
            "Now that we have verified that we can use the DeiT model to classify\nimages, let\u2019s see how to modify the model so it can run on iOS and\nAndroid apps.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Optimizing Vision Transformer Model for Deployment->Scripting DeiT": [
        [
            "To use the model on mobile, we first need to script the\nmodel. See the  for a\nquick overview. Run the code below to convert the DeiT model used in the\nprevious step to the TorchScript format that can run on mobile.",
            "markdown"
        ],
        [
            "model = ('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n()\nscripted_model = (model)\n(\"fbdeit_scripted.pt\")",
            "code"
        ],
        [
            "Using cache found in /var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main",
            "code"
        ],
        [
            "The scripted model file fbdeit_scripted.pt of size about 346MB is\ngenerated.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Optimizing Vision Transformer Model for Deployment->Quantizing DeiT": [
        [
            "To reduce the trained model size significantly while\nkeeping the inference accuracy about the same, quantization can be\napplied to the model. Thanks to the transformer model used in DeiT, we\ncan easily apply dynamic-quantization to the model, because dynamic\nquantization works best for LSTM and transformer models (see \nfor more details).",
            "markdown"
        ],
        [
            "Now run the code below:",
            "markdown"
        ],
        [
            "# Use 'x86' for server inference (the old 'fbgemm' is still available but 'x86' is the recommended default) and 'qnnpack' for mobile inference.\nbackend = \"x86\" # replaced with qnnpack causing much worse inference speed for quantized model on this notebook\n = torch.quantization.get_default_qconfig(backend)\ntorch.backends.quantized.engine = backend\n\nquantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={}, dtype=)\nscripted_quantized_model = (quantized_model)\n(\"fbdeit_scripted_quantized.pt\")",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning:\n\nPlease use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.",
            "code"
        ],
        [
            "This generates the scripted and quantized version of the model\nfbdeit_quantized_scripted.pt, with size about 89MB, a 74% reduction of\nthe non-quantized model size of 346MB!",
            "markdown"
        ],
        [
            "You can use the scripted_quantized_model to generate the same\ninference result:",
            "markdown"
        ],
        [
            " = scripted_quantized_model()\n = ()\nprint(.item())\n# The same output 269 should be printed",
            "code"
        ],
        [
            "269",
            "code"
        ]
    ],
    "torch->Model Optimization->Optimizing Vision Transformer Model for Deployment->Optimizing DeiT": [
        [
            "The final step before using the quantized and scripted\nmodel on mobile is to optimize it:",
            "markdown"
        ],
        [
            "from torch.utils.mobile_optimizer import \noptimized_scripted_quantized_model = (scripted_quantized_model)\n(\"fbdeit_optimized_scripted_quantized.pt\")",
            "code"
        ],
        [
            "The generated fbdeit_optimized_scripted_quantized.pt file has about the\nsame size as the quantized, scripted, but non-optimized model. The\ninference result remains the same.",
            "markdown"
        ],
        [
            " = optimized_scripted_quantized_model()\n = ()\nprint(.item())\n# Again, the same output 269 should be printed",
            "code"
        ],
        [
            "269",
            "code"
        ]
    ],
    "torch->Model Optimization->Optimizing Vision Transformer Model for Deployment->Using Lite Interpreter": [
        [
            "To see how much model size reduction and inference speed up the Lite\nInterpreter can result in, let\u2019s create the lite version of the model.",
            "markdown"
        ],
        [
            "optimized_scripted_quantized_model._save_for_lite_interpreter(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\nptl = (\"fbdeit_optimized_scripted_quantized_lite.ptl\")",
            "code"
        ],
        [
            "Although the lite model size is comparable to the non-lite version, when\nrunning the lite version on mobile, the inference speed up is expected.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Optimizing Vision Transformer Model for Deployment->Comparing Inference Speed": [
        [
            "To see how the inference speed differs for the four models - the\noriginal model, the scripted model, the quantized-and-scripted model,\nthe optimized-quantized-and-scripted model - run the code below:",
            "markdown"
        ],
        [
            "with (use_cuda=False) as :\n     = model()\nwith (use_cuda=False) as :\n     = scripted_model()\nwith (use_cuda=False) as :\n     = scripted_quantized_model()\nwith (use_cuda=False) as :\n     = optimized_scripted_quantized_model()\nwith (use_cuda=False) as :\n     = ptl()\n\nprint(\"original model: {:.2f}ms\".format(/1000))\nprint(\"scripted model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized &amp; optimized model: {:.2f}ms\".format(/1000))\nprint(\"lite model: {:.2f}ms\".format(/1000))",
            "code"
        ],
        [
            "original model: 309.06ms\nscripted model: 335.36ms\nscripted &amp; quantized model: 240.60ms\nscripted &amp; quantized &amp; optimized model: 201.37ms\nlite model: 206.63ms",
            "code"
        ],
        [
            "The results running on a Google Colab are:",
            "markdown"
        ],
        [
            "original model: 1236.69ms\nscripted model: 1226.72ms\nscripted &amp; quantized model: 593.19ms\nscripted &amp; quantized &amp; optimized model: 598.01ms\nlite model: 600.72ms",
            "code"
        ],
        [
            "The following results summarize the inference time taken by each model\nand the percentage reduction of each model relative to the original\nmodel.",
            "markdown"
        ],
        [
            "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Model': ['original model','scripted model', 'scripted &amp; quantized model', 'scripted &amp; quantized &amp; optimized model', 'lite model']})\ndf = pd.concat([df, pd.DataFrame([\n    [\"{:.2f}ms\".format(/1000), \"0%\"],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)]],\n    columns=['Inference Time', 'Reduction'])], axis=1)\n\nprint(df)\n\n\"\"\"\n        Model                             Inference Time    Reduction\n0   original model                             1236.69ms           0%\n1   scripted model                             1226.72ms        0.81%\n2   scripted &amp; quantized model                  593.19ms       52.03%\n3   scripted &amp; quantized &amp; optimized model      598.01ms       51.64%\n4   lite model                                  600.72ms       51.43%\n\"\"\"",
            "code"
        ],
        [
            "                                    Model Inference Time Reduction\n0                          original model       309.06ms        0%\n1                          scripted model       335.36ms    -8.51%\n2              scripted &amp; quantized model       240.60ms    22.15%\n3  scripted &amp; quantized &amp; optimized model       201.37ms    34.84%\n4                              lite model       206.63ms    33.14%\n\n'\\n        Model                             Inference Time    Reduction\\n0\\toriginal model                             1236.69ms           0%\\n1\\tscripted model                             1226.72ms        0.81%\\n2\\tscripted &amp; quantized model                  593.19ms       52.03%\\n3\\tscripted &amp; quantized &amp; optimized model      598.01ms       51.64%\\n4\\tlite model                                  600.72ms       51.43%\\n'",
            "code"
        ]
    ],
    "torch->Model Optimization->Optimizing Vision Transformer Model for Deployment->Comparing Inference Speed->Learn More": [
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  26.562 seconds)",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Parametrizations Tutorial": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "Regularizing deep-learning models is a surprisingly challenging task.\nClassical techniques such as penalty methods often fall short when applied\non deep models due to the complexity of the function being optimized.\nThis is particularly problematic when working with ill-conditioned models.\nExamples of these are RNNs trained on long sequences and GANs. A number\nof techniques have been proposed in recent years to regularize these\nmodels and improve their convergence. On recurrent models, it has been\nproposed to control the singular values of the recurrent kernel for the\nRNN to be well-conditioned. This can be achieved, for example, by making\nthe recurrent kernel .\nAnother way to regularize recurrent models is via\n\u201c\u201d.\nThis approach proposes to decouple the learning of the parameters from the\nlearning of their norms.  To do so, the parameter is divided by its\n\nand a separate parameter encoding its norm is learnt.\nA similar regularization was proposed for GANs under the name of\n\u201c\u201d. This method\ncontrols the Lipschitz constant of the network by dividing its parameters by\ntheir ,\nrather than their Frobenius norm.",
            "markdown"
        ],
        [
            "All these methods have a common pattern: they all transform a parameter\nin an appropriate way before using it. In the first case, they make it orthogonal by\nusing a function that maps matrices to orthogonal matrices. In the case of weight\nand spectral normalization, they divide the original parameter by its norm.",
            "markdown"
        ],
        [
            "More generally, all these examples use a function to put extra structure on the parameters.\nIn other words, they use a function to constrain the parameters.",
            "markdown"
        ],
        [
            "In this tutorial, you will learn how to implement and use this pattern to put\nconstraints on your model. Doing so is as easy as writing your own nn.Module.",
            "markdown"
        ],
        [
            "Requirements: torch&gt;=1.9.0",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Parametrizations Tutorial->Implementing parametrizations by hand": [
        [
            "Assume that we want to have a square linear layer with symmetric weights, that is,\nwith weights X such that X = X\u1d40. One way to do so is\nto copy the upper-triangular part of the matrix into its lower-triangular part",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\nimport torch.nn.utils.parametrize as parametrize\n\ndef symmetric(X):\n    return X.triu() + X.triu(1).transpose(-1, -2)\n\nX = (3, 3)\nA = symmetric(X)\nassert (A, A.T)  # A is symmetric\nprint(A)                       # Quick visual check",
            "code"
        ],
        [
            "We can then use this idea to implement a linear layer with symmetric weights",
            "markdown"
        ],
        [
            "class LinearSymmetric():\n    def __init__(self, n_features):\n        super().__init__()\n        self.weight = nn.Parameter((n_features, n_features))\n\n    def forward(self, x):\n        A = symmetric(self.weight)\n        return x @ A",
            "code"
        ],
        [
            "The layer can be then used as a regular linear layer",
            "markdown"
        ],
        [
            "layer = LinearSymmetric(3)\nout = layer((8, 3))",
            "code"
        ],
        [
            "This implementation, although correct and self-contained, presents a number of problems:",
            "markdown"
        ],
        [
            "It reimplements the layer. We had to implement the linear layer as x @ A. This is\nnot very problematic for a linear layer, but imagine having to reimplement a CNN or a\nTransformer\u2026",
            "markdown"
        ],
        [
            "It does not separate the layer and the parametrization.  If the parametrization were\nmore difficult, we would have to rewrite its code for each layer that we want to use it\nin.",
            "markdown"
        ],
        [
            "It recomputes the parametrization everytime we use the layer. If we use the layer\nseveral times during the forward pass, (imagine the recurrent kernel of an RNN), it\nwould compute the same A every time that the layer is called.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Parametrizations Tutorial->Introduction to parametrizations": [
        [
            "Parametrizations can solve all these problems as well as others.",
            "markdown"
        ],
        [
            "Let\u2019s start by reimplementing the code above using torch.nn.utils.parametrize.\nThe only thing that we have to do is to write the parametrization as a regular nn.Module",
            "markdown"
        ],
        [
            "class Symmetric():\n    def forward(self, X):\n        return X.triu() + X.triu(1).transpose(-1, -2)",
            "code"
        ],
        [
            "This is all we need to do. Once we have this, we can transform any regular layer into a\nsymmetric layer by doing",
            "markdown"
        ],
        [
            "layer = (3, 3)\n(layer, \"weight\", Symmetric())",
            "code"
        ],
        [
            "Now, the matrix of the linear layer is symmetric",
            "markdown"
        ],
        [
            "A = layer.weight\nassert (A, A.T)  # A is symmetric\nprint(A)                       # Quick visual check",
            "code"
        ],
        [
            "We can do the same thing with any other layer. For example, we can create a CNN with\n kernels.\nWe use a similar parametrization, copying the upper-triangular part with signs\nreversed into the lower-triangular part",
            "markdown"
        ],
        [
            "class Skew():\n    def forward(self, X):\n        A = X.triu(1)\n        return A - A.transpose(-1, -2)\n\n\ncnn = (in_channels=5, out_channels=8, kernel_size=3)\n(cnn, \"weight\", Skew())\n# Print a few kernels\nprint(cnn.weight[0, 1])\nprint(cnn.weight[2, 2])",
            "code"
        ]
    ],
    "torch->Model Optimization->Parametrizations Tutorial->Inspecting a parametrized module": [
        [
            "When a module is parametrized, we find that the module has changed in three ways:",
            "markdown"
        ],
        [
            "model.weight is now a property",
            "markdown"
        ],
        [
            "It has a new module.parametrizations attribute",
            "markdown"
        ],
        [
            "The unparametrized weight has been moved to module.parametrizations.weight.original\n\n\n<br/>",
            "markdown"
        ],
        [
            "After parametrizing weight, layer.weight is turned into a\n.\nThis property computes parametrization(weight) every time we request layer.weight\njust as we did in our implementation of LinearSymmetric above.",
            "markdown"
        ],
        [
            "Registered parametrizations are stored under a parametrizations attribute within the module.",
            "markdown"
        ],
        [
            "layer = (3, 3)\nprint(f\"Unparametrized:\\n{layer}\")\n(layer, \"weight\", Symmetric())\nprint(f\"\\nParametrized:\\n{layer}\")",
            "code"
        ],
        [
            "This parametrizations attribute is an nn.ModuleDict, and it can be accessed as such",
            "markdown"
        ],
        [
            "print(layer.parametrizations)\nprint(layer.parametrizations.weight)",
            "code"
        ],
        [
            "Each element of this nn.ModuleDict is a ParametrizationList, which behaves like an\nnn.Sequential. This list will allow us to concatenate parametrizations on one weight.\nSince this is a list, we can access the parametrizations indexing it. Here\u2019s\nwhere our Symmetric parametrization sits",
            "markdown"
        ],
        [
            "print(layer.parametrizations.weight[0])",
            "code"
        ],
        [
            "The other thing that we notice is that, if we print the parameters, we see that the\nparameter weight has been moved",
            "markdown"
        ],
        [
            "print(dict(layer.named_parameters()))",
            "code"
        ],
        [
            "It now sits under layer.parametrizations.weight.original",
            "markdown"
        ],
        [
            "print(layer.parametrizations.weight.original)",
            "code"
        ],
        [
            "Besides these three small differences, the parametrization is doing exactly the same\nas our manual implementation",
            "markdown"
        ],
        [
            "symmetric = Symmetric()\nweight_orig = layer.parametrizations.weight.original\nprint((layer.weight, symmetric(weight_orig)))",
            "code"
        ]
    ],
    "torch->Model Optimization->Parametrizations Tutorial->Parametrizations are first-class citizens": [
        [
            "Since layer.parametrizations is an nn.ModuleList, it means that the parametrizations\nare properly registered as submodules of the original module. As such, the same rules\nfor registering parameters in a module apply to register a parametrization.\nFor example, if a parametrization has parameters, these will be moved from CPU\nto CUDA when calling model = model.cuda().",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Parametrizations Tutorial->Caching the value of a parametrization": [
        [
            "Parametrizations come with an inbuilt caching system via the context manager\nparametrize.cached()",
            "markdown"
        ],
        [
            "class NoisyParametrization():\n    def forward(self, X):\n        print(\"Computing the Parametrization\")\n        return X\n\nlayer = (4, 4)\n(layer, \"weight\", NoisyParametrization())\nprint(\"Here, layer.weight is recomputed every time we call it\")\nfoo = layer.weight + layer.weight.T\nbar = layer.weight.sum()\nwith ():\n    print(\"Here, it is computed just the first time layer.weight is called\")\n    foo = layer.weight + layer.weight.T\n    bar = layer.weight.sum()",
            "code"
        ]
    ],
    "torch->Model Optimization->Parametrizations Tutorial->Concatenating parametrizations": [
        [
            "Concatenating two parametrizations is as easy as registering them on the same tensor.\nWe may use this to create more complex parametrizations from simpler ones. For example, the\n\nmaps the skew-symmetric matrices to the orthogonal matrices of positive determinant. We can\nconcatenate Skew and a parametrization that implements the Cayley map to get a layer with\northogonal weights",
            "markdown"
        ],
        [
            "class CayleyMap():\n    def __init__(self, n):\n        super().__init__()\n        self.register_buffer(\"Id\", (n))\n\n    def forward(self, X):\n        # (I + X)(I - X)^{-1}\n        return torch.solve(self.Id + X, self.Id - X).solution\n\nlayer = (3, 3)\n(layer, \"weight\", Skew())\n(layer, \"weight\", CayleyMap(3))\nX = layer.weight\nprint((X.T @ X, (3)))  # X is orthogonal",
            "code"
        ],
        [
            "This may also be used to prune a parametrized module, or to reuse parametrizations. For example,\nthe matrix exponential maps the symmetric matrices to the Symmetric Positive Definite (SPD) matrices\nBut the matrix exponential also maps the skew-symmetric matrices to the orthogonal matrices.\nUsing these two facts, we may reuse the parametrizations before to our advantage",
            "markdown"
        ],
        [
            "class MatrixExponential():\n    def forward(self, X):\n        return (X)\n\nlayer_orthogonal = (3, 3)\n(layer_orthogonal, \"weight\", Skew())\n(layer_orthogonal, \"weight\", MatrixExponential())\nX = layer_orthogonal.weight\nprint((X.T @ X, (3)))         # X is orthogonal\n\nlayer_spd = (3, 3)\n(layer_spd, \"weight\", Symmetric())\n(layer_spd, \"weight\", MatrixExponential())\nX = layer_spd.weight\nprint((X, X.T))                        # X is symmetric\nprint((torch.symeig(X).eigenvalues &gt; 0.).all())  # X is positive definite",
            "code"
        ]
    ],
    "torch->Model Optimization->Parametrizations Tutorial->Intializing parametrizations": [
        [
            "Parametrizations come with a mechanism to initialize them. If we implement a method\nright_inverse with signature",
            "markdown"
        ],
        [
            "def right_inverse(self, X: Tensor) -&gt; Tensor",
            "code"
        ],
        [
            "it will be used when assigning to the parametrized tensor.",
            "markdown"
        ],
        [
            "Let\u2019s upgrade our implementation of the Skew class to support this",
            "markdown"
        ],
        [
            "class Skew():\n    def forward(self, X):\n        A = X.triu(1)\n        return A - A.transpose(-1, -2)\n\n    def right_inverse(self, A):\n        # We assume that A is skew-symmetric\n        # We take the upper-triangular elements, as these are those used in the forward\n        return A.triu(1)",
            "code"
        ],
        [
            "We may now initialize a layer that is parametrized with Skew",
            "markdown"
        ],
        [
            "layer = (3, 3)\n(layer, \"weight\", Skew())\nX = (3, 3)\nX = X - X.T                             # X is now skew-symmetric\nlayer.weight = X                        # Initialize layer.weight to be X\nprint((layer.weight, X))      # layer.weight == X",
            "code"
        ],
        [
            "This right_inverse works as expected when we concatenate parametrizations.\nTo see this, let\u2019s upgrade the Cayley parametrization to also support being initialized",
            "markdown"
        ],
        [
            "class CayleyMap():\n    def __init__(self, n):\n        super().__init__()\n        self.register_buffer(\"Id\", (n))\n\n    def forward(self, X):\n        # Assume X skew-symmetric\n        # (I + X)(I - X)^{-1}\n        return torch.solve(self.Id + X, self.Id - X).solution\n\n    def right_inverse(self, A):\n        # Assume A orthogonal\n        # See https://en.wikipedia.org/wiki/Cayley_transform#Matrix_map\n        # (X - I)(X + I)^{-1}\n        return torch.solve(X - self.Id, self.Id + X).solution\n\nlayer_orthogonal = (3, 3)\n(layer_orthogonal, \"weight\", Skew())\n(layer_orthogonal, \"weight\", CayleyMap(3))\n# Sample an orthogonal matrix with positive determinant\nX = (3, 3)\n(X)\nif X.det() &lt; 0.:\n    X[0].neg_()\nlayer_orthogonal.weight = X\nprint((layer_orthogonal.weight, X))  # layer_orthogonal.weight == X",
            "code"
        ],
        [
            "This initialization step can be written more succinctly as",
            "markdown"
        ],
        [
            "layer_orthogonal.weight = (layer_orthogonal.weight)",
            "code"
        ],
        [
            "The name of this method comes from the fact that we would often expect\nthat forward(right_inverse(X)) == X. This is a direct way of rewriting that\nthe forward afer the initalization with value X should return the value X.\nThis constraint is not strongly enforced in practice. In fact, at times, it might be of\ninterest to relax this relation. For example, consider the following implementation\nof a randomized pruning method:",
            "markdown"
        ],
        [
            "class PruningParametrization():\n    def __init__(self, X, p_drop=0.2):\n        super().__init__()\n        # sample zeros with probability p_drop\n        mask = (X, 1.0 - p_drop)\n        self.mask = (mask)\n\n    def forward(self, X):\n        return X * self.mask\n\n    def right_inverse(self, A):\n        return A",
            "code"
        ],
        [
            "In this case, it is not true that for every matrix A forward(right_inverse(A)) == A.\nThis is only true when the matrix A has zeros in the same positions as the mask.\nEven then, if we assign a tensor to a pruned parameter, it will comes as no surprise\nthat tensor will be, in fact, pruned",
            "markdown"
        ],
        [
            "layer = (3, 4)\nX = (layer.weight)\nprint(f\"Initialization matrix:\\n{X}\")\n(layer, \"weight\", PruningParametrization(layer.weight))\nlayer.weight = X\nprint(f\"\\nInitialized weight:\\n{layer.weight}\")",
            "code"
        ]
    ],
    "torch->Model Optimization->Parametrizations Tutorial->Removing parametrizations": [
        [
            "We may remove all the parametrizations from a parameter or a buffer in a module\nby using parametrize.remove_parametrizations()",
            "markdown"
        ],
        [
            "layer = (3, 3)\nprint(\"Before:\")\nprint(layer)\nprint(layer.weight)\n(layer, \"weight\", Skew())\nprint(\"\\nParametrized:\")\nprint(layer)\nprint(layer.weight)\n(layer, \"weight\")\nprint(\"\\nAfter. Weight has skew-symmetric values but it is unconstrained:\")\nprint(layer)\nprint(layer.weight)",
            "code"
        ],
        [
            "When removing a parametrization, we may choose to leave the original parameter (i.e. that in\nlayer.parametriations.weight.original) rather than its parametrized version by setting\nthe flag leave_parametrized=False",
            "markdown"
        ],
        [
            "layer = (3, 3)\nprint(\"Before:\")\nprint(layer)\nprint(layer.weight)\n(layer, \"weight\", Skew())\nprint(\"\\nParametrized:\")\nprint(layer)\nprint(layer.weight)\n(layer, \"weight\", leave_parametrized=False)\nprint(\"\\nAfter. Same as Before:\")\nprint(layer)\nprint(layer.weight)",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Pruning Tutorial": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "State-of-the-art deep learning techniques rely on over-parametrized models\nthat are hard to deploy. On the contrary, biological neural networks are\nknown to use efficient sparse connectivity. Identifying optimal\ntechniques to compress models by reducing the number of parameters in them is\nimportant in order to reduce memory, battery, and hardware consumption without\nsacrificing accuracy. This in turn allows you to deploy lightweight models on device, and guarantee\nprivacy with private on-device computation. On the research front, pruning is\nused to investigate the differences in learning dynamics between\nover-parametrized and under-parametrized networks, to study the role of lucky\nsparse subnetworks and initializations\n(\u201c\u201d) as a destructive\nneural architecture search technique, and more.",
            "markdown"
        ],
        [
            "In this tutorial, you will learn how to use torch.nn.utils.prune to\nsparsify your neural networks, and how to extend it to implement your\nown custom pruning technique.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Pruning Tutorial->Requirements": [
        [
            "\"torch&gt;=1.4.0a0+8e8a5e0\"",
            "markdown"
        ],
        [
            "import torch\nfrom torch import nn\nimport torch.nn.utils.prune as prune\nimport torch.nn.functional as F",
            "code"
        ]
    ],
    "torch->Model Optimization->Pruning Tutorial->Create a model": [
        [
            "In this tutorial, we use the  architecture from\nLeCun et al., 1998.",
            "markdown"
        ],
        [
            " = (\"cuda\" if () else \"cpu\")\n\nclass LeNet():\n    def __init__(self):\n        super(, self).__init__()\n        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n        self.conv1 = (1, 6, 3)\n        self.conv2 = (6, 16, 3)\n        self.fc1 = (16 * 5 * 5, 120)  # 5x5 image dimension\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = ((self.conv1(x)), (2, 2))\n        x = ((self.conv2(x)), 2)\n        x = x.view(-1, int(x.nelement() / x.shape[0]))\n        x = (self.fc1(x))\n        x = (self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nmodel = ().to(=)",
            "code"
        ]
    ],
    "torch->Model Optimization->Pruning Tutorial->Inspect a Module": [
        [
            "Let\u2019s inspect the (unpruned) conv1 layer in our LeNet model. It will contain two\nparameters weight and bias, and no buffers, for now.",
            "markdown"
        ],
        [
            " = \nprint(list(()))",
            "code"
        ],
        [
            "[('weight', Parameter containing:\ntensor([[[[ 0.1435, -0.0650, -0.0823],\n          [ 0.0881, -0.1054,  0.0788],\n          [-0.0560,  0.2107, -0.1601]]],\n\n\n        [[[-0.2217,  0.1166, -0.1881],\n          [-0.2317,  0.2586,  0.0918],\n          [ 0.0984, -0.1276, -0.1021]]],\n\n\n        [[[ 0.1736, -0.1684,  0.0646],\n          [-0.3216,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0352,  0.3101],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.2093,  0.2674, -0.0505],\n          [-0.2793,  0.1896,  0.1743],\n          [ 0.1594,  0.0905,  0.1410]]],\n\n\n        [[[-0.1667,  0.3219, -0.0257],\n          [ 0.0208, -0.0809,  0.1365],\n          [-0.1979,  0.1987, -0.1775]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.3213, -0.1174,  0.1717,  0.2642, -0.3046,  0.3249], device='cuda:0',\n       requires_grad=True))]",
            "code"
        ],
        [
            "print(list(()))",
            "code"
        ],
        [
            "[]",
            "code"
        ]
    ],
    "torch->Model Optimization->Pruning Tutorial->Pruning a Module": [
        [
            "To prune a module (in this example, the conv1 layer of our LeNet\narchitecture), first select a pruning technique among those available in\ntorch.nn.utils.prune (or\n\nyour own by subclassing\nBasePruningMethod). Then, specify the module and the name of the parameter to\nprune within that module. Finally, using the adequate keyword arguments\nrequired by the selected pruning technique, specify the pruning parameters.",
            "markdown"
        ],
        [
            "In this example, we will prune at random 30% of the connections in\nthe parameter named weight in the conv1 layer.\nThe module is passed as the first argument to the function; name\nidentifies the parameter within that module using its string identifier; and\namount indicates either the percentage of connections to prune (if it\nis a float between 0. and 1.), or the absolute number of connections to\nprune (if it is a non-negative integer).",
            "markdown"
        ],
        [
            "(, name=\"weight\", amount=0.3)",
            "code"
        ],
        [
            "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))",
            "code"
        ],
        [
            "Pruning acts by removing weight from the parameters and replacing it with\na new parameter called weight_orig (i.e. appending \"_orig\" to the\ninitial parameter name). weight_orig stores the unpruned version of\nthe tensor. The bias was not pruned, so it will remain intact.",
            "markdown"
        ],
        [
            "print(list(()))",
            "code"
        ],
        [
            "[('bias', Parameter containing:\ntensor([-0.3213, -0.1174,  0.1717,  0.2642, -0.3046,  0.3249], device='cuda:0',\n       requires_grad=True)), ('weight_orig', Parameter containing:\ntensor([[[[ 0.1435, -0.0650, -0.0823],\n          [ 0.0881, -0.1054,  0.0788],\n          [-0.0560,  0.2107, -0.1601]]],\n\n\n        [[[-0.2217,  0.1166, -0.1881],\n          [-0.2317,  0.2586,  0.0918],\n          [ 0.0984, -0.1276, -0.1021]]],\n\n\n        [[[ 0.1736, -0.1684,  0.0646],\n          [-0.3216,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0352,  0.3101],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.2093,  0.2674, -0.0505],\n          [-0.2793,  0.1896,  0.1743],\n          [ 0.1594,  0.0905,  0.1410]]],\n\n\n        [[[-0.1667,  0.3219, -0.0257],\n          [ 0.0208, -0.0809,  0.1365],\n          [-0.1979,  0.1987, -0.1775]]]], device='cuda:0', requires_grad=True))]",
            "code"
        ],
        [
            "The pruning mask generated by the pruning technique selected above is saved\nas a module buffer named weight_mask (i.e. appending \"_mask\" to the\ninitial parameter name).",
            "markdown"
        ],
        [
            "print(list(()))",
            "code"
        ],
        [
            "[('weight_mask', tensor([[[[0., 1., 0.],\n          [1., 0., 0.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 1.],\n          [1., 1., 0.],\n          [1., 0., 1.]]],\n\n\n        [[[1., 0., 0.],\n          [0., 1., 1.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 0.],\n          [1., 1., 1.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 1.],\n          [1., 1., 1.],\n          [0., 1., 1.]]],\n\n\n        [[[1., 1., 1.],\n          [1., 1., 0.],\n          [1., 1., 0.]]]], device='cuda:0'))]",
            "code"
        ],
        [
            "For the forward pass to work without modification, the weight attribute\nneeds to exist. The pruning techniques implemented in\ntorch.nn.utils.prune compute the pruned version of the weight (by\ncombining the mask with the original parameter) and store them in the\nattribute weight. Note, this is no longer a parameter of the module,\nit is now simply an attribute.",
            "markdown"
        ],
        [
            "print()",
            "code"
        ],
        [
            "tensor([[[[ 0.0000, -0.0650, -0.0000],\n          [ 0.0881, -0.0000,  0.0000],\n          [-0.0560,  0.2107, -0.1601]]],\n\n\n        [[[-0.2217,  0.0000, -0.1881],\n          [-0.2317,  0.2586,  0.0000],\n          [ 0.0984, -0.0000, -0.1021]]],\n\n\n        [[[ 0.1736, -0.0000,  0.0000],\n          [-0.0000,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0000,  0.0000],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.2093,  0.0000, -0.0505],\n          [-0.2793,  0.1896,  0.1743],\n          [ 0.0000,  0.0905,  0.1410]]],\n\n\n        [[[-0.1667,  0.3219, -0.0257],\n          [ 0.0208, -0.0809,  0.0000],\n          [-0.1979,  0.1987, -0.0000]]]], device='cuda:0',\n       grad_fn=&lt;MulBackward0&gt;)",
            "code"
        ],
        [
            "Finally, pruning is applied prior to each forward pass using PyTorch\u2019s\nforward_pre_hooks. Specifically, when the module is pruned, as we\nhave done here, it will acquire a forward_pre_hook for each parameter\nassociated with it that gets pruned. In this case, since we have so far\nonly pruned the original parameter named weight, only one hook will be\npresent.",
            "markdown"
        ],
        [
            "print(._forward_pre_hooks)",
            "code"
        ],
        [
            "OrderedDict([(0, &lt;torch.nn.utils.prune.RandomUnstructured object at 0x7ff5c140bf70&gt;)])",
            "code"
        ],
        [
            "For completeness, we can now prune the bias too, to see how the\nparameters, buffers, hooks, and attributes of the module change.\nJust for the sake of trying out another pruning technique, here we prune the\n3 smallest entries in the bias by L1 norm, as implemented in the\nl1_unstructured pruning function.",
            "markdown"
        ],
        [
            "(, name=\"bias\", amount=3)",
            "code"
        ],
        [
            "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))",
            "code"
        ],
        [
            "We now expect the named parameters to include both weight_orig (from\nbefore) and bias_orig. The buffers will include weight_mask and\nbias_mask. The pruned versions of the two tensors will exist as\nmodule attributes, and the module will now have two forward_pre_hooks.",
            "markdown"
        ],
        [
            "print(list(()))",
            "code"
        ],
        [
            "[('weight_orig', Parameter containing:\ntensor([[[[ 0.1435, -0.0650, -0.0823],\n          [ 0.0881, -0.1054,  0.0788],\n          [-0.0560,  0.2107, -0.1601]]],\n\n\n        [[[-0.2217,  0.1166, -0.1881],\n          [-0.2317,  0.2586,  0.0918],\n          [ 0.0984, -0.1276, -0.1021]]],\n\n\n        [[[ 0.1736, -0.1684,  0.0646],\n          [-0.3216,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0352,  0.3101],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.2093,  0.2674, -0.0505],\n          [-0.2793,  0.1896,  0.1743],\n          [ 0.1594,  0.0905,  0.1410]]],\n\n\n        [[[-0.1667,  0.3219, -0.0257],\n          [ 0.0208, -0.0809,  0.1365],\n          [-0.1979,  0.1987, -0.1775]]]], device='cuda:0', requires_grad=True)), ('bias_orig', Parameter containing:\ntensor([-0.3213, -0.1174,  0.1717,  0.2642, -0.3046,  0.3249], device='cuda:0',\n       requires_grad=True))]",
            "code"
        ],
        [
            "print(list(()))",
            "code"
        ],
        [
            "[('weight_mask', tensor([[[[0., 1., 0.],\n          [1., 0., 0.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 1.],\n          [1., 1., 0.],\n          [1., 0., 1.]]],\n\n\n        [[[1., 0., 0.],\n          [0., 1., 1.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 0.],\n          [1., 1., 1.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 1.],\n          [1., 1., 1.],\n          [0., 1., 1.]]],\n\n\n        [[[1., 1., 1.],\n          [1., 1., 0.],\n          [1., 1., 0.]]]], device='cuda:0')), ('bias_mask', tensor([1., 0., 0., 0., 1., 1.], device='cuda:0'))]",
            "code"
        ],
        [
            "print()",
            "code"
        ],
        [
            "tensor([-0.3213, -0.0000,  0.0000,  0.0000, -0.3046,  0.3249], device='cuda:0',\n       grad_fn=&lt;MulBackward0&gt;)",
            "code"
        ],
        [
            "print(._forward_pre_hooks)",
            "code"
        ],
        [
            "OrderedDict([(0, &lt;torch.nn.utils.prune.RandomUnstructured object at 0x7ff5c140bf70&gt;), (1, &lt;torch.nn.utils.prune.L1Unstructured object at 0x7ff5c140b760&gt;)])",
            "code"
        ]
    ],
    "torch->Model Optimization->Pruning Tutorial->Iterative Pruning": [
        [
            "The same parameter in a module can be pruned multiple times, with the\neffect of the various pruning calls being equal to the combination of the\nvarious masks applied in series.\nThe combination of a new mask with the old mask is handled by the\nPruningContainer\u2019s compute_mask method.",
            "markdown"
        ],
        [
            "Say, for example, that we now want to further prune module.weight, this\ntime using structured pruning along the 0th axis of the tensor (the 0th axis\ncorresponds to the output channels of the convolutional layer and has\ndimensionality 6 for conv1), based on the channels\u2019 L2 norm. This can be\nachieved using the ln_structured function, with n=2 and dim=0.",
            "markdown"
        ],
        [
            "(, name=\"weight\", amount=0.5, n=2, dim=0)\n\n# As we can verify, this will zero out all the connections corresponding to\n# 50% (3 out of 6) of the channels, while preserving the action of the\n# previous mask.\nprint()",
            "code"
        ],
        [
            "tensor([[[[ 0.0000, -0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000]]],\n\n\n        [[[-0.2217,  0.0000, -0.1881],\n          [-0.2317,  0.2586,  0.0000],\n          [ 0.0984, -0.0000, -0.1021]]],\n\n\n        [[[ 0.1736, -0.0000,  0.0000],\n          [-0.0000,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0000,  0.0000],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.0000,  0.0000, -0.0000],\n          [-0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000]]],\n\n\n        [[[-0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000]]]], device='cuda:0',\n       grad_fn=&lt;MulBackward0&gt;)",
            "code"
        ],
        [
            "The corresponding hook will now be of type\ntorch.nn.utils.prune.PruningContainer, and will store the history of\npruning applied to the weight parameter.",
            "markdown"
        ],
        [
            "for  in ._forward_pre_hooks.values():\n    if ._tensor_name == \"weight\":  # select out the correct hook\n        break\n\nprint(list())  # pruning history in the container",
            "code"
        ],
        [
            "[&lt;torch.nn.utils.prune.RandomUnstructured object at 0x7ff5c140bf70&gt;, &lt;torch.nn.utils.prune.LnStructured object at 0x7ff5c140bfd0&gt;]",
            "code"
        ]
    ],
    "torch->Model Optimization->Pruning Tutorial->Serializing a pruned model": [
        [
            "All relevant tensors, including the mask buffers and the original parameters\nused to compute the pruned tensors are stored in the model\u2019s state_dict\nand can therefore be easily serialized and saved, if needed.",
            "markdown"
        ],
        [
            "print(().keys())",
            "code"
        ],
        [
            "odict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])",
            "code"
        ]
    ],
    "torch->Model Optimization->Pruning Tutorial->Remove pruning re-parametrization": [
        [
            "To make the pruning permanent, remove the re-parametrization in terms\nof weight_orig and weight_mask, and remove the forward_pre_hook,\nwe can use the remove functionality from torch.nn.utils.prune.\nNote that this doesn\u2019t undo the pruning, as if it never happened. It simply\nmakes it permanent, instead, by reassigning the parameter weight to the\nmodel parameters, in its pruned version.",
            "markdown"
        ],
        [
            "Prior to removing the re-parametrization:",
            "markdown"
        ],
        [
            "print(list(()))",
            "code"
        ],
        [
            "[('weight_orig', Parameter containing:\ntensor([[[[ 0.1435, -0.0650, -0.0823],\n          [ 0.0881, -0.1054,  0.0788],\n          [-0.0560,  0.2107, -0.1601]]],\n\n\n        [[[-0.2217,  0.1166, -0.1881],\n          [-0.2317,  0.2586,  0.0918],\n          [ 0.0984, -0.1276, -0.1021]]],\n\n\n        [[[ 0.1736, -0.1684,  0.0646],\n          [-0.3216,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0352,  0.3101],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.2093,  0.2674, -0.0505],\n          [-0.2793,  0.1896,  0.1743],\n          [ 0.1594,  0.0905,  0.1410]]],\n\n\n        [[[-0.1667,  0.3219, -0.0257],\n          [ 0.0208, -0.0809,  0.1365],\n          [-0.1979,  0.1987, -0.1775]]]], device='cuda:0', requires_grad=True)), ('bias_orig', Parameter containing:\ntensor([-0.3213, -0.1174,  0.1717,  0.2642, -0.3046,  0.3249], device='cuda:0',\n       requires_grad=True))]",
            "code"
        ],
        [
            "print(list(()))",
            "code"
        ],
        [
            "[('weight_mask', tensor([[[[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]]],\n\n\n        [[[1., 0., 1.],\n          [1., 1., 0.],\n          [1., 0., 1.]]],\n\n\n        [[[1., 0., 0.],\n          [0., 1., 1.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 0.],\n          [1., 1., 1.],\n          [1., 1., 1.]]],\n\n\n        [[[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]]],\n\n\n        [[[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]]]], device='cuda:0')), ('bias_mask', tensor([1., 0., 0., 0., 1., 1.], device='cuda:0'))]",
            "code"
        ],
        [
            "print()",
            "code"
        ],
        [
            "tensor([[[[ 0.0000, -0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000]]],\n\n\n        [[[-0.2217,  0.0000, -0.1881],\n          [-0.2317,  0.2586,  0.0000],\n          [ 0.0984, -0.0000, -0.1021]]],\n\n\n        [[[ 0.1736, -0.0000,  0.0000],\n          [-0.0000,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0000,  0.0000],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.0000,  0.0000, -0.0000],\n          [-0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000]]],\n\n\n        [[[-0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000]]]], device='cuda:0',\n       grad_fn=&lt;MulBackward0&gt;)",
            "code"
        ],
        [
            "After removing the re-parametrization:",
            "markdown"
        ],
        [
            "(, 'weight')\nprint(list(()))",
            "code"
        ],
        [
            "[('bias_orig', Parameter containing:\ntensor([-0.3213, -0.1174,  0.1717,  0.2642, -0.3046,  0.3249], device='cuda:0',\n       requires_grad=True)), ('weight', Parameter containing:\ntensor([[[[ 0.0000, -0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000]]],\n\n\n        [[[-0.2217,  0.0000, -0.1881],\n          [-0.2317,  0.2586,  0.0000],\n          [ 0.0984, -0.0000, -0.1021]]],\n\n\n        [[[ 0.1736, -0.0000,  0.0000],\n          [-0.0000,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0000,  0.0000],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.0000,  0.0000, -0.0000],\n          [-0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000]]],\n\n\n        [[[-0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000]]]], device='cuda:0', requires_grad=True))]",
            "code"
        ],
        [
            "print(list(()))",
            "code"
        ],
        [
            "[('bias_mask', tensor([1., 0., 0., 0., 1., 1.], device='cuda:0'))]",
            "code"
        ]
    ],
    "torch->Model Optimization->Pruning Tutorial->Pruning multiple parameters in a model": [
        [
            "By specifying the desired pruning technique and parameters, we can easily\nprune multiple tensors in a network, perhaps according to their type, as we\nwill see in this example.",
            "markdown"
        ],
        [
            "new_model = ()\nfor name,  in ():\n    # prune 20% of connections in all 2D-conv layers\n    if isinstance(, ):\n        (, name='weight', amount=0.2)\n    # prune 40% of connections in all linear layers\n    elif isinstance(, ):\n        (, name='weight', amount=0.4)\n\nprint(dict(()).keys())  # to verify that all masks exist",
            "code"
        ],
        [
            "dict_keys(['conv1.weight_mask', 'conv2.weight_mask', 'fc1.weight_mask', 'fc2.weight_mask', 'fc3.weight_mask'])",
            "code"
        ]
    ],
    "torch->Model Optimization->Pruning Tutorial->Global pruning": [
        [
            "So far, we only looked at what is usually referred to as \u201clocal\u201d pruning,\ni.e. the practice of pruning tensors in a model one by one, by\ncomparing the statistics (weight magnitude, activation, gradient, etc.) of\neach entry exclusively to the other entries in that tensor. However, a\ncommon and perhaps more powerful technique is to prune the model all at\nonce, by removing (for example) the lowest 20% of connections across the\nwhole model, instead of removing the lowest 20% of connections in each\nlayer. This is likely to result in different pruning percentages per layer.\nLet\u2019s see how to do that using global_unstructured from\ntorch.nn.utils.prune.",
            "markdown"
        ],
        [
            "model = ()\n\nparameters_to_prune = (\n    (, 'weight'),\n    (, 'weight'),\n    (, 'weight'),\n    (, 'weight'),\n    (, 'weight'),\n)\n\n(\n    parameters_to_prune,\n    pruning_method=,\n    amount=0.2,\n)",
            "code"
        ],
        [
            "Now we can check the sparsity induced in every pruned parameter, which will\nnot be equal to 20% in each layer. However, the global sparsity will be\n(approximately) 20%.",
            "markdown"
        ],
        [
            "print(\n    \"Sparsity in conv1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in conv2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc3.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Global sparsity: {:.2f}%\".format(\n        100. * float(\n            ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n        )\n        / float(\n            .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n        )\n    )\n)",
            "code"
        ],
        [
            "Sparsity in conv1.weight: 0.00%\nSparsity in conv2.weight: 7.52%\nSparsity in fc1.weight: 22.14%\nSparsity in fc2.weight: 11.78%\nSparsity in fc3.weight: 10.71%\nGlobal sparsity: 20.00%",
            "code"
        ]
    ],
    "torch->Model Optimization->Pruning Tutorial->Extending torch.nn.utils.prune with custom pruning functions": [
        [
            "To implement your own pruning function, you can extend the\nnn.utils.prune module by subclassing the BasePruningMethod\nbase class, the same way all other pruning methods do. The base class\nimplements the following methods for you: __call__, apply_mask,\napply, prune, and remove. Beyond some special cases, you shouldn\u2019t\nhave to reimplement these methods for your new pruning technique.\nYou will, however, have to implement __init__ (the constructor),\nand compute_mask (the instructions on how to compute the mask\nfor the given tensor according to the logic of your pruning\ntechnique). In addition, you will have to specify which type of\npruning this technique implements (supported options are global,\nstructured, and unstructured). This is needed to determine\nhow to combine masks in the case in which pruning is applied\niteratively. In other words, when pruning a pre-pruned parameter,\nthe current prunining techique is expected to act on the unpruned\nportion of the parameter. Specifying the PRUNING_TYPE will\nenable the PruningContainer (which handles the iterative\napplication of pruning masks) to correctly identify the slice of the\nparameter to prune.",
            "markdown"
        ],
        [
            "Let\u2019s assume, for example, that you want to implement a pruning\ntechnique that prunes every other entry in a tensor (or \u2013 if the\ntensor has previously been pruned \u2013 in the remaining unpruned\nportion of the tensor). This will be of PRUNING_TYPE='unstructured'\nbecause it acts on individual connections in a layer and not on entire\nunits/channels ('structured'), or across different parameters\n('global').",
            "markdown"
        ],
        [
            "class FooBarPruningMethod():\n    \"\"\"Prune every other entry in a tensor\n    \"\"\"\n    PRUNING_TYPE = 'unstructured'\n\n    def compute_mask(self, t, default_mask):\n        mask = default_mask.clone()\n        mask.view(-1)[::2] = 0\n        return mask",
            "code"
        ],
        [
            "Now, to apply this to a parameter in an nn.Module, you should\nalso provide a simple function that instantiates the method and\napplies it.",
            "markdown"
        ],
        [
            "def foobar_unstructured(, name):\n    \"\"\"Prunes tensor corresponding to parameter called `name` in `module`\n    by removing every other entry in the tensors.\n    Modifies module in place (and also return the modified module)\n    by:\n    1) adding a named buffer called `name+'_mask'` corresponding to the\n    binary mask applied to the parameter `name` by the pruning method.\n    The parameter `name` is replaced by its pruned version, while the\n    original (unpruned) parameter is stored in a new parameter named\n    `name+'_orig'`.\n\n    Args:\n        module (nn.Module): module containing the tensor to prune\n        name (string): parameter name within `module` on which pruning\n                will act.\n\n    Returns:\n        module (nn.Module): modified (i.e. pruned) version of the input\n            module\n\n    Examples:\n        &gt;&gt;&gt; m = nn.Linear(3, 4)\n        &gt;&gt;&gt; foobar_unstructured(m, name='bias')\n    \"\"\"\n    (, name)\n    return ",
            "code"
        ],
        [
            "Let\u2019s try it out!",
            "markdown"
        ],
        [
            "model = ()\nfoobar_unstructured(, name='bias')\n\nprint()",
            "code"
        ],
        [
            "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  2.603 seconds)",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "<strong>Edited by</strong>: ",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->Introduction": [
        [
            "Quantization involves converting the weights and activations of your model from float\nto int, which can result in smaller model size and faster inference with only a small\nhit to accuracy.",
            "markdown"
        ],
        [
            "In this tutorial, we will apply the easiest form of quantization -\n -\nto an LSTM-based next word-prediction model, closely following the\n\nfrom the PyTorch examples.",
            "markdown"
        ],
        [
            "# imports\nimport os\nfrom io import open\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->1. Define the model": [
        [
            "Here we define the LSTM model architecture, following the\n\nfrom the word language model example.",
            "markdown"
        ],
        [
            "class LSTMModel():\n    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n\n    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n        super(, self).__init__()\n        self.drop = (dropout)\n        self.encoder = (ntoken, ninp)\n        self.rnn = (ninp, nhid, nlayers, dropout=dropout)\n        self.decoder = (nhid, ntoken)\n\n        self.init_weights()\n\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        , hidden = self.rnn(emb, hidden)\n         = self.drop()\n        decoded = self.decoder()\n        return decoded, hidden\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters())\n        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n                weight.new_zeros(self.nlayers, bsz, self.nhid))",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->2. Load in the text data": [
        [
            "Next, we load the\n into a <cite>Corpus</cite>,\nagain following the\n\nfrom the word language model example.",
            "markdown"
        ],
        [
            "class Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\nclass Corpus(object):\n    def __init__(self, path):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n\n    def tokenize(self, path):\n        \"\"\"Tokenizes a text file.\"\"\"\n        assert os.path.exists(path)\n        # Add words to the dictionary\n        with open(path, 'r', encoding=\"utf8\") as f:\n            for line in f:\n                words = line.split() + ['&lt;eos&gt;']\n                for word in words:\n                    self.dictionary.add_word(word)\n\n        # Tokenize file content\n        with open(path, 'r', encoding=\"utf8\") as f:\n            idss = []\n            for line in f:\n                words = line.split() + ['&lt;eos&gt;']\n                ids = []\n                for word in words:\n                    ids.append(self.dictionary.word2idx[word])\n                idss.append((ids).type())\n            ids = (idss)\n\n        return ids\n\nmodel_data_filepath = 'data/'\n\ncorpus = Corpus(model_data_filepath + 'wikitext-2')",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->3. Load the pre-trained model": [
        [
            "This is a tutorial on dynamic quantization, a quantization technique\nthat is applied after a model has been trained. Therefore, we\u2019ll simply load some\npre-trained weights into this model architecture; these weights were obtained\nby training for five epochs using the default settings in the word language model\nexample.",
            "markdown"
        ],
        [
            "ntokens = len(corpus.dictionary)\n\nmodel = (\n    ntoken = ntokens,\n    ninp = 512,\n    nhid = 256,\n    nlayers = 5,\n)\n\n(\n    (\n        model_data_filepath + 'word_language_model_quantize.pth',\n        map_location=('cpu')\n        )\n    )\n\n()\nprint(model)",
            "code"
        ],
        [
            "LSTMModel(\n  (drop): Dropout(p=0.5, inplace=False)\n  (encoder): Embedding(33278, 512)\n  (rnn): LSTM(512, 256, num_layers=5, dropout=0.5)\n  (decoder): Linear(in_features=256, out_features=33278, bias=True)\n)",
            "code"
        ],
        [
            "Now let\u2019s generate some text to ensure that the pre-trained model is working\nproperly - similarly to before, we follow",
            "markdown"
        ],
        [
            " = (ntokens, (1, 1), dtype=)\nhidden = model.init_hidden(1)\ntemperature = 1.0\nnum_words = 1000\n\nwith open(model_data_filepath + 'out.txt', 'w') as outf:\n    with ():  # no tracking history\n        for i in range(num_words):\n            , hidden = model(, hidden)\n             = .squeeze().div(temperature).exp().cpu()\n             = (, 1)[0]\n            .fill_()\n\n            word = corpus.dictionary.idx2word[]\n\n            outf.write(str(word.encode('utf-8')) + ('\\n' if i % 20 == 19 else ' '))\n\n            if i % 100 == 0:\n                print('| Generated {}/{} words'.format(i, 1000))\n\nwith open(model_data_filepath + 'out.txt', 'r') as outf:\n    all_output = outf.read()\n    print(all_output)",
            "code"
        ],
        [
            "| Generated 0/1000 words\n| Generated 100/1000 words\n| Generated 200/1000 words\n| Generated 300/1000 words\n| Generated 400/1000 words\n| Generated 500/1000 words\n| Generated 600/1000 words\n| Generated 700/1000 words\n| Generated 800/1000 words\n| Generated 900/1000 words\nb'independently' b'concentrates' b',' b'conducted' b'by' b'&lt;unk&gt;' b'Lancaster' b'as' b'one' b'of' b'his' b'own' b'Pacer' b'classmates' b'agent' b'from' b'&lt;unk&gt;' b'as' b'the' b'\"'\nb'principal' b'eukaryotic' b'but' b'D' b'\"' b'television' b'@-@' b'talking' b',' b'right' b'of' b'Cards' b'soon' b'bid' b'from' b'1953' b'from' b'1996' b'.' b'A'\nb'brief' b'year' b'in' b'Squadron' b'held' b'possibly' b'in' b'252' b'through' b'the' b'rubble' b'\"' b'which' b'vanished' b'longer' b',' b'I' b'think' b'known' b'on'\nb'the' b'North' b'or' b'a' b'opportunity' b'of' b'German' b'air' b'signal' b',' b'searching' b'.' b'Tanaka' b'that' b'headed' b'old' b'Mars' b'on' b'&lt;unk&gt;' b'and'\nb'even' b'upon' b'their' b'literary' b'work' b'.' b'\"' b'Along' b'after' b'goaltender' b'advances' b',' b'and' b'stated' b'it' b'underwent' b'the' b'description' b'played' b'after'\nb'there' b',' b'19th-' b'&lt;unk&gt;' b',' b'and' b'minor' b'story' b'@-@' b'&lt;unk&gt;' b'works' b'.' b'The' b'image' b'of' b'fifth' b'of' b'public' b'schools' b'from'\nb'roads' b'were' b'asked' b'by' b'loving' b'70' b'for' b'its' b'North' b'manuscripts' b'.' b'For' b'example' b'in' b'her' b'location' b',' b'he' b'Peru' b'addicted'\nb'with' b'five' b'years' b',' b'leaving' b'they' b'pitch' b'at' b'his' b'cartoon' b'Oxford' b'&lt;unk&gt;' b'&lt;unk&gt;' b'&lt;unk&gt;' b'(' b'&lt;unk&gt;' b'@-@' b'&lt;unk&gt;' b')' b','\nb'super' b'Werneth' b'(' b'1973' b')' b',' b'and' b'even' b'strained' b'as' b'more' b'approached' b'.' b'Two' b'saw' b'large' b'classes' b',' b'including' b'various'\nb'medical' b'schools' b',' b'or' b'surrounding' b',' b'and' b'Ravi' b'&lt;unk&gt;' b'(' b'died' b'Denham' b')' b'.' b'violacea' b'smoke' b'one' b'of' b'larger' b'notice'\nb',' b'only' b'in' b'Cloud' b',' b'but' b'Liam' b'preferring' b'his' b'home' b'scheme' b'under' b'the' b'&lt;unk&gt;' b'alone' b'was' b'given' b'to' b'means' b'when'\nb'feeling' b',' b'from' b'the' b'Flower' b'drum' b'study' b'of' b'lions' b'and' b'Gatrell' b'.' b'The' b'Gilmore' b\"'s\" b'&lt;unk&gt;' b'comprises' b'graphics' b'is' b'created'\nb'in' b'from' b'Lawson' b'.' b'&lt;eos&gt;' b'Diplocystis' b'and' b'Earth' b',' b'including' b'same' b'men' b',' b'concluded' b'his' b'neighbors' b',' b'performing' b'discourage' b'or'\nb'subsequent' b'licences' b'.' b'Wuzhu' b',' b'Piazzi' b'Thatgamecompany' b',' b'The' b'Cyrus' b'led' b'to' b'him' b'in' b'1217' b'for' b'storylines' b'for' b'the' b'HNC'\nb'or' b'could' b'be' b'put' b'to' b'games' b'by' b'that' b'time' b'so' b'the' b'EMI' b'blocks' b'marriages' b'on' b'their' b'website' b'.' b'Whether' b'the'\nb'crisis' b'played' b'by' b'du' b'Ma\\xc3\\xadl' b'Marc' b'Robinson' b'creates' b'David' b'teen' b',' b'observing' b'his' b'troops' b'and' b'all' b'two' b'novels' b',' b'calling'\nb'high' b'scenario' b'to' b'&lt;unk&gt;' b'.' b'In' b'Virginia' b'his' b'Personality' b',' b'elects' b'total' b'policemen' b'from' b'Finkelstein' b\"'s\" b'reluctance' b'has' b'long' b'finds'\nb'it' b'marked' b'attempts' b'to' b'continue' b'to' b'feed' b'from' b'the' b'Palmyrene' b'vote' b'from' b'reality' b',' b'who' b'\"' b'fuses' b'out' b'of' b'treason'\nb'for' b'the' b'support' b'of' b'convict' b'\"' b'.' b'He' b'then' b'says' b'that' b'&lt;unk&gt;' b'and' b'Technologies' b'were' b'buck' b'wanted' b'to' b'secure' b'short'\nb'support' b':' b'\"' b'I' b'think' b'the' b'large' b'Assmann' b'Bertin' b'has' b'bought' b'his' b'job' b'for' b'every' b'foreign' b',' b'and' b'good' b','\nb'such' b',' b'he' b'could' b'be' b'1804' b'in' b'Universities' b'in' b'a' b'whole' b'financial' b'manner' b',' b'more' b'brutal' b'admirals' b'run' b'and' b'painter'\nb'order' b'differently' b'customs' b'Amnesty' b'I' b'returned' b'to' b'a' b'hurricane' b'.' b'It' b'also' b'repeatedly' b'Church' b'\"' b'.' b'This' b'glancing' b'saw' b'to'\nb'towns' b\"'s\" b'friends' b'have' b'widely' b'been' b'spent' b'out' b'to' b'pass' b'version' b'of' b'elect' b'or' b'combined' b'that' b'people' b'concludes' b'.' b'&lt;eos&gt;'\nb'barring' b'that' b'producer' b\"'\" b'Jenova' b'is' b'questioned' b'.' b'East' b'supply' b'of' b'culinary' b'ports' b'estimated' b'its' b'training' b'calls' b',' b'with' b'competition'\nb'an' b'sudden' b'structure' b'by' b'The' b'other' b'\"' b'worst' b'school' b'be' b'the' b'&lt;unk&gt;' b'or' b'sensitive' b'colour' b'that' b'he' b'is' b'sort' b'of'\nb'affluent' b'&lt;unk&gt;' b'.' b'\"' b'He' b'commented' b'\"' b'It' b'is' b'one' b'of' b'his' b'&lt;unk&gt;' b',' b'at' b'the' b'Barcelona' b'entrance' b'and' b'that'\nb'the' b'people' b'implicated' b'away' b'into' b'\"' b',' b'it' b'considers' b'eclipse' b'a' b'greater' b'sort' b'of' b'Hairan' b'Carter' b'is' b'also' b'evident' b'.'\nb'In' b'western' b'and' b'early' b'1918' b'the' b'character' b'found' b'to' b'take' b'ammunition' b'when' b'security' b'or' b'earthworms' b'Transit' b'.' b'&lt;eos&gt;' b'Due' b'to'\nb'&lt;unk&gt;' b'&lt;unk&gt;' b'died' b'this' b'earlier' b'warrior' b'Mosley' b',' b'railways' b'this' b'Go' b'military' b'is' b'also' b'a' b'DRS' b',' b'but' b'Pelagius' b'chronicles'\nb'until' b'this' b'moment' b'.' b'Inside' b'there' b'they' b'Reala' b'sacrifices' b'of' b'five' b'minutes' b'a' b'lot' b'of' b'scale' b'suffers' b'the' b'papillae' b'.'\nb'Because' b'would' b'return' b'a' b'characteristic' b'in' b'their' b'explicit' b'business' b';' b'it' b'is' b'commonly' b'only' b'sheet' b'at' b'any' b'other' b'female' b'@-@'\nb'navigational' b'zoos' b'at' b'apparent' b'above' b'night' b'.' b'&lt;unk&gt;' b'changes' b',' b'etc' b',' b'necessitated' b'Ludacris' b'.' b'The' b'universal' b'form' b'is' b'usually'\nb'easy' b',' b'quite' b'after' b'a' b'able' b'for' b'keyboard' b'movement' b'.' b'In' b'the' b'same' b'year' b',' b'due' b'to' b'Raffles' b',' b'placing'\nb'&lt;unk&gt;' b',' b'fellow' b'or' b'fades' b'calling' b',' b'they' b'are' b'\"' b'OK' b'inclined' b',' b'to' b'contain' b'his' b'natural' b'trouble' b'\"' b'.'\nb'For' b'example' b'all' b'to' b'Love' b',' b'the' b'character' b'honorable' b'exactly' b'by' b'his' b'body' b'and' b'goes' b'down' b'and' b'climb' b'the' b'last'\nb'to' b'gravity' b'when' b'they' b'are' b'very' b'somewhat' b'at' b'a' b'large' b'end' b'.' b'A' b'anonymous' b'Athene' b'particle' b'supernatural' b'often' b'marked' b'the'\nb'&lt;unk&gt;' b'&lt;unk&gt;' b'possessing' b'&lt;unk&gt;' b'.' b'&lt;eos&gt;' b'Now' b'for' b'comedic' b'behaviour' b'into' b'Andrew' b'sexpunctatus' b'(' b'converted' b',' b'and' b'the' b'Lake' b'version'\nb')' b',' b'have' b'been' b'beside' b'that' b'on' b'weather' b'regions' b',' b'they' b'are' b'used' b'to' b'learn' b'in' b'their' b'size' b'due' b'to'\nb'a' b'behaviour' b'from' b'&lt;unk&gt;' b'for' b'525' b'\\xc2\\xb0' b'weeks' b'.' b'Later' b'there' b'is' b'reported' b'that' b'Authors' b'only' b'consider' b'very' b'one' b'of'\nb'its' b'reasons' b'from' b'a' b'tolerate' b'jumping' b'.' b'The' b'flanks' b'have' b'always' b'been' b'widely' b'compared' b'to' b'account' b\"'\" b'Perhaps' b',' b'handheld'\nb',' b'but' b'some' b'have' b'increase' b'alone' b'.' b'A' b'blast' b'on' b'either' b'ridge' b'is' b'probably' b'effective' b',' b'with' b'a' b'elaborate' b'eye'\nb'(' b'such' b'as' b'sulfide' b'or' b'external' b'wing' b'and' b'Bhakti' b'.' b')' b'flipped' b'Barrier' b'NW' b'expressed' b'it' b'as' b'\"' b'\"' b'I'\nb'adopted' b'you' b'Division' b'Mitchell' b'\"' b'.' b'In' b'his' b'lifetime' b'the' b'name' b'therapy' b'.' b'&lt;eos&gt;' b'&lt;eos&gt;' b'=' b'=' b'=' b'Selective' b'bodies'\nb'=' b'=' b'=' b'&lt;eos&gt;' b'&lt;eos&gt;' b'King' b'Meta' b'The' b'idea' b'that' b'year' b'\"' b'knew' b'it' b'a' b'tactical' b'one' b'root' b'wall' b'on'\nb'natural' b'or' b'&lt;unk&gt;' b',' b'wind' b',' b'and' b'grassy' b'cooking' b'words' b'that' b'were' b'&lt;unk&gt;' b'.' b'\"' b'&lt;eos&gt;' b'Because' b'they' b'go' b'a'\nb'same' b'situation' b'member' b'for' b'these' b'brood' b'depend' b'.' b'Its' b'pests' b'is' b'gradually' b'slowed' b'.' b'Tech' b'then' b'presents' b'it' b'back' b'as'\nb'it' b'appears' b'clicking' b'with' b'it' b',' b'breaking' b'him' b'to' b'make' b'pairs' b'of' b'community' b'corporal' b'patron' b'shape' b'of' b'clinical' b'mating' b'.'\nb'It' b'could' b'often' b'want' b'to' b'be' b'discovered' b'even' b'at' b',' b'though' b'there' b'are' b'harmonies' b'and' b'attaining' b'food' b',' b'most' b'lanthanides'\nb'be' b'won' b'that' b'bare' b'males' b'also' b'did' b'.' b'Although' b'their' b'diet' b'greatly' b'Various' b'sufficiently' b'on' b'another' b'hand' b'needs' b'alive' b','\nb'they' b'think' b'&lt;unk&gt;' b'and' b'sends' b'up' b'the' b'nominate' b'calls' b',' b'about' b'75' b'metres' b'(' b'22' b'to' b'9' b'@.@' b'4' b'mph'",
            "code"
        ],
        [
            "It\u2019s no GPT-2, but it looks like the model has started to learn the structure of\nlanguage!",
            "markdown"
        ],
        [
            "We\u2019re almost ready to demonstrate dynamic quantization. We just need to define a few more\nhelper functions:",
            "markdown"
        ],
        [
            "bptt = 25\n = ()\neval_batch_size = 1\n\n# create test data set\ndef batchify(data, bsz):\n    # Work out how cleanly we can divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    return data.view(bsz, -1).t().contiguous()\n\n = batchify(, eval_batch_size)\n\n# Evaluation functions\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].reshape(-1)\n    return data, target\n\ndef repackage_hidden(h):\n  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n\n  if isinstance(h, ):\n      return h.detach()\n  else:\n      return tuple(repackage_hidden(v) for v in h)\n\ndef evaluate(model_, data_source):\n    # Turn on evaluation mode which disables dropout.\n    model_.eval()\n    total_loss = 0.\n    hidden = model_.init_hidden(eval_batch_size)\n    with ():\n        for i in range(0, data_source.size(0) - 1, bptt):\n            data, targets = get_batch(data_source, i)\n            , hidden = model_(data, hidden)\n            hidden = repackage_hidden(hidden)\n            output_flat = .view(-1, ntokens)\n            total_loss += len(data) * (output_flat, targets).item()\n    return total_loss / (len(data_source) - 1)",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->4. Test dynamic quantization": [
        [
            "Finally, we can call torch.quantization.quantize_dynamic on the model!\nSpecifically,",
            "markdown"
        ],
        [
            "We specify that we want the nn.LSTM and nn.Linear modules in our\nmodel to be quantized",
            "markdown"
        ],
        [
            "We specify that we want weights to be converted to int8 values",
            "markdown"
        ],
        [
            "import torch.quantization\n\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {, }, dtype=\n)\nprint(quantized_model)",
            "code"
        ],
        [
            "LSTMModel(\n  (drop): Dropout(p=0.5, inplace=False)\n  (encoder): Embedding(33278, 512)\n  (rnn): DynamicQuantizedLSTM(512, 256, num_layers=5, dropout=0.5)\n  (decoder): DynamicQuantizedLinear(in_features=256, out_features=33278, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n)",
            "code"
        ],
        [
            "The model looks the same; how has this benefited us? First, we see a\nsignificant reduction in model size:",
            "markdown"
        ],
        [
            "def print_size_of_model(model):\n    ((), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model)",
            "code"
        ],
        [
            "Size (MB): 113.943637\nSize (MB): 79.738057",
            "code"
        ],
        [
            "Second, we see faster inference time, with no difference in evaluation loss:",
            "markdown"
        ],
        [
            "Note: we set the number of threads to one for single threaded comparison, since quantized\nmodels run single threaded.",
            "markdown"
        ],
        [
            "(1)\n\ndef time_model_evaluation(model, ):\n    s = time.time()\n    loss = evaluate(model, )\n    elapsed = time.time() - s\n    print('''loss: {0:.3f}\\nelapsed time (seconds): {1:.1f}'''.format(loss, elapsed))\n\ntime_model_evaluation(model, )\ntime_model_evaluation(quantized_model, )",
            "code"
        ],
        [
            "loss: 5.167\nelapsed time (seconds): 199.7\nloss: 5.168\nelapsed time (seconds): 115.0",
            "code"
        ],
        [
            "Running this locally on a MacBook Pro, without quantization, inference takes about 200 seconds,\nand with quantization it takes just about 100 seconds.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->Conclusion": [
        [
            "Dynamic quantization can be an easy way to reduce model size while only\nhaving a limited effect on accuracy.",
            "markdown"
        ],
        [
            "Thanks for reading! As always, we welcome any feedback, so please create an issue\n if you have any.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 5 minutes  23.575 seconds)",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT": [
        [
            "Tip",
            "markdown"
        ],
        [
            "To get the most of this tutorial, we suggest using this\n. This will allow you to experiment with the information presented below.",
            "markdown"
        ],
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "<strong>Reviewed by</strong>: ",
            "markdown"
        ],
        [
            "<strong>Edited by</strong>: ",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->Introduction": [
        [
            "In this tutorial, we will apply the dynamic quantization on a BERT\nmodel, closely following the BERT model from .\nWith this step-by-step journey, we would like to demonstrate how to\nconvert a well-known state-of-the-art model like BERT into dynamic\nquantized model.",
            "markdown"
        ],
        [
            "BERT, or Bidirectional Embedding Representations from Transformers,\nis a new method of pre-training language representations which\nachieves the state-of-the-art accuracy results on many popular\nNatural Language Processing (NLP) tasks, such as question answering,\ntext classification, and others. The original paper can be found\n.",
            "markdown"
        ],
        [
            "Dynamic quantization support in PyTorch converts a float model to a\nquantized model with static int8 or float16 data types for the\nweights and dynamic quantization for the activations. The activations\nare quantized dynamically (per batch) to int8 when the weights are\nquantized to int8. In PyTorch, we have ,\nwhich replaces specified modules with dynamic weight-only quantized\nversions and output the quantized model.",
            "markdown"
        ],
        [
            "We demonstrate the accuracy and inference performance results on the\n\nin the General Language Understanding Evaluation benchmark . The MRPC (Dolan and Brockett, 2005) is\na corpus of sentence pairs automatically extracted from online news\nsources, with human annotations of whether the sentences in the pair\nare semantically equivalent. As the classes are imbalanced (68%\npositive, 32% negative), we follow the common practice and report\n.\nMRPC is a common NLP task for language pair classification, as shown\nbelow.\n\n<img alt=\"../_images/bert.png\" src=\"../_images/bert.png\"/>",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->1. Setup->1.1 Install PyTorch and HuggingFace Transformers": [
        [
            "To start this tutorial, let\u2019s first follow the installation instructions\nin PyTorch  and HuggingFace Github Repo .\nIn addition, we also install  package, as we will reuse its\nbuilt-in F1 score calculation helper function.",
            "markdown"
        ],
        [
            "pip install sklearn\npip install transformers",
            "code"
        ],
        [
            "Because we will be using the beta parts of the PyTorch, it is\nrecommended to install the latest version of torch and torchvision. You\ncan find the most recent instructions on local installation . For example, to install on\nMac:",
            "markdown"
        ],
        [
            "yes y | pip uninstall torch tochvision\nyes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->1. Setup->1.2 Import the necessary modules": [
        [
            "In this step we import the necessary Python modules for the tutorial.",
            "markdown"
        ],
        [
            "from __future__ import absolute_import, division, print_function\n\nimport logging\nimport numpy as np\nimport os\nimport random\nimport sys\nimport time\nimport torch\n\nfrom argparse import Namespace\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom tqdm import tqdm\nfrom transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)\nfrom transformers import glue_compute_metrics as compute_metrics\nfrom transformers import glue_output_modes as output_modes\nfrom transformers import glue_processors as processors\nfrom transformers import glue_convert_examples_to_features as convert_examples_to_features\n\n# Setup logging\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                    datefmt = '%m/%d/%Y %H:%M:%S',\n                    level = logging.WARN)\n\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(\n   logging.WARN)  # Reduce logging\n\nprint(torch.__version__)",
            "code"
        ],
        [
            "We set the number of threads to compare the single thread performance between FP32 and INT8 performance.\nIn the end of the tutorial, the user can set other number of threads by building PyTorch with right parallel backend.",
            "markdown"
        ],
        [
            "torch.set_num_threads(1)\nprint(torch.__config__.parallel_info())",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->1. Setup->1.3 Learn about helper functions": [
        [
            "The helper functions are built-in in transformers library. We mainly use\nthe following helper functions: one for converting the text examples\ninto the feature vectors; The other one for measuring the F1 score of\nthe predicted result.",
            "markdown"
        ],
        [
            "The  function converts the texts into input features:",
            "markdown"
        ],
        [
            "Tokenize the input sequences;",
            "markdown"
        ],
        [
            "Insert [CLS] in the beginning;",
            "markdown"
        ],
        [
            "Insert [SEP] between the first sentence and the second sentence, and\nin the end;",
            "markdown"
        ],
        [
            "Generate token type ids to indicate whether a token belongs to the\nfirst sequence or the second sequence.",
            "markdown"
        ],
        [
            "The   function has the compute metrics with\nthe , which\ncan be interpreted as a weighted average of the precision and recall,\nwhere an F1 score reaches its best value at 1 and worst score at 0. The\nrelative contribution of precision and recall to the F1 score are equal.",
            "markdown"
        ],
        [
            "The equation for the F1 score is:\n\n\n\\[F1 = 2 * (\\text{precision} * \\text{recall}) / (\\text{precision} + \\text{recall})\n\n\\]",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->1. Setup->1.4 Download the dataset": [
        [
            "Before running MRPC tasks we download the  by running \nand unpack it to a directory glue_data.",
            "markdown"
        ],
        [
            "python download_glue_data.py --data_dir='glue_data' --tasks='MRPC'",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->2. Fine-tune the BERT model": [
        [
            "The spirit of BERT is to pre-train the language representations and then\nto fine-tune the deep bi-directional representations on a wide range of\ntasks with minimal task-dependent parameters, and achieves\nstate-of-the-art results. In this tutorial, we will focus on fine-tuning\nwith the pre-trained BERT model to classify semantically equivalent\nsentence pairs on MRPC task.",
            "markdown"
        ],
        [
            "To fine-tune the pre-trained BERT model (bert-base-uncased model in\nHuggingFace transformers) for the MRPC task, you can follow the command\nin :",
            "markdown"
        ],
        [
            "export GLUE_DIR=./glue_data\nexport TASK_NAME=MRPC\nexport OUT_DIR=./$TASK_NAME/\npython ./run_glue.py \\\n    --model_type bert \\\n    --model_name_or_path bert-base-uncased \\\n    --task_name $TASK_NAME \\\n    --do_train \\\n    --do_eval \\\n    --do_lower_case \\\n    --data_dir $GLUE_DIR/$TASK_NAME \\\n    --max_seq_length 128 \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 3.0 \\\n    --save_steps 100000 \\\n    --output_dir $OUT_DIR",
            "code"
        ],
        [
            "We provide the fined-tuned BERT model for MRPC task .\nTo save time, you can download the model file (~400 MB) directly into your local folder $OUT_DIR.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->2. Fine-tune the BERT model->2.1 Set global configurations": [
        [
            "Here we set the global configurations for evaluating the fine-tuned BERT\nmodel before and after the dynamic quantization.",
            "markdown"
        ],
        [
            "configs = Namespace()\n\n# The output directory for the fine-tuned model, $OUT_DIR.\nconfigs.output_dir = \"./MRPC/\"\n\n# The data directory for the MRPC task in the GLUE benchmark, $GLUE_DIR/$TASK_NAME.\nconfigs.data_dir = \"./glue_data/MRPC\"\n\n# The model name or path for the pre-trained model.\nconfigs.model_name_or_path = \"bert-base-uncased\"\n# The maximum length of an input sequence\nconfigs.max_seq_length = 128\n\n# Prepare GLUE task.\nconfigs.task_name = \"MRPC\".lower()\nconfigs.processor = processors[configs.task_name]()\nconfigs.output_mode = output_modes[configs.task_name]\nconfigs.label_list = configs.processor.get_labels()\nconfigs.model_type = \"bert\".lower()\nconfigs.do_lower_case = True\n\n# Set the device, batch size, topology, and caching flags.\nconfigs.device = \"cpu\"\nconfigs.per_gpu_eval_batch_size = 8\nconfigs.n_gpu = 0\nconfigs.local_rank = -1\nconfigs.overwrite_cache = False\n\n\n# Set random seed for reproducibility.\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\nset_seed(42)",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->2. Fine-tune the BERT model->2.2 Load the fine-tuned BERT model": [
        [
            "We load the tokenizer and fine-tuned BERT sequence classifier model\n(FP32) from the configs.output_dir.",
            "markdown"
        ],
        [
            "tokenizer = BertTokenizer.from_pretrained(\n    configs.output_dir, do_lower_case=configs.do_lower_case)\n\nmodel = BertForSequenceClassification.from_pretrained(configs.output_dir)\nmodel.to(configs.device)",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->2. Fine-tune the BERT model->2.3 Define the tokenize and evaluation function": [
        [
            "We reuse the tokenize and evaluation function from .",
            "markdown"
        ],
        [
            "# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ndef evaluate(args, model, tokenizer, prefix=\"\"):\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + '-MM') if args.task_name == \"mnli\" else (args.output_dir,)\n\n    results = {}\n    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        # Note that DistributedSampler samples randomly\n        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n        # multi-gpu eval\n        if args.n_gpu &gt; 1:\n            model = torch.nn.DataParallel(model)\n\n        # Eval!\n        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n        logger.info(\"  Num examples = %d\", len(eval_dataset))\n        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            model.eval()\n            batch = tuple(t.to(args.device) for t in batch)\n\n            with torch.no_grad():\n                inputs = {'input_ids':      batch[0],\n                          'attention_mask': batch[1],\n                          'labels':         batch[3]}\n                if args.model_type != 'distilbert':\n                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n                outputs = model(**inputs)\n                tmp_eval_loss, logits = outputs[:2]\n\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs['labels'].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n\n        eval_loss = eval_loss / nb_eval_steps\n        if args.output_mode == \"classification\":\n            preds = np.argmax(preds, axis=1)\n        elif args.output_mode == \"regression\":\n            preds = np.squeeze(preds)\n        result = compute_metrics(eval_task, preds, out_label_ids)\n        results.update(result)\n\n        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n        with open(output_eval_file, \"w\") as writer:\n            logger.info(\"***** Eval results {} *****\".format(prefix))\n            for key in sorted(result.keys()):\n                logger.info(\"  %s = %s\", key, str(result[key]))\n                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return results\n\n\ndef load_and_cache_examples(args, task, tokenizer, evaluate=False):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format(\n        'dev' if evaluate else 'train',\n        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n        str(args.max_seq_length),\n        str(task)))\n    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n        logger.info(\"Loading features from cached file %s\", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n        label_list = processor.get_labels()\n        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta']:\n            # HACK(label indices are swapped in RoBERTa pretrained model)\n            label_list[1], label_list[2] = label_list[2], label_list[1]\n        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n        features = convert_examples_to_features(examples,\n                                                tokenizer,\n                                                label_list=label_list,\n                                                max_length=args.max_seq_length,\n                                                output_mode=output_mode,\n                                                pad_on_left=bool(args.model_type in ['xlnet']),                 # pad on the left for xlnet\n                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n                                                pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0,\n        )\n        if args.local_rank in [-1, 0]:\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            torch.save(features, cached_features_file)\n\n    if args.local_rank == 0 and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    if output_mode == \"classification\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == \"regression\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n    return dataset",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->3. Apply the dynamic quantization": [
        [
            "We call torch.quantization.quantize_dynamic on the model to apply\nthe dynamic quantization on the HuggingFace BERT model. Specifically,",
            "markdown"
        ],
        [
            "We specify that we want the torch.nn.Linear modules in our model to\nbe quantized;",
            "markdown"
        ],
        [
            "We specify that we want weights to be converted to quantized int8\nvalues.",
            "markdown"
        ],
        [
            "quantized_model = torch.quantization.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8\n)\nprint(quantized_model)",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->3. Apply the dynamic quantization->3.1 Check the model size": [
        [
            "Let\u2019s first check the model size. We can observe a significant reduction\nin model size (FP32 total size: 438 MB; INT8 total size: 181 MB):",
            "markdown"
        ],
        [
            "def print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model)",
            "code"
        ],
        [
            "The BERT model used in this tutorial (bert-base-uncased) has a\nvocabulary size V of 30522. With the embedding size of 768, the total\nsize of the word embedding table is ~ 4 (Bytes/FP32) * 30522 * 768 =\n90 MB. So with the help of quantization, the model size of the\nnon-embedding table part is reduced from 350 MB (FP32 model) to 90 MB\n(INT8 model).",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->3. Apply the dynamic quantization->3.2 Evaluate the inference accuracy and time": [
        [
            "Next, let\u2019s compare the inference time as well as the evaluation\naccuracy between the original FP32 model and the INT8 model after the\ndynamic quantization.",
            "markdown"
        ],
        [
            "def time_model_evaluation(model, configs, tokenizer):\n    eval_start_time = time.time()\n    result = evaluate(configs, model, tokenizer, prefix=\"\")\n    eval_end_time = time.time()\n    eval_duration_time = eval_end_time - eval_start_time\n    print(result)\n    print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n\n# Evaluate the original FP32 BERT model\ntime_model_evaluation(model, configs, tokenizer)\n\n# Evaluate the INT8 BERT model after the dynamic quantization\ntime_model_evaluation(quantized_model, configs, tokenizer)",
            "code"
        ],
        [
            "Running this locally on a MacBook Pro, without quantization, inference\n(for all 408 examples in MRPC dataset) takes about 160 seconds, and with\nquantization it takes just about 90 seconds. We summarize the results\nfor running the quantized BERT model inference on a Macbook Pro as the\nfollows:",
            "markdown"
        ],
        [
            "| Prec | F1 score | Model Size | 1 thread | 4 threads |\n| FP32 |  0.9019  |   438 MB   | 160 sec  | 85 sec    |\n| INT8 |  0.902   |   181 MB   |  90 sec  | 46 sec    |",
            "code"
        ],
        [
            "We have 0.6% lower F1 score accuracy after applying the post-training dynamic\nquantization on the fine-tuned BERT model on the MRPC task. As a\ncomparison, in a  (Table 1),\nit achieved 0.8788 by\napplying the post-training dynamic quantization and 0.8956 by applying\nthe quantization-aware training. The main difference is that we support the\nasymmetric quantization in PyTorch while that paper supports the\nsymmetric quantization only.",
            "markdown"
        ],
        [
            "Note that we set the number of threads to 1 for the single-thread\ncomparison in this tutorial. We also support the intra-op\nparallelization for these quantized INT8 operators. The users can now\nset multi-thread by torch.set_num_threads(N) (N is the number of\nintra-op parallelization threads). One preliminary requirement to enable\nthe intra-op parallelization support is to build PyTorch with the right\n\nsuch as OpenMP, Native or TBB.\nYou can use torch.__config__.parallel_info() to check the\nparallelization settings. On the same MacBook Pro using PyTorch with\nNative backend for parallelization, we can get about 46 seconds for\nprocessing the evaluation of MRPC dataset.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->3. Apply the dynamic quantization->3.3 Serialize the quantized model": [
        [
            "We can serialize and save the quantized model for the future use using\n<cite>torch.jit.save</cite> after tracing the model.",
            "markdown"
        ],
        [
            "input_ids = ids_tensor([8, 128], 2)\ntoken_type_ids = ids_tensor([8, 128], 2)\nattention_mask = ids_tensor([8, 128], vocab_size=2)\ndummy_input = (input_ids, attention_mask, token_type_ids)\ntraced_model = torch.jit.trace(quantized_model, dummy_input)\ntorch.jit.save(traced_model, \"bert_traced_eager_quant.pt\")",
            "code"
        ],
        [
            "To load the quantized model, we can use <cite>torch.jit.load</cite>",
            "markdown"
        ],
        [
            "loaded_quantized_model = torch.jit.load(\"bert_traced_eager_quant.pt\")",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->Conclusion": [
        [
            "In this tutorial, we demonstrated how to convert a\nwell-known state-of-the-art NLP model like BERT into dynamic quantized\nmodel. Dynamic quantization can reduce the size of the model while only\nhaving a limited implication on accuracy.",
            "markdown"
        ],
        [
            "Thanks for reading! As always, we welcome any feedback, so please create\nan issue  if you have\nany.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Dynamic Quantization on BERT->References": [
        [
            "[1] J.Devlin, M. Chang, K. Lee and K. Toutanova, .",
            "markdown"
        ],
        [
            "[2] .",
            "markdown"
        ],
        [
            "[3] O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat (2019). .",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial": [
        [
            "Tip",
            "markdown"
        ],
        [
            "To get the most of this tutorial, we suggest using this\n.\nThis will allow you to experiment with the information presented below.",
            "markdown"
        ],
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "<strong>Reviewed by</strong>: ",
            "markdown"
        ],
        [
            "<strong>Edited by</strong>: ",
            "markdown"
        ],
        [
            "This tutorial builds on the original \ntutorial, written by .",
            "markdown"
        ],
        [
            "Transfer learning refers to techniques that make use of a pretrained model for\napplication on a different data-set.\nThere are two main ways the transfer learning is used:",
            "markdown"
        ],
        [
            "<strong>ConvNet as a fixed feature extractor</strong>: Here, you \nthe weights of all the parameters in the network except that of the final\nseveral layers (aka \u201cthe head\u201d, usually fully connected layers).\nThese last layers are replaced with new ones initialized with random\nweights and only these layers are trained.",
            "markdown"
        ],
        [
            "<strong>Finetuning the ConvNet</strong>: Instead of random initializaion, the model is\ninitialized using a pretrained network, after which the training proceeds as\nusual but with a different dataset.\nUsually the head (or part of it) is also replaced in the network in\ncase there is a different number of outputs.\nIt is common in this method to set the learning rate to a smaller number.\nThis is done because the network is already trained, and only minor changes\nare required to \u201cfinetune\u201d it to a new dataset.",
            "markdown"
        ],
        [
            "You can also combine the above two methods:\nFirst you can freeze the feature extractor, and train the head. After\nthat, you can unfreeze the feature extractor (or part of it), set the\nlearning rate to something smaller, and continue training.",
            "markdown"
        ],
        [
            "In this part you will use the first method \u2013 extracting the features\nusing a quantized model.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites": [
        [
            "Before diving into the transfer learning, let us review the \u201cprerequisites\u201d,\nsuch as installations and data loading/visualizations.",
            "markdown"
        ],
        [
            "# Imports\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\nplt.ion()",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites->Installing the Nightly Build": [
        [
            "Because you will be using the beta parts of the PyTorch, it is\nrecommended to install the latest version of torch and\ntorchvision. You can find the most recent instructions on local\ninstallation .\nFor example, to install without GPU support:",
            "markdown"
        ],
        [
            "pip install numpy\npip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n# For CUDA support use https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites->Load Data": [
        [
            "Note",
            "markdown"
        ],
        [
            "This section is identical to the original transfer learning tutorial.",
            "markdown"
        ],
        [
            "We will use torchvision and torch.utils.data packages to load\nthe data.",
            "markdown"
        ],
        [
            "The problem you are going to solve today is classifying <strong>ants</strong> and\n<strong>bees</strong> from images. The dataset contains about 120 training images\neach for ants and bees. There are 75 validation images for each class.\nThis is considered a very small dataset to generalize on. However, since\nwe are using transfer learning, we should be able to generalize\nreasonably well.",
            "markdown"
        ],
        [
            "<em>This dataset is a very small subset of imagenet.</em>",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Download the data from \nand extract it to the data directory.",
            "markdown"
        ],
        [
            "import torch\nfrom torchvision import transforms, datasets\n\n# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize(224),\n        transforms.RandomCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n                                              shuffle=True, num_workers=8)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites->Visualize a few images": [
        [
            "Let\u2019s visualize a few training images so as to understand the data\naugmentations.",
            "markdown"
        ],
        [
            "import torchvision\n\ndef imshow(inp, title=None, ax=None, figsize=(5, 5)):\n  \"\"\"Imshow for Tensor.\"\"\"\n  inp = inp.numpy().transpose((1, 2, 0))\n  mean = np.array([0.485, 0.456, 0.406])\n  std = np.array([0.229, 0.224, 0.225])\n  inp = std * inp + mean\n  inp = np.clip(inp, 0, 1)\n  if ax is None:\n    fig, ax = plt.subplots(1, figsize=figsize)\n  ax.imshow(inp)\n  ax.set_xticks([])\n  ax.set_yticks([])\n  if title is not None:\n    ax.set_title(title)\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs, nrow=4)\n\nfig, ax = plt.subplots(1, figsize=(10, 10))\nimshow(out, title=[class_names[x] for x in classes], ax=ax)",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites->Support Function for Model Training": [
        [
            "Below is a generic function for model training.\nThis function also",
            "markdown"
        ],
        [
            "Schedules the learning rate",
            "markdown"
        ],
        [
            "Saves the best model",
            "markdown"
        ],
        [
            "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):\n  \"\"\"\n  Support function for model training.\n\n  Args:\n    model: Model to be trained\n    criterion: Optimization criterion (loss)\n    optimizer: Optimizer to use for training\n    scheduler: Instance of ``torch.optim.lr_scheduler``\n    num_epochs: Number of epochs\n    device: Device to run the training on. Must be 'cpu' or 'cuda'\n  \"\"\"\n  since = time.time()\n\n  best_model_wts = copy.deepcopy(model.state_dict())\n  best_acc = 0.0\n\n  for epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n\n    # Each epoch has a training and validation phase\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()  # Set model to training mode\n      else:\n        model.eval()   # Set model to evaluate mode\n\n      running_loss = 0.0\n      running_corrects = 0\n\n      # Iterate over data.\n      for inputs, labels in dataloaders[phase]:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward\n        # track history if only in train\n        with torch.set_grad_enabled(phase == 'train'):\n          outputs = model(inputs)\n          _, preds = torch.max(outputs, 1)\n          loss = criterion(outputs, labels)\n\n          # backward + optimize only if in training phase\n          if phase == 'train':\n            loss.backward()\n            optimizer.step()\n\n        # statistics\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n      if phase == 'train':\n        scheduler.step()\n\n      epoch_loss = running_loss / dataset_sizes[phase]\n      epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n      print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n        phase, epoch_loss, epoch_acc))\n\n      # deep copy the model\n      if phase == 'val' and epoch_acc &gt; best_acc:\n        best_acc = epoch_acc\n        best_model_wts = copy.deepcopy(model.state_dict())\n\n    print()\n\n  time_elapsed = time.time() - since\n  print('Training complete in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))\n  print('Best val Acc: {:4f}'.format(best_acc))\n\n  # load best model weights\n  model.load_state_dict(best_model_wts)\n  return model",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites->Support Function for Visualizing the Model Predictions": [
        [
            "Generic function to display predictions for a few images",
            "markdown"
        ],
        [
            "def visualize_model(model, rows=3, cols=3):\n  was_training = model.training\n  model.eval()\n  current_row = current_col = 0\n  fig, ax = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n\n  with torch.no_grad():\n    for idx, (imgs, lbls) in enumerate(dataloaders['val']):\n      imgs = imgs.cpu()\n      lbls = lbls.cpu()\n\n      outputs = model(imgs)\n      _, preds = torch.max(outputs, 1)\n\n      for jdx in range(imgs.size()[0]):\n        imshow(imgs.data[jdx], ax=ax[current_row, current_col])\n        ax[current_row, current_col].axis('off')\n        ax[current_row, current_col].set_title('predicted: {}'.format(class_names[preds[jdx]]))\n\n        current_col += 1\n        if current_col &gt;= cols:\n          current_row += 1\n          current_col = 0\n        if current_row &gt;= rows:\n          model.train(mode=was_training)\n          return\n    model.train(mode=was_training)",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 1. Training a Custom Classifier based on a Quantized Feature Extractor": [
        [
            "In this section you will use a \u201cfrozen\u201d quantized feature extractor, and\ntrain a custom classifier head on top of it. Unlike floating point\nmodels, you don\u2019t need to set requires_grad=False for the quantized\nmodel, as it has no trainable parameters. Please, refer to the\n for\nmore details.",
            "markdown"
        ],
        [
            "Load a pretrained model: for this exercise you will be using\n.",
            "markdown"
        ],
        [
            "import torchvision.models.quantization as models\n\n# You will need the number of filters in the `fc` for future use.\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\nmodel_fe = models.resnet18(pretrained=True, progress=True, quantize=True)\nnum_ftrs = model_fe.fc.in_features",
            "code"
        ],
        [
            "At this point you need to modify the pretrained model. The model\nhas the quantize/dequantize blocks in the beginning and the end. However,\nbecause you will only use the feature extractor, the dequantization layer has\nto move right before the linear layer (the head). The easiest way to do that\nis to wrap the model in the nn.Sequential module.",
            "markdown"
        ],
        [
            "The first step is to isolate the feature extractor in the ResNet\nmodel. Although in this example you are tasked to use all layers except\nfc as the feature extractor, in reality, you can take as many parts\nas you need. This would be useful in case you would like to replace some\nof the convolutional layers as well.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "When separating the feature extractor from the rest of a quantized\nmodel, you have to manually place the quantizer/dequantized in the\nbeginning and the end of the parts you want to keep quantized.",
            "markdown"
        ],
        [
            "The function below creates a model with a custom head.",
            "markdown"
        ],
        [
            "from torch import nn\n\ndef create_combined_model(model_fe):\n  # Step 1. Isolate the feature extractor.\n  model_fe_features = nn.Sequential(\n    model_fe.quant,  # Quantize the input\n    model_fe.conv1,\n    model_fe.bn1,\n    model_fe.relu,\n    model_fe.maxpool,\n    model_fe.layer1,\n    model_fe.layer2,\n    model_fe.layer3,\n    model_fe.layer4,\n    model_fe.avgpool,\n    model_fe.dequant,  # Dequantize the output\n  )\n\n  # Step 2. Create a new \"head\"\n  new_head = nn.Sequential(\n    nn.Dropout(p=0.5),\n    nn.Linear(num_ftrs, 2),\n  )\n\n  # Step 3. Combine, and don't forget the quant stubs.\n  new_model = nn.Sequential(\n    model_fe_features,\n    nn.Flatten(1),\n    new_head,\n  )\n  return new_model",
            "code"
        ],
        [
            "Warning",
            "markdown"
        ],
        [
            "Currently the quantized models can only be run on CPU.\nHowever, it is possible to send the non-quantized parts of the model to a GPU.",
            "markdown"
        ],
        [
            "import torch.optim as optim\nnew_model = create_combined_model(model_fe)\nnew_model = new_model.to('cpu')\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are only training the head.\noptimizer_ft = optim.SGD(new_model.parameters(), lr=0.01, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 1. Training a Custom Classifier based on a Quantized Feature Extractor->Train and evaluate": [
        [
            "This step takes around 15-25 min on CPU. Because the quantized model can\nonly run on the CPU, you cannot run the training on GPU.",
            "markdown"
        ],
        [
            "new_model = train_model(new_model, criterion, optimizer_ft, exp_lr_scheduler,\n                        num_epochs=25, device='cpu')\n\nvisualize_model(new_model)\nplt.tight_layout()",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model": [
        [
            "In this part, we fine tune the feature extractor used for transfer\nlearning, and quantize the feature extractor. Note that in both part 1\nand 2, the feature extractor is quantized. The difference is that in\npart 1, we use a pretrained quantized model. In this part, we create a\nquantized feature extractor after fine tuning on the data-set of\ninterest, so this is a way to get better accuracy with transfer learning\nwhile having the benefits of quantization. Note that in our specific\nexample, the training set is really small (120 images) so the benefits\nof fine tuning the entire model is not apparent. However, the procedure\nshown here will improve accuracy for transfer learning with larger\ndatasets.",
            "markdown"
        ],
        [
            "The pretrained feature extractor must be quantizable.\nTo make sure it is quantizable, perform the following steps:\n<blockquote>",
            "markdown"
        ],
        [
            "Fuse (Conv, BN, ReLU), (Conv, BN), and (Conv, ReLU) using\ntorch.quantization.fuse_modules.",
            "markdown"
        ],
        [
            "Connect the feature extractor with a custom head.\nThis requires dequantizing the output of the feature extractor.",
            "markdown"
        ],
        [
            "Insert fake-quantization modules at appropriate locations\nin the feature extractor to mimic quantization during training.\n\n</blockquote>",
            "markdown"
        ],
        [
            "For step (1), we use models from torchvision/models/quantization, which\nhave a member method fuse_model. This function fuses all the conv,\nbn, and relu modules. For custom models, this would require calling\nthe torch.quantization.fuse_modules API with the list of modules to fuse\nmanually.",
            "markdown"
        ],
        [
            "Step (2) is performed by the create_combined_model function\nused in the previous section.",
            "markdown"
        ],
        [
            "Step (3) is achieved by using torch.quantization.prepare_qat, which\ninserts fake-quantization modules.",
            "markdown"
        ],
        [
            "As step (4), you can start \u201cfinetuning\u201d the model, and after that convert\nit to a fully quantized version (Step 5).",
            "markdown"
        ],
        [
            "To convert the fine tuned model into a quantized model you can call the\ntorch.quantization.convert function (in our case only\nthe feature extractor is quantized).",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Because of the random initialization your results might differ from\nthe results shown in this tutorial.",
            "markdown"
        ],
        [
            "# notice `quantize=False`\nmodel = models.resnet18(pretrained=True, progress=True, quantize=False)\nnum_ftrs = model.fc.in_features\n\n# Step 1\nmodel.train()\nmodel.fuse_model()\n# Step 2\nmodel_ft = create_combined_model(model)\nmodel_ft[0].qconfig = torch.quantization.default_qat_qconfig  # Use default QAT configuration\n# Step 3\nmodel_ft = torch.quantization.prepare_qat(model_ft, inplace=True)",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model->Finetuning the model": [
        [
            "In the current tutorial the whole model is fine tuned. In\ngeneral, this will lead to higher accuracy. However, due to the small\ntraining set used here, we end up overfitting to the training set.",
            "markdown"
        ],
        [
            "Step 4. Fine tune the model",
            "markdown"
        ],
        [
            "for param in model_ft.parameters():\n  param.requires_grad = True\n\nmodel_ft.to(device)  # We can fine-tune on GPU if available\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are training everything, so the learning rate is lower\n# Notice the smaller learning rate\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.3 every several epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n\nmodel_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                             num_epochs=25, device=device)",
            "code"
        ],
        [
            "Step 5. Convert to quantized model",
            "markdown"
        ],
        [
            "from torch.quantization import convert\nmodel_ft_tuned.cpu()\n\nmodel_quantized_and_trained = convert(model_ft_tuned, inplace=False)",
            "code"
        ],
        [
            "Lets see how the quantized model performs on a few images",
            "markdown"
        ],
        [
            "visualize_model(model_quantized_and_trained)\n\nplt.ioff()\nplt.tight_layout()\nplt.show()",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch": [
        [
            "<strong>Author</strong>: \n<strong>Edited by</strong>: , ",
            "markdown"
        ],
        [
            "This tutorial shows how to do post-training static quantization, as well as illustrating\ntwo more advanced techniques - per-channel quantization and quantization-aware training -\nto further improve the model\u2019s accuracy. Note that quantization is currently only supported\nfor CPUs, so we will not be utilizing GPUs / CUDA in this tutorial.\nBy the end of this tutorial, you will see how quantization in PyTorch can result in\nsignificant decreases in model size while increasing speed. Furthermore, you\u2019ll see how\nto easily apply some advanced quantization techniques shown\n so that your quantized models take much less\nof an accuracy hit than they would otherwise.\nWarning: we use a lot of boilerplate code from other PyTorch repos to, for example,\ndefine the MobileNetV2 model architecture, define data loaders, and so on. We of course\nencourage you to read it; but if you want to get to the quantization features, feel free\nto skip to the \u201c4. Post-training static quantization\u201d section.\nWe\u2019ll start by doing the necessary imports:",
            "markdown"
        ],
        [
            "import os\nimport sys\nimport time\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\n# Set up warnings\nimport warnings\nwarnings.filterwarnings(\n    action='ignore',\n    category=DeprecationWarning,\n    module=r'.*'\n)\nwarnings.filterwarnings(\n    action='default',\n    module=r'torch.ao.quantization'\n)\n\n# Specify random seed for repeatable results\ntorch.manual_seed(191009)",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->1. Model architecture": [
        [
            "We first define the MobileNetV2 model architecture, with several notable modifications\nto enable quantization:",
            "markdown"
        ],
        [
            "Replacing addition with nn.quantized.FloatFunctional",
            "markdown"
        ],
        [
            "Insert QuantStub and DeQuantStub at the beginning and end of the network.",
            "markdown"
        ],
        [
            "Replace ReLU6 with ReLU",
            "markdown"
        ],
        [
            "Note: this code is taken from\n.",
            "markdown"
        ],
        [
            "from torch.ao.quantization import QuantStub, DeQuantStub\n\ndef _make_divisible(v, divisor, min_value=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:\n    :return:\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v &lt; 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n        padding = (kernel_size - 1) // 2\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n            nn.BatchNorm2d(out_planes, momentum=0.1),\n            # Replace with ReLU\n            nn.ReLU(inplace=False)\n        )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n        layers.extend([\n            # dw\n            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n            # pw-linear\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup, momentum=0.1),\n        ])\n        self.conv = nn.Sequential(*layers)\n        # Replace torch.add with floatfunctional\n        self.skip_add = nn.quantized.FloatFunctional()\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return self.skip_add.add(x, self.conv(x))\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n        \"\"\"\n        MobileNet V2 main class\n        Args:\n            num_classes (int): Number of classes\n            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n            inverted_residual_setting: Network structure\n            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n            Set to 1 to turn off rounding\n        \"\"\"\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n\n        if inverted_residual_setting is None:\n            inverted_residual_setting = [\n                # t, c, n, s\n                [1, 16, 1, 1],\n                [6, 24, 2, 2],\n                [6, 32, 3, 2],\n                [6, 64, 4, 2],\n                [6, 96, 3, 1],\n                [6, 160, 3, 2],\n                [6, 320, 1, 1],\n            ]\n\n        # only check the first element, assuming user knows t,c,n,s are required\n        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n            raise ValueError(\"inverted_residual_setting should be non-empty \"\n                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n\n        # building first layer\n        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n        features = [ConvBNReLU(3, input_channel, stride=2)]\n        # building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c * width_mult, round_nearest)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*features)\n        self.quant = QuantStub()\n        self.dequant = DeQuantStub()\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, num_classes),\n        )\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.features(x)\n        x = x.mean([2, 3])\n        x = self.classifier(x)\n        x = self.dequant(x)\n        return x\n\n    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization\n    # This operation does not change the numerics\n    def fuse_model(self):\n        for m in self.modules():\n            if type(m) == ConvBNReLU:\n                torch.ao.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n            if type(m) == InvertedResidual:\n                for idx in range(len(m.conv)):\n                    if type(m.conv[idx]) == nn.Conv2d:\n                        torch.ao.quantization.fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->2. Helper functions": [
        [
            "We next define several helper functions to help with model evaluation. These mostly come from\n.",
            "markdown"
        ],
        [
            "class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef evaluate(model, criterion, data_loader, neval_batches):\n    model.eval()\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    cnt = 0\n    with torch.no_grad():\n        for image, target in data_loader:\n            output = model(image)\n            loss = criterion(output, target)\n            cnt += 1\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            print('.', end = '')\n            top1.update(acc1[0], image.size(0))\n            top5.update(acc5[0], image.size(0))\n            if cnt &gt;= neval_batches:\n                 return top1, top5\n\n    return top1, top5\n\ndef load_model(model_file):\n    model = MobileNetV2()\n    state_dict = torch.load(model_file)\n    model.load_state_dict(state_dict)\n    model.to('cpu')\n    return model\n\ndef print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')",
            "code"
        ]
    ],
    "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders": [
        [
            "As our last major setup step, we define our dataloaders for our training and testing set.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders->ImageNet Data": [
        [
            "To run the code in this tutorial using the entire ImageNet dataset, first download imagenet by following the instructions at here . Unzip the downloaded file into the \u2018data_path\u2019 folder.",
            "markdown"
        ],
        [
            "With the data downloaded, we show functions below that define dataloaders we\u2019ll use to read\nin this data. These functions mostly come from\n.",
            "markdown"
        ],
        [
            "def prepare_data_loaders(data_path):\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    dataset = torchvision.datasets.ImageNet(\n        data_path, split=\"train\", transform=transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n    dataset_test = torchvision.datasets.ImageNet(\n        data_path, split=\"val\", transform=transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    train_sampler = torch.utils.data.RandomSampler(dataset)\n    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=train_batch_size,\n        sampler=train_sampler)\n\n    data_loader_test = torch.utils.data.DataLoader(\n        dataset_test, batch_size=eval_batch_size,\n        sampler=test_sampler)\n\n    return data_loader, data_loader_test",
            "code"
        ],
        [
            "Next, we\u2019ll load in the pre-trained MobileNetV2 model. We provide the URL to download the model\n.",
            "markdown"
        ],
        [
            "data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
            "code"
        ],
        [
            "Finally to get a \u201cbaseline\u201d accuracy, let\u2019s see the accuracy of our un-quantized model\nwith fused modules",
            "markdown"
        ],
        [
            "num_eval_batches = 1000\n\nprint(\"Size of baseline model\")\nprint_size_of_model(float_model)\n\ntop1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)",
            "code"
        ],
        [
            "On the entire model, we get an accuracy of 71.9% on the eval dataset of 50,000 images.",
            "markdown"
        ],
        [
            "This will be our baseline to compare to. Next, let\u2019s try different quantization methods",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->4. Post-training static quantization": [
        [
            "Post-training static quantization involves not just converting the weights from float to int,\nas in dynamic quantization, but also performing the additional step of first feeding batches\nof data through the network and computing the resulting distributions of the different activations\n(specifically, this is done by inserting <cite>observer</cite> modules at different points that record this\ndata). These distributions are then used to determine how the specifically the different activations\nshould be quantized at inference time (a simple technique would be to simply divide the entire range\nof activations into 256 levels, but we support more sophisticated methods as well). Importantly,\nthis additional step allows us to pass quantized values between operations instead of converting these\nvalues to floats - and then back to ints - between every operation, resulting in a significant speed-up.",
            "markdown"
        ],
        [
            "num_calibration_batches = 32\n\nmyModel = load_model(saved_model_dir + float_model_file).to('cpu')\nmyModel.eval()\n\n# Fuse Conv, bn and relu\nmyModel.fuse_model()\n\n# Specify quantization configuration\n# Start with simple min/max range estimation and per-tensor quantization of weights\nmyModel.qconfig = torch.ao.quantization.default_qconfig\nprint(myModel.qconfig)\ntorch.ao.quantization.prepare(myModel, inplace=True)\n\n# Calibrate first\nprint('Post Training Quantization Prepare: Inserting Observers')\nprint('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1].conv)\n\n# Calibrate with the training set\nevaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\nprint('Post Training Quantization: Calibration done')\n\n# Convert to quantized model\ntorch.ao.quantization.convert(myModel, inplace=True)\nprint('Post Training Quantization: Convert done')\nprint('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel.features[1].conv)\n\nprint(\"Size of model after quantization\")\nprint_size_of_model(myModel)\n\ntop1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))",
            "code"
        ],
        [
            "For this quantized model, we see an accuracy of 56.7% on the eval dataset. This is because we used a simple min/max observer to determine quantization parameters. Nevertheless, we did reduce the size of our model down to just under 3.6 MB, almost a 4x decrease.",
            "markdown"
        ],
        [
            "In addition, we can significantly improve on the accuracy simply by using a different\nquantization configuration. We repeat the same exercise with the recommended configuration for\nquantizing for x86 architectures. This configuration does the following:",
            "markdown"
        ],
        [
            "Quantizes weights on a per-channel basis",
            "markdown"
        ],
        [
            "Uses a histogram observer that collects a histogram of activations and then picks\nquantization parameters in an optimal manner.",
            "markdown"
        ],
        [
            "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\nper_channel_quantized_model.eval()\nper_channel_quantized_model.fuse_model()\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nper_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\nprint(per_channel_quantized_model.qconfig)\n\ntorch.ao.quantization.prepare(per_channel_quantized_model, inplace=True)\nevaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches)\ntorch.ao.quantization.convert(per_channel_quantized_model, inplace=True)\ntop1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)",
            "code"
        ],
        [
            "Changing just this quantization configuration method resulted in an increase\nof the accuracy to over 67.3%! Still, this is 4% worse than the baseline of 71.9% achieved above.\nSo lets try quantization aware training.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->5. Quantization-aware training": [
        [
            "Quantization-aware training (QAT) is the quantization method that typically results in the highest accuracy.\nWith QAT, all weights and activations are \u201cfake quantized\u201d during both the forward and backward passes of\ntraining: that is, float values are rounded to mimic int8 values, but all computations are still done with\nfloating point numbers. Thus, all the weight adjustments during training are made while \u201caware\u201d of the fact\nthat the model will ultimately be quantized; after quantizing, therefore, this method will usually yield\nhigher accuracy than either dynamic quantization or post-training static quantization.",
            "markdown"
        ],
        [
            "The overall workflow for actually performing QAT is very similar to before:",
            "markdown"
        ],
        [
            "We can use the same model as before: there is no additional preparation needed for quantization-aware\ntraining.",
            "markdown"
        ],
        [
            "We need to use a qconfig specifying what kind of fake-quantization is to be inserted after weights\nand activations, instead of specifying observers",
            "markdown"
        ],
        [
            "We first define a training function:",
            "markdown"
        ],
        [
            "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n    model.train()\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    avgloss = AverageMeter('Loss', '1.5f')\n\n    cnt = 0\n    for image, target in data_loader:\n        start_time = time.time()\n        print('.', end = '')\n        cnt += 1\n        image, target = image.to(device), target.to(device)\n        output = model(image)\n        loss = criterion(output, target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n        top1.update(acc1[0], image.size(0))\n        top5.update(acc5[0], image.size(0))\n        avgloss.update(loss, image.size(0))\n        if cnt &gt;= ntrain_batches:\n            print('Loss', avgloss.avg)\n\n            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n                  .format(top1=top1, top5=top5))\n            return\n\n    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n          .format(top1=top1, top5=top5))\n    return",
            "code"
        ],
        [
            "We fuse modules as before",
            "markdown"
        ],
        [
            "qat_model = load_model(saved_model_dir + float_model_file)\nqat_model.fuse_model()\n\noptimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nqat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')",
            "code"
        ],
        [
            "Finally, prepare_qat performs the \u201cfake quantization\u201d, preparing the model for quantization-aware training",
            "markdown"
        ],
        [
            "torch.ao.quantization.prepare_qat(qat_model, inplace=True)\nprint('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv)",
            "code"
        ],
        [
            "Training a quantized model with high accuracy requires accurate modeling of numerics at\ninference. For quantization aware training, therefore, we modify the training loop by:",
            "markdown"
        ],
        [
            "Switch batch norm to use running mean and variance towards the end of training to better\nmatch inference numerics.",
            "markdown"
        ],
        [
            "We also freeze the quantizer parameters (scale and zero-point) and fine tune the weights.",
            "markdown"
        ],
        [
            "num_train_batches = 20\n\n# QAT takes time and one needs to train over a few epochs.\n# Train and check accuracy after each epoch\nfor nepoch in range(8):\n    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches)\n    if nepoch &gt; 3:\n        # Freeze quantizer parameters\n        qat_model.apply(torch.ao.quantization.disable_observer)\n    if nepoch &gt; 2:\n        # Freeze batch norm mean and variance estimates\n        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n\n    # Check the accuracy after each epoch\n    quantized_model = torch.ao.quantization.convert(qat_model.eval(), inplace=False)\n    quantized_model.eval()\n    top1, top5 = evaluate(quantized_model,criterion, data_loader_test, neval_batches=num_eval_batches)\n    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))",
            "code"
        ],
        [
            "Quantization-aware training yields an accuracy of over 71.5% on the entire imagenet dataset, which is close to the floating point accuracy of 71.9%.",
            "markdown"
        ],
        [
            "More on quantization-aware training:",
            "markdown"
        ],
        [
            "QAT is a super-set of post training quant techniques that allows for more debugging.\nFor example, we can analyze if the accuracy of the model is limited by weight or activation\nquantization.",
            "markdown"
        ],
        [
            "We can also simulate the accuracy of a quantized model in floating point since\nwe are using fake-quantization to model the numerics of actual quantized arithmetic.",
            "markdown"
        ],
        [
            "We can mimic post training quantization easily too.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->5. Quantization-aware training->Speedup from quantization": [
        [
            "Finally, let\u2019s confirm something we alluded to above: do our quantized models actually perform inference\nfaster? Let\u2019s test:",
            "markdown"
        ],
        [
            "def run_benchmark(model_file, img_loader):\n    elapsed = 0\n    model = torch.jit.load(model_file)\n    model.eval()\n    num_batches = 5\n    # Run the scripted model on a few batches of images\n    for i, (images, target) in enumerate(img_loader):\n        if i &lt; num_batches:\n            start = time.time()\n            output = model(images)\n            end = time.time()\n            elapsed = elapsed + (end-start)\n        else:\n            break\n    num_images = images.size()[0] * num_batches\n\n    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n    return elapsed\n\nrun_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)\n\nrun_benchmark(saved_model_dir + scripted_quantized_model_file, data_loader_test)",
            "code"
        ],
        [
            "Running this locally on a MacBook pro yielded 61 ms for the regular model, and\njust 20 ms for the quantized model, illustrating the typical 2-4x speedup\nwe see for quantized models compared to floating point ones.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->Conclusion": [
        [
            "In this tutorial, we showed two quantization methods - post-training static quantization,\nand quantization-aware training - describing what they do \u201cunder the hood\u201d and how to use\nthem in PyTorch.",
            "markdown"
        ],
        [
            "Thanks for reading! As always, we welcome any feedback, so please create an issue\n if you have any.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles": [
        [
            "A case study on the TorchServe inference framework optimized with .",
            "markdown"
        ],
        [
            "Authors: Min Jean Cho, Mark Saroufim",
            "markdown"
        ],
        [
            "Reviewers: Ashok Emani, Jiong Gong",
            "markdown"
        ],
        [
            "Getting a strong out-of-box performance for deep learning on CPUs can be tricky but it\u2019s much easier if you\u2019re aware of the main problems that affect performance, how to measure them and how to solve them.",
            "markdown"
        ],
        [
            "TL;DR",
            "markdown"
        ],
        [
            "<em>GEMM (General Matrix Multiply)</em> run on fused-multiply-add (FMA) or dot-product (DP) execution units which will be bottlenecked and cause delays in thread waiting/<em>spinning at synchronization</em> barrier when <em>hyperthreading</em> is enabled - because using logical cores causes insufficient concurrency for all working threads as each logical thread <em>contends for the same core resources</em>. Instead, if we use 1 thread per physical core, we avoid this contention. So we generally recommend <em>avoiding logical cores</em> by setting CPU <em>thread affinity</em> to physical cores via <em>core pinning</em>.",
            "markdown"
        ],
        [
            "Multi-socket systems have <em>Non-Uniform Memory Access (NUMA)</em> which is a shared memory architecture that describes the placement of main memory modules with respect to processors. But if a process is not NUMA-aware, slow <em>remote memory</em> is frequently accessed when <em>threads migrate</em> cross socket via <em>Intel Ultra Path Interconnect (UPI)</em> during run time. We address this problem by setting CPU <em>thread affinity</em> to a specific socket via <em>core pinning</em>.",
            "markdown"
        ],
        [
            "Knowing these principles in mind, proper CPU runtime configuration can significantly boost out-of-box performance.",
            "markdown"
        ],
        [
            "In this blog, we\u2019ll walk you through the important runtime configurations you should be aware of from , explain how they work, how to profile them and how to integrate them within a model serving framework like  via an easy to use  which we\u2019ve  <sup>1</sup> natively.",
            "markdown"
        ],
        [
            "We\u2019ll explain all of these ideas <strong>visually</strong> from <strong>first principles</strong> with lots of <strong>profiles</strong> and show you how we applied our learnings to make out of the box CPU performance on TorchServe better.",
            "markdown"
        ],
        [
            "The feature has to be explicitly enabled by setting <em>cpu_launcher_enable=true</em> in <em>config.properties</em>.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles->Avoid logical cores for deep learning": [
        [
            "Avoiding logical cores for deep learning workloads generally improves performance. To understand this, let us take a step back to GEMM.",
            "markdown"
        ],
        [
            "<strong>Optimizing GEMM optimizes deep learning</strong>",
            "markdown"
        ],
        [
            "The majority of time in deep learning training or inference is spent on millions of repeated operations of GEMM which is at the core of fully connected layers. Fully connected layers have been used for decades since multi-layer perceptrons (MLP) . Any MLP can be entirely represented as GEMM. And even a convolution can be represented as a GEMM by using a .",
            "markdown"
        ],
        [
            "Returning to the original topic, most GEMM operators benefit from using non-hyperthreading, because the majority of time in deep learning training or inference is spent on millions of repeated operations of GEMM running on fused-multiply-add (FMA) or dot-product (DP) execution units shared by hyperthreading cores. With hyperthreading enabled, OpenMP threads will contend for the same GEMM execution units.",
            "markdown"
        ],
        [
            "And if 2 logical threads run GEMM at the same time, they will be sharing the same core resources causing front end bound, such that the overhead from this front end bound is greater than the gain from running both logical threads at the same time.",
            "markdown"
        ],
        [
            "Therefore we generally recommend avoiding using logical cores for deep learning workloads to achieve good performance. The launch script by default uses physical cores only; however, users can easily experiment with logical vs. physical cores by simply toggling the --use_logical_core launch script knob.",
            "markdown"
        ],
        [
            "<strong>Exercise</strong>",
            "markdown"
        ],
        [
            "We\u2019ll use the following example of feeding ResNet50 dummy tensor:",
            "markdown"
        ],
        [
            "import torch\nimport torchvision.models as models\nimport time\n\nmodel = models.resnet50(pretrained=False)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\n# warm up\nfor _ in range(100):\n    model(data)\n\nstart = time.time()\nfor _ in range(100):\n    model(data)\nend = time.time()\nprint('Inference took {:.2f} ms in average'.format((end-start)/100*1000))",
            "code"
        ],
        [
            "Throughout the blog, we\u2019ll use  to profile and verify optimizations. And we\u2019ll run all exercises on a machine with two Intel(R) Xeon(R) Platinum 8180M CPUs. The CPU information is shown in Figure 2.1.",
            "markdown"
        ],
        [
            "Environment variable OMP_NUM_THREADS is used to set the number of threads for parallel region. We\u2019ll compare OMP_NUM_THREADS=2 with (1) use of logical cores and (2) use of physical cores only.",
            "markdown"
        ],
        [
            "Both OpenMP threads trying to utilize the same GEMM execution units shared by hyperthreading cores (0, 56)",
            "markdown"
        ],
        [
            "We can visualize this by running htop command on Linux as shown below.",
            "markdown"
        ],
        [
            "We notice that the Spin Time is flagged, and Imbalance or Serial Spinning contributed to the majority of it - 4.980 seconds out of the 8.982 seconds total. The Imbalance or Serial Spinning when using logical cores is due to insufficient concurrency of working threads as each logical thread contends for the same core resources.",
            "markdown"
        ],
        [
            "The Top Hotspots section of the execution summary indicates that __kmp_fork_barrier took 4.589 seconds of CPU time - during 9.33% of the CPU execution time, threads were just spinning at this barrier due to thread synchronization.",
            "markdown"
        ],
        [
            "Each OpenMP thread utilizing GEMM execution units in respective physical cores (0,1)",
            "markdown"
        ],
        [
            "We first note that the execution time dropped from 32 seconds to 23 seconds by avoiding logical cores. While there\u2019s still some non-negligible Imbalance or Serial Spinning, we note relative improvement from 4.980 seconds to 3.887 seconds.",
            "markdown"
        ],
        [
            "By not using logical threads (instead, using 1 thread per physical core), we avoid logical threads contending for the same core resources. The Top Hotspots section also indicates relative improvement of __kmp_fork_barrier time from 4.589 seconds to 3.530 seconds.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles->Local memory access is always faster than remote memory access": [
        [
            "We generally recommend binding a process to a local socket such that the process does not migrate across sockets. Generally the goal of doing so is to utilize high speed cache on local memory and to avoid remote memory access which can be ~2x slower.",
            "markdown"
        ],
        [
            "Figure 1. Two-socket configuration",
            "markdown"
        ],
        [
            "Figure 1. shows a typical two-socket configuration. Notice that each socket has its own local memory. Sockets are connected to each other via Intel Ultra Path Interconnect (UPI) which allows each socket to access the local memory of another socket called remote memory. Local memory access is always faster than remote memory access.",
            "markdown"
        ],
        [
            "Figure 2.1. CPU information",
            "markdown"
        ],
        [
            "Users can get their CPU information by running lscpu command on their Linux machine. Figure 2.1. shows an example of lscpu  execution on a machine with two Intel(R) Xeon(R) Platinum 8180M CPUs. Notice that there are 28 cores per socket, and 2 threads per core (i.e., hyperthreading is enabled). In other words, there are 28 logical cores in addition to 28 physical cores, giving a total of 56 cores per socket. And there are 2 sockets, giving a total of 112 cores (Thread(s) per core x Core(s) per socket x Socket(s)).",
            "markdown"
        ],
        [
            "Figure 2.2. CPU information",
            "markdown"
        ],
        [
            "The 2 sockets are mapped to 2 NUMA nodes (NUMA node 0, NUMA node 1) respectively.  Physical cores are indexed prior to logical cores. As shown in Figure 2.2., the first 28 physical cores (0-27) and the first 28 logical cores (56-83) on the first socket are on NUMA node 0. And the second 28 physical cores (28-55) and the second 28 logical cores (84-111) on the second socket are on NUMA node 1. Cores on the same socket share local memory and last level cache (LLC) which is much faster than cross-socket communication via Intel UPI.",
            "markdown"
        ],
        [
            "Now that we understand NUMA, cross-socket (UPI) traffic, local vs. remote memory access in multi-processor systems, let\u2019s profile and verify our understanding.",
            "markdown"
        ],
        [
            "<strong>Exercise</strong>",
            "markdown"
        ],
        [
            "We\u2019ll reuse the ResNet50 example above.",
            "markdown"
        ],
        [
            "As we did not pin threads to processor cores of a specific socket, the operating system periodically schedules threads on processor cores located in different sockets.",
            "markdown"
        ],
        [
            "Figure 3. CPU usage of non NUMA-aware application. 1 main worker thread was launched, then it launched a physical core number (56) of threads on all cores, including logical cores.",
            "markdown"
        ],
        [
            "(Aside: If the number of threads is not set by , the default number of threads is the number of physical cores in a hyperthreading enabled system. This can be verified by . Hence we see above about half of the cores busy running the example script.)",
            "markdown"
        ],
        [
            "Figure 4. Non-Uniform Memory Access Analysis graph",
            "markdown"
        ],
        [
            "Figure 4. compares local vs. remote memory access over time. We verify usage of remote memory which could result in sub-optimal performance.",
            "markdown"
        ],
        [
            "<strong>Set thread affinity to reduce remote memory access and cross-socket (UPI) traffic</strong>",
            "markdown"
        ],
        [
            "Pinning threads to cores on the same socket helps maintain locality of memory access. In this example, we\u2019ll pin to the physical cores on the first NUMA node (0-27). With the launch script, users can easily experiment with NUMA nodes configuration by simply toggling the --node_id launch script knob.",
            "markdown"
        ],
        [
            "Let\u2019s visualize the CPU usage now.",
            "markdown"
        ],
        [
            "Figure 5. CPU usage of NUMA-aware application",
            "markdown"
        ],
        [
            "1 main worker thread was launched, then it launched threads on all physical cores on the first numa node.",
            "markdown"
        ],
        [
            "Figure 6. Non-Uniform Memory Access Analysis graph",
            "markdown"
        ],
        [
            "As shown in Figure 6., now almost all memory accesses are local accesses.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles->Efficient CPU usage with core pinning for multi-worker inference": [
        [
            "When running multi-worker inference, cores are overlapped (or shared) between workers causing inefficient CPU usage. To address this problem, the launch script equally divides the number of available cores by the number of workers such that each worker is pinned to assigned cores during runtime.",
            "markdown"
        ],
        [
            "<strong>Exercise with TorchServe</strong>",
            "markdown"
        ],
        [
            "For this exercise, let\u2019s apply the CPU performance tuning principles and recommendations that we have discussed so far to .",
            "markdown"
        ],
        [
            "We\u2019ll use ResNet50 with 4 workers, concurrency 100, requests 10,000. All other parameters (e.g., batch_size, input, etc) are the same as the .",
            "markdown"
        ],
        [
            "We\u2019ll compare the following three configurations:",
            "markdown"
        ],
        [
            "default TorchServe setting (no core pinning)",
            "markdown"
        ],
        [
            " = number of physical cores / number of workers (no core pinning)",
            "markdown"
        ],
        [
            "core pinning via the launch script",
            "markdown"
        ],
        [
            "After this exercise, we\u2019ll have verified that we prefer avoiding logical cores and prefer local memory access via core pinning with a real TorchServe use case.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles->1. Default TorchServe setting (no core pinning)": [
        [
            "The  doesn\u2019t explicitly set . Hence the default number of threads is the number of physical CPU cores as described . Users can check the number of threads by  in the base_handler. Each of the 4 main worker threads launches a physical core number (56) of threads, launching a total of 56x4 = 224 threads, which is more than the total number of cores 112.  Therefore cores are guaranteed to be heavily overlapped with high logical core utilization- multiple workers using multiple cores at the same time. Furthermore, because threads are not affinitized to specific CPU cores, the operating system periodically schedules threads to cores located in different sockets.",
            "markdown"
        ],
        [
            "CPU usage",
            "markdown"
        ],
        [
            "4 main worker threads were launched, then each launched a physical core number (56) of threads on all cores, including logical cores.",
            "markdown"
        ],
        [
            "Core Bound stalls",
            "markdown"
        ],
        [
            "We observe a very high Core Bound stall of 88.4%, decreasing pipeline efficiency. Core Bound stalls indicate sub-optimal use of available execution units in the CPU. For example, several GEMM instructions in a row competing for fused-multiply-add (FMA) or dot-product (DP) execution units shared by hyperthreading cores could cause Core Bound stalls. And as described in the previous section, use of logical cores amplifies this problem.",
            "markdown"
        ],
        [
            "An empty pipeline slot not filled with micro-ops (uOps) is attributed to a stall. For example, without core pinning CPU usage may not effectively be on compute but on other operations like thread scheduling from Linux kernel. We see above that __sched_yield contributed to the majority of the Spin Time.",
            "markdown"
        ],
        [
            "Thread Migration",
            "markdown"
        ],
        [
            "Without core pinning, scheduler may migrate thread executing on a core to a different core. Thread migration can disassociate the thread from data that has already been fetched into the caches resulting in longer data access latencies. This problem is exacerbated in NUMA systems when thread migrates across sockets. Data that has been fetched to high speed cache on local memory now becomes remote memory, which is much slower.",
            "markdown"
        ],
        [
            "Generally the total number of threads should be less than or equal to the total number of threads supported by the core. In the above example, we notice a large number of threads executing on core_51 instead of the expected 2 threads (since hyperthreading is enabled in Intel(R) Xeon(R) Platinum 8180 CPUs) . This indicates thread migration.",
            "markdown"
        ],
        [
            "Additionally, notice that thread (TID:97097) was executing on a large number of CPU cores, indicating CPU migration. For example, this thread was executing on cpu_81, then migrated to cpu_14, then migrated to cpu_5, and so on. Furthermore, note that this thread migrated cross socket back and forth many times, resulting in very inefficient memory access. For example, this thread executed on cpu_70 (NUMA node 0), then migrated to cpu_100 (NUMA node 1), then migrated to cpu_24 (NUMA node 0).",
            "markdown"
        ],
        [
            "Non Uniform Memory Access Analysis",
            "markdown"
        ],
        [
            "Compare local vs. remote memory access over time. We observe that about half, 51.09%, of the memory accesses were remote accesses, indicating sub-optimal NUMA configuration.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles->2. torch.set_num_threads = number of physical cores / number of workers (no core pinning)": [
        [
            "For an apple-to-apple comparison with launcher\u2019s core pinning, we\u2019ll set the number of threads to the number of cores divided by the number of workers (launcher does this internally). Add the following code snippet in the :",
            "markdown"
        ],
        [
            "torch.set_num_threads(num_physical_cores/num_workers)",
            "code"
        ],
        [
            "As before without core pinning, these threads are not affinitized to specific CPU cores, causing the operating system to periodically schedule threads on cores located in different sockets.",
            "markdown"
        ],
        [
            "CPU usage",
            "markdown"
        ],
        [
            "4 main worker threads were launched, then each launched a num_physical_cores/num_workers number (14) of threads on all cores, including logical cores.",
            "markdown"
        ],
        [
            "Core Bound stalls",
            "markdown"
        ],
        [
            "Although the percentage of Core Bound stalls has decreased from 88.4% to 73.5%, the Core Bound is still very high.",
            "markdown"
        ],
        [
            "Thread Migration",
            "markdown"
        ],
        [
            "Similar as before, without core pinning thread (TID:94290) was executing on a large number of CPU cores, indicating CPU migration. We notice again cross-socket thread migration, resulting in very inefficient memory access. For example, this thread executed on cpu_78 (NUMA node 0), then migrated to cpu_108 (NUMA node 1).",
            "markdown"
        ],
        [
            "Non Uniform Memory Access Analysis",
            "markdown"
        ],
        [
            "Although an improvement from the original 51.09%, still 40.45% of memory access is remote, indicating sub-optimal NUMA configuration.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles->3. launcher core pinning": [
        [
            "Launcher will internally equally distribute physical cores to workers, and bind them to each worker. As a reminder, launcher by default uses physical cores only. In this example, launcher will bind worker 0 to cores 0-13 (NUMA node 0), worker 1 to cores 14-27 (NUMA node 0), worker 2 to cores 28-41 (NUMA node 1), and worker 3 to cores 42-55 (NUMA node 1). Doing so ensures that cores are not overlapped among workers and avoids logical core usage.",
            "markdown"
        ],
        [
            "CPU usage",
            "markdown"
        ],
        [
            "4 main worker threads were launched, then each launched a num_physical_cores/num_workers number (14) of threads affinitized to the assigned physical cores.",
            "markdown"
        ],
        [
            "Core Bound stalls",
            "markdown"
        ],
        [
            "Core Bound stalls has decreased significantly from the original 88.4% to 46.2% - almost a 2x improvement.",
            "markdown"
        ],
        [
            "We verify that with core binding, most CPU time is effectively used on compute - Spin Time of 0.256s.",
            "markdown"
        ],
        [
            "Thread Migration",
            "markdown"
        ],
        [
            "We verify that <cite>OMP Primary Thread #0</cite> was bound to assigned physical cores (42-55), and did not migrate cross-socket.",
            "markdown"
        ],
        [
            "Non Uniform Memory Access Analysis",
            "markdown"
        ],
        [
            "Now almost all, 89.52%, memory accesses are local accesses.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles->Conclusion": [
        [
            "In this blog, we\u2019ve showcased that properly setting your CPU runtime configuration can significantly boost out-of-box CPU performance.",
            "markdown"
        ],
        [
            "We have walked through some general CPU performance tuning principles and recommendations:",
            "markdown"
        ],
        [
            "In a hyperthreading enabled system, avoid logical cores by setting thread affinity to physical cores only via core pinning.",
            "markdown"
        ],
        [
            "In a multi-socket system with NUMA, avoid cross-socket remote memory access by setting thread affinity to a specific socket via core pinning.",
            "markdown"
        ],
        [
            "We have visually explained these ideas from first principles and have verified the performance boost with profiling. And finally, we have applied all of our learnings to TorchServe to boost out-of-box TorchServe CPU performance.",
            "markdown"
        ],
        [
            "These principles can be automatically configured via an easy to use launch script which has already been integrated into TorchServe.",
            "markdown"
        ],
        [
            "For interested readers, please check out the following documents:",
            "markdown"
        ],
        [
            "And stay tuned for a follow-up posts on optimized kernels on CPU via  and advanced launcher configurations such as memory allocator.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles->Acknowledgement": [
        [
            "We would like to thank Ashok Emani (Intel) and Jiong Gong (Intel) for their immense guidance and support, and thorough feedback and reviews throughout many steps of this blog. We would also like to thank Hamid Shojanazeri (Meta), Li Ning (AWS) and Jing Xu (Intel) for helpful feedback in code review. And Suraj Subramanian (Meta) and Geeta Chauhan (Meta) for helpful feedback on the blog.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)": [
        [
            "Authors: , , ",
            "markdown"
        ],
        [
            "In the  tutorial\n, we have introduced how to tune CPU runtime configurations, how to profile them, and how to integrate them into  for optimized CPU performance.",
            "markdown"
        ],
        [
            "In this tutorial, we will demonstrate boosting performance with memory allocator via the \n, and optimized kernels on CPU via \n, and apply them to TorchServe showcasing 7.71x throughput speedup for ResNet50 and 2.20x throughput speedup for BERT.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->Prerequisites": [
        [
            "Throughout this tutorial, we will use  to profile and show that the Back End Bound (Memory Bound, Core Bound) is often the primary bottleneck for under-optimized or under-tuned deep learning workloads, and demonstrate optimization techniques via Intel\u00ae Extension for PyTorch* for improving Back End Bound. We will use  , a tool part of  built on top of , for TMA.",
            "markdown"
        ],
        [
            "We will also use  to profile at finer granularity.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->Prerequisites->Top-down Microarchitecture Analysis Method (TMA)": [
        [
            "When tuning CPU for optimal performance, it\u2019s useful to know where the bottleneck is. Most CPU cores have on-chip Performance Monitoring Units (PMUs). PMUs are dedicated pieces of logic within a CPU core that count specific hardware events as they occur on the system. Examples of these events may be Cache Misses or Branch Mispredictions. PMUs are used for Top-down Microarchitecture Analysis (TMA) to identify the bottlenecks. TMA consists of hierarchical levels as shown:",
            "markdown"
        ],
        [
            "The top level, level-1, metrics collect <em>Retiring</em>, <em>Bad Speculation</em>, <em>Front End Bound</em>, <em>Back End Bound</em>. The pipeline of CPU can conceptually be simplified and divided into two: the frontend and the backend. The <em>frontend</em> is responsible for fetching the program code and decoding them into low-level hardware operations called micro-ops (uOps). The uOps are then fed to the <em>backend</em> in a process called allocation. Once allocated, the backend is responsible for executing the uOp in an available execution unit. A completion of uOp\u2019s execution is called <em>retirement</em>. In contrast, a <em>bad speculation</em> is when speculatively fetched uOps are canceled before retiring such as in the case of mispredicted branches. Each of these metrics can further be broken down in the subsequent levels to pinpoint the bottleneck.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->Prerequisites->Top-down Microarchitecture Analysis Method (TMA)->Tune for the Back End Bound": [
        [
            "The majority of untuned deep learning workloads will be Back End Bound. Resolving Back End bound is often resolving sources of latency causing retirement to take longer than necessary. As shown above, Back End Bound has two sub-metrics \u2013 Core Bound and Memory Bound.",
            "markdown"
        ],
        [
            "Memory Bound stalls have causes related to the memory subsystem. For example, last-level cache (LLC or L3 cache) miss causing access to DRAM. Scaling deep learning models often requires significant compute. And high compute utilization requires that data is available when the execution units need it to execute the uOps. This requires prefetching the data and reusing the data in cache instead of fetching that same data multiple times from main memory which causes execution units to be starved while data is being returned. Throughout this tutorial, we wll show that a more efficient memory allocator, operator fusion, memory layout format optimization reduce overhead on Memory Bound with better cache locality.",
            "markdown"
        ],
        [
            "Core Bound stalls indicate sub-optimal use of available execution units while there are no uncompleted memory accesses. For example, several general matrix-matrix multiplication (GEMM) instructions in a row competing for fused-multiply-add (FMA) or dot-product (DP) execution units could cause Core Bound stalls. Key deep learning kernels, including the DP kernels, have been well optimized by  (oneAPI Deep Neural Network Library), reducing overhead on Core Bound.",
            "markdown"
        ],
        [
            "Operations like GEMM, convolution, deconvolution are compute-intensive. While operations like pooling, batch normalization, activation functions like ReLU are memory-bound.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->Prerequisites->Intel\u00ae VTune\u2122 Profiler\u2019s Instrumentation and Tracing Technology (ITT)": [
        [
            "The ITT APIs of Intel\u00ae VTune Profiler is a useful tool to annotate a region of your workload for tracing to profile and visualize at a finer granularity of your annotation \u2013 OP/function/sub-function granularity. By annotating at the granularity of your PyTorch model\u2019s OPs, Intel\u00ae VTune Profiler\u2019s ITT enables op-level profiling. Intel\u00ae VTune Profiler\u2019s ITT has been integrated into . <sup>1</sup>",
            "markdown"
        ],
        [
            "The feature has to be explicitly enabled by <em>with torch.autograd.profiler.emit_itt()</em>.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*": [
        [
            " is a Python package to extend PyTorch with optimizations for extra performance boost on Intel hardware.",
            "markdown"
        ],
        [
            "Intel\u00ae Extension for PyTorch* has already been integrated into TorchServe to improve the performance out-of-box. <sup>2</sup> For custom handler scripts, we recommend adding the <em>intel_extension_for_pytorch</em> package in.",
            "markdown"
        ],
        [
            "The feature has to be explicitly enabled by setting <em>ipex_enable=true</em> in <em>config.properties</em>.",
            "markdown"
        ],
        [
            "Throughout this section, we will show that Back End Bound is often the primary bottleneck for under-optimized or under-tuned deep learning workloads, and demonstrate optimization techniques via Intel\u00ae Extension for PyTorch* for improving Back End Bound, which has two submetrics - Memory Bound, and Core Bound. A more efficient memory allocator, operator fusion, memory layout format optimization improve Memory Bound. Ideally, Memory Bound can be improved to Core Bound by optimized operators and better cache locality. And key deep learning primitives, such as convolution, matrix multiplication, dot-product, have been well optimized by Intel\u00ae Extension for PyTorch* and oneDNN library, improving Core Bound.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*->Leveraging Advanced Launcher Configuration: Memory Allocator": [
        [
            "Memory allocator plays an important role from performance perspective. A more efficient memory usage reduces overhead on unnecessary memory allocations or destructions, and thus faster execution. For deep learning workloads in practice, especially those running on large multi-core systems or servers like TorchServe, TCMalloc, or JeMalloc can generally get better memory usage than the default PyTorch memory allocator, PTMalloc.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*->Leveraging Advanced Launcher Configuration: Memory Allocator->TCMalloc, JeMalloc, PTMalloc": [
        [
            "Both TCMalloc and JeMalloc use thread-local caches to reduce overhead on thread synchronization, and lock contention by using spinlocks and per-thread arenas respectively. TCMalloc and JeMalloc reduce overhead on unnecessary memory allocation and deallocation. Both allocators categorize memory allocations by sizes to reduce overhead on memory fragmentation.",
            "markdown"
        ],
        [
            "With the launcher, users can easily experiment with different memory allocators by choosing one of the three launcher knobs <em>\u2013enable_tcmalloc</em> (TCMalloc), <em>\u2013enable_jemalloc</em> (JeMalloc), <em>\u2013use_default_allocator</em> (PTMalloc).",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*->Leveraging Advanced Launcher Configuration: Memory Allocator->TCMalloc, JeMalloc, PTMalloc->Exercise": [
        [
            "Let\u2019s profile PTMalloc vs. JeMalloc.",
            "markdown"
        ],
        [
            "We will use the launcher to designate the memory allocator, and to bind the workload to physical cores of the first socket to avoid any NUMA complication \u2013 to profile the effect of memory allocator only.",
            "markdown"
        ],
        [
            "The following example measures the average inference time of ResNet50:",
            "markdown"
        ],
        [
            "import torch\nimport torchvision.models as models\nimport time\n\nmodel = models.resnet50(pretrained=False)\nmodel.eval()\nbatch_size = 32\ndata = torch.rand(batch_size, 3, 224, 224)\n\n# warm up\nfor _ in range(100):\n    model(data)\n\n# measure\n# Intel\u00ae VTune Profiler's ITT context manager\nwith torch.autograd.profiler.emit_itt():\n    start = time.time()\n    for i in range(100):\n   # Intel\u00ae VTune Profiler's ITT to annotate each step\n        torch.profiler.itt.range_push('step_{}'.format(i))\n        model(data)\n        torch.profiler.itt.range_pop()\n    end = time.time()\n\nprint('Inference took {:.2f} ms in average'.format((end-start)/100*1000))",
            "code"
        ],
        [
            "Let\u2019s collect level-1 TMA metrics.",
            "markdown"
        ],
        [
            "Level-1 TMA shows that both PTMalloc and JeMalloc are bounded by the backend. More than half of the execution time was stalled by the backend. Let\u2019s go one level deeper.",
            "markdown"
        ],
        [
            "Level-2 TMA shows that the Back End Bound was caused by Memory Bound. Let\u2019s go one level deeper.",
            "markdown"
        ],
        [
            "Most of the metrics under the Memory Bound identify which level of the memory hierarchy from the L1 cache to main memory is the bottleneck. A hotspot bounded at a given level indicates that most of the data was being retrieved from that cache or memory-level. Optimizations should focus on moving data closer to the core. Level-3 TMA shows that PTMalloc was bottlenecked by DRAM Bound. On the other hand, JeMalloc was bottlenecked by L1 Bound \u2013 JeMalloc moved data closer to the core, and thus faster execution.",
            "markdown"
        ],
        [
            "Let\u2019s look at Intel\u00ae VTune Profiler ITT trace. In the example script, we have annotated each <em>step_x</em> of the inference loop.",
            "markdown"
        ],
        [
            "Each step is traced in the timeline graph. The duration of model inference on the last step (step_99) decreased from 304.308 ms to 261.843 ms.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*->Leveraging Advanced Launcher Configuration: Memory Allocator->TCMalloc, JeMalloc, PTMalloc->Exercise with TorchServe": [
        [
            "Let\u2019s profile PTMalloc vs. JeMalloc with TorchServe.",
            "markdown"
        ],
        [
            "We will use  with ResNet50 FP32, batch size 32, concurrency 32, requests 8960. All other parameters are the same as the .",
            "markdown"
        ],
        [
            "As in the previous exercise, we will use the launcher to designate the memory allocator, and to bind the workload to physical cores of the first socket. To do so, user simply needs to add a few lines in :",
            "markdown"
        ],
        [
            "PTMalloc",
            "markdown"
        ],
        [
            "cpu_launcher_enable=true\ncpu_launcher_args=--node_id 0 --use_default_allocator",
            "code"
        ],
        [
            "JeMalloc",
            "markdown"
        ],
        [
            "cpu_launcher_enable=true\ncpu_launcher_args=--node_id 0 --enable_jemalloc",
            "code"
        ],
        [
            "Let\u2019s collect level-1 TMA metrics.",
            "markdown"
        ],
        [
            "Let\u2019s go one level deeper.",
            "markdown"
        ],
        [
            "Let\u2019s use Intel\u00ae VTune Profiler ITT to annotate  to profile at inference-level granularity. As  consists of several sub-components, including the Java frontend for handling request/response, and the Python backend for running the actual inference on the models, it is helpful to use Intel\u00ae VTune Profiler ITT to limit the collection of trace data at inference-level.",
            "markdown"
        ],
        [
            "Each inference call is traced in the timeline graph. The duration of the last model inference decreased from 561.688 ms to 251.287 ms - 2.2x speedup.",
            "markdown"
        ],
        [
            "The timeline graph can be expanded to see op-level profiling results. The duration of <em>aten::conv2d</em> decreased from 16.401 ms to 6.392 ms - 2.6x speedup.",
            "markdown"
        ],
        [
            "In this section, we have demonstrated that JeMalloc can give better performance than the default PyTorch memory allocator, PTMalloc, with efficient thread-local caches improving Back-End-Bound.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*->Intel\u00ae Extension for PyTorch*": [
        [
            "The three major  optimization techniques, Operator, Graph, Runtime, are as shown:",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*->Intel\u00ae Extension for PyTorch*->Operator Optimization": [
        [
            "Optimized operators and kernels are registered through PyTorch dispatching mechanism. These operators and kernels are accelerated from native vectorization feature and matrix calculation feature of Intel hardware. During execution, Intel\u00ae Extension for PyTorch* intercepts invocation of ATen operators, and replaces the original ones with these optimized ones. Popular operators like Convolution, Linear have been optimized in Intel\u00ae Extension for PyTorch*.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*->Intel\u00ae Extension for PyTorch*->Operator Optimization->Exercise": [
        [
            "Let\u2019s profile optimized operator with Intel\u00ae Extension for PyTorch*. We will compare with and without the lines in code changes.",
            "markdown"
        ],
        [
            "As in the previous exercises, we will bind the workload to physical cores of the first socket.",
            "markdown"
        ],
        [
            "import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(16, 33, 3, stride=2)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x\n\nmodel = Model()\nmodel.eval()\ndata = torch.rand(20, 16, 50, 100)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model)\n######################################################\n\nprint(model)",
            "code"
        ],
        [
            "The model consists of two operations\u2014Conv2d and ReLU. By printing the model object, we get the following output.",
            "markdown"
        ],
        [
            "Let\u2019s collect level-1 TMA metrics.",
            "markdown"
        ],
        [
            "Notice the Back End Bound reduced from 68.9 to 38.5 \u2013 1.8x speedup.",
            "markdown"
        ],
        [
            "Additionally, let\u2019s profile with PyTorch Profiler.",
            "markdown"
        ],
        [
            "Notice the CPU time reduced from 851 us to 310 us \u2013 2.7X speedup.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*->Intel\u00ae Extension for PyTorch*->Graph Optimization": [
        [
            "It is highly recommended for users to take advantage of Intel\u00ae Extension for PyTorch* with  for further graph optimizations. To optimize performance further with TorchScript, Intel\u00ae Extension for PyTorch* supports oneDNN fusion of frequently used FP32/BF16 operator patterns, like Conv2D+ReLU, Linear+ReLU, and more to reduce operator/kernel invocation overheads, and for better cache locality. Some operator fusions allow to maintain temporary calculations, data type conversions, data layouts for better cache locality. As well as for INT8, Intel\u00ae Extension for PyTorch* has built-in quantization recipes to deliver good statistical accuracy for popular DL workloads including CNN, NLP and recommendation models. The quantized model is then optimized with oneDNN fusion support.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*->Intel\u00ae Extension for PyTorch*->Graph Optimization->Exercise": [
        [
            "Let\u2019s profile FP32 graph optimization with TorchScript.",
            "markdown"
        ],
        [
            "As in the previous exercises, we will bind the workload to physical cores of the first socket.",
            "markdown"
        ],
        [
            "import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(16, 33, 3, stride=2)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x\n\nmodel = Model()\nmodel.eval()\ndata = torch.rand(20, 16, 50, 100)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model)\n######################################################\n\n# torchscript\nwith torch.no_grad():\n    model = torch.jit.trace(model, data)\n    model = torch.jit.freeze(model)",
            "code"
        ],
        [
            "Let\u2019s collect level-1 TMA metrics.",
            "markdown"
        ],
        [
            "Notice the Back End Bound reduced from 67.1 to 37.5 \u2013 1.8x speedup.",
            "markdown"
        ],
        [
            "Additionally, let\u2019s profile with PyTorch Profiler.",
            "markdown"
        ],
        [
            "Notice that with Intel\u00ae Extension for PyTorch*  Conv + ReLU operators are fused, and the CPU time reduced from 803 us to 248 us \u2013 3.2X speedup. The oneDNN eltwise post-op enables fusing a primitive with an elementwise primitive. This is one of the most popular kinds of fusion: an eltwise (typically an activation function such as ReLU) with preceding convolution or inner product. Have a look at the oneDNN verbose log shown in the next section.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*->Intel\u00ae Extension for PyTorch*->Channels Last Memory Format": [
        [
            "When invoking <em>ipex.optimize</em> on model, Intel\u00ae Extension for PyTorch* automatically converts the model to optimized memory format, channels last. Channels last is a memory format that is more friendly to Intel Architecture. Compared to PyTorch default channels first NCHW (batch, channels, height, width) memory format, channels last NHWC (batch, height, width, channels) memory format generally accelerates convolutional neural networks with better cache locality.",
            "markdown"
        ],
        [
            "One thing to note is that it is expensive to convert memory format. So it\u2019s better to convert the memory format prior to deployment once, and keep the memory format conversion minimum during deployment. As the data propagates through model\u2019s layers the channels last memory format is preserved through consecutive channels last supported layers (for example, Conv2d -&gt; ReLU -&gt; Conv2d) and conversions are only made in between channels last unsupported layers. See  for more details.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*->Intel\u00ae Extension for PyTorch*->Channels Last Memory Format->Exercise": [
        [
            "Let\u2019s demonstrate channels last optimization.",
            "markdown"
        ],
        [
            "import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(16, 33, 3, stride=2)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x\n\nmodel = Model()\nmodel.eval()\ndata = torch.rand(20, 16, 50, 100)\n\nimport intel_extension_for_pytorch as ipex\n############################### code changes ###############################\nipex.disable_auto_channels_last() # omit this line for channels_last (default)\n############################################################################\nmodel = ipex.optimize(model)\n\nwith torch.no_grad():\n    model = torch.jit.trace(model, data)\n    model = torch.jit.freeze(model)",
            "code"
        ],
        [
            "We will use , a tool to help collect information at oneDNN graph level such as operator fusions, kernel execution time spent on executing oneDNN primitives. For more information, refer to the .",
            "markdown"
        ],
        [
            "Above is oneDNN verbose from channels first. We can verify that there are reorders from weight and data, then do computation, and finally reorder output back.",
            "markdown"
        ],
        [
            "Above is oneDNN verbose from channels last. We can verify that channels last memory format avoids unnecessary reorders.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*->Performance Boost with Intel\u00ae Extension for PyTorch*": [
        [
            "Below summarizes performance boost of TorchServe with Intel\u00ae Extension for PyTorch* for ResNet50 and BERT-base-uncased.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->TorchServe with Intel\u00ae Extension for PyTorch*->Exercise with TorchServe": [
        [
            "Let\u2019s profile Intel\u00ae Extension for PyTorch* optimizations with TorchServe.",
            "markdown"
        ],
        [
            "We will use  with ResNet50 FP32 TorchScript, batch size 32, concurrency 32, requests 8960. All other parameters are the same as the .",
            "markdown"
        ],
        [
            "As in the previous exercise, we will use the launcher to bind the workload to physical cores of the first socket. To do so, user simply needs to add a few lines in :",
            "markdown"
        ],
        [
            "cpu_launcher_enable=true\ncpu_launcher_args=--node_id 0",
            "code"
        ],
        [
            "Let\u2019s collect level-1 TMA metrics.",
            "markdown"
        ],
        [
            "Level-1 TMA shows that both are bounded by the backend. As discussed earlier, the majority of untuned deep learning workloads will be Back End Bound. Notice the Back End Bound reduced from 70.0 to 54.1. Let\u2019s go one level deeper.",
            "markdown"
        ],
        [
            "As discussed earlier, Back End Bound has two submetrics \u2013 Memory Bound and Core Bound. Memory Bound indicates the workload is under-optimized or under-utilized, and ideally memory-bound operations can be improved to core-bound by optimizing the OPs and improving cache locality. Level-2 TMA shows that the Back End Bound improved from Memory Bound to Core Bound. Let\u2019s go one level deeper.",
            "markdown"
        ],
        [
            "Scaling deep learning models for production on a model serving framework like TorchServe requires high compute utilization. This requires that data is available through prefetching and reusing the data in cache when the execution units need it to execute the uOps. Level-3 TMA shows that the Back End Memory Bound improved from DRAM Bound to Core Bound.",
            "markdown"
        ],
        [
            "As in the previous exercise with TorchServe, let\u2019s use Intel\u00ae VTune Profiler ITT to annotate  to profile at inference-level granularity.",
            "markdown"
        ],
        [
            "Each inference call is traced in the timeline graph. The duration of the last inference call decreased from 215.731 ms to 95.634 ms - 2.3x speedup.",
            "markdown"
        ],
        [
            "The timeline graph can be expanded to see op-level profiling results. Notice that Conv + ReLU has been fused, and the duration decreased from 6.393 ms + 1.731 ms to 3.408 ms - 2.4x speedup.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->Conclusion": [
        [
            "In this tutorial, we have used Top-down Microarchitecture Analysis (TMA) and Intel\u00ae VTune\u2122 Profiler\u2019s Instrumentation and Tracing Technology (ITT) to demonstrate that",
            "markdown"
        ],
        [
            "Often the primary bottleneck of under-optimized or under-tuned deep learning workloads are Back End Bound, which has two submetrics, Memory Bound and Core Bound.",
            "markdown"
        ],
        [
            "A more efficient memory allocator, operator fusion, memory layout format optimization by Intel\u00ae Extension for PyTorch* improve Memory Bound.",
            "markdown"
        ],
        [
            "Key deep learning primitives, such as convolution, matrix multiplication, dot-product, etc have been well optimized by Intel\u00ae Extension for PyTorch* and oneDNN library, improving Core Bound.",
            "markdown"
        ],
        [
            "Intel\u00ae Extension for PyTorch* has been integrated into TorchServe with an ease-of-use API.",
            "markdown"
        ],
        [
            "TorchServe with Intel\u00ae Extension for PyTorch* shows 7.71x throughput speedup for ResNet50, and 2.20x throughput speedup for BERT.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->Related Readings": [],
    "torch->Model Optimization->Grokking PyTorch Intel CPU performance from first principles (Part 2)->Acknowledgement": [
        [
            "We would like to thank Ashok Emani (Intel) and Jiong Gong (Intel) for their immense guidance and support, and thorough feedback and reviews throughout many steps of this tutorial. We would also like to thank Hamid Shojanazeri (Meta) and Li Ning (AWS) for their helpful feedback in code review and the tutorial.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser": [
        [
            "<strong>Authors</strong>: \n\n\n\n\n<cite>Neal Vaidya</cite>",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Introduction": [
        [
            "This tutorial will demonstrate how you can accelerate your networks\nwith nvFuser. nvFuser is a Deep Learning Compiler that just-in-time\ncompiles fast and flexible GPU specific code to reliably accelerate\nusers\u2019 networks automatically, providing speedups for deep learning\nnetworks running on Volta and later CUDA accelerators by generating\nfast custom \u201cfusion\u201d kernels at runtime. nvFuser is specifically\ndesigned to meet the unique requirements of the PyTorch community,\nand it supports diverse network architectures and programs with\ndynamic inputs of varying shapes and strides.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Importing Packages and Selecting a Device": [
        [
            "In order to run this tutorial and see the benefits of using nvFuser,\nyou would need to install the <cite>1.12.0</cite> PyTorch release as well as\n<cite>functorch</cite> <cite>0.2</cite> or newer version of them. <cite>functorch</cite> also needs\n<cite>networkx</cite> for its smart recomputation heuristics which you can\ninstall via <cite>pip install networkx</cite>.\nAdditionally, a GPU is required.",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn.functional as F\nimport functorch\nfrom functorch.compile import memory_efficient_fusion\nfrom copy import deepcopy\nfrom typing import List\nimport time\nimport functools\nimport random\n\nrandom.seed(42)\n\nif torch.__version__ &lt; (1, 12, 0):\n    raise RuntimeError(\n        \"PyTorch &gt;= 1.12.0 required, but your environment uses torch=={}\".format(\n            torch.__version__\n        )\n    )\n\nmajor, minor, _ = functorch.__version__.split(\".\")\nif int(major) == 0 and int(minor) &lt; 2:\n    raise RuntimeError(\n        \"FuncTorch &gt;= 0.2.0 required, but your environment uses functorch=={}\".format(\n            functorch.__version__\n        )\n    )",
            "code"
        ]
    ],
    "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->The Transformer Block": [
        [
            "The network topology we\u2019re going to focus on is the Transformer\nBlock for networks like BERT. As of writing this tutorial, nvFuser\nprovides acceleration of pointwise, reduction, and normalization\noperations. These simple operations are the backbone of large\nnetworks, so improving the speed of these operations can improve\noverall network training speed. Future releases of nvFuser will\nimprove the performance of Linear Layers, but for now we will\nspecifically look at the Bias-Dropout-Add-LayerNorm section of this\nTransformer Block.\n\n<img alt=\"../_images/nvfuser_transformer_block.png\" src=\"../_images/nvfuser_transformer_block.png\"/>",
            "markdown"
        ],
        [
            "First, let\u2019s define the forward pass for this section of our network.\nFor when we\u2019ll use TorchScript on this function, we decorate the\nfunction with type information of the function parameters. This isn\u2019t\nalways required, but it can often help to provide this information to\nTorchScript because it is a strictly typed system. Since we have\nPyTorch\u2019s autograd system, we don\u2019t need to explicitly define the\nbackwards pass.",
            "markdown"
        ],
        [
            "def composite_definition(\n    : ,\n    : ,\n    : ,\n    : ,\n    : ,\n    normalization_axis: int,\n    dropout_prob: float,\n) -&gt; :\n    bias1_out =  + \n    dropout_out = (bias1_out, dropout_prob, training=True)\n    norm_input = dropout_out + \n    norm_output = (\n        norm_input, (.size(normalization_axis),), , \n    )\n    return norm_output",
            "code"
        ]
    ],
    "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Setup and Performance Metrics": [
        [
            "Next, we initialize some inputs, parameters, and a simulated gradient\noutput tensor for the backwards pass since we aren\u2019t including a\nloss function.",
            "markdown"
        ],
        [
            "# Setup initial tensors and parameters\ninput_size = [64, 128, 1024]\ndevice = \"cuda\"\n = \n\n# Create sample inputs\n = (*input_size, device=device, =, requires_grad=True)\n = ().requires_grad_()\n\n# Precompute a grad output tensor, for this example it's the same size\n# as the inputs\n = ()\n\n# Randomly initialize the model parameters\n = ((input_size[2], =, device=device))\n = ((input_size[2], =, device=device))\n = ((input_size[2], =, device=device))\n\nparameters = [, , , , ]",
            "code"
        ],
        [
            "To produce a baseline performance we will measure the speed of our\nforward and backward passes in PyTorch\u2019s default eager mode. To get\naccurate and comparable measurements, we perform a few warm up\niterations. Then, we time many iterations of the forward and backward\npass using performance counters combined with proper GPU\nsynchronization, then compute the average iterations per second.\nIt\u2019s important to be very careful when measuring performance on the\nGPU, as we want to remove any initialization costs and need\nsynchronization since it\u2019s an asynchronous device. Since we will\nmeasure many variations of this problem with and without nvFuser we\ndefine a helper method called <cite>profile_workload</cite> and will use\n<cite>functool.partial</cite> to concisely profile the workload.",
            "markdown"
        ],
        [
            "# Utility to profile the workload\ndef profile_workload(forward_func, , iteration_count=100, label=\"\"):\n    # Perform warm-up iterations\n    for _ in range(3):\n        # Run model, forward and backward\n         = forward_func()\n        ()\n        # delete gradiens to avoid profiling the gradient accumulation\n        for  in parameters:\n            .grad = None\n\n    # Synchronize the GPU before starting the timer\n    ()\n    start = time.perf_counter()\n    for _ in range(iteration_count):\n        # Run model, forward and backward\n         = forward_func()\n        ()\n        # delete gradiens to avoid profiling the gradient accumulation\n        for  in parameters:\n            .grad = None\n\n    # Synchronize the GPU before stopping the timer\n    ()\n    stop = time.perf_counter()\n    iters_per_second = iteration_count / (stop - start)\n    if label:\n        print(label)\n    print(\"Average iterations per second: {:.2f}\".format(iters_per_second))",
            "code"
        ],
        [
            "We can now measure a baseline performance of PyTorch\u2019s eager mode\n(without nvFuser).",
            "markdown"
        ],
        [
            "# Run and profile eager mode execution on the composite definition of our\n# operations.\nfunc = functools.partial(\n    composite_definition,\n    ,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n)\nprofile_workload(\n    func, , iteration_count=100, label=\"Eager Mode - Composite definition\"\n)",
            "code"
        ],
        [
            "Eager Mode - Composite definition\nAverage iterations per second: 193.33",
            "code"
        ],
        [
            "It\u2019s important for PyTorch and nvFuser to work well across diverse\nGPU architectures. For our measurements we\u2019ve run this tutorial on\nfive GPUs ranging from consumer to enterprise grade. Our baseline\ngeometric mean (geomean) performance across these GPUs is 850\niterations per second, plotted in the figure below.\n\n<img alt=\"../_images/nvfuser_tutorial_0.png\" src=\"../_images/nvfuser_tutorial_0.png\"/>",
            "markdown"
        ],
        [
            "As we run different variations of this script with nvFuser, we will\ncontinue to add the results to this figure for the same GPUs.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->TorchScript & nvFuser": [
        [
            "nvFuser is the default fusion system in TorchScript since PyTorch\nversion 1.12, so to turn on nvFuser we need to enable TorchScript.\nThis will allow nvFuser to automatically generate fast kernels and\ntake over execution of these operations. TorchScript can be a\nchallenging system to get working, but with our current definition\nof our operators, all we need to do is wrap our function in the\n<cite>torch.jit.script</cite> compile function. We can then simply run our\nworkload as before.",
            "markdown"
        ],
        [
            " = (composite_definition)\nfunc = functools.partial(\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n)\nprofile_workload(\n    func, , iteration_count=100, label=\"TorchScript - Composite definition\"\n)",
            "code"
        ],
        [
            "TorchScript - Composite definition\nAverage iterations per second: 248.66",
            "code"
        ],
        [
            "Before we get to the results, it is important to mention here that\nnvFuser does not generate the exact same sequence of random numbers,\nas random number generation in PyTorch is dependent on the precise\nparallelization scheme used for the GPU function. Therefore, if you\nwant to validate the output of nvFuser with the output without\nnvFuser, it would require disabling the random number generation\nfunctions. In this example, we would simply need to change\n<cite>dropout_out = F.dropout(bias1_out, dropout_prob, training=True)</cite>\nto\n<cite>dropout_out = F.dropout(bias1_out, dropout_prob, training=False)</cite>\nas the dropout function is the only function in this example that\ndepends on random number generation.\n\n<img alt=\"../_images/nvfuser_tutorial_1.png\" src=\"../_images/nvfuser_tutorial_1.png\"/>",
            "markdown"
        ],
        [
            "Our geomean performance with nvFuser is 1,394 images per second\nwhich is a geomean of 1.64x faster than eager mode. We did not\ninclude the time that TorchScript and nvFuser take to compile the\nprogram and GPU functions. For real end-to-end training the\ncompile time of TorchScript and nvFuser are negligible. For\nexample, in this tutorial the combination of TorchScript and\nnvFuser took around 2.4s in total to compile these high speed\nGPU functions.",
            "markdown"
        ],
        [
            "nvFuser\u2019s capabilities extend well beyond this initial performance gain.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->nvFuser & Dynamic Shapes": [
        [
            "It is challenging for Deep Learning Compilers to provide performance\ngains when the user changes the input sizes of the tensors. However,\nsupporting changing shapes has always been a fundamental design\ncriteria for nvFuser, as processing different-sized input tensors is\ncritical to many applications like Natural Language Processing and\nGraph Neural Networks.",
            "markdown"
        ],
        [
            "To use nvFuser on inputs that change shape from iteration, we\ngenerate new input and output gradient tensors and make a few\ndifferent sizes. Since the last dimension is shared with the\nparameters and cannot be changed dynamically in LayerNorm, we\nperturb the first two dimensions of the input and gradient tensors.",
            "markdown"
        ],
        [
            "SHAPE_COUNT = 20\ndynamic_sizes = deepcopy(input_size)\n\ninputs1: List[] = []\ninputs2: List[] = []\ngrad_outputs: List[] = []\n\n\n# Create some random shapes\nfor _ in range(SHAPE_COUNT):\n    dynamic_sizes[0] = input_size[0] + random.randrange(-2, 3)\n    dynamic_sizes[1] = input_size[1] + random.randrange(-2, 3)\n    input = (*dynamic_sizes, device=device, =, requires_grad=True)\n    inputs1.append(input)\n    inputs2.append((input))\n    grad_outputs.append((input))",
            "code"
        ],
        [
            "No changes from before are required for running with TorchScript, we\nsimply reuse the previous definition that we wrapped in\n<cite>torch.jit.script</cite>.",
            "markdown"
        ],
        [
            "We\u2019ll start as usual by performing some warm-up iterations, however\nwe won\u2019t show nvFuser all of the input sizes, we\u2019ll only show one\nsize for the warm-up.",
            "markdown"
        ],
        [
            "# Perform warm-up iterations\nfor _ in range(3):\n     = inputs1[0]\n     = inputs2[0]\n     = grad_outputs[0]\n    # Run model, forward and backward\n     = (\n        ,\n        ,\n        ,\n        ,\n        ,\n        normalization_axis=2,\n        dropout_prob=0.1,\n    )\n    ()",
            "code"
        ],
        [
            "Now, we can measure the performance metrics of nvFuser as we have\npreviously.",
            "markdown"
        ],
        [
            "# Profile manually as our helper function expects static inputs\niteration_count = 100\n# Synchronize the GPU before starting the timer\n()\nstart = time.perf_counter()\nfor i in range(iteration_count):\n     = inputs1[i % SHAPE_COUNT]\n     = inputs2[i % SHAPE_COUNT]\n     = grad_outputs[i % SHAPE_COUNT]\n    dynamic_parameters = [, , , , ]\n\n    # Run model, forward and backward\n     = (\n        ,\n        ,\n        ,\n        ,\n        ,\n        normalization_axis=2,\n        dropout_prob=0.1,\n    )\n    ()\n    # Delete the gradients to avoid profiling the gradient accumulation\n    for  in dynamic_parameters:\n        .grad = None\n\n# Synchronize the GPU before stopping the timer\n()\nstop = time.perf_counter()\niters_per_second = iteration_count / (stop - start)\nprint(\"TorchScript - Random Sizes\")\nprint(\"Average iterations per second: {:.2f}\".format(iters_per_second))",
            "code"
        ],
        [
            "TorchScript - Random Sizes\nAverage iterations per second: 244.98",
            "code"
        ],
        [
            "Performance across our GPUs is very similar to the previous\nperformance seen. Only the performance of the A100 degraded\nslightly, but is still much higher than without nvFuser. The small\nchange in performance of the A100 is actually related to the\nadditional CPU overhead that dynamic shapes cause in nvFuser.\nnvFuser at runtime has to infer how to run the different sized\nkernels, so additional CPU time is consumed. This CPU time is\npresent with all GPUs, but since the A100 runs its functions so fast\nthis CPU overhead cannot be fully hidden by the asynchronous nature\nof GPU execution.\n\n<img alt=\"../_images/nvfuser_tutorial_2.png\" src=\"../_images/nvfuser_tutorial_2.png\"/>",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Today, nvFuser in TorchScript is the only exposure of\nnvFuser that allows for dynamic shape changes, although we will\nexpand this capability to other systems in the future. For more\ninsight into how dynamic shapes are implemented in nvFuser, you can\nview this presentation from GTC 2021:",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Defining novel operations with nvFuser and FuncTorch": [
        [
            "One of the primary benefits of nvFuser is the ability to define\nnovel operations composed of PyTorch \u201cprimitives\u201d which are then\njust-in-time compiled into efficient kernels.",
            "markdown"
        ],
        [
            "PyTorch has strong performance for any individual operation,\nespecially composite operations like LayerNorm. However, if\nLayerNorm wasn\u2019t already implemented in PyTorch as a composite\noperation, then you\u2019d have to define it as a series of simpler\n(primitive) operations. Let\u2019s make such a definition and run it\nwithout nvFuser.",
            "markdown"
        ],
        [
            "def primitive_definition(\n    : ,\n    : ,\n    : ,\n    : ,\n    : ,\n    normalization_axis: int,\n    dropout_prob: float,\n    keepdim: bool,\n) -&gt; :\n    bias1_out =  + \n    dropout_out = (bias1_out, dropout_prob, training=True)\n    norm_input = dropout_out + \n    mean = norm_input.mean(normalization_axis, keepdim=keepdim)\n    diff = norm_input - mean\n    diff_sq = diff * diff\n    var = diff_sq.mean(normalization_axis, keepdim=keepdim)\n    pre_shift_scale_norm_output = (norm_input - mean) / (var + 1e-12)\n    norm_output =  * pre_shift_scale_norm_output + \n    return norm_output\n\n\n# Profile primitive definition\nfunc = functools.partial(\n    primitive_definition,\n    ,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(\n    func, , iteration_count=100, label=\"Eager Mode - Primitive Definition\"\n)",
            "code"
        ],
        [
            "Eager Mode - Primitive Definition\nAverage iterations per second: 61.56",
            "code"
        ],
        [
            "While the above is mathematically equivalent to our previous\ndefinition, benchmarking our new function with the original static\nshape using TorchScript and nvFuser shows the iterations per second\ndecreases \u2013 mostly due to the cost of accessing memory to save\nintermediate results.\n\n<img alt=\"../_images/nvfuser_tutorial_3.png\" src=\"../_images/nvfuser_tutorial_3.png\"/>",
            "markdown"
        ],
        [
            "The geomean iterations per second is 260 iterations per second,\n3.26x slower than the composite definition in eager mode and 5.35x\nslower than the nvFuser composite operation! For more information on\nwhy there\u2019s such a drastic decrease in compute speed please see this\npresentation from GTC 2022:",
            "markdown"
        ],
        [
            "nvFuser with TorchScript can improve the performance of this\noperation even though it\u2019s defined with primitive PyTorch\noperations. Simply by enabling TorchScript on the new function\n(just like before), we can see much of the performance returns.",
            "markdown"
        ],
        [
            "# Profile scripted primitive definition\n = (primitive_definition)\nfunc = functools.partial(\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(\n    func, , iteration_count=100, label=\"TorchScript - Primitive definition\"\n)",
            "code"
        ],
        [
            "TorchScript - Primitive definition\nAverage iterations per second: 120.92\n\n\n\n<img alt=\"../_images/nvfuser_tutorial_4.png\" src=\"../_images/nvfuser_tutorial_4.png\"/>",
            "code"
        ],
        [
            "However, the performance is still slower than the original eager\nmode performance of the composite definition. TorchScript works well\nwhen predefined composite operations are used, however TorchScript\u2019s\napplication of Autograd saves all of the activations for each\noperator in the fusion for re-use in the backwards pass. However,\nthis is not typically the optimal choice. Especially when chaining\ntogether multiple simple operations, it is often much faster to\nrecompute some intermediate tensors rather than spend the time\nstoring and retrieving several saved results from memory.",
            "markdown"
        ],
        [
            "It\u2019s possible to optimize away many of these unnecessary memory\naccesses, but it requires building a connected forward and backward\ngraph which isn\u2019t possible with TorchScript. The\n<cite>memory_efficient_fusion</cite> pass in FuncTorch, however, is such an\noptimization pass. To use this pass, we have to redefine our\nfunction to pull the constants inside (for now it\u2019s easiest to make\nnon-tensor constants literals in the function definition):",
            "markdown"
        ],
        [
            "def primitive_definition_for_memory_efficient_fusion(\n    : ,\n    : ,\n    : ,\n    : ,\n    : ,\n) -&gt; :\n    bias1_out =  + \n    dropout_out = (bias1_out, 0.1, training=True)\n    norm_input = dropout_out + \n    mean = norm_input.mean(2, keepdim=True)\n    diff = norm_input - mean\n    diff_sq = diff * diff\n    var = diff_sq.mean(2, keepdim=True)\n    pre_shift_scale_norm_output = (norm_input - mean) / (var + 1e-12)\n    norm_output =  * pre_shift_scale_norm_output + \n    return norm_output",
            "code"
        ],
        [
            "Now, instead of passing our function to TorchScript, we will pass it\nto FuncTorch\u2019s optimization pass.",
            "markdown"
        ],
        [
            "# Optimize the model with FuncTorch tracing and the memory efficiency\n# optimization pass\nmemory_efficient_primitive_definition = memory_efficient_fusion(\n    primitive_definition_for_memory_efficient_fusion\n)\n\n# Profile memory efficient primitive definition\nfunc = functools.partial(\n    memory_efficient_primitive_definition, , , , , \n)\nprofile_workload(\n    func,\n    ,\n    iteration_count=100,\n    label=\"FuncTorch - Primitive definition\",\n)",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torch/jit/_check.py:172: UserWarning:\n\nThe TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n\nFuncTorch - Primitive definition\nAverage iterations per second: 125.71",
            "code"
        ],
        [
            "This recovers even more speed, but it\u2019s still not as fast as\nTorchScripts original performance with the composite definition.\nHowever, this is still faster than running this new definition\nwithout nvFuser, and is still faster than the composite definition\nwithout nvFuser.\n\n<img alt=\"../_images/nvfuser_tutorial_5.png\" src=\"../_images/nvfuser_tutorial_5.png\"/>",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "FuncTorch\u2019s memory efficient pass is experimental and still\nactively in development.\nFuture versions of the API are expected to achieve performance\ncloser to that of TorchScript with the composite definition.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "FuncTorch\u2019s memory efficient pass specializes on the shapes of\nthe inputs to the function. If new inputs are provided with\ndifferent shapes, then you need to construct a new function\nusing <cite>memory_efficient_fusion</cite> and apply it to the new inputs.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Transformer Block With a Novel Normalization": [
        [
            "The ability to quickly execute chains of simple operations is\nimportant as not every operation has a composite operation defined\nin PyTorch. Previously, this meant researchers either had to define\nan entirely new operation in PyTorch \u2013 which takes a lot of time and\nknowledge of the lower-level PyTorch code as well as parallel\nprogramming \u2013 or writing the operation in simpler PyTorch ops and\nsettling for poor performance. For example, let\u2019s replace LayerNorm\nin our example with RMSNorm. Even though RMSNorm is a bit simpler\nthan LayerNorm, it doesn\u2019t have an existing compound operation in\nPyTorch. See the  paper for more information about RMSNorm.\nAs before, we\u2019ll define our new transformer block with\nprimitive PyTorch operations.",
            "markdown"
        ],
        [
            "def with_rms_norm(\n    : ,\n    : ,\n    : ,\n    bias: ,\n    normalization_axis: int,\n    dropout_prob: float,\n    keepdim: bool,\n) -&gt; :\n    bias_out =  + bias\n    dropout_out = (bias_out, dropout_prob, training=True)\n    norm_input = dropout_out + \n    var = norm_input.mul(norm_input).mean(normalization_axis, keepdim)\n    pre_shift_scale_norm_output = norm_input / (var + 1e-12)\n    norm_output =  * pre_shift_scale_norm_output\n    return norm_output",
            "code"
        ],
        [
            "As before, we\u2019ll get a baseline by running PyTorch without nvFuser.",
            "markdown"
        ],
        [
            "# Profile rms_norm\nfunc = functools.partial(\n    with_rms_norm,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"Eager Mode - RMS Norm\")",
            "code"
        ],
        [
            "Eager Mode - RMS Norm\nAverage iterations per second: 83.70",
            "code"
        ],
        [
            "With nvFuser through TorchScript.",
            "markdown"
        ],
        [
            "# Profile scripted rms_norm\n = (with_rms_norm)\nfunc = functools.partial(\n    ,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"TorchScript - RMS Norm\")",
            "code"
        ],
        [
            "TorchScript - RMS Norm\nAverage iterations per second: 172.33",
            "code"
        ],
        [
            "With nvFuser through Functorch.",
            "markdown"
        ],
        [
            "def with_rms_norm_for_memory_efficient_fusion(\n    : , : , : , bias: \n) -&gt; :\n    bias_out =  + bias\n    dropout_out = (bias_out, 0.1)\n    norm_input = dropout_out + \n    var = norm_input.mul(norm_input).mean(2, keepdim=True)\n    pre_shift_scale_norm_output = norm_input / (var + 1e-12)\n    norm_output =  * pre_shift_scale_norm_output\n    return norm_output\n\n\n# Profile memory efficient rms_norm\nmemory_efficient_rms_norm = memory_efficient_fusion(\n    with_rms_norm_for_memory_efficient_fusion\n)\nfunc = functools.partial(memory_efficient_rms_norm, , , , )\nprofile_workload(func, , iteration_count=100, label=\"FuncTorch - RMS Norm\")",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torch/jit/_check.py:172: UserWarning:\n\nThe TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n\nFuncTorch - RMS Norm\nAverage iterations per second: 201.74\n\n\n\n<img alt=\"../_images/nvfuser_tutorial_6.png\" src=\"../_images/nvfuser_tutorial_6.png\"/>",
            "code"
        ],
        [
            "Since RMSNorm is simpler than LayerNorm the performance of our new\ntransformer block is a little higher than the primitive definition\nwithout nvFuser (354 iterations per second compared with 260\niterations per second). With TorchScript, the iterations per second\nincreases by 2.68x and 3.36x to 952 iterations per second and 1,191\niterations per second with TorchScript and FuncTorch\u2019s memory\nefficient optimization pass, respectively. The performance of this\nnew operation nearly matches the performance of the composite Layer\nNorm definition with TorchScript.",
            "markdown"
        ],
        [
            "nvFuser is here to provide the ability to define novel operations in\nsimple PyTorch and get performance that\u2019s close to a highly optimized\ncomposite operation in PyTorch. We believe this will enable research\ninto novel network topologies without paying for sometimes devastating\neffects on speed of training. nvFuser provides this unique ability as\nit\u2019s able to analyze users\u2019 programs to provide performance as fast as a\nhighly hand tuned implementation, regardless of how the operations are\ndefined. nvFuser still cannot support every operation in PyTorch,\nhowever its capabilities will continue to grow over time.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  19.393 seconds)",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Multi-Objective NAS with Ax": [
        [
            "<strong>Authors:</strong> ,\n,\nand the Adaptive Experimentation team at Meta.",
            "markdown"
        ],
        [
            "In this tutorial, we show how to use  to run\nmulti-objective neural architecture search (NAS) for a simple neural\nnetwork model on the popular MNIST dataset. While the underlying\nmethodology would typically be used for more complicated models and\nlarger datasets, we opt for a tutorial that is easily runnable\nend-to-end on a laptop in less than 20 minutes.",
            "markdown"
        ],
        [
            "In many NAS applications, there is a natural tradeoff between multiple\nobjectives of interest. For instance, when deploying models on-device\nwe may want to maximize model performance (for example, accuracy), while\nsimultaneously minimizing competing metrics like power consumption,\ninference latency, or model size in order to satisfy deployment\nconstraints. Often, we may be able to reduce computational requirements\nor latency of predictions substantially by accepting minimally lower\nmodel performance. Principled methods for exploring such tradeoffs\nefficiently are key enablers of scalable and sustainable AI, and have\nmany successful applications at Meta - see for instance our\n\non a Natural Language Understanding model.",
            "markdown"
        ],
        [
            "In our example here, we will tune the widths of two hidden layers,\nthe learning rate, the dropout probability, the batch size, and the\nnumber of training epochs. The goal is to trade off performance\n(accuracy on the validation set) and model size (the number of\nmodel parameters).",
            "markdown"
        ],
        [
            "This tutorial makes use of the following PyTorch libraries:",
            "markdown"
        ],
        [
            " (specifying the model and training loop)",
            "markdown"
        ],
        [
            " (for running training jobs remotely / asynchronously)",
            "markdown"
        ],
        [
            " (the Bayesian Optimization library powering Ax\u2019s algorithms)",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Multi-Objective NAS with Ax->Defining the TorchX App": [
        [
            "Our goal is to optimize the PyTorch Lightning training job defined in\n.\nTo do this using TorchX, we write a helper function that takes in\nthe values of the architcture and hyperparameters of the training\njob and creates a \nwith the appropriate settings.",
            "markdown"
        ],
        [
            "from pathlib import Path\n\nimport torchx\n\nfrom torchx import specs\nfrom torchx.components import utils\n\n\ndef trainer(\n    log_path: str,\n    hidden_size_1: int,\n    hidden_size_2: int,\n    learning_rate: float,\n    epochs: int,\n    dropout: float,\n    batch_size: int,\n    trial_idx: int = -1,\n) -&gt; specs.AppDef:\n\n    # define the log path so we can pass it to the TorchX AppDef\n    if trial_idx &gt;= 0:\n        log_path = Path(log_path).joinpath(str(trial_idx)).absolute().as_posix()\n\n    return utils.python(\n        # command line args to the training script\n        \"--log_path\",\n        log_path,\n        \"--hidden_size_1\",\n        str(hidden_size_1),\n        \"--hidden_size_2\",\n        str(hidden_size_2),\n        \"--learning_rate\",\n        str(learning_rate),\n        \"--epochs\",\n        str(epochs),\n        \"--dropout\",\n        str(dropout),\n        \"--batch_size\",\n        str(batch_size),\n        # other config options\n        name=\"trainer\",\n        script=\"mnist_train_nas.py\",\n        image=torchx.version.TORCHX_IMAGE,\n    )",
            "code"
        ]
    ],
    "torch->Model Optimization->Multi-Objective NAS with Ax->Setting up the Runner": [
        [
            "Ax\u2019s \nabstraction allows writing interfaces to various backends.\nAx already comes with Runner for TorchX, and so we just need to\nconfigure it. For the purpose of this tutorial we run jobs locally\nin a fully asynchronous fashion.",
            "markdown"
        ],
        [
            "In order to launch them on a cluster, you can instead specify a\ndifferent TorchX scheduler and adjust the configuration appropriately.\nFor example, if you have a Kubernetes cluster, you just need to change the\nscheduler from local_cwd to kubernetes).",
            "markdown"
        ],
        [
            "import tempfile\nfrom ax.runners.torchx import TorchXRunner\n\n# Make a temporary dir to log our results into\nlog_dir = tempfile.mkdtemp()\n\nax_runner = TorchXRunner(\n    tracker_base=\"/tmp/\",\n    component=trainer,\n    # NOTE: To launch this job on a cluster instead of locally you can\n    # specify a different scheduler and adjust args appropriately.\n    scheduler=\"local_cwd\",\n    component_const_params={\"log_path\": log_dir},\n    cfg={},\n)",
            "code"
        ]
    ],
    "torch->Model Optimization->Multi-Objective NAS with Ax->Setting up the SearchSpace": [
        [
            "First, we define our search space. Ax supports both range parameters\nof type integer and float as well as choice parameters which can have\nnon-numerical types such as strings.\nWe will tune the hidden sizes, learning rate, dropout, and number of\nepochs as range parameters and tune the batch size as an ordered choice\nparameter to enforce it to be a power of 2.",
            "markdown"
        ],
        [
            "from ax.core import (\n    ChoiceParameter,\n    ParameterType,\n    RangeParameter,\n    SearchSpace,\n)\n\nparameters = [\n    # NOTE: In a real-world setting, hidden_size_1 and hidden_size_2\n    # should probably be powers of 2, but in our simple example this\n    # would mean that num_params can't take on that many values, which\n    # in turn makes the Pareto frontier look pretty weird.\n    RangeParameter(\n        name=\"hidden_size_1\",\n        lower=16,\n        upper=128,\n        parameter_type=ParameterType.INT,\n        log_scale=True,\n    ),\n    RangeParameter(\n        name=\"hidden_size_2\",\n        lower=16,\n        upper=128,\n        parameter_type=ParameterType.INT,\n        log_scale=True,\n    ),\n    RangeParameter(\n        name=\"learning_rate\",\n        lower=1e-4,\n        upper=1e-2,\n        parameter_type=ParameterType.FLOAT,\n        log_scale=True,\n    ),\n    RangeParameter(\n        name=\"epochs\",\n        lower=1,\n        upper=4,\n        parameter_type=ParameterType.INT,\n    ),\n    RangeParameter(\n        name=\"dropout\",\n        lower=0.0,\n        upper=0.5,\n        parameter_type=ParameterType.FLOAT,\n    ),\n    ChoiceParameter(  # NOTE: ChoiceParameters don't require log-scale\n        name=\"batch_size\",\n        values=[32, 64, 128, 256],\n        parameter_type=ParameterType.INT,\n        is_ordered=True,\n        sort_values=True,\n    ),\n]\n\nsearch_space = SearchSpace(\n    parameters=parameters,\n    # NOTE: In practice, it may make sense to add a constraint\n    # hidden_size_2 &lt;= hidden_size_1\n    parameter_constraints=[],\n)",
            "code"
        ]
    ],
    "torch->Model Optimization->Multi-Objective NAS with Ax->Setting up Metrics": [
        [
            "Ax has the concept of a \nthat defines properties of outcomes and how observations are obtained\nfor these outcomes. This allows e.g. encodig how data is fetched from\nsome distributed execution backend and post-processed before being\npassed as input to Ax.",
            "markdown"
        ],
        [
            "In this tutorial we will use\n\nwith the goal of maximizing the validation accuracy and minimizing\nthe number of model parameters. The latter represents a simple proxy\nof model latency, which is hard to estimate accurately for small ML\nmodels (in an actual application we would benchmark the latency while\nrunning the model on-device).",
            "markdown"
        ],
        [
            "In our example TorchX will run the training jobs in a fully asynchronous\nfashion locally and write the results to the log_dir based on the trial\nindex (see the trainer() function above). We will define a metric\nclass that is aware of that logging directory. By subclassing\n\nwe get the logic to read and parse the Tensorboard logs for free.",
            "markdown"
        ],
        [
            "from ax.metrics.tensorboard import TensorboardCurveMetric\n\n\nclass MyTensorboardMetric(TensorboardCurveMetric):\n\n    # NOTE: We need to tell the new Tensorboard metric how to get the id /\n    # file handle for the tensorboard logs from a trial. In this case\n    # our convention is to just save a separate file per trial in\n    # the pre-specified log dir.\n    @classmethod\n    def get_ids_from_trials(cls, trials):\n        return {\n            trial.index: Path(log_dir).joinpath(str(trial.index)).as_posix()\n            for trial in trials\n        }\n\n    # This indicates whether the metric is queryable while the trial is\n    # still running. We don't use this in the current tutorial, but Ax\n    # utilizes this to implement trial-level early-stopping functionality.\n    @classmethod\n    def is_available_while_running(cls):\n        return False",
            "code"
        ],
        [
            "Now we can instatiate the metrics for accuracy and the number of\nmodel parameters. Here <cite>curve_name</cite> is the name of the metric in the\nTensorboard logs, while <cite>name</cite> is the metric name used internally\nby Ax. We also specify <cite>lower_is_better</cite> to indicate the favorable\ndirection of the two metrics.",
            "markdown"
        ],
        [
            "val_acc = MyTensorboardMetric(\n    name=\"val_acc\",\n    curve_name=\"val_acc\",\n    lower_is_better=False,\n)\nmodel_num_params = MyTensorboardMetric(\n    name=\"num_params\",\n    curve_name=\"num_params\",\n    lower_is_better=True,\n)",
            "code"
        ]
    ],
    "torch->Model Optimization->Multi-Objective NAS with Ax->Setting up the OptimizationConfig": [
        [
            "The way to tell Ax what it should optimize is by means of an\n.\nHere we use a MultiObjectiveOptimizationConfig as we will\nbe performing multi-objective optimization.",
            "markdown"
        ],
        [
            "Additionally, Ax supports placing constraints on the different\nmetrics by specifying objective thresholds, which bound the region\nof interest in the outcome space that we want to explore. For this\nexample, we will constrain the validation accuracy to be at least\n0.94 (94%) and the number of model parameters to be at most 80,000.",
            "markdown"
        ],
        [
            "from ax.core import MultiObjective, Objective, ObjectiveThreshold\nfrom ax.core.optimization_config import MultiObjectiveOptimizationConfig\n\n\nopt_config = MultiObjectiveOptimizationConfig(\n    objective=MultiObjective(\n        objectives=[\n            Objective(metric=val_acc, minimize=False),\n            Objective(metric=model_num_params, minimize=True),\n        ],\n    ),\n    objective_thresholds=[\n        ObjectiveThreshold(metric=val_acc, bound=0.94, relative=False),\n        ObjectiveThreshold(metric=model_num_params, bound=80_000, relative=False),\n    ],\n)",
            "code"
        ]
    ],
    "torch->Model Optimization->Multi-Objective NAS with Ax->Creating the Ax Experiment": [
        [
            "In Ax, the \nobject is the object that stores all the information about the problem\nsetup.",
            "markdown"
        ],
        [
            "from ax.core import Experiment\n\nexperiment = Experiment(\n    name=\"torchx_mnist\",\n    search_space=search_space,\n    optimization_config=opt_config,\n    runner=ax_runner,\n)",
            "code"
        ]
    ],
    "torch->Model Optimization->Multi-Objective NAS with Ax->Choosing the GenerationStrategy": [
        [
            "A \nis the abstract representation of how we would like to perform the\noptimization. While this can be customized (if you\u2019d like to do so, see\n),\nin most cases Ax can automatically determine an appropriate strategy\nbased on the search space, optimization config, and the total number\nof trials we want to run.",
            "markdown"
        ],
        [
            "Typically, Ax chooses to evaluate a number of random configurations\nbefore starting a model-based Bayesian Optimization strategy.",
            "markdown"
        ],
        [
            "total_trials = 48  # total evaluation budget\n\nfrom ax.modelbridge.dispatch_utils import choose_generation_strategy\n\ngs = choose_generation_strategy(\n    search_space=experiment.search_space,\n    optimization_config=experiment.optimization_config,\n    num_trials=total_trials,\n  )",
            "code"
        ]
    ],
    "torch->Model Optimization->Multi-Objective NAS with Ax->Configuring the Scheduler": [
        [
            "The <cite>Scheduler</cite> (TODO: link) acts as the loop control for the optimization.\nIt communicates with the backend to launch trials, check their status,\nand retrieve results. In the case of this tutorial, it is simply reading\nand parsing the locally saved logs. In a remote execution setting,\nit would call APIs. The following illustration from the Ax\n\nsummarizes how the Scheduler interacts with external systems used to run\ntrial evaluations:\n<img alt=\"../_static/img/ax_scheduler_illustration.png\" src=\"../_static/img/ax_scheduler_illustration.png\"/>",
            "markdown"
        ],
        [
            "The Scheduler requires the Experiment and the GenerationStrategy.\nA set of options can be passed in via SchedulerOptions. Here, we\nconfigure the number of total evaluations as well as max_pending_trials,\nthe maximum number of trials that should run concurrently. In our\nlocal setting, this is the number of training jobs running as individual\nprocesses, while in a remote execution setting, this would be the number\nof machines you want to use in parallel.",
            "markdown"
        ],
        [
            "from ax.service.scheduler import Scheduler, SchedulerOptions\n\nscheduler = Scheduler(\n    experiment=experiment,\n    generation_strategy=gs,\n    options=SchedulerOptions(\n        total_trials=total_trials, max_pending_trials=4\n    ),\n)",
            "code"
        ]
    ],
    "torch->Model Optimization->Multi-Objective NAS with Ax->Running the optimization": [
        [
            "Now that everything is configured, we can let Ax run the optimization\nin a fully automated fashion. The Scheduler will periodially check\nthe logs for the status of all currently running trials, and if a\ntrial completes the scheduler will update its status on the\nexperiment and fetch the observations needed for the Bayesian\noptimization algorithm.",
            "markdown"
        ],
        [
            "scheduler.run_all_trials()",
            "code"
        ]
    ],
    "torch->Model Optimization->Multi-Objective NAS with Ax->Evaluating the results": [
        [
            "We can now inspect the result of the optimization using helper\nfunctions and visualizations included with Ax.",
            "markdown"
        ],
        [
            "First, we generate a dataframe with a summary of the results\nof the experiment. Each row in this dataframe corresponds to a\ntrial (that is, a training job that was run), and contains information\non the status of the trial, the parameter configuration that was\nevaluated, and the metric values that were observed. This provides\nan easy way to sanity check the optimization.",
            "markdown"
        ],
        [
            "from ax.service.utils.report_utils import exp_to_df\n\ndf = exp_to_df(experiment)\ndf.head(10)",
            "code"
        ],
        [
            "We can also visualize the Pareto frontier of tradeoffs between the\nvalidation accuracy and the number of model parameters.",
            "markdown"
        ],
        [
            "Tip",
            "markdown"
        ],
        [
            "Ax uses Plotly to produce interactive plots, which allow you to\ndo things like zoom, crop, or hover in order to view details\nof components of the plot. Try it out, and take a look at the\n\nif you\u2019d like to learn more).",
            "markdown"
        ],
        [
            "The final optimization results are shown in the figure below where\nthe color corresponds to the iteration number for each trial.\nWe see that our method was able to successfully explore the\ntrade-offs and found both large models with high validation\naccuracy as well as small models with comparatively lower\nvalidation accuracy.",
            "markdown"
        ],
        [
            "from ax.service.utils.report_utils import _pareto_frontier_scatter_2d_plotly\n\n_pareto_frontier_scatter_2d_plotly(experiment)",
            "code"
        ],
        [
            "To better understand what our surrogate models have learned about\nthe black box objectives, we can take a look at the leave-one-out\ncross validation results. Since our models are Gaussian Processes,\nthey not only provide point predictions but also uncertainty estimates\nabout these predictions. A good model means that the predicted means\n(the points in the figure) are close to the 45 degree line and that the\nconfidence intervals cover the 45 degree line with the expected frequency\n(here we use 95% confidence intervals, so we would expect them to contain\nthe true observation 95% of the time).",
            "markdown"
        ],
        [
            "As the figures below show, the model size (num_params) metric is\nmuch easier to model than the validation accuracy (val_acc) metric.",
            "markdown"
        ],
        [
            "from ax.modelbridge.cross_validation import compute_diagnostics, cross_validate\nfrom ax.plot.diagnostic import interact_cross_validation_plotly\nfrom ax.utils.notebook.plotting import init_notebook_plotting, render\n\ncv = cross_validate(model=gs.model)  # The surrogate model is stored on the GenerationStrategy\ncompute_diagnostics(cv)\n\ninteract_cross_validation_plotly(cv)",
            "code"
        ],
        [
            "We can also make contour plots to better understand how the different\nobjectives depend on two of the input parameters. In the figure below,\nwe show the validation accuracy predicted by the model as a function\nof the two hidden sizes. The validation accuracy clearly increases\nas the hidden sizes increase.",
            "markdown"
        ],
        [
            "from ax.plot.contour import interact_contour_plotly\n\ninteract_contour_plotly(model=gs.model, metric_name=\"val_acc\")",
            "code"
        ],
        [
            "Similarly, we show the number of model parameters as a function of\nthe hidden sizes in the figure below and see that it also increases\nas a function of the hidden sizes (the dependency on hidden_size_1\nis much larger).",
            "markdown"
        ],
        [
            "interact_contour_plotly(model=gs.model, metric_name=\"num_params\")",
            "code"
        ]
    ],
    "torch->Model Optimization->Multi-Objective NAS with Ax->Acknowledgements": [
        [
            "We thank the TorchX team (in particular Kiuk Chung and Tristan Rice)\nfor their help with integrating TorchX with Ax.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->Model Optimization->torch.compile Tutorial": [
        [
            "<strong>Author:</strong> William Wen",
            "markdown"
        ],
        [
            "torch.compile is the latest method to speed up your PyTorch code!\ntorch.compile makes PyTorch code run faster by\nJIT-compiling PyTorch code into optimized kernels,\nall while requiring minimal code changes.",
            "markdown"
        ],
        [
            "In this tutorial, we cover basic torch.compile usage,\nand demonstrate the advantages of torch.compile over\nprevious PyTorch compiler solutions, such as\n and\n.",
            "markdown"
        ],
        [
            "<strong>Contents</strong>",
            "markdown"
        ],
        [
            "Basic Usage",
            "markdown"
        ],
        [
            "Demonstrating Speedups",
            "markdown"
        ],
        [
            "Comparison to TorchScript and FX Tracing",
            "markdown"
        ],
        [
            "TorchDynamo and FX Graphs",
            "markdown"
        ],
        [
            "Conclusion",
            "markdown"
        ],
        [
            "<strong>Required pip Dependencies</strong>",
            "markdown"
        ],
        [
            "torch &gt;= 2.0",
            "markdown"
        ],
        [
            "torchvision",
            "markdown"
        ],
        [
            "numpy",
            "markdown"
        ],
        [
            "scipy",
            "markdown"
        ],
        [
            "tabulate",
            "markdown"
        ],
        [
            "Note: a modern NVIDIA GPU (Volta or Ampere) is recommended for this tutorial.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->torch.compile Tutorial->Basic Usage": [
        [
            "torch.compile is included in the latest PyTorch nightlies.\nRunning TorchInductor on GPU requires Triton, which is included with the PyTorch 2.0 nightly\nbinary. If Triton is still missing, try installing torchtriton via pip\n(pip install torchtriton --extra-index-url \"https://download.pytorch.org/whl/nightly/cu117\"\nfor CUDA 11.7).",
            "markdown"
        ],
        [
            "Arbitrary Python functions can be optimized by passing the callable to\ntorch.compile. We can then call the returned optimized\nfunction in place of the original function.",
            "markdown"
        ],
        [
            "import torch\n\ndef foo(x, y):\n    a = (x)\n    b = (x)\n    return a + b\nopt_foo1 = (foo)\nprint(opt_foo1((10, 10), (10, 10)))",
            "code"
        ],
        [
            "tensor([[ 0.1798,  1.2844,  1.1928, -0.6577,  1.2001,  1.2458,  1.3295,  1.0679,\n         -0.5651,  1.2174],\n        [ 0.4180,  0.8621,  0.4904,  1.2344,  1.3135,  1.0205,  1.1573,  1.4137,\n          0.6963,  0.1998],\n        [ 0.8186,  1.3638,  1.1769,  1.1858,  0.1157,  0.5435,  1.2660,  0.0452,\n          1.4071,  1.2790],\n        [ 0.1720,  0.7816,  1.1149,  0.2946,  1.0323,  1.3251,  1.2137,  0.7413,\n          1.3243,  1.3410],\n        [-0.5763, -0.8852,  0.0997,  0.5206,  1.2721, -0.8215,  1.1307,  0.6280,\n          0.9548, -0.4519],\n        [ 0.5277,  1.1179,  0.8299,  1.1068,  1.1056, -0.9118,  1.1369,  0.1787,\n          1.4070,  1.2695],\n        [ 1.1402,  1.3995,  1.3867,  1.4065, -0.1616,  1.3379,  0.5723,  1.4052,\n          0.9187,  0.7396],\n        [ 1.4136,  0.9392,  1.1151, -0.1921,  0.6561,  1.3462, -0.8778,  0.0633,\n         -0.8164,  1.3842],\n        [-0.2066,  1.3659,  1.4083,  1.1795,  0.8535, -1.2620,  0.5423, -0.1378,\n          0.9112,  1.4097],\n        [-0.7677,  0.8684, -0.4427,  1.2253,  1.3321,  1.2597,  1.1444,  0.5330,\n          1.2131,  0.6155]])",
            "code"
        ],
        [
            "Alternatively, we can decorate the function.",
            "markdown"
        ],
        [
            "@torch.compile\ndef opt_foo2(x, y):\n    a = (x)\n    b = (x)\n    return a + b\nprint(opt_foo2((10, 10), (10, 10)))",
            "code"
        ],
        [
            "tensor([[ 1.3819, -0.0023,  0.3889, -0.3917,  0.3476,  0.8949, -0.1183,  1.2301,\n         -0.7860,  0.5604],\n        [ 1.2832,  0.9980, -1.3994,  0.3217,  1.4017,  1.1034,  1.3659,  1.2822,\n          0.8117, -1.4010],\n        [ 0.0598,  1.3736,  1.1738,  1.4119,  0.9839,  1.2038,  1.4142, -0.9450,\n         -0.6137,  1.3810],\n        [ 0.1338,  1.4103,  0.9247,  0.5043,  1.4142, -0.2322,  1.0996,  0.8513,\n          0.9817, -0.7160],\n        [ 0.8685,  1.1138, -0.5582,  0.7401,  1.0455,  0.6791,  1.3404,  0.9539,\n          0.6790,  0.5284],\n        [ 0.8021,  1.3120, -1.0966,  1.2934, -0.0197, -1.1680,  0.5190,  1.3328,\n         -0.6529, -0.8327],\n        [ 0.2134, -0.8958,  1.2135, -0.7837,  0.8079,  0.9630,  1.3625,  0.7420,\n          1.1731,  1.3631],\n        [ 1.3967, -0.9851,  0.8375,  1.0817, -0.0596, -0.4823,  1.0790,  0.1602,\n         -0.3130,  1.2681],\n        [ 1.3737,  1.0730,  1.3989,  0.8458, -1.1168,  0.7682,  1.2389,  0.8003,\n          1.2728,  1.3231],\n        [-0.3265,  1.1508, -0.0631,  1.3986,  1.0830,  0.8830,  1.1943, -0.0505,\n         -0.8125,  0.9741]])",
            "code"
        ],
        [
            "We can also optimize torch.nn.Module instances.",
            "markdown"
        ],
        [
            "class MyModule():\n    def __init__(self):\n        super().__init__()\n        self.lin = (100, 10)\n\n    def forward(self, x):\n        return (self.lin(x))\n\nmod = ()\n = (mod)\nprint(((10, 100)))",
            "code"
        ],
        [
            "tensor([[0.6872, -0.0000, 0.6799, 0.8060, -0.0000, -0.0000, 0.4238, 0.1070, 0.1104,\n         -0.0000],\n        [-0.0000, 0.0631, -0.0000, 1.3911, 0.0685, 0.0367, -0.0000, 0.4435, -0.0000,\n         -0.0000],\n        [-0.0000, 0.8475, -0.0000, -0.0000, 1.2670, 0.0052, -0.0000, -0.0000, 1.4696,\n         0.7722],\n        [-0.0000, 0.3604, 0.7189, 0.1741, -0.0000, 0.0089, 0.4830, -0.0000, -0.0000,\n         -0.0000],\n        [0.2388, -0.0000, 0.3756, 0.1995, -0.0000, 0.3563, 1.1686, -0.0000, -0.0000,\n         -0.0000],\n        [-0.0000, 0.3833, 0.2159, -0.0000, 0.0479, -0.0000, -0.0000, -0.0000, 0.3953,\n         0.7013],\n        [-0.0000, 0.6361, -0.0000, -0.0000, 1.6845, 0.3640, -0.0000, 0.0496, 0.7504,\n         0.4540],\n        [-0.0000, -0.0000, 0.8502, -0.0000, 0.1043, -0.0000, -0.0000, 0.1842, 0.0391,\n         0.3200],\n        [-0.0000, -0.0000, -0.0000, 0.7361, -0.0000, -0.0000, -0.0000, 0.4034, -0.0000,\n         1.0752],\n        [0.7645, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, 1.0489,\n         -0.0000]], grad_fn=&lt;CompiledFunctionBackward&gt;)",
            "code"
        ]
    ],
    "torch->Model Optimization->torch.compile Tutorial->Demonstrating Speedups": [
        [
            "Let\u2019s now demonstrate that using torch.compile can speed\nup real models. We will compare standard eager mode and\ntorch.compile by evaluating and training ResNet-18 on random data.",
            "markdown"
        ],
        [
            "Before we start, we need to define some utility functions.",
            "markdown"
        ],
        [
            "# Returns the result of running `fn()` and the time it took for `fn()` to run,\n# in seconds. We use CUDA events and synchronization for the most accurate\n# measurements.\ndef timed(fn):\n    start = (enable_timing=True)\n    end = (enable_timing=True)\n    start.record()\n    result = fn()\n    end.record()\n    ()\n    return result, start.elapsed_time(end) / 1000\n\n# Generates random input and targets data for the model, where `b` is\n# batch size.\ndef generate_data(b):\n    return (\n        (b, 3, 128, 128).to().cuda(),\n        (1000, (b,)).cuda(),\n    )\n\nN_ITERS = 10\n\nfrom torchvision.models import \ndef init_model():\n    return ().to().cuda()",
            "code"
        ],
        [
            "First, let\u2019s compare inference.",
            "markdown"
        ],
        [
            "Note that in the call to torch.compile, we have have the additional\nmode kwarg, which we will discuss below.",
            "markdown"
        ],
        [
            "def evaluate(mod, inp):\n    return mod(inp)\n\nmodel = init_model()\n\n# Reset since we are using a different mode.\nimport torch._dynamo\n()\n\nevaluate_opt = (evaluate, mode=\"reduce-overhead\")\n\ninp = generate_data(16)[0]\nprint(\"eager:\", timed(lambda: evaluate(model, inp))[1])\nprint(\"compile:\", timed(lambda: evaluate_opt(model, inp))[1])",
            "code"
        ],
        [
            "eager: 2.106290283203125\ncompile: 10.0058740234375",
            "code"
        ],
        [
            "Notice that torch.compile takes a lot longer to complete\ncompared to eager. This is because torch.compile compiles\nthe model into optimized kernels as it executes. In our example, the\nstructure of the model doesn\u2019t change, and so recompilation is not\nneeded. So if we run our optimized model several more times, we should\nsee a significant improvement compared to eager.",
            "markdown"
        ],
        [
            "eager_times = []\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)[0]\n    _, eager_time = timed(lambda: evaluate(model, inp))\n    eager_times.append(eager_time)\n    print(f\"eager eval time {i}: {eager_time}\")\n\nprint(\"~\" * 10)\n\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)[0]\n    _, compile_time = timed(lambda: evaluate_opt(model, inp))\n    compile_times.append(compile_time)\n    print(f\"compile eval time {i}: {compile_time}\")\nprint(\"~\" * 10)\n\nimport numpy as np\neager_med = np.median(eager_times)\ncompile_med = np.median(compile_times)\nspeedup = eager_med / compile_med\nprint(f\"(eval) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\nprint(\"~\" * 10)",
            "code"
        ],
        [
            "eager eval time 0: 0.009543840408325194\neager eval time 1: 0.008958080291748046\neager eval time 2: 0.0089717435836792\neager eval time 3: 0.008945695877075195\neager eval time 4: 0.008947936058044434\neager eval time 5: 0.008927264213562013\neager eval time 6: 0.008937503814697266\neager eval time 7: 0.008948351860046387\neager eval time 8: 0.008941663742065429\neager eval time 9: 0.00894159984588623\n~~~~~~~~~~\ncompile eval time 0: 0.009375519752502441\ncompile eval time 1: 0.009378111839294434\ncompile eval time 2: 0.009247039794921875\ncompile eval time 3: 0.009245856285095215\ncompile eval time 4: 0.008823455810546875\ncompile eval time 5: 0.00714246416091919\ncompile eval time 6: 0.007155712127685547\ncompile eval time 7: 0.007184415817260742\ncompile eval time 8: 0.007147552013397217\ncompile eval time 9: 0.007140128135681152\n~~~~~~~~~~\n(eval) eager median: 0.008946815967559814, compile median: 0.00800393581390381, speedup: 1.1178020633321555x\n~~~~~~~~~~",
            "code"
        ],
        [
            "And indeed, we can see that running our model with torch.compile\nresults in a significant speedup. Speedup mainly comes from reducing Python overhead and\nGPU read/writes, and so the observed speedup may vary on factors such as model\narchitecture and batch size. For example, if a model\u2019s architecture is simple\nand the amount of data is large, then the bottleneck would be\nGPU compute and the observed speedup may be less significant.",
            "markdown"
        ],
        [
            "You may also see different speedup results depending on the chosen mode\nkwarg. Since our model and data are small, we want to reduce overhead as\nmuch as possible, and so we chose \"reduce-overhead\". For your own models,\nyou may need to experiment with different modes to maximize speedup. You can\nread more about modes .",
            "markdown"
        ],
        [
            "For general PyTorch benchmarking, you can try using torch.utils.benchmark instead of the timed\nfunction we defined above. We wrote our own timing function in this tutorial to show\ntorch.compile\u2019s compilation latency.",
            "markdown"
        ],
        [
            "Now, let\u2019s consider comparing training.",
            "markdown"
        ],
        [
            "model = init_model()\n = (())\n\ndef train(mod, data):\n    (True)\n    pred = mod(data[0])\n    loss = ()(pred, data[1])\n    loss.backward()\n    ()\n\neager_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)\n    _, eager_time = timed(lambda: train(model, inp))\n    eager_times.append(eager_time)\n    print(f\"eager train time {i}: {eager_time}\")\nprint(\"~\" * 10)\n\nmodel = init_model()\n = (())\ntrain_opt = (train, mode=\"reduce-overhead\")\n\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)\n    _, compile_time = timed(lambda: train_opt(model, inp))\n    compile_times.append(compile_time)\n    print(f\"compile train time {i}: {compile_time}\")\nprint(\"~\" * 10)\n\neager_med = np.median(eager_times)\ncompile_med = np.median(compile_times)\nspeedup = eager_med / compile_med\nprint(f\"(train) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\nprint(\"~\" * 10)",
            "code"
        ],
        [
            "eager train time 0: 0.4737531127929687\neager train time 1: 0.02575574493408203\neager train time 2: 0.025766271591186524\neager train time 3: 0.02569219207763672\neager train time 4: 0.02566761589050293\neager train time 5: 0.026013599395751954\neager train time 6: 0.025726463317871092\neager train time 7: 0.025708160400390624\neager train time 8: 0.025737247467041015\neager train time 9: 0.02180371284484863\n~~~~~~~~~~\ncompile train time 0: 22.97644140625\ncompile train time 1: 0.02164531135559082\ncompile train time 2: 0.02067158317565918\ncompile train time 3: 0.02125984001159668\ncompile train time 4: 0.021174720764160156\ncompile train time 5: 0.020945056915283203\ncompile train time 6: 0.021309440612792968\ncompile train time 7: 0.021041824340820314\ncompile train time 8: 0.020995264053344728\ncompile train time 9: 0.021283327102661134\n~~~~~~~~~~\n(train) eager median: 0.02573185539245605, compile median: 0.02121728038787842, speedup: 1.2127782129493299x\n~~~~~~~~~~",
            "code"
        ],
        [
            "Again, we can see that torch.compile takes longer in the first\niteration, as it must compile the model, but in subsequent iterations, we see\nsignificant speedups compared to eager.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->torch.compile Tutorial->Comparison to TorchScript and FX Tracing": [
        [
            "We have seen that torch.compile can speed up PyTorch code.\nWhy else should we use torch.compile over existing PyTorch\ncompiler solutions, such as TorchScript or FX Tracing? Primarily, the\nadvantage of torch.compile lies in its ability to handle\narbitrary Python code with minimal changes to existing code.",
            "markdown"
        ],
        [
            "One case that torch.compile can handle that other compiler\nsolutions struggle with is data-dependent control flow (the\nif x.sum() &lt; 0: line below).",
            "markdown"
        ],
        [
            "def f1(x, y):\n    if x.sum() &lt; 0:\n        return -y\n    return y\n\n# Test that `fn1` and `fn2` return the same result, given\n# the same arguments `args`. Typically, `fn1` will be an eager function\n# while `fn2` will be a compiled function (torch.compile, TorchScript, or FX graph).\ndef test_fns(fn1, fn2, args):\n    out1 = fn1(*args)\n    out2 = fn2(*args)\n    return (out1, out2)\n\n = (5, 5)\n = (5, 5)",
            "code"
        ],
        [
            "TorchScript tracing f1 results in\nsilently incorrect results, since only the actual control flow path\nis traced.",
            "markdown"
        ],
        [
            " = (f1, (, ))\nprint(\"traced 1, 1:\", test_fns(f1, , (, )))\nprint(\"traced 1, 2:\", test_fns(f1, , (-, )))",
            "code"
        ],
        [
            "/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py:254: TracerWarning:\n\nConverting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n\ntraced 1, 1: True\ntraced 1, 2: False",
            "code"
        ],
        [
            "FX tracing f1 results in an error due to the presence of\ndata-dependent control flow.",
            "markdown"
        ],
        [
            "import traceback as tb\ntry:\n    (f1)\nexcept:\n    tb.print_exc()",
            "code"
        ],
        [
            "Traceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 284, in &lt;module&gt;\n    torch.fx.symbolic_trace(f1)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 1109, in symbolic_trace\n    graph = tracer.trace(root, concrete_args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 778, in trace\n    (self.create_arg(fn(*args)),),\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 254, in f1\n    if x.sum() &lt; 0:\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 413, in __bool__\n    return self.tracer.to_bool(self)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 276, in to_bool\n    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow",
            "code"
        ],
        [
            "If we provide a value for x as we try to FX trace f1, then\nwe run into the same problem as TorchScript tracing, as the data-dependent\ncontrol flow is removed in the traced function.",
            "markdown"
        ],
        [
            "fx_f1 = (f1, concrete_args={\"x\": })\nprint(\"fx 1, 1:\", test_fns(f1, fx_f1, (, )))\nprint(\"fx 1, 2:\", test_fns(f1, fx_f1, (-, )))",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:602: UserWarning:\n\nWas not able to add assertion to guarantee correct input x to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n\nfx 1, 1: True\nfx 1, 2: False",
            "code"
        ],
        [
            "Now we can see that torch.compile correctly handles\ndata-dependent control flow.",
            "markdown"
        ],
        [
            "# Reset since we are using a different mode.\n()\n\ncompile_f1 = (f1)\nprint(\"compile 1, 1:\", test_fns(f1, compile_f1, (, )))\nprint(\"compile 1, 2:\", test_fns(f1, compile_f1, (-, )))\nprint(\"~\" * 10)",
            "code"
        ],
        [
            "compile 1, 1: True\ncompile 1, 2: True\n~~~~~~~~~~",
            "code"
        ],
        [
            "TorchScript scripting can handle data-dependent control flow, but this\nsolution comes with its own set of problems. Namely, TorchScript scripting\ncan require major code changes and will raise errors when unsupported Python\nis used.",
            "markdown"
        ],
        [
            "In the example below, we forget TorchScript type annotations and we receive\na TorchScript error because the input type for argument y, an int,\ndoes not match with the default argument type, torch.Tensor.",
            "markdown"
        ],
        [
            "def f2(x, y):\n    return x + y\n\n = (5, 5)\n = 3\n\n = (f2)\ntry:\n    (, )\nexcept:\n    tb.print_exc()",
            "code"
        ],
        [
            "Traceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 327, in &lt;module&gt;\n    script_f2(inp1, inp2)\nRuntimeError: f2() Expected a value of type 'Tensor (inferred)' for argument 'y' but instead found type 'int'.\nInferred 'y' to be of type 'Tensor' because it was not annotated with an explicit type.\nPosition: 1\nValue: 3\nDeclaration: f2(Tensor x, Tensor y) -&gt; Tensor\nCast error details: Unable to cast 3 to Tensor",
            "code"
        ],
        [
            "However, torch.compile is easily able to handle f2.",
            "markdown"
        ],
        [
            "compile_f2 = (f2)\nprint(\"compile 2:\", test_fns(f2, compile_f2, (, )))\nprint(\"~\" * 10)",
            "code"
        ],
        [
            "compile 2: True\n~~~~~~~~~~",
            "code"
        ],
        [
            "Another case that torch.compile handles well compared to\nprevious compilers solutions is the usage of non-PyTorch functions.",
            "markdown"
        ],
        [
            "import scipy\ndef f3(x):\n    x = x * 2\n    x = scipy.fft.dct(x.numpy())\n    x = (x)\n    x = x * 2\n    return x",
            "code"
        ],
        [
            "TorchScript tracing treats results from non-PyTorch function calls\nas constants, and so our results can be silently wrong.",
            "markdown"
        ],
        [
            " = (5, 5)\n = (5, 5)\n = (f3, (,))\nprint(\"traced 3:\", test_fns(f3, , (,)))",
            "code"
        ],
        [
            "/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py:345: TracerWarning:\n\nConverting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n\n/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py:346: TracerWarning:\n\ntorch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n\ntraced 3: False",
            "code"
        ],
        [
            "TorchScript scripting and FX tracing disallow non-PyTorch function calls.",
            "markdown"
        ],
        [
            "try:\n    (f3)\nexcept:\n    tb.print_exc()\n\ntry:\n    (f3)\nexcept:\n    tb.print_exc()",
            "code"
        ],
        [
            "Traceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 363, in &lt;module&gt;\n    torch.jit.script(f3)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/jit/_script.py\", line 1341, in script\n    fn = torch._C._jit_script_compile(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_jit_internal.py\", line 1198, in _try_get_dispatched_fn\n    return boolean_dispatched.get(fn)\n  File \"/opt/conda/lib/python3.10/weakref.py\", line 453, in get\n    return self.data.get(ref(key),default)\nTypeError: cannot create weak reference to 'uarray._Function' object\nTraceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 368, in &lt;module&gt;\n    torch.fx.symbolic_trace(f3)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 1109, in symbolic_trace\n    graph = tracer.trace(root, concrete_args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 778, in trace\n    (self.create_arg(fn(*args)),),\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 345, in f3\n    x = scipy.fft.dct(x.numpy())\n  File \"/opt/conda/lib/python3.10/site-packages/scipy/fft/_backend.py\", line 25, in __ua_function__\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/scipy/fft/_pocketfft/realtransforms.py\", line 19, in _r2r\n    tmp = _asfarray(x)\n  File \"/opt/conda/lib/python3.10/site-packages/scipy/fft/_pocketfft/helper.py\", line 89, in _asfarray\n    if x.dtype == np.float16:\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 518, in impl\n    return tracer.create_proxy('call_function', target, args, kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 151, in create_proxy\n    args_ = self.create_arg(args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 373, in create_arg\n    return super().create_arg(a)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 239, in create_arg\n    return type(a)(self.create_arg(elem) for elem in a)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 239, in &lt;genexpr&gt;\n    return type(a)(self.create_arg(elem) for elem in a)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 373, in create_arg\n    return super().create_arg(a)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 267, in create_arg\n    raise NotImplementedError(f\"argument of type: {type(a)}\")\nNotImplementedError: argument of type: &lt;class 'type'&gt;",
            "code"
        ],
        [
            "In comparison, torch.compile is easily able to handle\nthe non-PyTorch function call.",
            "markdown"
        ],
        [
            "compile_f3 = (f3)\nprint(\"compile 3:\", test_fns(f3, compile_f3, (,)))",
            "code"
        ],
        [
            "compile 3: True",
            "code"
        ]
    ],
    "torch->Model Optimization->torch.compile Tutorial->TorchDynamo and FX Graphs": [
        [
            "One important component of torch.compile is TorchDynamo.\nTorchDynamo is responsible for JIT compiling arbitrary Python code into\n, which can\nthen be further optimized. TorchDynamo extracts FX graphs by analyzing Python bytecode\nduring runtime and detecting calls to PyTorch operations.",
            "markdown"
        ],
        [
            "Normally, TorchInductor, another component of torch.compile,\nfurther compiles the FX graphs into optimized kernels,\nbut TorchDynamo allows for different backends to be used. In order to inspect\nthe FX graphs that TorchDynamo outputs, let us create a custom backend that\noutputs the FX graph and simply returns the graph\u2019s unoptimized forward method.",
            "markdown"
        ],
        [
            "from typing import List\ndef custom_backend(gm: , example_inputs: List[]):\n    print(\"custom backend called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward\n\n# Reset since we are using a different backend.\n()\n\n = (init_model(), backend=custom_backend)\n(generate_data(16)[0])",
            "code"
        ],
        [
            "custom backend called with FX graph:\nopcode         name                        target                                                      args                                             kwargs\n-------------  --------------------------  ----------------------------------------------------------  -----------------------------------------------  --------\nplaceholder    x                           x                                                           ()                                               {}\ncall_module    self_conv1                  self_conv1                                                  (x,)                                             {}\ncall_module    self_bn1                    self_bn1                                                    (self_conv1,)                                    {}\ncall_module    self_relu                   self_relu                                                   (self_bn1,)                                      {}\ncall_module    self_maxpool                self_maxpool                                                (self_relu,)                                     {}\ncall_module    self_layer1_0_conv1         self_layer1_0_conv1                                         (self_maxpool,)                                  {}\ncall_module    self_layer1_0_bn1           self_layer1_0_bn1                                           (self_layer1_0_conv1,)                           {}\ncall_module    self_layer1_0_relu          self_layer1_0_relu                                          (self_layer1_0_bn1,)                             {}\ncall_module    self_layer1_0_conv2         self_layer1_0_conv2                                         (self_layer1_0_relu,)                            {}\ncall_module    self_layer1_0_bn2           self_layer1_0_bn2                                           (self_layer1_0_conv2,)                           {}\ncall_function  iadd                        &lt;built-in function iadd&gt;                                    (self_layer1_0_bn2, self_maxpool)                {}\ncall_module    self_layer1_0_relu_1        self_layer1_0_relu                                          (iadd,)                                          {}\ncall_module    self_layer1_1_conv1         self_layer1_1_conv1                                         (self_layer1_0_relu_1,)                          {}\ncall_module    self_layer1_1_bn1           self_layer1_1_bn1                                           (self_layer1_1_conv1,)                           {}\ncall_module    self_layer1_1_relu          self_layer1_1_relu                                          (self_layer1_1_bn1,)                             {}\ncall_module    self_layer1_1_conv2         self_layer1_1_conv2                                         (self_layer1_1_relu,)                            {}\ncall_module    self_layer1_1_bn2           self_layer1_1_bn2                                           (self_layer1_1_conv2,)                           {}\ncall_function  iadd_1                      &lt;built-in function iadd&gt;                                    (self_layer1_1_bn2, self_layer1_0_relu_1)        {}\ncall_module    self_layer1_1_relu_1        self_layer1_1_relu                                          (iadd_1,)                                        {}\ncall_module    self_layer2_0_conv1         self_layer2_0_conv1                                         (self_layer1_1_relu_1,)                          {}\ncall_module    self_layer2_0_bn1           self_layer2_0_bn1                                           (self_layer2_0_conv1,)                           {}\ncall_module    self_layer2_0_relu          self_layer2_0_relu                                          (self_layer2_0_bn1,)                             {}\ncall_module    self_layer2_0_conv2         self_layer2_0_conv2                                         (self_layer2_0_relu,)                            {}\ncall_module    self_layer2_0_bn2           self_layer2_0_bn2                                           (self_layer2_0_conv2,)                           {}\ncall_module    self_layer2_0_downsample_0  self_layer2_0_downsample_0                                  (self_layer1_1_relu_1,)                          {}\ncall_module    self_layer2_0_downsample_1  self_layer2_0_downsample_1                                  (self_layer2_0_downsample_0,)                    {}\ncall_function  iadd_2                      &lt;built-in function iadd&gt;                                    (self_layer2_0_bn2, self_layer2_0_downsample_1)  {}\ncall_module    self_layer2_0_relu_1        self_layer2_0_relu                                          (iadd_2,)                                        {}\ncall_module    self_layer2_1_conv1         self_layer2_1_conv1                                         (self_layer2_0_relu_1,)                          {}\ncall_module    self_layer2_1_bn1           self_layer2_1_bn1                                           (self_layer2_1_conv1,)                           {}\ncall_module    self_layer2_1_relu          self_layer2_1_relu                                          (self_layer2_1_bn1,)                             {}\ncall_module    self_layer2_1_conv2         self_layer2_1_conv2                                         (self_layer2_1_relu,)                            {}\ncall_module    self_layer2_1_bn2           self_layer2_1_bn2                                           (self_layer2_1_conv2,)                           {}\ncall_function  iadd_3                      &lt;built-in function iadd&gt;                                    (self_layer2_1_bn2, self_layer2_0_relu_1)        {}\ncall_module    self_layer2_1_relu_1        self_layer2_1_relu                                          (iadd_3,)                                        {}\ncall_module    self_layer3_0_conv1         self_layer3_0_conv1                                         (self_layer2_1_relu_1,)                          {}\ncall_module    self_layer3_0_bn1           self_layer3_0_bn1                                           (self_layer3_0_conv1,)                           {}\ncall_module    self_layer3_0_relu          self_layer3_0_relu                                          (self_layer3_0_bn1,)                             {}\ncall_module    self_layer3_0_conv2         self_layer3_0_conv2                                         (self_layer3_0_relu,)                            {}\ncall_module    self_layer3_0_bn2           self_layer3_0_bn2                                           (self_layer3_0_conv2,)                           {}\ncall_module    self_layer3_0_downsample_0  self_layer3_0_downsample_0                                  (self_layer2_1_relu_1,)                          {}\ncall_module    self_layer3_0_downsample_1  self_layer3_0_downsample_1                                  (self_layer3_0_downsample_0,)                    {}\ncall_function  iadd_4                      &lt;built-in function iadd&gt;                                    (self_layer3_0_bn2, self_layer3_0_downsample_1)  {}\ncall_module    self_layer3_0_relu_1        self_layer3_0_relu                                          (iadd_4,)                                        {}\ncall_module    self_layer3_1_conv1         self_layer3_1_conv1                                         (self_layer3_0_relu_1,)                          {}\ncall_module    self_layer3_1_bn1           self_layer3_1_bn1                                           (self_layer3_1_conv1,)                           {}\ncall_module    self_layer3_1_relu          self_layer3_1_relu                                          (self_layer3_1_bn1,)                             {}\ncall_module    self_layer3_1_conv2         self_layer3_1_conv2                                         (self_layer3_1_relu,)                            {}\ncall_module    self_layer3_1_bn2           self_layer3_1_bn2                                           (self_layer3_1_conv2,)                           {}\ncall_function  iadd_5                      &lt;built-in function iadd&gt;                                    (self_layer3_1_bn2, self_layer3_0_relu_1)        {}\ncall_module    self_layer3_1_relu_1        self_layer3_1_relu                                          (iadd_5,)                                        {}\ncall_module    self_layer4_0_conv1         self_layer4_0_conv1                                         (self_layer3_1_relu_1,)                          {}\ncall_module    self_layer4_0_bn1           self_layer4_0_bn1                                           (self_layer4_0_conv1,)                           {}\ncall_module    self_layer4_0_relu          self_layer4_0_relu                                          (self_layer4_0_bn1,)                             {}\ncall_module    self_layer4_0_conv2         self_layer4_0_conv2                                         (self_layer4_0_relu,)                            {}\ncall_module    self_layer4_0_bn2           self_layer4_0_bn2                                           (self_layer4_0_conv2,)                           {}\ncall_module    self_layer4_0_downsample_0  self_layer4_0_downsample_0                                  (self_layer3_1_relu_1,)                          {}\ncall_module    self_layer4_0_downsample_1  self_layer4_0_downsample_1                                  (self_layer4_0_downsample_0,)                    {}\ncall_function  iadd_6                      &lt;built-in function iadd&gt;                                    (self_layer4_0_bn2, self_layer4_0_downsample_1)  {}\ncall_module    self_layer4_0_relu_1        self_layer4_0_relu                                          (iadd_6,)                                        {}\ncall_module    self_layer4_1_conv1         self_layer4_1_conv1                                         (self_layer4_0_relu_1,)                          {}\ncall_module    self_layer4_1_bn1           self_layer4_1_bn1                                           (self_layer4_1_conv1,)                           {}\ncall_module    self_layer4_1_relu          self_layer4_1_relu                                          (self_layer4_1_bn1,)                             {}\ncall_module    self_layer4_1_conv2         self_layer4_1_conv2                                         (self_layer4_1_relu,)                            {}\ncall_module    self_layer4_1_bn2           self_layer4_1_bn2                                           (self_layer4_1_conv2,)                           {}\ncall_function  iadd_7                      &lt;built-in function iadd&gt;                                    (self_layer4_1_bn2, self_layer4_0_relu_1)        {}\ncall_module    self_layer4_1_relu_1        self_layer4_1_relu                                          (iadd_7,)                                        {}\ncall_module    self_avgpool                self_avgpool                                                (self_layer4_1_relu_1,)                          {}\ncall_function  flatten                     &lt;built-in method flatten of type object at 0x7f051b9b3540&gt;  (self_avgpool, 1)                                {}\ncall_module    self_fc                     self_fc                                                     (flatten,)                                       {}\noutput         output                      output                                                      ((self_fc,),)                                    {}\n\ntensor([[-1.3284, -0.5548, -0.2535,  ...,  0.0955, -0.5363, -0.3258],\n        [-1.3473, -0.4153, -0.1871,  ...,  0.0867, -0.4691, -0.0939],\n        [-1.0568, -0.3973, -0.1293,  ...,  0.1281, -0.6533, -0.5252],\n        ...,\n        [-0.9553, -0.1680, -0.1153,  ...,  0.0278, -0.7660, -0.4412],\n        [-1.1365, -0.3513, -0.3417,  ..., -0.1511, -0.8385, -0.4293],\n        [-1.2228, -0.3285, -0.4461,  ...,  0.0856, -0.3251, -0.3988]],\n       device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)",
            "code"
        ],
        [
            "Using our custom backend, we can now see how TorchDynamo is able to handle\ndata-dependent control flow. Consider the function below, where the line\nif b.sum() &lt; 0 is the source of data-dependent control flow.",
            "markdown"
        ],
        [
            "def bar(a, b):\n    x = a / ((a) + 1)\n    if b.sum() &lt; 0:\n        b = b * -1\n    return x * b\n\nopt_bar = (bar, backend=custom_backend)\n = (10)\n = (10)\nopt_bar(, )\nopt_bar(, -)",
            "code"
        ],
        [
            "custom backend called with FX graph:\nopcode         name     target                                                  args              kwargs\n-------------  -------  ------------------------------------------------------  ----------------  --------\nplaceholder    a        a                                                       ()                {}\nplaceholder    b        b                                                       ()                {}\ncall_function  abs_1    &lt;built-in method abs of type object at 0x7f051b9b3540&gt;  (a,)              {}\ncall_function  add      &lt;built-in function add&gt;                                 (abs_1, 1)        {}\ncall_function  truediv  &lt;built-in function truediv&gt;                             (a, add)          {}\ncall_method    sum_1    sum                                                     (b,)              {}\ncall_function  lt       &lt;built-in function lt&gt;                                  (sum_1, 0)        {}\noutput         output   output                                                  ((truediv, lt),)  {}\ncustom backend called with FX graph:\nopcode         name    target                   args         kwargs\n-------------  ------  -----------------------  -----------  --------\nplaceholder    b       b                        ()           {}\nplaceholder    x       x                        ()           {}\ncall_function  mul     &lt;built-in function mul&gt;  (b, -1)      {}\ncall_function  mul_1   &lt;built-in function mul&gt;  (x, mul)     {}\noutput         output  output                   ((mul_1,),)  {}\ncustom backend called with FX graph:\nopcode         name    target                   args       kwargs\n-------------  ------  -----------------------  ---------  --------\nplaceholder    b       b                        ()         {}\nplaceholder    x       x                        ()         {}\ncall_function  mul     &lt;built-in function mul&gt;  (x, b)     {}\noutput         output  output                   ((mul,),)  {}\n\ntensor([-0.1855, -0.0932,  0.0438, -0.1475, -0.0601,  0.3393, -1.4583,  0.0015,\n         0.7143,  0.0911])",
            "code"
        ],
        [
            "The output reveals that TorchDynamo extracted 3 different FX graphs\ncorresponding the following code (order may differ from the output above):",
            "markdown"
        ],
        [
            "x = a / (torch.abs(a) + 1)",
            "markdown"
        ],
        [
            "b = b * -1; return x * b",
            "markdown"
        ],
        [
            "return x * b",
            "markdown"
        ],
        [
            "When TorchDynamo encounters unsupported Python features, such as data-dependent\ncontrol flow, it breaks the computation graph, lets the default Python\ninterpreter handle the unsupported code, then resumes capturing the graph.",
            "markdown"
        ],
        [
            "Let\u2019s investigate by example how TorchDynamo would step through bar.\nIf b.sum() &lt; 0, then TorchDynamo would run graph 1, let\nPython determine the result of the conditional, then run\ngraph 2. On the other hand, if not b.sum() &lt; 0, then TorchDynamo\nwould run graph 1, let Python determine the result of the conditional, then\nrun graph 3.",
            "markdown"
        ],
        [
            "This highlights a major difference between TorchDynamo and previous PyTorch\ncompiler solutions. When encountering unsupported Python features,\nprevious solutions either raise an error or silently fail.\nTorchDynamo, on the other hand, will break the computation graph.",
            "markdown"
        ],
        [
            "We can see where TorchDynamo breaks the graph by using torch._dynamo.explain:",
            "markdown"
        ],
        [
            "# Reset since we are using a different backend.\n()\nexplanation, out_guards, graphs, ops_per_graph, break_reasons, explanation_verbose = torch._dynamo.explain(\n    bar, (10), (10)\n)\nprint(explanation_verbose)",
            "code"
        ],
        [
            "Dynamo produced 2 graphs with 1 graph break and 6 ops\n Break reasons:\n\n1. generic_jump TensorVariable()\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 414, in bar\n    if b.sum() &lt; 0:\n\n2. return_value\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 416, in &lt;graph break in bar&gt;\n    return x * b\n\nTorchDynamo compilation metrics:\nFunction                        Runtimes (s)\n------------------------------  --------------\n_compile                        0.0122, 0.0055\nOutputGraph.call_user_compiler  0.0000, 0.0000",
            "code"
        ],
        [
            "In order to maximize speedup, graph breaks should be limited.\nWe can force TorchDynamo to raise an error upon the first graph\nbreak encountered by using fullgraph=True:",
            "markdown"
        ],
        [
            "opt_bar = (bar, fullgraph=True)\ntry:\n    opt_bar((10), (10))\nexcept:\n    tb.print_exc()",
            "code"
        ],
        [
            "Traceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 464, in &lt;module&gt;\n    opt_bar(torch.randn(10), torch.randn(10))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 337, in catch_errors\n    return callback(frame, cache_size, hooks)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 104, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 262, in _convert_frame_assert\n    return _compile(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 163, in time_wrapper\n    r = func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 324, in _compile\n    out_code = transform_code_object(code, transform)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 445, in transform_code_object\n    transformations(instructions, code_options)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 311, in transform\n    tracer.run()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1726, in run\n    super().run()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 576, in run\n    and self.step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 540, in step\n    getattr(self, inst.opname)(inst)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 327, in inner\n    unimplemented(f\"generic_jump {typestr(value)}\")\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 71, in unimplemented\n    raise Unsupported(msg)\ntorch._dynamo.exc.Unsupported: generic_jump TensorVariable()\n\nfrom user code:\n   File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 414, in bar\n    if b.sum() &lt; 0:\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True",
            "code"
        ],
        [
            "And below, we demonstrate that TorchDynamo does not break the graph on\nthe model we used above for demonstrating speedups.",
            "markdown"
        ],
        [
            " = (init_model(), fullgraph=True)\nprint((generate_data(16)[0]))",
            "code"
        ],
        [
            "tensor([[-0.0031, -0.6601,  0.7708,  ...,  0.5529, -0.0965,  0.2389],\n        [ 0.5157, -0.6799,  0.6711,  ...,  0.4190, -0.0511,  0.3566],\n        [ 0.4879, -0.5442,  0.6752,  ...,  0.2638, -0.2817,  0.6400],\n        ...,\n        [ 0.0579, -0.2516,  0.5776,  ...,  0.2413, -0.0513,  0.4131],\n        [ 0.2299, -0.3535,  0.3686,  ...,  0.4281, -0.1155,  0.4612],\n        [ 0.1491, -0.5038,  0.5811,  ...,  0.4087, -0.0058,  0.4349]],\n       device='cuda:0', grad_fn=&lt;CompiledFunctionBackward&gt;)",
            "code"
        ],
        [
            "Finally, if we simply want TorchDynamo to output the FX graph for export,\nwe can use torch._dynamo.export. Note that torch._dynamo.export, like\nfullgraph=True, raises an error if TorchDynamo breaks the graph.",
            "markdown"
        ],
        [
            "try:\n    torch._dynamo.export(bar, (10), (10))\nexcept:\n    tb.print_exc()\n\nmodel_exp = torch._dynamo.export(init_model(), generate_data(16)[0])\nprint(model_exp[0](generate_data(16)[0]))",
            "code"
        ],
        [
            "Traceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 481, in &lt;module&gt;\n    torch._dynamo.export(bar, torch.randn(10), torch.randn(10))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 601, in export\n    result_traced = opt_f(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 337, in catch_errors\n    return callback(frame, cache_size, hooks)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 104, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 262, in _convert_frame_assert\n    return _compile(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 163, in time_wrapper\n    r = func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 324, in _compile\n    out_code = transform_code_object(code, transform)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 445, in transform_code_object\n    transformations(instructions, code_options)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 311, in transform\n    tracer.run()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1726, in run\n    super().run()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 576, in run\n    and self.step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 540, in step\n    getattr(self, inst.opname)(inst)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 327, in inner\n    unimplemented(f\"generic_jump {typestr(value)}\")\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 71, in unimplemented\n    raise Unsupported(msg)\ntorch._dynamo.exc.Unsupported: generic_jump TensorVariable()\n\nfrom user code:\n   File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 414, in bar\n    if b.sum() &lt; 0:\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n\ntensor([[ 0.1920, -0.1323,  0.9817,  ...,  0.5318, -0.5725,  0.6707],\n        [ 0.1287, -0.2137,  0.7767,  ...,  0.4573, -0.3710,  0.4077],\n        [ 0.4535, -0.0487,  0.8373,  ...,  0.5423, -0.3619,  0.3945],\n        ...,\n        [ 0.1012, -0.2960,  0.6673,  ...,  0.4942, -0.4441,  0.5463],\n        [ 0.0787, -0.0200,  0.7137,  ...,  0.5139, -0.4842,  0.4904],\n        [ 0.2168, -0.1528,  0.7756,  ...,  0.7245, -0.3392,  0.6314]],\n       device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)",
            "code"
        ]
    ],
    "torch->Model Optimization->torch.compile Tutorial->Conclusion": [
        [
            "In this tutorial, we introduced torch.compile by covering\nbasic usage, demonstrating speedups over eager mode, comparing to previous\nPyTorch compiler solutions, and briefly investigating TorchDynamo and its interactions\nwith FX graphs. We hope that you will give torch.compile a try!",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 1 minutes  2.322 seconds)",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)": [
        [
            "<strong>Author:</strong> ",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)->Summary": [
        [
            "In this tutorial, we want to highlight a new torch.nn.functional function\nthat can be helpful for implementing transformer architectures. The\nfunction is named torch.nn.functional.scaled_dot_product_attention.\nFor detailed description of the function, see the .\nThis function has already been incorporated into torch.nn.MultiheadAttention and torch.nn.TransformerEncoderLayer.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)->Overview": [
        [
            "At a high level, this PyTorch function calculates the\nscaled dot product attention (SDPA) between query, key, and value according to\nthe definition found in the paper . While this function can\nbe written in PyTorch using existing functions, a fused implementation can provide\nlarge performance benefits over a naive implementation.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)->Fused implementations": [
        [
            "For CUDA tensor inputs, the function will dispatch into one of the following\nimplementations:",
            "markdown"
        ],
        [
            "A PyTorch implementation defined in C++",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This tutorial requires PyTorch 2.0.0 or later.",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ndevice = \"cuda\" if () else \"cpu\"\n\n# Example Usage:\n, ,  = (2, 3, 8, device=device), (2, 3, 8, device=device), (2, 3, 8, device=device)\n(, , )",
            "code"
        ],
        [
            "tensor([[[ 1.1316, -0.3213, -0.0318, -0.3642, -0.0631, -0.3050, -0.2020,\n          -0.8197],\n         [ 1.1733, -0.3907, -0.1463,  0.0058, -0.2564, -0.3103,  0.1328,\n          -0.9343],\n         [ 1.5885, -0.6964, -1.4466,  0.0382, -0.9734,  0.4265,  0.2052,\n          -2.3355]],\n\n        [[-1.1003, -0.6316,  0.0504,  0.3032, -0.1389,  0.6405,  0.6810,\n          -0.5061],\n         [-1.3532, -0.1394,  0.1714,  0.2172,  0.4992,  0.5198,  1.0496,\n          -0.5356],\n         [-0.7486, -1.4682, -0.2393,  0.4146, -1.0318,  0.9081, -0.0883,\n          -0.3074]]], device='cuda:0')",
            "code"
        ]
    ],
    "torch->Model Optimization->(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)->Explicit Dispatcher Control": [
        [
            "While the function will implicitly dispatch to one of the three\nimplementations, the user can also explicitly control the dispatch via\nthe use of a context manager. This context manager allows users to\nexplicitly disable certain implementations. If a user wants to ensure\nthe function is indeed using the fastest implementation for their\nspecific inputs, the context manager can be used to sweep through\nmeasuring performance.",
            "markdown"
        ],
        [
            "# Lets define a helpful benchmarking function:\nimport torch.utils.benchmark as benchmark\ndef benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n    t0 = (\n        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n    )\n    return t0.blocked_autorange().mean * 1e6\n\n# Lets define the hyper-parameters of our input\nbatch_size = 32\nmax_sequence_len = 1024\nnum_heads = 32\nembed_dimension = 32\n\n = \n\n = (batch_size, num_heads, max_sequence_len, embed_dimension, device=device, =)\n = (batch_size, num_heads, max_sequence_len, embed_dimension, device=device, =)\n = (batch_size, num_heads, max_sequence_len, embed_dimension, device=device, =)\n\nprint(f\"The default implementation runs in {benchmark_torch_function_in_microseconds(, , , ):.3f} microseconds\")\n\n# Lets explore the speed of each of the 3 implementations\nfrom torch.backends.cuda import , SDPBackend\n\n# Helpful arg mapper\nbackend_map = {\n    : {\"enable_math\": True, \"enable_flash\": False, \"enable_mem_efficient\": False},\n    : {\"enable_math\": False, \"enable_flash\": True, \"enable_mem_efficient\": False},\n    : {\n        \"enable_math\": False, \"enable_flash\": False, \"enable_mem_efficient\": True}\n}\n\nwith (**backend_map[]):\n    print(f\"The math implementation runs in {benchmark_torch_function_in_microseconds(, , , ):.3f} microseconds\")\n\n\nwith (**backend_map[]):\n    try:\n        print(f\"The flash attention implementation runs in {benchmark_torch_function_in_microseconds(, , , ):.3f} microseconds\")\n    except RuntimeError:\n        print(\"FlashAttention is not supported. See warnings for reasons.\")\n\nwith (**backend_map[]):\n    try:\n        print(f\"The memory efficient implementation runs in {benchmark_torch_function_in_microseconds(, , , ):.3f} microseconds\")\n    except RuntimeError:\n        print(\"EfficientAttention is not supported. See warnings for reasons.\")",
            "code"
        ],
        [
            "The default implementation runs in 1784774.944 microseconds\nThe math implementation runs in 143012.950 microseconds\n&lt;timeit-src&gt;:6: UserWarning:\n\nMemory efficient kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:527.)\n\n&lt;timeit-src&gt;:6: UserWarning:\n\nMemory Efficient attention has been runtime disabled. (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:338.)\n\n&lt;timeit-src&gt;:6: UserWarning:\n\nFlash attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:529.)\n\n&lt;timeit-src&gt;:6: UserWarning:\n\nFlash attention only supports sm75 and sm8x gpu architectures. Attempting to run on a sm 6.1 gpu. (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:352.)\n\nFlashAttention is not supported. See warnings for reasons.\nThe memory efficient implementation runs in 1786708.727 microseconds",
            "code"
        ]
    ],
    "torch->Model Optimization->(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)->Hardware dependence": [
        [
            "Depending on what machine you ran the above cell on and what hardware is\navailable, your results might be different.\n- If you don\u2019t have a GPU and are running on CPU then the context manager\nwill have no effect and all three runs should return similar timings.\n- Depending on what compute capability your graphics card supports\nflash attention or memory efficient might have failed.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)->Causal Self Attention": [
        [
            "Below is an example implementation of a multi-headed causal self\nattention block inspired by Andrej Karpathy\u2019s\n repository.",
            "markdown"
        ],
        [
            "class CausalSelfAttention():\n\n    def __init__(self, num_heads: int, embed_dimension: int, bias: bool=False, is_causal: bool=False, dropout:float=0.0):\n        super().__init__()\n        assert embed_dimension % num_heads == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = (embed_dimension, 3 * embed_dimension, bias=bias)\n        # output projection\n        self.c_proj = (embed_dimension, embed_dimension, bias=bias)\n        # regularization\n        self.dropout = dropout\n        self.resid_dropout = (dropout)\n        self.num_heads = num_heads\n        self.embed_dimension = embed_dimension\n        # Perform causal masking\n        self.is_causal = is_causal\n\n    def forward(self, ):\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        query_projected = self.c_attn()\n\n        batch_size = query_projected.size(0)\n        embed_dim = query_projected.size(2)\n        head_dim = embed_dim // (self.num_heads * 3)\n\n        , ,  = query_projected.chunk(3, -1)\n         = .view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n         = .view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n         = .view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n\n        if self.training:\n            dropout = self.dropout\n            is_causal = self.is_causal\n        else:\n            dropout = 0.0\n            is_causal = False\n\n        y = (, , , attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n        y = y.transpose(1, 2).view(batch_size, -1, self.num_heads * head_dim)\n\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\n\nnum_heads = 8\nheads_per_dim = 64\nembed_dimension = num_heads * heads_per_dim\n = \nmodel = (num_heads=num_heads, embed_dimension=embed_dimension, bias=False, is_causal=True, dropout=0.1).to(\"cuda\").to().eval()\nprint(model)",
            "code"
        ],
        [
            "CausalSelfAttention(\n  (c_attn): Linear(in_features=512, out_features=1536, bias=False)\n  (c_proj): Linear(in_features=512, out_features=512, bias=False)\n  (resid_dropout): Dropout(p=0.1, inplace=False)\n)",
            "code"
        ]
    ],
    "torch->Model Optimization->(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)->Causal Self Attention->NestedTensor and Dense tensor support": [
        [
            "SDPA supports both NestedTensor and Dense tensor inputs. NestedTensors handle the case where the input is a batch of variable length sequences\nwithout needing to pad each sequence to the maximum length in the batch. For more information about NestedTensors see\n and .",
            "markdown"
        ],
        [
            "import random\ndef generate_rand_batch(\n    batch_size,\n    max_sequence_len,\n    embed_dimension,\n    pad_percentage=None,\n    =,\n    device=\"cuda\",\n):\n    if not pad_percentage:\n        return (\n            (\n                batch_size,\n                max_sequence_len,\n                embed_dimension,\n                =,\n                device=device,\n            ),\n            None,\n        )\n    # Random sequence lengths\n    seq_len_list = [\n        int(max_sequence_len * (1 - random.gauss(pad_percentage, 0.01)))\n        for _ in range(batch_size)\n    ]\n    # Make random entry in the batch have max sequence length\n    seq_len_list[random.randint(0, batch_size - 1)] = max_sequence_len\n    return (\n        (\n            [\n                (seq_len, embed_dimension,\n                            =, device=device)\n                for seq_len in seq_len_list\n            ]\n        ),\n        seq_len_list,\n    )\n\n, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=0.5, =, device=device)\n, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=None, =, device=device)\n\n# Currently the fused implementations don't support NestedTensor for training\n()\n\nwith (**backend_map[]):\n    try:\n        print(f\"Random NT runs in {benchmark_torch_function_in_microseconds(model, ):.3f} microseconds\")\n        print(f\"Random Dense runs in {benchmark_torch_function_in_microseconds(model, ):.3f} microseconds\")\n    except RuntimeError:\n        print(\"FlashAttention is not supported. See warnings for reasons.\")",
            "code"
        ],
        [
            "/var/lib/jenkins/workspace/intermediate_source/scaled_dot_product_attention_tutorial.py:226: UserWarning:\n\nThe PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)\n\n/var/lib/jenkins/workspace/intermediate_source/scaled_dot_product_attention_tutorial.py:174: UserWarning:\n\nMemory efficient kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:527.)\n\n/var/lib/jenkins/workspace/intermediate_source/scaled_dot_product_attention_tutorial.py:174: UserWarning:\n\nMemory Efficient attention has been runtime disabled. (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:338.)\n\n/var/lib/jenkins/workspace/intermediate_source/scaled_dot_product_attention_tutorial.py:174: UserWarning:\n\nFlash attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:529.)\n\n/var/lib/jenkins/workspace/intermediate_source/scaled_dot_product_attention_tutorial.py:174: UserWarning:\n\nFlash attention only supports sm75 and sm8x gpu architectures. Attempting to run on a sm 6.1 gpu. (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:352.)\n\nFlashAttention is not supported. See warnings for reasons.",
            "code"
        ]
    ],
    "torch->Model Optimization->Using SDPA with torch.compile": [
        [
            "With the release of PyTorch 2.0, a new feature called\ntorch.compile() has been introduced, which can provide\nsignificant performance improvements over eager mode.\nScaled dot product attention is fully composable with torch.compile().\nTo demonstrate this, let\u2019s compile the CausalSelfAttention module using\ntorch.compile() and observe the resulting performance improvements.",
            "markdown"
        ],
        [
            "batch_size = 32\nmax_sequence_len = 256\n = (batch_size, max_sequence_len,\n               embed_dimension, device=device, =)\nprint(\n    f\"The non compiled module runs in  {benchmark_torch_function_in_microseconds(model, ):.3f} microseconds\")\n\n\n = (model)\n# Let's compile it\n()\nprint(\n    f\"The compiled module runs in  {benchmark_torch_function_in_microseconds(, ):.3f} microseconds\")",
            "code"
        ],
        [
            "The non compiled module runs in  42948.685 microseconds\nThe compiled module runs in  43165.903 microseconds",
            "code"
        ],
        [
            "The exact execution time is dependent on machine, however the results for mine:\nThe non compiled module runs in  166.616 microseconds\nThe compiled module runs in  166.726 microseconds\nThat is not what we were expecting. Let\u2019s dig a little deeper.\nPyTorch comes with an amazing built-in profiler that you can use to\ninspect the performance characteristics of your code.",
            "markdown"
        ],
        [
            "from torch.profiler import , record_function, ProfilerActivity\nactivities = []\nif device == 'cuda':\n    activities.append()\n\nwith (activities=activities, record_shapes=False) as :\n    with record_function(\" Non-Compilied Causal Attention\"):\n        for _ in range(25):\n            model()\nprint(().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n\nwith (activities=activities, record_shapes=False) as :\n    with record_function(\"Compiled Causal Attention\"):\n        for _ in range(25):\n            ()\nprint(().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n# For even more insights, you can export the trace and use ``chrome://tracing`` to view the results\n# prof.export_chrome_trace(\"compiled_causal_attention_trace.json\").",
            "code"
        ],
        [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n                         Non-Compilied Causal Attention         0.42%       4.576ms         1.61%      17.413ms      17.413ms       0.000us         0.00%        1.213s        1.213s             1\n                     aten::scaled_dot_product_attention         0.08%     902.000us         0.21%       2.240ms      89.600us       0.000us         0.00%     972.027ms      38.881ms            25\n          aten::_scaled_dot_product_efficient_attention         0.04%     454.000us         0.12%       1.338ms      53.520us       0.000us         0.00%     972.027ms      38.881ms            25\n                     aten::_efficient_attention_forward         0.03%     277.000us         0.08%     818.000us      32.720us     972.027ms        90.31%     972.027ms      38.881ms            25\nvoid attention_kernel_batched&lt;AttentionKernel&lt;cutlas...         0.00%       0.000us         0.00%       0.000us       0.000us     972.027ms        90.31%     972.027ms      38.881ms            25\n                                           aten::matmul         0.10%       1.042ms         0.75%       8.089ms     161.780us       0.000us         0.00%     241.153ms       4.823ms            50\n                                               aten::mm         0.41%       4.423ms         0.53%       5.733ms     114.660us     104.261ms         9.69%     241.153ms       4.823ms            50\n                                           aten::linear         0.04%     411.000us         0.79%       8.570ms     171.400us       0.000us         0.00%     234.146ms       4.683ms            50\ncudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.07%     812.000us         0.07%     812.000us       4.640us      96.656ms         8.98%      96.656ms     552.320us           175\n                      maxwell_fp16_sgemm_fp16_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us      80.984ms         7.52%      80.984ms       3.239ms            25\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 1.083s\nSelf CUDA time total: 1.076s\n\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n                              Compiled Causal Attention         0.32%       3.414ms         1.40%      15.124ms      15.124ms       0.000us         0.00%        1.361s        1.361s             1\n                                       CompiledFunction         0.68%       7.383ms         1.08%      11.605ms     464.200us       0.000us         0.00%        1.361s      54.437ms            25\n          aten::_scaled_dot_product_efficient_attention         0.03%     289.000us         0.11%       1.182ms      47.280us       0.000us         0.00%        1.049s      41.950ms            25\n                     aten::_efficient_attention_forward         0.03%     297.000us         0.07%     803.000us      32.120us     970.600ms        90.33%        1.049s      41.950ms            25\nvoid attention_kernel_batched&lt;AttentionKernel&lt;cutlas...         0.00%       0.000us         0.00%       0.000us       0.000us     970.600ms        90.33%     970.600ms      38.824ms            25\n                                               aten::mm         0.12%       1.305ms         0.18%       1.977ms      39.540us     103.919ms         9.67%     312.165ms       6.243ms            50\n                                       cudaLaunchKernel         0.08%     812.000us         0.08%     812.000us      10.827us     157.432ms        14.65%     157.432ms       2.099ms            75\ncudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.01%      71.000us         0.01%      71.000us       0.406us     128.971ms        12.00%     128.971ms     736.977us           175\n                      maxwell_fp16_sgemm_fp16_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us      80.978ms         7.54%      80.978ms       3.239ms            25\n                                 hgemm_128x128x8_NT_vec         0.00%       0.000us         0.00%       0.000us       0.000us      22.941ms         2.14%      22.941ms     917.640us            25\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 1.078s\nSelf CUDA time total: 1.075s",
            "code"
        ],
        [
            "The previous code snippet generates a report of the top 10 PyTorch functions\nthat consumed the most GPU execution time, for both the compiled and non-compiled module.\nThe analysis reveals that the majority of time spent on the GPU is concentrated\non the same set of functions for both modules.\nThe reason for this here is that torch.compile is very good at removing the\nframework overhead associated with PyTorch. If your model is launching\nlarge, efficient CUDA kernels, which in this case CausaulSelfAttention\nis, then the overhead of PyTorch can be hidden.",
            "markdown"
        ],
        [
            "In reality, your module does not normally consist of a singular\nCausalSelfAttention block. When experimenting with Andrej Karpathy\u2019s\n repository, compiling\nthe module took the time per train step from: 6090.49ms to\n3273.17ms! This was done on commit: ae3a8d5 of NanoGPT training on\nthe shakespeare dataset.",
            "markdown"
        ]
    ],
    "torch->Model Optimization->Conclusion": [
        [
            "In this tutorial, we have demonstrated the basic usage of\ntorch.nn.functional.scaled_dot_product_attention. We have shown how\nthe sdp_kernel context manager can be used to assert a certain\nimplementation is used on GPU. As well, we built a simple\nCausalSelfAttention module that works with NestedTensor and is torch\ncompilable. In the process we have shown how to the profiling tools can\nbe used to explore the performance characteristics of a user defined\nmodule.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  18.529 seconds)",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed and Parallel Training Tutorials": [
        [
            "Distributed training is a model training paradigm that involves\nspreading training workload across multiple worker nodes, therefore\nsignificantly improving the speed of training and model accuracy. While\ndistributed training can be used for any type of ML model training, it\nis most beneficial to use it for large models and compute demanding\ntasks as deep learning.",
            "markdown"
        ],
        [
            "There are a few ways you can perform distributed training in\nPyTorch with each method having their advantages in certain use cases:",
            "markdown"
        ],
        [
            "Read more about these options in .",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed and Parallel Training Tutorials->Learn DDP": [
        [
            "A step-by-step video series on how to get started with\n<cite>DistributedDataParallel</cite> and advance to more complex topics",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code <svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-square-fill\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M5.75 4A1.75 1.75 0 004 5.75v4.5c0 .966.784 1.75 1.75 1.75h4.5A1.75 1.75 0 0012 10.25v-4.5A1.75 1.75 0 0010.25 4h-4.5z\" fill-rule=\"evenodd\"></path></svg> <svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-video\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M1.75 3.5a.25.25 0 00-.25.25v8.5c0 .138.112.25.25.25h12.5a.25.25 0 00.25-.25v-8.5a.25.25 0 00-.25-.25H1.75zM0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0114.25 14H1.75A1.75 1.75 0 010 12.25v-8.5z\" fill-rule=\"evenodd\"></path><path d=\"M6 10.559V5.442a.25.25 0 01.379-.215l4.264 2.559a.25.25 0 010 .428l-4.264 2.559A.25.25 0 016 10.559z\"></path></svg> Video\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nGetting Started with Distributed Data Parallel",
            "markdown"
        ],
        [
            "This tutorial provides a short and gentle intro to the PyTorch\nDistributedData Parallel.",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nDistributed Training with Uneven Inputs Using\nthe Join Context Manager",
            "markdown"
        ],
        [
            "This tutorial describes the Join context manager and\ndemonstrates it\u2019s use with DistributedData Parallel.",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed and Parallel Training Tutorials->Learn FSDP": [
        [
            "This tutorial demonstrates how you can perform distributed training\nwith FSDP on a MNIST dataset.",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nFSDP Advanced",
            "markdown"
        ],
        [
            "In this tutorial, you will learn how to fine-tune a HuggingFace (HF) T5\nmodel with FSDP for text summarization.",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed and Parallel Training Tutorials->Learn RPC": [
        [
            "This tutorial demonstrates how to get started with RPC-based distributed\ntraining.",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nImplementing a Parameter Server Using Distributed RPC Framework",
            "markdown"
        ],
        [
            "This tutorial walks you through a simple example of implementing a\nparameter server using PyTorch\u2019s Distributed RPC framework.",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nImplementing Batch RPC Processing Using Asynchronous Executions",
            "markdown"
        ],
        [
            "In this tutorial you will build batch-processing RPC applications\nwith the @rpc.functions.async_execution decorator.",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nCombining Distributed DataParallel with Distributed RPC Framework",
            "markdown"
        ],
        [
            "In this tutorial you will learn how to combine distributed data\nparallelism with distributed model parallelism.",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed and Parallel Training Tutorials->Custom Extensions": [
        [
            "In this tutorial you will learn to implement a custom <cite>ProcessGroup</cite>\nbackend and plug that into PyTorch distributed package using\ncpp extensions.",
            "markdown"
        ],
        [
            "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->PyTorch Distributed Overview": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            " View and edit this tutorial in .",
            "markdown"
        ],
        [
            "This is the overview page for the torch.distributed package. The goal of\nthis page is to categorize documents into different topics and briefly\ndescribe each of them. If this is your first time building distributed training\napplications using PyTorch, it is recommended to use this document to navigate\nto the technology that can best serve your use case.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->PyTorch Distributed Overview->Introduction": [
        [
            "As of PyTorch v1.6.0, features in torch.distributed can be categorized into\nthree main components:",
            "markdown"
        ],
        [
            "(DDP) is a widely adopted single-program multiple-data training paradigm. With\nDDP, the model is replicated on every process, and every model replica will be\nfed with a different set of input data samples. DDP takes care of gradient\ncommunication to keep model replicas synchronized and overlaps it with the\ngradient computations to speed up training.",
            "markdown"
        ],
        [
            "(RPC) supports general training structures that cannot fit into\ndata-parallel training such as distributed pipeline parallelism, parameter\nserver paradigm, and combinations of DDP with other training paradigms. It\nhelps manage remote object lifetime and extends the\n beyond\nmachine boundaries.",
            "markdown"
        ],
        [
            "(c10d) library supports sending tensors across processes within a group. It\noffers both collective communication APIs (e.g.,\n\nand )\nand P2P communication APIs (e.g.,\n\nand ).\nDDP and RPC ()\nare built on c10d, where the former uses collective communications\nand the latter uses P2P communications. Usually, developers do not need to\ndirectly use this raw communication API, as the DDP and RPC APIs can serve\nmany distributed training scenarios. However, there are use cases where this API\nis still helpful. One example would be distributed parameter averaging, where\napplications would like to compute the average values of all model parameters\nafter the backward pass instead of using DDP to communicate gradients. This can\ndecouple communications from computations and allow finer-grain control over\nwhat to communicate, but on the other hand, it also gives up the performance\noptimizations offered by DDP.\n\nshows examples of using c10d communication APIs.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->PyTorch Distributed Overview->Data Parallel Training": [
        [
            "PyTorch provides several options for data-parallel training. For applications\nthat gradually grow from simple to complex and from prototype to production, the\ncommon development trajectory would be:",
            "markdown"
        ],
        [
            "Use single-device training if the data and model can fit in one GPU, and\ntraining speed is not a concern.",
            "markdown"
        ],
        [
            "Use single-machine multi-GPU\n\nto make use of multiple GPUs on a single machine to speed up training with\nminimal code changes.",
            "markdown"
        ],
        [
            "Use single-machine multi-GPU\n,\nif you would like to further speed up training and are willing to write a\nlittle more code to set it up.",
            "markdown"
        ],
        [
            "Use multi-machine \nand the ,\nif the application needs to scale across machine boundaries.",
            "markdown"
        ],
        [
            "Use \nto launch distributed training if errors (e.g., out-of-memory) are expected or if\nresources can join and leave dynamically during training.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Data-parallel training also works with .",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->PyTorch Distributed Overview->Data Parallel Training->torch.nn.DataParallel": [
        [
            "The \npackage enables single-machine multi-GPU parallelism with the lowest coding\nhurdle. It only requires a one-line change to the application code. The tutorial\n\nshows an example. Although DataParallel is very easy to\nuse, it usually does not offer the best performance because it replicates the\nmodel in every forward pass, and its single-process multi-thread parallelism\nnaturally suffers from\n contention. To get\nbetter performance, consider using\n.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->PyTorch Distributed Overview->Data Parallel Training->torch.nn.parallel.DistributedDataParallel": [
        [
            "Compared to ,\n\nrequires one more step to set up, i.e., calling\n.\nDDP uses multi-process parallelism, and hence there is no GIL contention across\nmodel replicas. Moreover, the model is broadcast at DDP construction time instead\nof in every forward pass, which also helps to speed up training. DDP is shipped\nwith several performance optimization technologies. For a more in-depth\nexplanation, refer to this\n (VLDB\u201920).",
            "markdown"
        ],
        [
            "DDP materials are listed below:",
            "markdown"
        ],
        [
            "offer a starter example and some brief descriptions of its design and\nimplementation. If this is your first time using DDP, start from this\ndocument.",
            "markdown"
        ],
        [
            "explains some common problems with DDP training, including unbalanced\nworkload, checkpointing, and multi-device models. Note that, DDP can be\neasily combined with single-machine multi-device model parallelism which is\ndescribed in the\n\ntutorial.",
            "markdown"
        ],
        [
            "The \ndocument shows how to use the DDP launching script.",
            "markdown"
        ],
        [
            "The \nrecipe demonstrates how \nhelps to reduce optimizer memory footprint.",
            "markdown"
        ],
        [
            "The \ntutorial walks through using the generic join context for distributed training with uneven inputs.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->PyTorch Distributed Overview->Data Parallel Training->torch.distributed.elastic": [
        [
            "With the growth of the application complexity and scale, failure recovery\nbecomes a requirement. Sometimes it is inevitable to hit errors\nlike out-of-memory (OOM) when using DDP, but DDP itself cannot recover from those errors,\nand it is not possible to handle them using a standard try-except construct.\nThis is because DDP requires all processes to operate in a closely synchronized manner\nand all AllReduce communications launched in different processes must match.\nIf one of the processes in the group\nthrows an exception, it is likely to lead to desynchronization (mismatched\nAllReduce operations) which would then cause a crash or hang.\n\nadds fault tolerance and the ability to make use of a dynamic pool of machines (elasticity).",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->PyTorch Distributed Overview->RPC-Based Distributed Training": [
        [
            "Many training paradigms do not fit into data parallelism, e.g.,\nparameter server paradigm, distributed pipeline parallelism, reinforcement\nlearning applications with multiple observers or agents, etc.\n aims at\nsupporting general distributed training scenarios.",
            "markdown"
        ],
        [
            "has four main pillars:",
            "markdown"
        ],
        [
            " supports running\na given function on a remote worker.",
            "markdown"
        ],
        [
            " helps to manage the\nlifetime of a remote object. The reference counting protocol is presented in the\n.",
            "markdown"
        ],
        [
            "extends the autograd engine beyond machine boundaries. Please refer to\n\nfor more details.",
            "markdown"
        ],
        [
            "automatically reaches out to all participating workers to update\nparameters using gradients computed by the distributed autograd engine.",
            "markdown"
        ],
        [
            "RPC Tutorials are listed below:",
            "markdown"
        ],
        [
            "The \ntutorial first uses a simple Reinforcement Learning (RL) example to\ndemonstrate RPC and RRef. Then, it applies a basic distributed model\nparallelism to an RNN example to show how to use distributed autograd and\ndistributed optimizer.",
            "markdown"
        ],
        [
            "The \ntutorial borrows the spirit of\n\nand applies it to an asynchronous parameter server (PS) training application.",
            "markdown"
        ],
        [
            "The \ntutorial extends the single-machine pipeline parallel example (presented in\n)\nto a distributed environment and shows how to implement it using RPC.",
            "markdown"
        ],
        [
            "The \ntutorial demonstrates how to implement RPC batch processing using the\n\ndecorator, which can help speed up inference and training. It uses\nRL and PS examples similar to those in the above tutorials 1 and 2.",
            "markdown"
        ],
        [
            "The \ntutorial demonstrates how to combine DDP with RPC to train a model using\ndistributed data parallelism combined with distributed model parallelism.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->PyTorch Distributed Overview->PyTorch Distributed Developers": [
        [
            "If you\u2019d like to contribute to PyTorch Distributed, please refer to our\n.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Single-Machine Model Parallel Best Practices": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "Model parallel is widely-used in distributed training\ntechniques. Previous posts have explained how to use\n\nto train a neural network on multiple GPUs; this feature replicates the\nsame model to all GPUs, where each GPU consumes a different partition of the\ninput data. Although it can significantly accelerate the training process, it\ndoes not work for some use cases where the model is too large to fit into a\nsingle GPU. This post shows how to solve that problem by using <strong>model parallel</strong>,\nwhich, in contrast to DataParallel, splits a single model onto different GPUs,\nrather than replicating the entire model on each GPU (to be concrete, say a model\nm contains 10 layers: when using DataParallel, each GPU will have a\nreplica of each of these 10 layers, whereas when using model parallel on two GPUs,\neach GPU could host 5 layers).",
            "markdown"
        ],
        [
            "The high-level idea of model parallel is to place different sub-networks of a\nmodel onto different devices, and implement the forward method accordingly\nto move intermediate outputs across devices. As only part of a model operates\non any individual device, a set of devices can collectively serve a larger\nmodel. In this post, we will not try to construct huge models and squeeze them\ninto a limited number of GPUs. Instead, this post focuses on showing the idea\nof model parallel. It is up to the readers to apply the ideas to real-world\napplications.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "For distributed model parallel training where a model spans multiple\nservers, please refer to\n\nfor examples and details.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Single-Machine Model Parallel Best Practices->Basic Usage": [
        [
            "Let us start with a toy model that contains two linear layers. To run this\nmodel on two GPUs, simply put each linear layer on a different GPU, and move\ninputs and intermediate outputs to match the layer devices accordingly.",
            "markdown"
        ],
        [
            "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\nclass ToyModel():\n    def __init__(self):\n        super(, self).__init__()\n        self.net1 = (10, 10).to('cuda:0')\n        self.relu = ()\n        self.net2 = (10, 5).to('cuda:1')\n\n    def forward(self, x):\n        x = self.relu(self.net1(x.to('cuda:0')))\n        return self.net2(x.to('cuda:1'))",
            "code"
        ],
        [
            "Note that, the above ToyModel looks very similar to how one would\nimplement it on a single GPU, except the four to(device) calls which\nplace linear layers and tensors on proper devices. That is the only place in\nthe model that requires changes. The backward() and torch.optim will\nautomatically take care of gradients as if the model is on one GPU. You only\nneed to make sure that the labels are on the same device as the outputs when\ncalling the loss function.",
            "markdown"
        ],
        [
            "model = ()\n = ()\n = ((), lr=0.001)\n\n()\n = model((20, 10))\n = (20, 5).to('cuda:1')\n(, ).backward()\n()",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Single-Machine Model Parallel Best Practices->Apply Model Parallel to Existing Modules": [
        [
            "It is also possible to run an existing single-GPU module on multiple GPUs\nwith just a few lines of changes. The code below shows how to decompose\ntorchvision.models.resnet50() to two GPUs. The idea is to inherit from\nthe existing ResNet module, and split the layers to two GPUs during\nconstruction. Then, override the forward method to stitch two\nsub-networks by moving the intermediate outputs accordingly.",
            "markdown"
        ],
        [
            "from torchvision.models.resnet import , \n\nnum_classes = 1000\n\n\nclass ModelParallelResNet50():\n    def __init__(self, *args, **kwargs):\n        super(, self).__init__(\n            , [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs)\n\n        self.seq1 = (\n            self.conv1,\n            self.bn1,\n            self.relu,\n            self.maxpool,\n\n            self.layer1,\n            self.layer2\n        ).to('cuda:0')\n\n        self.seq2 = (\n            self.layer3,\n            self.layer4,\n            self.avgpool,\n        ).to('cuda:1')\n\n        self.fc.to('cuda:1')\n\n    def forward(self, x):\n        x = self.seq2(self.seq1(x).to('cuda:1'))\n        return self.fc(x.view(x.size(0), -1))",
            "code"
        ],
        [
            "The above implementation solves the problem for cases where the model is too\nlarge to fit into a single GPU. However, you might have already noticed that\nit will be slower than running it on a single GPU if your model fits. It is\nbecause, at any point in time, only one of the two GPUs are working, while\nthe other one is sitting there doing nothing. The performance further\ndeteriorates as the intermediate outputs need to be copied from cuda:0 to\ncuda:1 between layer2 and layer3.",
            "markdown"
        ],
        [
            "Let us run an experiment to get a more quantitative view of the execution\ntime. In this experiment, we train ModelParallelResNet50 and the existing\ntorchvision.models.resnet50() by running random inputs and labels through\nthem. After the training, the models will not produce any useful predictions,\nbut we can get a reasonable understanding of the execution times.",
            "markdown"
        ],
        [
            "import torchvision.models as models\n\nnum_batches = 3\nbatch_size = 120\nimage_w = 128\nimage_h = 128\n\n\ndef train(model):\n    (True)\n     = ()\n     = ((), lr=0.001)\n\n    one_hot_indices = torch.LongTensor(batch_size) \\\n                           .random_(0, num_classes) \\\n                           .view(batch_size, 1)\n\n    for _ in range(num_batches):\n        # generate random inputs and labels\n        inputs = (batch_size, 3, image_w, image_h)\n         = (batch_size, num_classes) \\\n                      .scatter_(1, one_hot_indices, 1)\n\n        # run forward pass\n        ()\n         = model(inputs.to('cuda:0'))\n\n        # run backward pass\n         = .to()\n        (, ).backward()\n        ()",
            "code"
        ],
        [
            "The train(model) method above uses nn.MSELoss as the loss function,\nand optim.SGD as the optimizer. It mimics training on 128 X 128\nimages which are organized into 3 batches where each batch contains 120\nimages. Then, we use timeit to run the train(model) method 10 times\nand plot the execution times with standard deviations.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nplt.switch_backend('Agg')\nimport numpy as np\nimport timeit\n\nnum_repeat = 10\n\nstmt = \"train(model)\"\n\nsetup = \"model = ModelParallelResNet50()\"\nmp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nmp_mean, mp_std = np.mean(mp_run_times), np.std(mp_run_times)\n\nsetup = \"import torchvision.models as models;\" + \\\n        \"model = models.resnet50(num_classes=num_classes).to('cuda:0')\"\nrn_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nrn_mean, rn_std = np.mean(rn_run_times), np.std(rn_run_times)\n\n\ndef plot(means, stds, , fig_name):\n    fig, ax = plt.subplots()\n    ax.bar(np.arange(len(means)), means, yerr=stds,\n           align='center', alpha=0.5, ecolor='red', capsize=10, width=0.6)\n    ax.set_ylabel('ResNet50 Execution Time (Second)')\n    ax.set_xticks(np.arange(len(means)))\n    ax.set_xticklabels()\n    ax.yaxis.grid(True)\n    plt.tight_layout()\n    plt.savefig(fig_name)\n    plt.close(fig)\n\n\nplot([mp_mean, rn_mean],\n     [mp_std, rn_std],\n     ['Model Parallel', 'Single GPU'],\n     'mp_vs_rn.png')\n\n\n\n<img alt=\"\" src=\"../_images/mp_vs_rn.png\"/>",
            "code"
        ],
        [
            "The result shows that the execution time of model parallel implementation is\n4.02/3.75-1=7% longer than the existing single-GPU implementation. So we\ncan conclude there is roughly 7% overhead in copying tensors back and forth\nacross the GPUs. There are rooms for improvements, as we know one of the two\nGPUs is sitting idle throughout the execution. One option is to further\ndivide each batch into a pipeline of splits, such that when one split reaches\nthe second sub-network, the following split can be fed into the first\nsub-network. In this way, two consecutive splits can run concurrently on two\nGPUs.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Single-Machine Model Parallel Best Practices->Speed Up by Pipelining Inputs": [
        [
            "In the following experiments, we further divide each 120-image batch into\n20-image splits. As PyTorch launches CUDA operations asynchronously, the\nimplementation does not need to spawn multiple threads to achieve\nconcurrency.",
            "markdown"
        ],
        [
            "class PipelineParallelResNet50():\n    def __init__(self, split_size=20, *args, **kwargs):\n        super(, self).__init__(*args, **kwargs)\n        self.split_size = split_size\n\n    def forward(self, x):\n        splits = iter(x.split(self.split_size, dim=0))\n        s_next = next(splits)\n        s_prev = self.seq1(s_next).to('cuda:1')\n        ret = []\n\n        for s_next in splits:\n            # A. s_prev runs on cuda:1\n            s_prev = self.seq2(s_prev)\n            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n            # B. s_next runs on cuda:0, which can run concurrently with A\n            s_prev = self.seq1(s_next).to('cuda:1')\n\n        s_prev = self.seq2(s_prev)\n        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n        return (ret)\n\n\nsetup = \"model = PipelineParallelResNet50()\"\npp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\npp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n\nplot([mp_mean, rn_mean, pp_mean],\n     [mp_std, rn_std, pp_std],\n     ['Model Parallel', 'Single GPU', 'Pipelining Model Parallel'],\n     'mp_vs_rn_vs_pp.png')",
            "code"
        ],
        [
            "Please note, device-to-device tensor copy operations are synchronized on\ncurrent streams on the source and the destination devices. If you create\nmultiple streams, you have to make sure that copy operations are properly\nsynchronized. Writing the source tensor or reading/writing the destination\ntensor before finishing the copy operation can lead to undefined behavior.\nThe above implementation only uses default streams on both source and\ndestination devices, hence it is not necessary to enforce additional\nsynchronizations.\n\n<img alt=\"\" src=\"../_images/mp_vs_rn_vs_pp.png\"/>",
            "markdown"
        ],
        [
            "The experiment result shows that, pipelining inputs to model parallel\nResNet50 speeds up the training process by roughly 3.75/2.51-1=49%. It is\nstill quite far away from the ideal 100% speedup. As we have introduced a new\nparameter split_sizes in our pipeline parallel implementation, it is\nunclear how the new parameter affects the overall training time. Intuitively\nspeaking, using small split_size leads to many tiny CUDA kernel launch,\nwhile using large split_size results to relatively long idle times during\nthe first and last splits. Neither are optimal. There might be an optimal\nsplit_size configuration for this specific experiment. Let us try to find\nit by running experiments using several different split_size values.",
            "markdown"
        ],
        [
            "means = []\nstds = []\nsplit_sizes = [1, 3, 5, 8, 10, 12, 20, 40, 60]\n\nfor split_size in split_sizes:\n    setup = \"model = PipelineParallelResNet50(split_size=%d)\" % split_size\n    pp_run_times = timeit.repeat(\n        stmt, setup, number=1, repeat=num_repeat, globals=globals())\n    means.append(np.mean(pp_run_times))\n    stds.append(np.std(pp_run_times))\n\nfig, ax = plt.subplots()\nax.plot(split_sizes, means)\nax.errorbar(split_sizes, means, yerr=stds, ecolor='red', fmt='ro')\nax.set_ylabel('ResNet50 Execution Time (Second)')\nax.set_xlabel('Pipeline Split Size')\nax.set_xticks(split_sizes)\nax.yaxis.grid(True)\nplt.tight_layout()\nplt.savefig(\"split_size_tradeoff.png\")\nplt.close(fig)\n\n\n\n<img alt=\"\" src=\"../_images/split_size_tradeoff.png\"/>",
            "code"
        ],
        [
            "The result shows that setting split_size to 12 achieves the fastest\ntraining speed, which leads to 3.75/2.43-1=54% speedup. There are\nstill opportunities to further accelerate the training process. For example,\nall operations on cuda:0 is placed on its default stream. It means that\ncomputations on the next split cannot overlap with the copy operation of the\nprev split. However, as prev and next splits are different tensors, there is\nno problem to overlap one\u2019s computation with the other one\u2019s copy. The\nimplementation need to use multiple streams on both GPUs, and different\nsub-network structures require different stream management strategies. As no\ngeneral multi-stream solution works for all model parallel use cases, we will\nnot discuss it in this tutorial.",
            "markdown"
        ],
        [
            "<strong>Note:</strong>",
            "markdown"
        ],
        [
            "This post shows several performance measurements. You might see different\nnumbers when running the same code on your own machine, because the result\ndepends on the underlying hardware and software. To get the best performance\nfor your environment, a proper approach is to first generate the curve to\nfigure out the best split size, and then use that split size to pipeline\ninputs.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 3 minutes  50.597 seconds)",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Getting Started with Distributed Data Parallel": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "<strong>Edited by</strong>: ",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            " View and edit this tutorial in .",
            "markdown"
        ],
        [
            "Prerequisites:",
            "markdown"
        ],
        [
            "(DDP) implements data parallelism at the module level which can run across\nmultiple machines. Applications using DDP should spawn multiple processes and\ncreate a single DDP instance per process. DDP uses collective communications in the\n\npackage to synchronize gradients and buffers. More specifically, DDP registers\nan autograd hook for each parameter given by model.parameters() and the\nhook will fire when the corresponding gradient is computed in the backward\npass. Then DDP uses that signal to trigger gradient synchronization across\nprocesses. Please refer to\n for more details.",
            "markdown"
        ],
        [
            "The recommended way to use DDP is to spawn one process for each model replica,\nwhere a model replica can span multiple devices. DDP processes can be\nplaced on the same machine or across machines, but GPU devices cannot be\nshared across processes. This tutorial starts from a basic DDP use case and\nthen demonstrates more advanced use cases including checkpointing models and\ncombining DDP with model parallel.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The code in this tutorial runs on an 8-GPU server, but it can be easily\ngeneralized to other environments.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Getting Started with Distributed Data Parallel->Comparison between DataParallel and DistributedDataParallel": [
        [
            "Before we dive in, let\u2019s clarify why, despite the added complexity, you would\nconsider using DistributedDataParallel over DataParallel:",
            "markdown"
        ],
        [
            "First, DataParallel is single-process, multi-thread, and only works on a\nsingle machine, while DistributedDataParallel is multi-process and works\nfor both single- and multi- machine training. DataParallel is usually\nslower than DistributedDataParallel even on a single machine due to GIL\ncontention across threads, per-iteration replicated model, and additional\noverhead introduced by scattering inputs and gathering outputs.",
            "markdown"
        ],
        [
            "Recall from the\n\nthat if your model is too large to fit on a single GPU, you must use <strong>model parallel</strong>\nto split it across multiple GPUs. DistributedDataParallel works with\n<strong>model parallel</strong>; DataParallel does not at this time. When DDP is combined\nwith model parallel, each DDP process would use model parallel, and all processes\ncollectively would use data parallel.",
            "markdown"
        ],
        [
            "If your model needs to span multiple machines or if your use case does not fit\ninto data parallelism paradigm, please see \nfor more generic distributed training support.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Getting Started with Distributed Data Parallel->Basic Use Case": [
        [
            "To create a DDP module, you must first set up process groups properly. More details can\nbe found in\n.",
            "markdown"
        ],
        [
            "import os\nimport sys\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# On Windows platform, the torch.distributed package only\n# supports Gloo backend, FileStore and TcpStore.\n# For FileStore, set init_method parameter in init_process_group\n# to a local file. Example as follow:\n# init_method=\"file:///f:/libtmp/some_file\"\n# dist.init_process_group(\n#    \"gloo\",\n#    rank=rank,\n#    init_method=init_method,\n#    world_size=world_size)\n# For TcpStore, same way as on Linux.\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()",
            "code"
        ],
        [
            "Now, let\u2019s create a toy module, wrap it with DDP, and feed it some dummy\ninput data. Please note, as DDP broadcasts model states from rank 0 process to\nall other processes in the DDP constructor, you do not need to worry about\ndifferent DDP processes starting from different initial model parameter values.",
            "markdown"
        ],
        [
            "class ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef demo_basic(rank, world_size):\n    print(f\"Running basic DDP example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # create model and move it to GPU with id rank\n    model = ToyModel().to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(rank)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)",
            "code"
        ],
        [
            "As you can see, DDP wraps lower-level distributed communication details and\nprovides a clean API as if it were a local model. Gradient synchronization\ncommunications take place during the backward pass and overlap with the\nbackward computation. When the backward() returns, param.grad already\ncontains the synchronized gradient tensor. For basic use cases, DDP only\nrequires a few more LoCs to set up the process group. When applying DDP to more\nadvanced use cases, some caveats require caution.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Getting Started with Distributed Data Parallel->Skewed Processing Speeds": [
        [
            "In DDP, the constructor, the forward pass, and the backward pass are\ndistributed synchronization points. Different processes are expected to launch\nthe same number of synchronizations and reach these synchronization points in\nthe same order and enter each synchronization point at roughly the same time.\nOtherwise, fast processes might arrive early and timeout while waiting for\nstragglers. Hence, users are responsible for balancing workload distributions\nacross processes. Sometimes, skewed processing speeds are inevitable due to,\ne.g., network delays, resource contentions, or unpredictable workload spikes. To\navoid timeouts in these situations, make sure that you pass a sufficiently\nlarge timeout value when calling\n.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Getting Started with Distributed Data Parallel->Save and Load Checkpoints": [
        [
            "It\u2019s common to use torch.save and torch.load to checkpoint modules\nduring training and recover from checkpoints. See\n\nfor more details. When using DDP, one optimization is to save the model in\nonly one process and then load it to all processes, reducing write overhead.\nThis is correct because all processes start from the same parameters and\ngradients are synchronized in backward passes, and hence optimizers should keep\nsetting parameters to the same values. If you use this optimization, make sure no process starts\nloading before the saving is finished. Additionally, when\nloading the module, you need to provide an appropriate map_location\nargument to prevent a process from stepping into others\u2019 devices. If map_location\nis missing, torch.load will first load the module to CPU and then copy each\nparameter to where it was saved, which would result in all processes on the\nsame machine using the same set of devices. For more advanced failure recovery\nand elasticity support, please refer to .",
            "markdown"
        ],
        [
            "def demo_checkpoint(rank, world_size):\n    print(f\"Running DDP checkpoint example on rank {rank}.\")\n    setup(rank, world_size)\n\n    model = ToyModel().to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n\n\n    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n    if rank == 0:\n        # All processes should see same parameters as they all start from same\n        # random parameters and gradients are synchronized in backward passes.\n        # Therefore, saving it in one process is sufficient.\n        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n    # Use a barrier() to make sure that process 1 loads the model after process\n    # 0 saves it.\n    dist.barrier()\n    # configure map_location properly\n    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n    ddp_model.load_state_dict(\n        torch.load(CHECKPOINT_PATH, map_location=map_location))\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(rank)\n\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    # Not necessary to use a dist.barrier() to guard the file deletion below\n    # as the AllReduce ops in the backward pass of DDP already served as\n    # a synchronization.\n\n    if rank == 0:\n        os.remove(CHECKPOINT_PATH)\n\n    cleanup()",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Getting Started with Distributed Data Parallel->Combining DDP with Model Parallelism": [
        [
            "DDP also works with multi-GPU models. DDP wrapping multi-GPU models is especially\nhelpful when training large models with a huge amount of data.",
            "markdown"
        ],
        [
            "class ToyMpModel(nn.Module):\n    def __init__(self, dev0, dev1):\n        super(ToyMpModel, self).__init__()\n        self.dev0 = dev0\n        self.dev1 = dev1\n        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n\n    def forward(self, x):\n        x = x.to(self.dev0)\n        x = self.relu(self.net1(x))\n        x = x.to(self.dev1)\n        return self.net2(x)",
            "code"
        ],
        [
            "When passing a multi-GPU model to DDP, device_ids and output_device\nmust NOT be set. Input and output data will be placed in proper devices by\neither the application or the model forward() method.",
            "markdown"
        ],
        [
            "def demo_model_parallel(rank, world_size):\n    print(f\"Running DDP with model parallel example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # setup mp_model and devices for this process\n    dev0 = (rank * 2) % world_size\n    dev1 = (rank * 2 + 1) % world_size\n    mp_model = ToyMpModel(dev0, dev1)\n    ddp_mp_model = DDP(mp_model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    # outputs will be on dev1\n    outputs = ddp_mp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(dev1)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\n\nif __name__ == \"__main__\":\n    n_gpus = torch.cuda.device_count()\n    assert n_gpus &gt;= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n    world_size = n_gpus\n    run_demo(demo_basic, world_size)\n    run_demo(demo_checkpoint, world_size)\n    run_demo(demo_model_parallel, world_size)",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Getting Started with Distributed Data Parallel->Initialize DDP with torch.distributed.run/torchrun": [
        [
            "We can leverage PyTorch Elastic to simplify the DDP code and initialize the job more easily.\nLet\u2019s still use the Toymodel example and create a file named elastic_ddp.py.",
            "markdown"
        ],
        [
            "import torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef demo_basic():\n    dist.init_process_group(\"nccl\")\n    rank = dist.get_rank()\n    print(f\"Start running basic DDP example on rank {rank}.\")\n\n    # create model and move it to GPU with id rank\n    device_id = rank % torch.cuda.device_count()\n    model = ToyModel().to(device_id)\n    ddp_model = DDP(model, device_ids=[device_id])\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_id)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\nif __name__ == \"__main__\":\n    demo_basic()",
            "code"
        ],
        [
            "One can then run a  command\non all nodes to initialize the DDP job created above:",
            "markdown"
        ],
        [
            "torchrun --nnodes=2 --nproc_per_node=8 --rdzv_id=100 --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR:29400 elastic_ddp.py",
            "code"
        ],
        [
            "We are running the DDP script on two hosts, and each host we run with 8 processes, aka, we\nare running it on 16 GPUs. Note that $MASTER_ADDR must be the same across all nodes.",
            "markdown"
        ],
        [
            "Here torchrun will launch 8 process and invoke elastic_ddp.py\non each process on the node it is launched on, but user also needs to apply cluster\nmanagement tools like slurm to actually run this command on 2 nodes.",
            "markdown"
        ],
        [
            "For example, on a SLURM enabled cluster, we can write a script to run the command above\nand set MASTER_ADDR as:",
            "markdown"
        ],
        [
            "export MASTER_ADDR=$(scontrol show hostname ${SLURM_NODELIST} | head -n 1)",
            "code"
        ],
        [
            "Then we can just run this script using the SLURM command: srun --nodes=2 ./torchrun_script.sh.\nOf course, this is just an example; you can choose your own cluster scheduling tools\nto initiate the torchrun job.",
            "markdown"
        ],
        [
            "For more information about Elastic run, one can check this\n to learn more.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            " View and edit this tutorial in .",
            "markdown"
        ],
        [
            "Prerequisites:",
            "markdown"
        ],
        [
            "In this short tutorial, we will be going over the distributed package\nof PyTorch. We\u2019ll see how to set up the distributed setting, use the\ndifferent communication strategies, and go over some of the internals of\nthe package.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Setup": [
        [
            "The distributed package included in PyTorch (i.e.,\ntorch.distributed) enables researchers and practitioners to easily\nparallelize their computations across processes and clusters of\nmachines. To do so, it leverages message passing semantics\nallowing each process to communicate data to any of the other processes.\nAs opposed to the multiprocessing (torch.multiprocessing) package,\nprocesses can use different communication backends and are not\nrestricted to being executed on the same machine.",
            "markdown"
        ],
        [
            "In order to get started we need the ability to run multiple processes\nsimultaneously. If you have access to compute cluster you should check\nwith your local sysadmin or use your favorite coordination tool (e.g.,\n,\n, or\n). For the purpose of this\ntutorial, we will use a single machine and spawn multiple processes using\nthe following template.",
            "markdown"
        ],
        [
            "\"\"\"run.py:\"\"\"\n#!/usr/bin/env python\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\ndef run(rank, size):\n    \"\"\" Distributed function to be implemented later. \"\"\"\n    pass\n\ndef init_process(rank, size, fn, backend='gloo'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n    fn(rank, size)\n\n\nif __name__ == \"__main__\":\n    size = 2\n    processes = []\n    mp.set_start_method(\"spawn\")\n    for rank in range(size):\n        p = mp.Process(target=init_process, args=(rank, size, run))\n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()",
            "code"
        ],
        [
            "The above script spawns two processes who will each setup the\ndistributed environment, initialize the process group\n(dist.init_process_group), and finally execute the given run\nfunction.",
            "markdown"
        ],
        [
            "Let\u2019s have a look at the init_process function. It ensures that\nevery process will be able to coordinate through a master, using the\nsame ip address and port. Note that we used the gloo backend but\nother backends are available. (c.f.\n) We will go over the magic\nhappening in dist.init_process_group at the end of this tutorial,\nbut it essentially allows processes to communicate with each other by\nsharing their locations.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Point-to-Point Communication": [
        [
            "Send and Recv",
            "markdown"
        ],
        [
            "A transfer of data from one process to another is called a\npoint-to-point communication. These are achieved through the send\nand recv functions or their <em>immediate</em> counter-parts, isend and\nirecv.",
            "markdown"
        ],
        [
            "\"\"\"Blocking point-to-point communication.\"\"\"\n\ndef run(rank, size):\n    tensor = torch.zeros(1)\n    if rank == 0:\n        tensor += 1\n        # Send the tensor to process 1\n        dist.send(tensor=tensor, dst=1)\n    else:\n        # Receive tensor from process 0\n        dist.recv(tensor=tensor, src=0)\n    print('Rank ', rank, ' has data ', tensor[0])",
            "code"
        ],
        [
            "In the above example, both processes start with a zero tensor, then\nprocess 0 increments the tensor and sends it to process 1 so that they\nboth end up with 1.0. Notice that process 1 needs to allocate memory in\norder to store the data it will receive.",
            "markdown"
        ],
        [
            "Also notice that send/recv are <strong>blocking</strong>: both processes stop\nuntil the communication is completed. On the other hand immediates are\n<strong>non-blocking</strong>; the script continues its execution and the methods\nreturn a Work object upon which we can choose to\nwait().",
            "markdown"
        ],
        [
            "\"\"\"Non-blocking point-to-point communication.\"\"\"\n\ndef run(rank, size):\n    tensor = torch.zeros(1)\n    req = None\n    if rank == 0:\n        tensor += 1\n        # Send the tensor to process 1\n        req = dist.isend(tensor=tensor, dst=1)\n        print('Rank 0 started sending')\n    else:\n        # Receive tensor from process 0\n        req = dist.irecv(tensor=tensor, src=0)\n        print('Rank 1 started receiving')\n    req.wait()\n    print('Rank ', rank, ' has data ', tensor[0])",
            "code"
        ],
        [
            "When using immediates we have to be careful about how we use the sent and received tensors.\nSince we do not know when the data will be communicated to the other process,\nwe should not modify the sent tensor nor access the received tensor before req.wait() has completed.\nIn other words,",
            "markdown"
        ],
        [
            "writing to tensor after dist.isend() will result in undefined behaviour.",
            "markdown"
        ],
        [
            "reading from tensor after dist.irecv() will result in undefined behaviour.",
            "markdown"
        ],
        [
            "However, after req.wait()\nhas been executed we are guaranteed that the communication took place,\nand that the value stored in tensor[0] is 1.0.",
            "markdown"
        ],
        [
            "Point-to-point communication is useful when we want more fine-grained\ncontrol over the communication of our processes. They can be used to\nimplement fancy algorithms, such as the one used in  or\n.(c.f.\n)",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Collective Communication": [
        [
            "As opposed to point-to-point communcation, collectives allow for\ncommunication patterns across all processes in a <strong>group</strong>. A group is a\nsubset of all our processes. To create a group, we can pass a list of\nranks to dist.new_group(group). By default, collectives are executed\non all processes, also known as the <strong>world</strong>. For example, in order\nto obtain the sum of all tensors on all processes, we can use the\ndist.all_reduce(tensor, op, group) collective.",
            "markdown"
        ],
        [
            "\"\"\" All-Reduce example.\"\"\"\ndef run(rank, size):\n    \"\"\" Simple collective communication. \"\"\"\n    group = dist.new_group([0, 1])\n    tensor = torch.ones(1)\n    dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])",
            "code"
        ],
        [
            "Since we want the sum of all tensors in the group, we use\ndist.ReduceOp.SUM as the reduce operator. Generally speaking, any\ncommutative mathematical operation can be used as an operator.\nOut-of-the-box, PyTorch comes with 4 such operators, all working at the\nelement-wise level:",
            "markdown"
        ],
        [
            "dist.ReduceOp.SUM,",
            "markdown"
        ],
        [
            "dist.ReduceOp.PRODUCT,",
            "markdown"
        ],
        [
            "dist.ReduceOp.MAX,",
            "markdown"
        ],
        [
            "dist.ReduceOp.MIN.",
            "markdown"
        ],
        [
            "In addition to dist.all_reduce(tensor, op, group), there are a total\nof 6 collectives currently implemented in PyTorch.",
            "markdown"
        ],
        [
            "dist.broadcast(tensor, src, group): Copies tensor from\nsrc to all other processes.",
            "markdown"
        ],
        [
            "dist.reduce(tensor, dst, op, group): Applies op to every\ntensor and stores the result in dst.",
            "markdown"
        ],
        [
            "dist.all_reduce(tensor, op, group): Same as reduce, but the\nresult is stored in all processes.",
            "markdown"
        ],
        [
            "dist.scatter(tensor, scatter_list, src, group): Copies the\n\\(i^{\\text{th}}\\) tensor scatter_list[i] to the\n\\(i^{\\text{th}}\\) process.",
            "markdown"
        ],
        [
            "dist.gather(tensor, gather_list, dst, group): Copies tensor\nfrom all processes in dst.",
            "markdown"
        ],
        [
            "dist.all_gather(tensor_list, tensor, group): Copies tensor\nfrom all processes to tensor_list, on all processes.",
            "markdown"
        ],
        [
            "dist.barrier(group): Blocks all processes in <cite>group</cite> until each one has entered this function.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Distributed Training": [
        [
            "<strong>Note:</strong> You can find the example script of this section in .",
            "markdown"
        ],
        [
            "Now that we understand how the distributed module works, let us write\nsomething useful with it. Our goal will be to replicate the\nfunctionality of\n.\nOf course, this will be a didactic example and in a real-world\nsituation you should use the official, well-tested and well-optimized\nversion linked above.",
            "markdown"
        ],
        [
            "Quite simply we want to implement a distributed version of stochastic\ngradient descent. Our script will let all processes compute the\ngradients of their model on their batch of data and then average their\ngradients. In order to ensure similar convergence results when changing\nthe number of processes, we will first have to partition our dataset.\n(You could also use\n,\ninstead of the snippet below.)",
            "markdown"
        ],
        [
            "\"\"\" Dataset partitioning helper \"\"\"\nclass Partition(object):\n\n    def __init__(self, data, index):\n        self.data = data\n        self.index = index\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, index):\n        data_idx = self.index[index]\n        return self.data[data_idx]\n\n\nclass DataPartitioner(object):\n\n    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n        self.data = data\n        self.partitions = []\n        rng = Random()\n        rng.seed(seed)\n        data_len = len(data)\n        indexes = [x for x in range(0, data_len)]\n        rng.shuffle(indexes)\n\n        for frac in sizes:\n            part_len = int(frac * data_len)\n            self.partitions.append(indexes[0:part_len])\n            indexes = indexes[part_len:]\n\n    def use(self, partition):\n        return Partition(self.data, self.partitions[partition])",
            "code"
        ],
        [
            "With the above snippet, we can now simply partition any dataset using\nthe following few lines:",
            "markdown"
        ],
        [
            "\"\"\" Partitioning MNIST \"\"\"\ndef partition_dataset():\n    dataset = datasets.MNIST('./data', train=True, download=True,\n                             transform=transforms.Compose([\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.1307,), (0.3081,))\n                             ]))\n    size = dist.get_world_size()\n    bsz = 128 / float(size)\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    partition = partition.use(dist.get_rank())\n    train_set = torch.utils.data.DataLoader(partition,\n                                         batch_size=bsz,\n                                         shuffle=True)\n    return train_set, bsz",
            "code"
        ],
        [
            "Assuming we have 2 replicas, then each process will have a train_set\nof 60000 / 2 = 30000 samples. We also divide the batch size by the\nnumber of replicas in order to maintain the <em>overall</em> batch size of 128.",
            "markdown"
        ],
        [
            "We can now write our usual forward-backward-optimize training code, and\nadd a function call to average the gradients of our models. (The\nfollowing is largely inspired by the official .)",
            "markdown"
        ],
        [
            "\"\"\" Distributed Synchronous SGD Example \"\"\"\ndef run(rank, size):\n    torch.manual_seed(1234)\n    train_set, bsz = partition_dataset()\n    model = Net()\n    optimizer = optim.SGD(model.parameters(),\n                          lr=0.01, momentum=0.5)\n\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\n    for epoch in range(10):\n        epoch_loss = 0.0\n        for data, target in train_set:\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            average_gradients(model)\n            optimizer.step()\n        print('Rank ', dist.get_rank(), ', epoch ',\n              epoch, ': ', epoch_loss / num_batches)",
            "code"
        ],
        [
            "It remains to implement the average_gradients(model) function, which\nsimply takes in a model and averages its gradients across the whole\nworld.",
            "markdown"
        ],
        [
            "\"\"\" Gradient averaging. \"\"\"\ndef average_gradients(model):\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n        param.grad.data /= size",
            "code"
        ],
        [
            "<em>Et voil\u00e0</em>! We successfully implemented distributed synchronous SGD and\ncould train any model on a large computer cluster.",
            "markdown"
        ],
        [
            "<strong>Note:</strong> While the last sentence is <em>technically</em> true, there are  required to\nimplement a production-level implementation of synchronous SGD. Again,\nuse what .",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Distributed Training->Our Own Ring-Allreduce": [
        [
            "As an additional challenge, imagine that we wanted to implement\nDeepSpeech\u2019s efficient ring allreduce. This is fairly easy to implement\nusing point-to-point collectives.",
            "markdown"
        ],
        [
            "\"\"\" Implementation of a ring-reduce with addition. \"\"\"\ndef allreduce(send, recv):\n   rank = dist.get_rank()\n   size = dist.get_world_size()\n   send_buff = send.clone()\n   recv_buff = send.clone()\n   accum = send.clone()\n\n   left = ((rank - 1) + size) % size\n   right = (rank + 1) % size\n\n   for i in range(size - 1):\n       if i % 2 == 0:\n           # Send send_buff\n           send_req = dist.isend(send_buff, right)\n           dist.recv(recv_buff, left)\n           accum[:] += recv_buff[:]\n       else:\n           # Send recv_buff\n           send_req = dist.isend(recv_buff, right)\n           dist.recv(send_buff, left)\n           accum[:] += send_buff[:]\n       send_req.wait()\n   recv[:] = accum[:]",
            "code"
        ],
        [
            "In the above script, the allreduce(send, recv) function has a\nslightly different signature than the ones in PyTorch. It takes a\nrecv tensor and will store the sum of all send tensors in it. As\nan exercise left to the reader, there is still one difference between\nour version and the one in DeepSpeech: their implementation divides the\ngradient tensor into <em>chunks</em>, so as to optimally utilize the\ncommunication bandwidth. (Hint:\n)",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Advanced Topics": [
        [
            "We are now ready to discover some of the more advanced functionalities\nof torch.distributed. Since there is a lot to cover, this section is\ndivided into two subsections:",
            "markdown"
        ],
        [
            "Communication Backends: where we learn how to use MPI and Gloo for\nGPU-GPU communication.",
            "markdown"
        ],
        [
            "Initialization Methods: where we understand how to best set up the\ninitial coordination phase in dist.init_process_group().",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Advanced Topics->Communication Backends": [
        [
            "One of the most elegant aspects of torch.distributed is its ability\nto abstract and build on top of different backends. As mentioned before,\nthere are currently three backends implemented in PyTorch: Gloo, NCCL, and\nMPI. They each have different specifications and tradeoffs, depending\non the desired use case. A comparative table of supported functions can\nbe found\n.",
            "markdown"
        ],
        [
            "<strong>Gloo Backend</strong>",
            "markdown"
        ],
        [
            "So far we have made extensive usage of the .\nIt is quite handy as a development platform, as it is included in\nthe pre-compiled PyTorch binaries and works on both Linux (since 0.2)\nand macOS (since 1.3). It supports all point-to-point and collective\noperations on CPU, and all collective operations on GPU. The\nimplementation of the collective operations for CUDA tensors is not as\noptimized as the ones provided by the NCCL backend.",
            "markdown"
        ],
        [
            "As you have surely noticed, our\ndistributed SGD example does not work if you put model on the GPU.\nIn order to use multiple GPUs, let us also make the following\nmodifications:",
            "markdown"
        ],
        [
            "Use device = torch.device(\"cuda:{}\".format(rank))",
            "markdown"
        ],
        [
            "model = Net() \\(\\rightarrow\\) model = Net().to(device)",
            "markdown"
        ],
        [
            "Use data, target = data.to(device), target.to(device)",
            "markdown"
        ],
        [
            "With the above modifications, our model is now training on two GPUs and\nyou can monitor their utilization with watch nvidia-smi.",
            "markdown"
        ],
        [
            "<strong>MPI Backend</strong>",
            "markdown"
        ],
        [
            "The Message Passing Interface (MPI) is a standardized tool from the\nfield of high-performance computing. It allows to do point-to-point and\ncollective communications and was the main inspiration for the API of\ntorch.distributed. Several implementations of MPI exist (e.g.\n,\n, ) each\noptimized for different purposes. The advantage of using the MPI backend\nlies in MPI\u2019s wide availability - and high-level of optimization - on\nlarge computer clusters. \n\n are also able to take\nadvantage of CUDA IPC and GPU Direct technologies in order to avoid\nmemory copies through the CPU.",
            "markdown"
        ],
        [
            "Unfortunately, PyTorch\u2019s binaries cannot include an MPI implementation\nand we\u2019ll have to recompile it by hand. Fortunately, this process is\nfairly simple given that upon compilation, PyTorch will look <em>by itself</em>\nfor an available MPI implementation. The following steps install the MPI\nbackend, by installing PyTorch .",
            "markdown"
        ],
        [
            "Create and activate your Anaconda environment, install all the\npre-requisites following , but do\n<strong>not</strong> run python setup.py install yet.",
            "markdown"
        ],
        [
            "Choose and install your favorite MPI implementation. Note that\nenabling CUDA-aware MPI might require some additional steps. In our\ncase, we\u2019ll stick to Open-MPI <em>without</em> GPU support:\nconda install -c conda-forge openmpi",
            "markdown"
        ],
        [
            "Now, go to your cloned PyTorch repo and execute\npython setup.py install.",
            "markdown"
        ],
        [
            "In order to test our newly installed backend, a few modifications are\nrequired.",
            "markdown"
        ],
        [
            "Replace the content under if __name__ == '__main__': with\ninit_process(0, 0, run, backend='mpi').",
            "markdown"
        ],
        [
            "Run mpirun -n 4 python myscript.py.",
            "markdown"
        ],
        [
            "The reason for these changes is that MPI needs to create its own\nenvironment before spawning the processes. MPI will also spawn its own\nprocesses and perform the handshake described in , making the rankand size\narguments of init_process_group superfluous. This is actually quite\npowerful as you can pass additional arguments to mpirun in order to\ntailor computational resources for each process. (Things like number of\ncores per process, hand-assigning machines to specific ranks, and )\nDoing so, you should obtain the same familiar output as with the other\ncommunication backends.",
            "markdown"
        ],
        [
            "<strong>NCCL Backend</strong>",
            "markdown"
        ],
        [
            "The  provides an\noptimized implementation of collective operations against CUDA\ntensors. If you only use CUDA tensors for your collective operations,\nconsider using this backend for the best in class performance. The\nNCCL backend is included in the pre-built binaries with CUDA support.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Advanced Topics->Initialization Methods": [
        [
            "To finish this tutorial, let\u2019s talk about the very first function we\ncalled: dist.init_process_group(backend, init_method). In\nparticular, we will go over the different initialization methods which\nare responsible for the initial coordination step between each process.\nThose methods allow you to define how this coordination is done.\nDepending on your hardware setup, one of these methods should be\nnaturally more suitable than the others. In addition to the following\nsections, you should also have a look at the .",
            "markdown"
        ],
        [
            "<strong>Environment Variable</strong>",
            "markdown"
        ],
        [
            "We have been using the environment variable initialization method\nthroughout this tutorial. By setting the following four environment\nvariables on all machines, all processes will be able to properly\nconnect to the master, obtain information about the other processes, and\nfinally handshake with them.",
            "markdown"
        ],
        [
            "MASTER_PORT: A free port on the machine that will host the\nprocess with rank 0.",
            "markdown"
        ],
        [
            "MASTER_ADDR: IP address of the machine that will host the process\nwith rank 0.",
            "markdown"
        ],
        [
            "WORLD_SIZE: The total number of processes, so that the master\nknows how many workers to wait for.",
            "markdown"
        ],
        [
            "RANK: Rank of each process, so they will know whether it is the\nmaster of a worker.",
            "markdown"
        ],
        [
            "<strong>Shared File System</strong>",
            "markdown"
        ],
        [
            "The shared filesystem requires all processes to have access to a shared\nfile system, and will coordinate them through a shared file. This means\nthat each process will open the file, write its information, and wait\nuntil everybody did so. After that all required information will be\nreadily available to all processes. In order to avoid race conditions,\nthe file system must support locking through\n.",
            "markdown"
        ],
        [
            "dist.init_process_group(\n    init_method='file:///mnt/nfs/sharedfile',\n    rank=args.rank,\n    world_size=4)",
            "code"
        ],
        [
            "<strong>TCP</strong>",
            "markdown"
        ],
        [
            "Initializing via TCP can be achieved by providing the IP address of the process with rank 0 and a reachable port number.\nHere, all workers will be able to connect to the process\nwith rank 0 and exchange information on how to reach each other.",
            "markdown"
        ],
        [
            "dist.init_process_group(\n    init_method='tcp://10.1.1.20:23456',\n    rank=args.rank,\n    world_size=4)\n\n\n<!--\n## Internals\n* The magic behind init_process_group:\n\n1. validate and parse the arguments\n2. resolve the backend: name2channel.at()\n3. Drop GIL & THDProcessGroupInit: instantiate the channel and add address of master from config\n4. rank 0 inits master, others workers\n5. master: create sockets for all workers -> wait for all workers to connect -> send them each the info about location of other processes\n6. worker: create socket to master, send own info, receive info about each worker, and then handshake with each of them\n7. By this time everyone has handshake with everyone.\n--><center>",
            "code"
        ],
        [
            "<strong>Acknowledgements</strong>\n</center>",
            "markdown"
        ],
        [
            "I\u2019d like to thank the PyTorch developers for doing such a good job on\ntheir implementation, documentation, and tests. When the code was\nunclear, I could always count on the\n or the\n\nto find an answer. In particular, I\u2019d like to thank Soumith Chintala,\nAdam Paszke, and Natalia Gimelshein for providing insightful comments\nand answering questions on early drafts.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)": [
        [
            "<strong>Author</strong>: , , ",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            " View and edit this tutorial in .",
            "markdown"
        ],
        [
            "Training AI models at a large scale is a challenging task that requires a lot of compute power and resources.\nIt also comes with considerable engineering complexity to handle the training of these very large models.\n, released in PyTorch 1.11 makes this easier.",
            "markdown"
        ],
        [
            "In this tutorial, we show how to use , for simple MNIST models that can be extended to other larger models such as ,\n . The sample DDP MNIST code has been borrowed from .",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How FSDP works": [
        [
            "In , (DDP) training, each process/ worker owns a replica of the model and processes a batch of data, finally it uses all-reduce to sum up gradients over different workers. In DDP the model weights and optimizer states are replicated across all workers. FSDP is a type of data parallelism that shards model parameters, optimizer states and gradients across DDP ranks.",
            "markdown"
        ],
        [
            "FSDP GPU memory footprint would be smaller than DDP across all workers. This makes the training of some very large models feasible and helps to fit larger models or batch sizes for our training job. This would come with the cost of increased communication volume. The communication overhead is reduced by internal optimizations like communication and computation overlapping.",
            "markdown"
        ],
        [
            "FSDP Workflow",
            "markdown"
        ],
        [
            "At high level FSDP works as follow:",
            "markdown"
        ],
        [
            "<em>In constructor</em>",
            "markdown"
        ],
        [
            "Shard model parameters and each rank only keeps its own shard",
            "markdown"
        ],
        [
            "<em>In forward path</em>",
            "markdown"
        ],
        [
            "Run all_gather to collect all shards from all ranks to recover the full parameter in this FSDP unit",
            "markdown"
        ],
        [
            "Run forward computation",
            "markdown"
        ],
        [
            "Discard parameter shards it has just collected",
            "markdown"
        ],
        [
            "<em>In backward path</em>",
            "markdown"
        ],
        [
            "Run all_gather to collect all shards from all ranks to recover the full parameter in this FSDP unit",
            "markdown"
        ],
        [
            "Run backward computation",
            "markdown"
        ],
        [
            "Run reduce_scatter to sync gradients",
            "markdown"
        ],
        [
            "Discard parameters.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How to use FSDP": [
        [
            "Here we use a toy model to run training on MNIST dataset for demonstration purposes. Similarly the APIs and logic can be applied to larger models for training.",
            "markdown"
        ],
        [
            "<em>Setup</em>",
            "markdown"
        ],
        [
            "1.1 Install Pytorch along with Torchvision",
            "markdown"
        ],
        [
            "pip3 install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html",
            "code"
        ],
        [
            "We add the following code snippets to a python script \u201cFSDP_mnist.py\u201d.",
            "markdown"
        ],
        [
            "1.2  Import necessary packages",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This tutorial is intended for PyTorch versions 1.12 and later. If you are using an earlier version, replace all instances of <cite>size_based_auto_wrap_policy</cite> with <cite>default_auto_wrap_policy</cite>.",
            "markdown"
        ],
        [
            "# Based on: https://github.com/pytorch/examples/blob/master/mnist/main.py\nimport os\nimport argparse\nimport functools\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n\nfrom torch.optim.lr_scheduler import StepLR\n\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import (\n    CPUOffload,\n    BackwardPrefetch,\n)\nfrom torch.distributed.fsdp.wrap import (\n    size_based_auto_wrap_policy,\n    enable_wrap,\n    wrap,\n)",
            "code"
        ],
        [
            "1.3 Distributed training setup. As we mentioned FSDP is a type of data parallelism which requires a distributed training environment, so here we use two helper functions to initialize the processes for distributed training and clean up.",
            "markdown"
        ],
        [
            "def setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()",
            "code"
        ],
        [
            "2.1  Define our toy model for handwritten digit classification.",
            "markdown"
        ],
        [
            "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output",
            "code"
        ],
        [
            "2.2 define a train function",
            "markdown"
        ],
        [
            "def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n    model.train()\n    ddp_loss = torch.zeros(2).to(rank)\n    if sampler:\n        sampler.set_epoch(epoch)\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(rank), target.to(rank)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target, reduction='sum')\n        loss.backward()\n        optimizer.step()\n        ddp_loss[0] += loss.item()\n        ddp_loss[1] += len(data)\n\n    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n    if rank == 0:\n        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))",
            "code"
        ],
        [
            "2.3 Define a validation function",
            "markdown"
        ],
        [
            "def test(model, rank, world_size, test_loader):\n    model.eval()\n    correct = 0\n    ddp_loss = torch.zeros(3).to(rank)\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(rank), target.to(rank)\n            output = model(data)\n            ddp_loss[0] += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            ddp_loss[1] += pred.eq(target.view_as(pred)).sum().item()\n            ddp_loss[2] += len(data)\n\n    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n\n    if rank == 0:\n        test_loss = ddp_loss[0] / ddp_loss[2]\n        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n            test_loss, int(ddp_loss[1]), int(ddp_loss[2]),\n            100. * ddp_loss[1] / ddp_loss[2]))",
            "code"
        ],
        [
            "2.4 Define a distributed train function that wraps the model in FSDP",
            "markdown"
        ],
        [
            "<strong>Note: to save the FSDP model, we need to call the state_dict on each rank then on Rank 0 save the overall states. This is only available in Pytorch nightlies, current Pytorch release is 1.11 at the moment.</strong>",
            "markdown"
        ],
        [
            "def fsdp_main(rank, world_size, args):\n    setup(rank, world_size)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    dataset1 = datasets.MNIST('../data', train=True, download=True,\n                        transform=transform)\n    dataset2 = datasets.MNIST('../data', train=False,\n                        transform=transform)\n\n    sampler1 = DistributedSampler(dataset1, rank=rank, num_replicas=world_size, shuffle=True)\n    sampler2 = DistributedSampler(dataset2, rank=rank, num_replicas=world_size)\n\n    train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}\n    test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}\n    cuda_kwargs = {'num_workers': 2,\n                    'pin_memory': True,\n                    'shuffle': False}\n    train_kwargs.update(cuda_kwargs)\n    test_kwargs.update(cuda_kwargs)\n\n    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n    my_auto_wrap_policy = functools.partial(\n        size_based_auto_wrap_policy, min_num_params=100\n    )\n    torch.cuda.set_device(rank)\n\n\n    init_start_event = torch.cuda.Event(enable_timing=True)\n    init_end_event = torch.cuda.Event(enable_timing=True)\n\n    model = Net().to(rank)\n\n    model = FSDP(model)\n\n    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    init_start_event.record()\n    for epoch in range(1, args.epochs + 1):\n        train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n        test(model, rank, world_size, test_loader)\n        scheduler.step()\n\n    init_end_event.record()\n\n    if rank == 0:\n        print(f\"CUDA event elapsed time: {init_start_event.elapsed_time(init_end_event) / 1000}sec\")\n        print(f\"{model}\")\n\n    if args.save_model:\n        # use a barrier to make sure training is done on all ranks\n        dist_barrier()\n        # state_dict for FSDP model is only available on Nightlies for now\n        states = model.state_dict()\n        if rank == 0:\n            torch.save(states, \"mnist_cnn.pt\")\n\n    cleanup()",
            "code"
        ],
        [
            "2.5 Finally parsing the arguments and setting the main function",
            "markdown"
        ],
        [
            "if __name__ == '__main__':\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n                        help='number of epochs to train (default: 14)')\n    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n                        help='learning rate (default: 1.0)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--save-model', action='store_true', default=False,\n                        help='For Saving the current Model')\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n\n    WORLD_SIZE = torch.cuda.device_count()\n    mp.spawn(fsdp_main,\n        args=(WORLD_SIZE, args),\n        nprocs=WORLD_SIZE,\n        join=True)",
            "code"
        ],
        [
            "We have recorded cuda events to measure the time of FSDP model specifics. The CUDA event time was 110.85 seconds.",
            "markdown"
        ],
        [
            "python FSDP_mnist.py\n\nCUDA event elapsed time on training loop 40.67462890625sec",
            "code"
        ],
        [
            "Wrapping the model with FSDP, the model will look as follows, we can see the model has been wrapped in one FSDP unit.\nAlternatively, we will look at adding the fsdp_auto_wrap_policy next and will discuss the differences.",
            "markdown"
        ],
        [
            "   FullyShardedDataParallel(\n   (_fsdp_wrapped_module): FlattenParamsWrapper(\n       (_fpw_module): Net(\n       (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n       (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n       (dropout1): Dropout(p=0.25, inplace=False)\n       (dropout2): Dropout(p=0.5, inplace=False)\n       (fc1): Linear(in_features=9216, out_features=128, bias=True)\n       (fc2): Linear(in_features=128, out_features=10, bias=True)\n       )\n   )\n)",
            "code"
        ],
        [
            "Following is the peak memory usage from FSDP MNIST training on g4dn.12.xlarge AWS EC2 instance with 4 gpus captured from Pytorch Profiler.",
            "markdown"
        ],
        [
            "FSDP Peak Memory Usage",
            "markdown"
        ],
        [
            "<em>Applying fsdp_auto_wrap_policy</em> in FSDP otherwise, FSDP will put the entire model in one FSDP unit, which will reduce computation efficiency and memory efficiency.\nThe way it works is that, suppose your model contains 100 Linear layers. If you do FSDP(model), there will only be one FSDP unit which wraps the entire model.\nIn that case, the allgather would collect the full parameters for all 100 linear layers, and hence won\u2019t save CUDA memory for parameter sharding.\nAlso, there is only one blocking allgather call for the all 100 linear layers, there will not be communication and computation overlapping between layers.",
            "markdown"
        ],
        [
            "To avoid that, you can pass in an fsdp_auto_wrap_policy, which will seal the current FSDP unit and start a new one automatically when the specified condition is met (e.g., size limit).\nIn that way you will have multiple FSDP units, and only one FSDP unit needs to collect full parameters at a time. E.g., suppose you have 5 FSDP units, and each wraps 20 linear layers.\nThen, in the forward, the 1st FSDP unit will allgather parameters for the first 20 linear layers, do computation, discard the parameters and then move on to the next 20 linear layers. So, at any point in time, each rank only materializes parameters/grads for 20 linear layers instead of 100.",
            "markdown"
        ],
        [
            "To do so in 2.4 we define the auto_wrap_policy and pass it to FSDP wrapper, in the following example, my_auto_wrap_policy defines that a layer could be wrapped or sharded by FSDP if the number of parameters in this layer is larger than 100.\nIf the number of parameters in this layer is smaller than 100, it will be wrapped with other small layers together by FSDP.\nFinding an optimal auto wrap policy is challenging, PyTorch will add auto tuning for this config in the future. Without an auto tuning tool, it is good to profile your workflow using different auto wrap policies experimentally and find the optimal one.",
            "markdown"
        ],
        [
            "my_auto_wrap_policy = functools.partial(\n        size_based_auto_wrap_policy, min_num_params=20000\n    )\ntorch.cuda.set_device(rank)\nmodel = Net().to(rank)\n\nmodel = FSDP(model,\n    fsdp_auto_wrap_policy=my_auto_wrap_policy)",
            "code"
        ],
        [
            "Applying the FSDP_auto_wrap_policy, the model would be as follows:",
            "markdown"
        ],
        [
            "  FullyShardedDataParallel(\n(_fsdp_wrapped_module): FlattenParamsWrapper(\n  (_fpw_module): Net(\n    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n    (dropout1): Dropout(p=0.25, inplace=False)\n    (dropout2): Dropout(p=0.5, inplace=False)\n    (fc1): FullyShardedDataParallel(\n      (_fsdp_wrapped_module): FlattenParamsWrapper(\n        (_fpw_module): Linear(in_features=9216, out_features=128, bias=True)\n      )\n    )\n    (fc2): Linear(in_features=128, out_features=10, bias=True)\n  )\n)",
            "code"
        ],
        [
            "python FSDP_mnist.py\n\nCUDA event elapsed time on training loop 41.89130859375sec",
            "code"
        ],
        [
            "Following is the peak memory usage from FSDP with auto_wrap policy of MNIST training on g4dn.12.xlarge AWS EC2 instance with 4 gpus captured from Pytorch Profiler.\nIt can be observed that the peak memory usage on each device is smaller compared to FSDP without auto wrap policy applied, from ~75 MB to 66 MB.",
            "markdown"
        ],
        [
            "FSDP Peak Memory Usage using Auto_wrap policy",
            "markdown"
        ],
        [
            "<em>CPU Off-loading</em>: In case the model is very large that even with FSDP wouldn\u2019t fit into gpus, then CPU offload can be helpful here.",
            "markdown"
        ],
        [
            "Currently, only parameter and gradient CPU offload is supported. It can be enabled via passing in cpu_offload=CPUOffload(offload_params=True).",
            "markdown"
        ],
        [
            "Note that this currently implicitly enables gradient offloading to CPU in order for params and grads to be on the same device to work with the optimizer. This API is subject to change. Default is None in which case there will be no offloading.",
            "markdown"
        ],
        [
            "Using this feature may slow down the training considerably, due to frequent copying of tensors from host to device, but it could help improve memory efficiency and train larger scale models.",
            "markdown"
        ],
        [
            "In 2.4 we just add it to the FSDP wrapper",
            "markdown"
        ],
        [
            "model = FSDP(model,\n    fsdp_auto_wrap_policy=my_auto_wrap_policy,\n    cpu_offload=CPUOffload(offload_params=True))",
            "code"
        ],
        [
            "Compare it with DDP, if in 2.4 we just normally wrap the model in ddp, saving the changes in \u201cDDP_mnist.py\u201d.",
            "markdown"
        ],
        [
            "model = Net().to(rank)\nmodel = DDP(model)",
            "code"
        ],
        [
            "python DDP_mnist.py\n\nCUDA event elapsed time on training loop 39.77766015625sec",
            "code"
        ],
        [
            "Following is the peak memory usage from DDP MNIST training on g4dn.12.xlarge AWS EC2 instance with 4 gpus captured from Pytorch profiler.",
            "markdown"
        ],
        [
            "DDP Peak Memory Usage using Auto_wrap policy",
            "markdown"
        ],
        [
            "Considering the toy example and tiny MNIST model we defined here, we can observe the difference between peak memory usage of DDP and FSDP.\nIn DDP each process holds a replica of the model, so the memory footprint is higher compared to FSDP that shards the model parameter, optimizer states and gradients over DDP ranks.\nThe peak memory usage using FSDP with auto_wrap policy is the lowest followed by FSDP and DDP.",
            "markdown"
        ],
        [
            "Also, looking at timings, considering the small model and running the training on a single machine, FSDP with/out auto_wrap policy performed almost as fast as DDP.\nThis example does not represent most of the real applications, for detailed analysis and comparison between DDP and FSDP please refer to this  .",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)": [
        [
            "<strong>Author</strong>: , , , ",
            "markdown"
        ],
        [
            "This tutorial introduces more advanced features of Fully Sharded Data Parallel\n(FSDP) as part of the PyTorch 1.12 release. To get familiar with FSDP, please\nrefer to the .",
            "markdown"
        ],
        [
            "In this tutorial, we fine-tune a HuggingFace (HF) T5 model with FSDP for text\nsummarization as a working example.",
            "markdown"
        ],
        [
            "The example uses Wikihow and for simplicity, we will showcase the training on a\nsingle node, P4dn instance with 8 A100 GPUs. We will soon have a blog post on\nlarge scale FSDP training on a multi-node cluster, please stay tuned for that on\nthe PyTorch medium channel.",
            "markdown"
        ],
        [
            "FSDP is a production ready package with focus on ease of use, performance, and\nlong-term support.  One of the main benefits of FSDP is reducing the memory\nfootprint on each GPU. This enables training of larger models with lower total\nmemory vs DDP, and leverages the overlap of computation and communication to\ntrain models efficiently.\nThis reduced memory pressure can be leveraged to either train larger models or\nincrease batch size, potentially helping overall training throughput.  You can\nread more about PyTorch FSDP .",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->FSDP Features in This Tutorial": [
        [
            "Transformer Auto Wrap Policy",
            "markdown"
        ],
        [
            "Mixed Precision",
            "markdown"
        ],
        [
            "Initializing FSDP Model on Device",
            "markdown"
        ],
        [
            "Sharding Strategy",
            "markdown"
        ],
        [
            "Backward Prefetch",
            "markdown"
        ],
        [
            "Model Checkpoint Saving via Streaming to CPU",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Recap on How FSDP Works": [
        [
            "At a high level FDSP works as follow:",
            "markdown"
        ],
        [
            "<em>In constructor</em>",
            "markdown"
        ],
        [
            "Shard model parameters and each rank only keeps its own shard",
            "markdown"
        ],
        [
            "<em>In forward pass</em>",
            "markdown"
        ],
        [
            "Run <cite>all_gather</cite> to collect all shards from all ranks to recover the full\nparameter for this FSDP unit Run forward computation",
            "markdown"
        ],
        [
            "Discard non-owned parameter shards it has just collected to free memory",
            "markdown"
        ],
        [
            "<em>In backward pass</em>",
            "markdown"
        ],
        [
            "Run <cite>all_gather</cite> to collect all shards from all ranks to recover the full\nparameter in this FSDP unit Run backward computation",
            "markdown"
        ],
        [
            "Discard non-owned parameters to free memory.",
            "markdown"
        ],
        [
            "Run reduce_scatter to sync gradients",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Fine-tuning HF T5": [
        [
            "HF T5 pre-trained models are available in four different sizes, ranging from\nsmall with 60 Million parameters to XXL with 11 Billion parameters. In this\ntutorial, we demonstrate the fine-tuning of a T5 3B with FSDP for text\nsummarization using WikiHow dataset.  The main focus of this tutorial is to\nhighlight different available features in FSDP that are helpful for training\nlarge scale model above 3B parameters. Also, we cover specific features for\nTransformer based models. The code for this tutorial is available in  .",
            "markdown"
        ],
        [
            "<em>Setup</em>",
            "markdown"
        ],
        [
            "1.1 Install PyTorch Nightlies",
            "markdown"
        ],
        [
            "We will install PyTorch nightlies, as some of the features such as activation\ncheckpointing is available in nightlies and will be added in next PyTorch\nrelease after 1.12.",
            "markdown"
        ],
        [
            "pip3 install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html",
            "code"
        ],
        [
            "1.2 Dataset Setup",
            "markdown"
        ],
        [
            "Please create a <cite>data</cite> folder, download the WikiHow dataset from   and\n,\nand place them in the <cite>data</cite> folder.  We will use the wikihow dataset from\n.",
            "markdown"
        ],
        [
            "Next, we add the following code snippets to a Python script \u201cT5_training.py\u201d.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The full source code for this tutorial is available in .",
            "markdown"
        ],
        [
            "1.3  Import necessary packages:",
            "markdown"
        ],
        [
            "import os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom transformers import AutoTokenizer, GPT2TokenizerFast\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport functools\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom transformers.models.t5.modeling_t5 import T5Block\n\nfrom torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n checkpoint_wrapper,\n CheckpointImpl,\n apply_activation_checkpointing_wrapper)\n\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    MixedPrecision,\n    BackwardPrefetch,\n    ShardingStrategy,\n    FullStateDictConfig,\n    StateDictType,\n)\nfrom torch.distributed.fsdp.wrap import (\n    transformer_auto_wrap_policy,\n    enable_wrap,\n    wrap,\n)\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nfrom summarization_dataset import *\nfrom transformers.models.t5.modeling_t5 import T5Block\nfrom typing import Type\nimport time\nimport tqdm\nfrom datetime import datetime",
            "code"
        ],
        [
            "1.4 Distributed training setup.\nHere we use two helper functions to initialize the processes for distributed\ntraining,  and then to clean up after training completion.  In this tutorial, we\nare going to use torch elastic, using  , which will set the\nworker <cite>RANK</cite> and <cite>WORLD_SIZE</cite> automatically.",
            "markdown"
        ],
        [
            "def setup():\n    # initialize the process group\n    dist.init_process_group(\"nccl\")\n\ndef cleanup():\n    dist.destroy_process_group()",
            "code"
        ],
        [
            "2.1  Set up the HuggingFace T5 model:",
            "markdown"
        ],
        [
            "def setup_model(model_name):\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    tokenizer =  T5Tokenizer.from_pretrained(model_name)\n    return model, tokenizer",
            "code"
        ],
        [
            "We also, add couple of helper functions here for date and formatting memory\nmetrics.",
            "markdown"
        ],
        [
            "def get_date_of_run():\n    \"\"\"create date and time for file save uniqueness\n    example: 2022-05-07-08:31:12_PM'\n    \"\"\"\n    date_of_run = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n    print(f\"--&gt; current date and time of run = {date_of_run}\")\n    return date_of_run\n\ndef format_metrics_to_gb(item):\n    \"\"\"quick function to format numbers to gigabyte and round to 4 digit precision\"\"\"\n    metric_num = item / g_gigabyte\n    metric_num = round(metric_num, ndigits=4)\n    return metric_num",
            "code"
        ],
        [
            "2.2 Define a train function:",
            "markdown"
        ],
        [
            "def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n    model.train()\n    local_rank = int(os.environ['LOCAL_RANK'])\n    fsdp_loss = torch.zeros(2).to(local_rank)\n\n    if sampler:\n        sampler.set_epoch(epoch)\n    if rank==0:\n        inner_pbar = tqdm.tqdm(\n            range(len(train_loader)), colour=\"blue\", desc=\"r0 Training Epoch\"\n        )\n    for batch in train_loader:\n        for key in batch.keys():\n            batch[key] = batch[key].to(local_rank)\n        optimizer.zero_grad()\n        output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"] )\n        loss = output[\"loss\"]\n        loss.backward()\n        optimizer.step()\n        fsdp_loss[0] += loss.item()\n        fsdp_loss[1] += len(batch)\n        if rank==0:\n            inner_pbar.update(1)\n\n    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)\n    train_accuracy = fsdp_loss[0] / fsdp_loss[1]\n\n\n    if rank == 0:\n        inner_pbar.close()\n        print(\n                f\"Train Epoch: \\t{epoch}, Loss: \\t{train_accuracy:.4f}\"\n            )\n    return train_accuracy",
            "code"
        ],
        [
            "2.3 Define a validation function:",
            "markdown"
        ],
        [
            "def validation(model, rank, world_size, val_loader):\n    model.eval()\n    correct = 0\n    local_rank = int(os.environ['LOCAL_RANK'])\n    fsdp_loss = torch.zeros(3).to(local_rank)\n    if rank == 0:\n        inner_pbar = tqdm.tqdm(\n            range(len(val_loader)), colour=\"green\", desc=\"Validation Epoch\"\n        )\n    with torch.no_grad():\n        for batch in val_loader:\n            for key in batch.keys():\n                batch[key] = batch[key].to(local_rank)\n            output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"])\n            fsdp_loss[0] += output[\"loss\"].item()  # sum up batch loss\n            fsdp_loss[1] += len(batch)\n\n            if rank==0:\n                inner_pbar.update(1)\n\n    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)\n    val_loss = fsdp_loss[0] / fsdp_loss[1]\n    if rank == 0:\n        inner_pbar.close()\n        print(f\"Validation Loss: {val_loss:.4f}\")\n    return val_loss",
            "code"
        ],
        [
            "2.4 Define a distributed train function that wraps the model in FSDP:",
            "markdown"
        ],
        [
            "def fsdp_main(args):\n\n    model, tokenizer = setup_model(\"t5-base\")\n\n    local_rank = int(os.environ['LOCAL_RANK'])\n    rank = int(os.environ['RANK'])\n    world_size = int(os.environ['WORLD_SIZE'])\n\n\n    dataset = load_dataset('wikihow', 'all', data_dir='data/')\n    print(dataset.keys())\n    print(\"Size of train dataset: \", dataset['train'].shape)\n    print(\"Size of Validation dataset: \", dataset['validation'].shape)\n\n\n    #wikihow(tokenizer, type_path, num_samples, input_length, output_length, print_text=False)\n    train_dataset = wikihow(tokenizer, 'train', 1500, 512, 150, False)\n    val_dataset = wikihow(tokenizer, 'validation', 300, 512, 150, False)\n\n    sampler1 = DistributedSampler(train_dataset, rank=rank, num_replicas=world_size, shuffle=True)\n    sampler2 = DistributedSampler(val_dataset, rank=rank, num_replicas=world_size)\n\n    setup()\n\n\n    train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}\n    test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}\n    cuda_kwargs = {'num_workers': 2,\n                    'pin_memory': True,\n                    'shuffle': False}\n    train_kwargs.update(cuda_kwargs)\n    test_kwargs.update(cuda_kwargs)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)\n    val_loader = torch.utils.data.DataLoader(val_dataset, **test_kwargs)\n\n    t5_auto_wrap_policy = functools.partial(\n        transformer_auto_wrap_policy,\n        transformer_layer_cls={\n            T5Block,\n        },\n    )\n    sharding_strategy: ShardingStrategy = ShardingStrategy.SHARD_GRAD_OP #for Zero2 and FULL_SHARD for Zero3\n    torch.cuda.set_device(local_rank)\n\n\n    #init_start_event = torch.cuda.Event(enable_timing=True)\n    #init_end_event = torch.cuda.Event(enable_timing=True)\n\n    #init_start_event.record()\n\n    bf16_ready = (\n    torch.version.cuda\n    and torch.cuda.is_bf16_supported()\n    and LooseVersion(torch.version.cuda) &gt;= \"11.0\"\n    and dist.is_nccl_available()\n    and nccl.version() &gt;= (2, 10)\n    )\n\n    if bf16_ready:\n        mp_policy = bfSixteen\n    else:\n        mp_policy = None # defaults to fp32\n\n    # model is on CPU before input to FSDP\n    model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=mp_policy,\n        #sharding_strategy=sharding_strategy,\n        device_id=torch.cuda.current_device())\n\n    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    best_val_loss = float(\"inf\")\n    curr_val_loss = float(\"inf\")\n    file_save_name = \"T5-model-\"\n\n    if rank == 0:\n        time_of_run = get_date_of_run()\n        dur = []\n        train_acc_tracking = []\n        val_acc_tracking = []\n        training_start_time = time.time()\n\n    if rank == 0 and args.track_memory:\n        mem_alloc_tracker = []\n        mem_reserved_tracker = []\n\n    for epoch in range(1, args.epochs + 1):\n        t0 = time.time()\n        train_accuracy = train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n        if args.run_validation:\n            curr_val_loss = validation(model, rank, world_size, val_loader)\n        scheduler.step()\n\n        if rank == 0:\n\n            print(f\"--&gt; epoch {epoch} completed...entering save and stats zone\")\n\n            dur.append(time.time() - t0)\n            train_acc_tracking.append(train_accuracy.item())\n\n            if args.run_validation:\n                val_acc_tracking.append(curr_val_loss.item())\n\n            if args.track_memory:\n                mem_alloc_tracker.append(\n                    format_metrics_to_gb(torch.cuda.memory_allocated())\n                )\n                mem_reserved_tracker.append(\n                    format_metrics_to_gb(torch.cuda.memory_reserved())\n                )\n            print(f\"completed save and stats zone...\")\n\n        if args.save_model and curr_val_loss &lt; best_val_loss:\n\n            # save\n            if rank == 0:\n                print(f\"--&gt; entering save model state\")\n\n            save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n            with FSDP.state_dict_type(\n                model, StateDictType.FULL_STATE_DICT, save_policy\n            ):\n                cpu_state = model.state_dict()\n            #print(f\"saving process: rank {rank}  done w state_dict\")\n\n\n            if rank == 0:\n                print(f\"--&gt; saving model ...\")\n                currEpoch = (\n                    \"-\" + str(epoch) + \"-\" + str(round(curr_val_loss.item(), 4)) + \".pt\"\n                )\n                print(f\"--&gt; attempting to save model prefix {currEpoch}\")\n                save_name = file_save_name + \"-\" + time_of_run + \"-\" + currEpoch\n                print(f\"--&gt; saving as model name {save_name}\")\n\n                torch.save(cpu_state, save_name)\n\n        if curr_val_loss &lt; best_val_loss:\n\n            best_val_loss = curr_val_loss\n            if rank==0:\n                print(f\"--&gt;&gt;&gt;&gt; New Val Loss Record: {best_val_loss}\")\n\n    dist.barrier()\n    cleanup()",
            "code"
        ],
        [
            "2.5 Parse the arguments and set the main function:",
            "markdown"
        ],
        [
            "if __name__ == '__main__':\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch T5 FSDP Example')\n    parser.add_argument('--batch-size', type=int, default=4, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=4, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=2, metavar='N',\n                        help='number of epochs to train (default: 3)')\n    parser.add_argument('--lr', type=float, default=.002, metavar='LR',\n                        help='learning rate (default: .002)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--track_memory', action='store_false', default=True,\n                        help='track the gpu memory')\n    parser.add_argument('--run_validation', action='store_false', default=True,\n                        help='running the validation')\n    parser.add_argument('--save-model', action='store_false', default=True,\n                        help='For Saving the current Model')\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n\n    fsdp_main(args)",
            "code"
        ],
        [
            "To run the the training using torchrun:",
            "markdown"
        ],
        [
            "torchrun --nnodes 1 --nproc_per_node 4  T5_training.py",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Transformer Wrapping Policy": [
        [
            "As discussed in the ,\nauto_wrap_policy is one of the FSDP features that make it easy to automatically\nshard a given model and put the model, optimizer and gradient shards into\ndistinct FSDP units.",
            "markdown"
        ],
        [
            "For some architectures such as Transformer encoder-decoders, some parts of the\nmodel such as embedding table is being shared with both encoder and decoder.  In\nthis case, we need to place the embedding table in the outer FSDP unit so that\nit could be accessed from both encoder and decoder.  In addition, by registering\nthe layer class for a transformer, the sharding plan can be made much more\ncommunication efficient.  In PyTorch 1.12, FSDP added this support and now we\nhave a wrapping policy for transfomers.",
            "markdown"
        ],
        [
            "It can be created as follows, where the T5Block represents the T5 transformer\nlayer class (holding MHSA and FFN).",
            "markdown"
        ],
        [
            "t5_auto_wrap_policy = functools.partial(\n        transformer_auto_wrap_policy,\n        transformer_layer_cls={\n            T5Block,\n        },\n    )\ntorch.cuda.set_device(local_rank)\n\n\nmodel = FSDP(model,\n    fsdp_auto_wrap_policy=t5_auto_wrap_policy)",
            "code"
        ],
        [
            "To see the wrapped model, you can easily print the model and visually inspect\nthe sharding and FSDP units as well.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Mixed Precision": [
        [
            "FSDP supports flexible mixed precision training allowing for arbitrary reduced\nprecision types (such as fp16 or bfloat16). Currently BFloat16 is only available\non Ampere GPUs, so you need to confirm native support before you use it. On\nV100s for example, BFloat16 can still be run but due to it running non-natively,\nit can result in significant slowdowns.",
            "markdown"
        ],
        [
            "To check if BFloat16 is natively supported, you can use the following :",
            "markdown"
        ],
        [
            "bf16_ready = (\n    torch.version.cuda\n    and torch.cuda.is_bf16_supported()\n    and LooseVersion(torch.version.cuda) &gt;= \"11.0\"\n    and dist.is_nccl_available()\n    and nccl.version() &gt;= (2, 10)\n)",
            "code"
        ],
        [
            "One of the advantages of mixed percision in FSDP is providing granular control\nover different precision levels for parameters, gradients, and buffers as\nfollows:",
            "markdown"
        ],
        [
            "fpSixteen = MixedPrecision(\n    param_dtype=torch.float16,\n    # Gradient communication precision.\n    reduce_dtype=torch.float16,\n    # Buffer precision.\n    buffer_dtype=torch.float16,\n)\n\nbfSixteen = MixedPrecision(\n    param_dtype=torch.bfloat16,\n    # Gradient communication precision.\n    reduce_dtype=torch.bfloat16,\n    # Buffer precision.\n    buffer_dtype=torch.bfloat16,\n)\n\nfp32_policy = MixedPrecision(\n    param_dtype=torch.float32,\n    # Gradient communication precision.\n    reduce_dtype=torch.float32,\n    # Buffer precision.\n    buffer_dtype=torch.float32,\n)",
            "code"
        ],
        [
            "Note that if a certain type (parameter, reduce, buffer) is not specified, they\nwill not be casted at all.",
            "markdown"
        ],
        [
            "This flexibility allows users fine grained control, such as only setting\ngradient communication to happen in reduced precision, and all parameters /\nbuffer computation to be done in full precision. This is potentially useful in\ncases where intra-node communication is the main bottleneck and parameters /\nbuffers must be in full precision to avoid accuracy issues. This can be done\nwith the following policy:",
            "markdown"
        ],
        [
            "grad_bf16 = MixedPrecision(reduce_dtype=torch.bfloat16)",
            "code"
        ],
        [
            "In 2.4 we just add the relevant mixed precision policy to the FSDP wrapper:",
            "markdown"
        ],
        [
            "model = FSDP(model,\n       auto_wrap_policy=t5_auto_wrap_policy,\n       mixed_precision=bfSixteen)",
            "code"
        ],
        [
            "In our experiments, we have observed up to 4x speed up by using BFloat16 for\ntraining and memory reduction of approximately 30% in some experiments that can\nbe used for batch size increases.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Intializing FSDP Model on Device": [
        [
            "In 1.12, FSDP supports a <cite>device_id</cite> argument meant to initialize input CPU\nmodule on the device given by <cite>device_id</cite>. This is useful when the entire model\ndoes not fit on a single GPU, but fits in a host\u2019s CPU memory. When <cite>device_id</cite>\nis specified, FSDP will move the model to the specified device on a per-FSDP\nunit basis, avoiding GPU OOM issues while initializing several times faster than\nCPU-based initialization:",
            "markdown"
        ],
        [
            "torch.cuda.set_device(local_rank)\n\n model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=bfSixteen,\n        device_id=torch.cuda.current_device())",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Sharding Strategy": [
        [
            "FSDP sharding strategy by default is set to fully shard the model parameters,\ngradients and optimizer states get sharded across all ranks. (also termed Zero3\nsharding). In case you are interested to have the Zero2 sharding strategy, where\nonly optimizer states and gradients are sharded, FSDP support this feature by\npassing the Sharding strategy by using  \u201cShardingStrategy.SHARD_GRAD_OP\u201d,\ninstead of \u201cShardingStrategy.FULL_SHARD\u201d to the FSDP initialization  as follows:",
            "markdown"
        ],
        [
            "torch.cuda.set_device(local_rank)\n\n model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=bfSixteen,\n        device_id=torch.cuda.current_device(),\n        sharding_strategy=ShardingStrategy.SHARD_GRAD_OP # ZERO2)",
            "code"
        ],
        [
            "This will reduce the communication overhead in FSDP, in this case, it holds full\nparameters after forward and through the backwards pass.",
            "markdown"
        ],
        [
            "This saves an all_gather during backwards so there is less communication at the\ncost of a higher memory footprint. Note that full model params are freed at the\nend of backwards and all_gather will happen on the next forward pass.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Backward Prefetch": [
        [
            "The backward prefetch setting controls the timing of when the next FSDP unit\u2019s\nparameters should be requested.  By setting it to <cite>BACKWARD_PRE</cite>, the next\nFSDP\u2019s unit params can begin to be requested and arrive sooner before the\ncomputation of the current unit starts. This overlaps the <cite>all_gather</cite>\ncommunication and gradient computation which can increase the training speed in\nexchange for slightly higher memory consumption. It can be utilized in the FSDP\nwrapper in 2.4 as follows:",
            "markdown"
        ],
        [
            "torch.cuda.set_device(local_rank)\n\n model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=bfSixteen,\n        device_id=torch.cuda.current_device(),\n        backward_prefetch = BackwardPrefetch.BACKWARD_PRE)",
            "code"
        ],
        [
            "<cite>backward_prefetch</cite> has two modes, <cite>BACKWARD_PRE</cite> and <cite>BACKWARD_POST</cite>.\n<cite>BACKWARD_POST</cite> means that the next FSDP unit\u2019s params will not be requested\nuntil the current FSDP unit processing is complete, thus minimizing memory\noverhead.  In some cases, using <cite>BACKWARD_PRE</cite> can increase model training speed\nup to 2-10%, with even higher speed improvements noted for larger models.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Model Checkpoint Saving, by streaming to the Rank0 CPU": [
        [
            "To save model checkpoints using FULL_STATE_DICT saving which saves model in the\nsame fashion as a local model, PyTorch 1.12 offers a few utilities to support\nthe saving of larger models.",
            "markdown"
        ],
        [
            "First, a FullStateDictConfig can be specified, allowing the state_dict to be\npopulated on rank 0 only and offloaded to the CPU.",
            "markdown"
        ],
        [
            "When using this configuration, FSDP will allgather model parameters, offloading\nthem to the CPU one by one, only on rank 0. When the state_dict is finally\nsaved, it will only be populated on rank 0 and contain CPU tensors. This avoids\npotential OOM for models that are larger than a single GPU memory and allows\nusers to checkpoint models whose size is roughly the available CPU RAM on the\nuser\u2019s machine.",
            "markdown"
        ],
        [
            "This feature can be run as follows:",
            "markdown"
        ],
        [
            "save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\nwith FSDP.state_dict_type(\n            model, StateDictType.FULL_STATE_DICT, save_policy\n        ):\n            cpu_state = model.state_dict()\nif rank == 0:\n save_name = file_save_name + \"-\" + time_of_run + \"-\" + currEpoch\n torch.save(cpu_state, save_name)",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Summary": [
        [
            "In this tutorial, we have introduced many new features for FSDP available in\nPytorch 1.12 and used HF T5 as the running example.  Using the proper wrapping\npolicy especially for transformer models, along with mixed precision and\nbackward prefetch should speed up your training runs. Also, features such as\ninitializing the model on device, and checkpoint saving via streaming to CPU\nshould help to avoid OOM error in dealing with large models.",
            "markdown"
        ],
        [
            "We are actively working to add new features to FSDP for the next release. If\nyou have feedback, feature requests, questions or are encountering issues\nusing FSDP, please feel free to contact us by opening an issue in the\n.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Customize Process Group Backends Using Cpp Extensions": [
        [
            "<strong>Author</strong>: , , ",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            " View and edit this tutorial in .",
            "markdown"
        ],
        [
            "Prerequisites:",
            "markdown"
        ],
        [
            "This tutorial demonstrates how to implement a custom ProcessGroup\nbackend and plug that into\n using\n. This is helpful when you need a specialized software\nstack for your hardware, or when you would like to experiment with new\ncollective communication algorithms.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Customize Process Group Backends Using Cpp Extensions->Basics": [
        [
            "PyTorch collective communications power several widely adopted distributed\ntraining features, including\n,\n,\n.\nIn order to make the same collective communication API work with\ndifferent communication backends, the distributed package abstracts collective\ncommunication operations into a\n\nclass. Different backends can\nthen be implemented as subclasses of ProcessGroup using preferred\nthird-party libraries. PyTorch distributed comes with three default backends,\nProcessGroupNCCL, ProcessGroupGloo, and ProcessGroupMPI. However,\nbeyond these three backends, there are also other communication libraries\n(e.g., ,\n), different types of hardware\n(e.g., ,\n), and emerging\ncommunication algorithms (e.g.,\n,\n).\nTherefore, the distributed package exposes extension APIs to allow customizing\ncollective communication backends.",
            "markdown"
        ],
        [
            "The 4 steps below show how to implement a dummy ProcessGroup backend\nand use that in Python application code. Please note that this tutorial focuses\non demonstrating the extension APIs, instead of developing a functioning\ncommunication backend. Hence, the dummy backend just covers a subset of the\nAPIs (all_reduce and all_gather), and simply sets the values of tensors\nto 0.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Customize Process Group Backends Using Cpp Extensions->Step 1: Implement a Subclass of ProcessGroup": [
        [
            "This first step is to implement a ProcessGroup subclass that overrides\ntarget collective communication APIs and runs the custom communication algorithm.\nThe extension also needs to implement a Work subclass, which\nserves as a future of communication results and allows asynchronous execution in\napplication code. If the extension uses third-party libraries, it can\ninclude the headers and call into the library APIs from the ProcessGroupDummy\nsubclass. The two code snippets below present the implementation of dummy.h and\ndummy.cpp. See the \nrepository for the full implementation.",
            "markdown"
        ],
        [
            "// file name: dummy.hpp\n#include &lt;torch/python.h&gt;\n\n#include &lt;torch/csrc/distributed/c10d/ProcessGroup.hpp&gt;\n#include &lt;torch/csrc/distributed/c10d/Work.hpp&gt;\n#include &lt;torch/csrc/distributed/c10d/Store.hpp&gt;\n#include &lt;torch/csrc/distributed/c10d/Types.hpp&gt;\n#include &lt;torch/csrc/distributed/c10d/Utils.hpp&gt;\n\n#include &lt;pybind11/chrono.h&gt;\n\nnamespace c10d {\n\nclass ProcessGroupDummy : public ProcessGroup {\n  public:\n    ProcessGroupDummy(int rank, int size);\n\n    c10::intrusive_ptr&lt;Work&gt; allgather(\n        std::vector&lt;std::vector&lt;at::Tensor&gt;&gt;&amp; outputTensors,\n        std::vector&lt;at::Tensor&gt;&amp; inputTensors,\n        const AllgatherOptions&amp; opts = AllgatherOptions()) override;\n\n    c10::intrusive_ptr&lt;Work&gt; allreduce(\n        std::vector&lt;at::Tensor&gt;&amp; tensors,\n        const AllreduceOptions&amp; opts = AllreduceOptions()) override;\n\n    // The collective communication APIs without a custom implementation\n    // will error out if invoked by application code.\n};\n\nclass WorkDummy : public Work {\n  public:\n    WorkDummy(\n      OpType opType,\n      c10::intrusive_ptr&lt;c10::ivalue::Future&gt; future) // future of the output\n      : Work(\n          -1, // rank, only used by recvAnySource, irrelevant in this demo\n          opType),\n      future_(std::move(future)) {}\n    // There are several additional helper functions that need to be\n    // implemented. Please refer to https://github.com/mrshenli/dummy_collectives\n    // for the full implementation.\n\n  private:\n    c10::intrusive_ptr&lt;c10::ivalue::Future&gt; future_;\n};\n} // namespace c10d",
            "code"
        ],
        [
            "// file name: dummy.cpp\n#include \"dummy.hpp\"\n\nnamespace c10d {\n\n// This is a dummy allgather that sets all output tensors to zero\n// Modify the implementation to conduct real communication asynchronously\nc10::intrusive_ptr&lt;Work&gt; ProcessGroupDummy::allgather(\n        std::vector&lt;std::vector&lt;at::Tensor&gt;&gt;&amp; outputTensors,\n        std::vector&lt;at::Tensor&gt;&amp; inputTensors,\n        const AllgatherOptions&amp; /* unused */) {\n    for (auto&amp; outputTensorVec : outputTensors) {\n        for (auto&amp; outputTensor : outputTensorVec) {\n            outputTensor.zero_();\n        }\n    }\n\n    auto future = c10::make_intrusive&lt;c10::ivalue::Future&gt;(\n        c10::ListType::create(c10::ListType::create(c10::TensorType::get())));\n    future-&gt;markCompleted(c10::IValue(outputTensors));\n    return c10::make_intrusive&lt;WorkDummy&gt;(OpType::ALLGATHER, std::move(future));\n}\n\n// This is a dummy allreduce that sets all output tensors to zero\n// Modify the implementation to conduct real communication asynchronously\nc10::intrusive_ptr&lt;Work&gt; ProcessGroupDummy::allreduce(\n        std::vector&lt;at::Tensor&gt;&amp; tensors,\n        const AllreduceOptions&amp; opts) {\n    for (auto&amp; tensor : tensors) {\n        tensor.zero_();\n    }\n\n    auto future = c10::make_intrusive&lt;c10::ivalue::Future&gt;(\n        c10::ListType::create(c10::TensorType::get()));\n    future-&gt;markCompleted(c10::IValue(tensors));\n    return c10::make_intrusive&lt;WorkDummy&gt;(OpType::ALLGATHER, std::move(future));\n}\n} // namespace c10d",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Customize Process Group Backends Using Cpp Extensions->Step 2: Expose The Extension Python APIs": [
        [
            "The backend constructors are called\n,\nso the extension also needs to expose the constructor APIs to Python. This can\nbe done by adding the following methods. In this example, store and\ntimeout are ignored by the ProcessGroupDummy instantiation method, as\nthose are not used in this dummy implementation. However, real-world extensions\nshould consider using the store to perform rendezvous and supporting the\ntimeout argument.",
            "markdown"
        ],
        [
            "class ProcessGroupDummy : public ProcessGroup {\n    static c10::intrusive_ptr&lt;ProcessGroup&gt; createProcessGroupDummy(\n        const c10::intrusive_ptr&lt;::c10d::Store&gt;&amp; store,\n        int rank,\n        int size,\n        const std::chrono::duration&lt;float&gt;&amp; timeout);\n\n    static void ProcessGroupDummyConstructor() __attribute__((constructor)) {\n        py::object module = py::module::import(\"torch.distributed\");\n        py::object register_backend =\n            module.attr(\"Backend\").attr(\"register_backend\");\n        // torch.distributed.Backend.register_backend will add `dummy` as a\n        // new valid backend.\n        register_backend(\"dummy\", py::cpp_function(createProcessGroupDummy));\n    }\n}",
            "code"
        ],
        [
            "c10::intrusive_ptr&lt;ProcessGroup&gt; ProcessGroupDummy::createProcessGroupDummy(\n        const c10::intrusive_ptr&lt;::c10d::Store&gt;&amp; /* unused */,\n        int rank,\n        int size,\n        const std::chrono::duration&lt;float&gt;&amp; /* unused */) {\n    return c10::make_intrusive&lt;ProcessGroupDummy&gt;(rank, size);\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"createProcessGroupDummy\", &amp;ProcessGroupDummy::createProcessGroupDummy);\n}",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Customize Process Group Backends Using Cpp Extensions->Step 3: Build The Custom Extension": [
        [
            "Now, the extension source code files are ready. We can then use\n\nto build it. To do that, create a setup.py file that prepares the paths and\ncommands. Then call python setup.py install to install the extension.",
            "markdown"
        ],
        [
            "If the extension depends on third-party libraries, you can also specify\nlibraries_dirs and libraries to the cpp extension APIs. See the\n\nproject as a real-world example.",
            "markdown"
        ],
        [
            "# file name: setup.py\nimport os\nimport sys\nimport torch\nfrom setuptools import setup\nfrom torch.utils import cpp_extension\n\nsources = [\"src/dummy.cpp\"]\ninclude_dirs = [f\"{os.path.dirname(os.path.abspath(__file__))}/include/\"]\n\nif torch.cuda.is_available():\n    module = cpp_extension.CUDAExtension(\n        name = \"dummy_collectives\",\n        sources = sources,\n        include_dirs = include_dirs,\n    )\nelse:\n    module = cpp_extension.CppExtension(\n        name = \"dummy_collectives\",\n        sources = sources,\n        include_dirs = include_dirs,\n    )\n\nsetup(\n    name = \"Dummy-Collectives\",\n    version = \"0.0.1\",\n    ext_modules = [module],\n    cmdclass={'build_ext': cpp_extension.BuildExtension}\n)",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Customize Process Group Backends Using Cpp Extensions->Step 4: Use The Extension in Application": [
        [
            "After installation, you can conveniently use the dummy backend when calling\n\nas if it is an builtin backend.",
            "markdown"
        ],
        [
            "import os\n\nimport torch\n# importing dummy_collectives makes torch.distributed recognize `dummy`\n# as a valid backend.\nimport dummy_collectives\n\nimport torch.distributed as dist\n\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '29500'\n\ndist.init_process_group(\"dummy\", rank=0, world_size=1)\n\nx = torch.ones(6)\ndist.all_reduce(x)\nprint(f\"cpu allreduce: {x}\")\nif torch.cuda.is_available():\n    y = x.cuda()\n    dist.all_reduce(y)\n    print(f\"cuda allreduce: {y}\")\n\ntry:\n    dist.broadcast(x, 0)\nexcept RuntimeError:\n    print(\"got RuntimeError as broadcast is not implemented in Dummy ProcessGroup\")",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Getting Started with Distributed RPC Framework": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            " View and edit this tutorial in .",
            "markdown"
        ],
        [
            "Prerequisites:",
            "markdown"
        ],
        [
            "This tutorial uses two simple examples to demonstrate how to build distributed\ntraining with the \npackage which was first introduced as an experimental feature in PyTorch v1.4.\nSource code of the two examples can be found in\n.",
            "markdown"
        ],
        [
            "Previous tutorials,\n\nand ,\ndescribed \nwhich supports a specific training paradigm where the model is replicated across\nmultiple processes and each process handles a split of the input data.\nSometimes, you might run into scenarios that require different training\nparadigms. For example:",
            "markdown"
        ],
        [
            "In reinforcement learning, it might be relatively expensive to acquire\ntraining data from environments while the model itself can be quite small. In\nthis case, it might be useful to spawn multiple observers running in parallel\nand share a single agent. In this case, the agent takes care of the training\nlocally, but the application would still need libraries to send and receive\ndata between observers and the trainer.",
            "markdown"
        ],
        [
            "Your model might be too large to fit in GPUs on a single machine, and hence\nwould need a library to help split the model onto multiple machines. Or you\nmight be implementing a \ntraining framework, where model parameters and trainers live on different\nmachines.",
            "markdown"
        ],
        [
            "The  package\ncan help with the above scenarios. In case 1, \nand  allow sending data\nfrom one worker to another while easily referencing remote data objects. In\ncase 2, \nand \nmake executing backward pass and optimizer step as if it is local training. In\nthe next two sections, we will demonstrate APIs of\n using a\nreinforcement learning example and a language model example. Please note, this\ntutorial does not aim at building the most accurate or efficient models to\nsolve given problems, instead, the main goal here is to show how to use the\n package to\nbuild distributed training applications.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Getting Started with Distributed RPC Framework->Distributed Reinforcement Learning using RPC and RRef": [
        [
            "This section describes steps to build a toy distributed reinforcement learning\nmodel using RPC to solve CartPole-v1 from .\nThe policy code is mostly borrowed from the existing single-thread\n\nas shown below. We will skip details of the Policy design, and focus on RPC\nusages.",
            "markdown"
        ],
        [
            "import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Policy(nn.Module):\n\n    def __init__(self):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.dropout = nn.Dropout(p=0.6)\n        self.affine2 = nn.Linear(128, 2)\n\n    def forward(self, x):\n        x = self.affine1(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        action_scores = self.affine2(x)\n        return F.softmax(action_scores, dim=1)",
            "code"
        ],
        [
            "We are ready to present the observer. In this example, each observer creates its\nown environment, and waits for the agent\u2019s command to run an episode. In each\nepisode, one observer loops at most n_steps iterations, and in each\niteration, it uses RPC to pass its environment state to the agent and gets an\naction back. Then it applies that action to its environment, and gets the reward\nand the next state from the environment. After that, the observer uses another\nRPC to report the reward to the agent. Again, please note that, this is\nobviously not the most efficient observer implementation. For example, one\nsimple optimization could be packing current state and last reward in one RPC to\nreduce the communication overhead. However, the goal is to demonstrate RPC API\ninstead of building the best solver for CartPole. So, let\u2019s keep the logic\nsimple and the two steps explicit in this example.",
            "markdown"
        ],
        [
            "import argparse\nimport gym\nimport torch.distributed.rpc as rpc\n\nparser = argparse.ArgumentParser(\n    description=\"RPC Reinforcement Learning Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument('--world_size', default=2, type=int, metavar='W',\n                    help='number of workers')\nparser.add_argument('--log_interval', type=int, default=10, metavar='N',\n                    help='interval between training status logs')\nparser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n                    help='how much to value future rewards')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed  for reproducibility')\nargs = parser.parse_args()\n\nclass Observer:\n\n    def __init__(self):\n        self.id = rpc.get_worker_info().id\n        self.env = gym.make('CartPole-v1')\n        self.env.seed(args.seed)\n\n    def run_episode(self, agent_rref):\n        state, ep_reward = self.env.reset(), 0\n        for _ in range(10000):\n            # send the state to the agent to get an action\n            action = agent_rref.rpc_sync().select_action(self.id, state)\n\n            # apply the action to the environment, and get the reward\n            state, reward, done, _ = self.env.step(action)\n\n            # report the reward to the agent for training purpose\n            agent_rref.rpc_sync().report_reward(self.id, reward)\n\n            # finishes after the number of self.env._max_episode_steps\n            if done:\n                break",
            "code"
        ],
        [
            "The code for agent is a little more complex, and we will break it into multiple\npieces. In this example, the agent serves as both the trainer and the master,\nsuch that it sends command to multiple distributed observers to run episodes,\nand it also records all actions and rewards locally which will be used during\nthe training phase after each episode. The code below shows Agent\nconstructor where most lines are initializing various components. The loop at\nthe end initializes observers remotely on other workers, and holds RRefs to\nthose observers locally. The agent will use those observer RRefs later to\nsend commands. Applications don\u2019t need to worry about the lifetime of RRefs.\nThe owner of each RRef maintains a reference counting map to track its\nlifetime, and guarantees the remote data object will not be deleted as long as\nthere is any live user of that RRef. Please refer to the RRef\n for details.",
            "markdown"
        ],
        [
            "import gym\nimport numpy as np\n\nimport torch\nimport torch.distributed.rpc as rpc\nimport torch.optim as optim\nfrom torch.distributed.rpc import RRef, rpc_async, remote\nfrom torch.distributions import Categorical\n\nclass Agent:\n    def __init__(self, world_size):\n        self.ob_rrefs = []\n        self.agent_rref = RRef(self)\n        self.rewards = {}\n        self.saved_log_probs = {}\n        self.policy = Policy()\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n        self.eps = np.finfo(np.float32).eps.item()\n        self.running_reward = 0\n        self.reward_threshold = gym.make('CartPole-v1').spec.reward_threshold\n        for ob_rank in range(1, world_size):\n            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))\n            self.ob_rrefs.append(remote(ob_info, Observer))\n            self.rewards[ob_info.id] = []\n            self.saved_log_probs[ob_info.id] = []",
            "code"
        ],
        [
            "Next, the agent exposes two APIs to observers for selecting actions and\nreporting rewards. Those functions only run locally on the agent, but will\nbe triggered by observers through RPC.",
            "markdown"
        ],
        [
            "class Agent:\n    ...\n    def select_action(self, ob_id, state):\n        state = torch.from_numpy(state).float().unsqueeze(0)\n        probs = self.policy(state)\n        m = Categorical(probs)\n        action = m.sample()\n        self.saved_log_probs[ob_id].append(m.log_prob(action))\n        return action.item()\n\n    def report_reward(self, ob_id, reward):\n        self.rewards[ob_id].append(reward)",
            "code"
        ],
        [
            "Let\u2019s add a run_episode function on agent which tells all observers\nto execute an episode. In this function, it first creates a list to collect\nfutures from asynchronous RPCs, and then loop over all observer RRefs to\nmake asynchronous RPCs. In these RPCs, the agent also passes an RRef of\nitself to the observer, so that the observer can call functions on the agent as\nwell. As shown above, each observer will make RPCs back to the agent, which are\nnested RPCs. After each episode, the saved_log_probs and rewards will\ncontain the recorded action probs and rewards.",
            "markdown"
        ],
        [
            "class Agent:\n    ...\n    def run_episode(self):\n        futs = []\n        for ob_rref in self.ob_rrefs:\n            # make async RPC to kick off an episode on all observers\n            futs.append(\n                rpc_async(\n                    ob_rref.owner(),\n                    ob_rref.rpc_sync().run_episode,\n                    args=(self.agent_rref,)\n                )\n            )\n\n        # wait until all obervers have finished this episode\n        for fut in futs:\n            fut.wait()",
            "code"
        ],
        [
            "Finally, after one episode, the agent needs to train the model, which\nis implemented in the finish_episode function below. There is no RPCs in\nthis function and it is mostly borrowed from the single-thread\n.\nHence, we skip describing its contents.",
            "markdown"
        ],
        [
            "class Agent:\n    ...\n    def finish_episode(self):\n      # joins probs and rewards from different observers into lists\n      R, probs, rewards = 0, [], []\n      for ob_id in self.rewards:\n          probs.extend(self.saved_log_probs[ob_id])\n          rewards.extend(self.rewards[ob_id])\n\n      # use the minimum observer reward to calculate the running reward\n      min_reward = min([sum(self.rewards[ob_id]) for ob_id in self.rewards])\n      self.running_reward = 0.05 * min_reward + (1 - 0.05) * self.running_reward\n\n      # clear saved probs and rewards\n      for ob_id in self.rewards:\n          self.rewards[ob_id] = []\n          self.saved_log_probs[ob_id] = []\n\n      policy_loss, returns = [], []\n      for r in rewards[::-1]:\n          R = r + args.gamma * R\n          returns.insert(0, R)\n      returns = torch.tensor(returns)\n      returns = (returns - returns.mean()) / (returns.std() + self.eps)\n      for log_prob, R in zip(probs, returns):\n          policy_loss.append(-log_prob * R)\n      self.optimizer.zero_grad()\n      policy_loss = torch.cat(policy_loss).sum()\n      policy_loss.backward()\n      self.optimizer.step()\n      return min_reward",
            "code"
        ],
        [
            "With Policy, Observer, and Agent classes, we are ready to launch\nmultiple processes to perform the distributed training. In this example, all\nprocesses run the same run_worker function, and they use the rank to\ndistinguish their role. Rank 0 is always the agent, and all other ranks are\nobservers. The agent serves as master by repeatedly calling run_episode and\nfinish_episode until the running reward surpasses the reward threshold\nspecified by the environment. All observers passively waiting for commands\nfrom the agent. The code is wrapped by\n and\n,\nwhich initializes and terminates RPC instances respectively. More details are\navailable in the .",
            "markdown"
        ],
        [
            "import os\nfrom itertools import count\n\nimport torch.multiprocessing as mp\n\nAGENT_NAME = \"agent\"\nOBSERVER_NAME=\"obs{}\"\n\ndef run_worker(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 0:\n        # rank0 is the agent\n        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)\n\n        agent = Agent(world_size)\n        print(f\"This will run until reward threshold of {agent.reward_threshold}\"\n                \" is reached. Ctrl+C to exit.\")\n        for i_episode in count(1):\n            agent.run_episode()\n            last_reward = agent.finish_episode()\n\n            if i_episode % args.log_interval == 0:\n                print(f\"Episode {i_episode}\\tLast reward: {last_reward:.2f}\\tAverage reward: \"\n                    f\"{agent.running_reward:.2f}\")\n            if agent.running_reward &gt; agent.reward_threshold:\n                print(f\"Solved! Running reward is now {agent.running_reward}!\")\n                break\n    else:\n        # other ranks are the observer\n        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)\n        # observers passively waiting for instructions from the agent\n\n    # block until all rpcs finish, and shutdown the RPC instance\n    rpc.shutdown()\n\n\nmp.spawn(\n    run_worker,\n    args=(args.world_size, ),\n    nprocs=args.world_size,\n    join=True\n)",
            "code"
        ],
        [
            "Below are some sample outputs when training with <cite>world_size=2</cite>.",
            "markdown"
        ],
        [
            "This will run until reward threshold of 475.0 is reached. Ctrl+C to exit.\nEpisode 10      Last reward: 26.00      Average reward: 10.01\nEpisode 20      Last reward: 16.00      Average reward: 11.27\nEpisode 30      Last reward: 49.00      Average reward: 18.62\nEpisode 40      Last reward: 45.00      Average reward: 26.09\nEpisode 50      Last reward: 44.00      Average reward: 30.03\nEpisode 60      Last reward: 111.00     Average reward: 42.23\nEpisode 70      Last reward: 131.00     Average reward: 70.11\nEpisode 80      Last reward: 87.00      Average reward: 76.51\nEpisode 90      Last reward: 86.00      Average reward: 95.93\nEpisode 100     Last reward: 13.00      Average reward: 123.93\nEpisode 110     Last reward: 33.00      Average reward: 91.39\nEpisode 120     Last reward: 73.00      Average reward: 76.38\nEpisode 130     Last reward: 137.00     Average reward: 88.08\nEpisode 140     Last reward: 89.00      Average reward: 104.96\nEpisode 150     Last reward: 97.00      Average reward: 98.74\nEpisode 160     Last reward: 150.00     Average reward: 100.87\nEpisode 170     Last reward: 126.00     Average reward: 104.38\nEpisode 180     Last reward: 500.00     Average reward: 213.74\nEpisode 190     Last reward: 322.00     Average reward: 300.22\nEpisode 200     Last reward: 165.00     Average reward: 272.71\nEpisode 210     Last reward: 168.00     Average reward: 233.11\nEpisode 220     Last reward: 184.00     Average reward: 195.02\nEpisode 230     Last reward: 284.00     Average reward: 208.32\nEpisode 240     Last reward: 395.00     Average reward: 247.37\nEpisode 250     Last reward: 500.00     Average reward: 335.42\nEpisode 260     Last reward: 500.00     Average reward: 386.30\nEpisode 270     Last reward: 500.00     Average reward: 405.29\nEpisode 280     Last reward: 500.00     Average reward: 443.29\nEpisode 290     Last reward: 500.00     Average reward: 464.65\nSolved! Running reward is now 475.3163778435275!",
            "code"
        ],
        [
            "In this example, we show how to use RPC as the communication vehicle to pass\ndata across workers, and how to use RRef to reference remote objects. It is true\nthat you could build the entire structure directly on top of ProcessGroup\nsend and recv APIs or use other communication/RPC libraries. However,\nby using <cite>torch.distributed.rpc</cite>, you can get the native support and\ncontinuously optimized performance under the hood.",
            "markdown"
        ],
        [
            "Next, we will show how to combine RPC and RRef with distributed autograd and\ndistributed optimizer to perform distributed model parallel training.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Getting Started with Distributed RPC Framework->Distributed RNN using Distributed Autograd and Distributed Optimizer": [
        [
            "In this section, we use an RNN model to show how to build distributed model\nparallel training with the RPC API. The example RNN model is very small and\ncan easily fit into a single GPU, but we still divide its layers onto two\ndifferent workers to demonstrate the idea. Developer can apply the similar\ntechniques to distribute much larger models across multiple devices and\nmachines.",
            "markdown"
        ],
        [
            "The RNN model design is borrowed from the word language model in PyTorch\n\nrepository, which contains three main components, an embedding table, an\nLSTM layer, and a decoder. The code below wraps the embedding table and the\ndecoder into sub-modules, so that their constructors can be passed to the RPC\nAPI. In the EmbeddingTable sub-module, we intentionally put the\nEmbedding layer on GPU to cover the use case. In v1.4, RPC always creates\nCPU tensor arguments or return values on the destination worker. If the function\ntakes a GPU tensor, you need to move it to the proper device explicitly.",
            "markdown"
        ],
        [
            "class EmbeddingTable(nn.Module):\n    r\"\"\"\n    Encoding layers of the RNNModel\n    \"\"\"\n    def __init__(self, ntoken, ninp, dropout):\n        super(EmbeddingTable, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp).cuda()\n        self.encoder.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, input):\n        return self.drop(self.encoder(input.cuda()).cpu()\n\n\nclass Decoder(nn.Module):\n    def __init__(self, ntoken, nhid, dropout):\n        super(Decoder, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.decoder = nn.Linear(nhid, ntoken)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, output):\n        return self.decoder(self.drop(output))",
            "code"
        ],
        [
            "With the above sub-modules, we can now piece them together using RPC to\ncreate an RNN model. In the code below ps represents a parameter server,\nwhich hosts parameters of the embedding table and the decoder. The constructor\nuses the \nAPI to create an EmbeddingTable object and a Decoder object on the\nparameter server, and locally creates the LSTM sub-module. During the\nforward pass, the trainer uses the EmbeddingTable RRef to find the\nremote sub-module and passes the input data to the EmbeddingTable using RPC\nand fetches the lookup results. Then, it runs the embedding through the local\nLSTM layer, and finally uses another RPC to send the output to the\nDecoder sub-module. In general, to implement distributed model parallel\ntraining, developers can divide the model into sub-modules, invoke RPC to create\nsub-module instances remotely, and use on RRef to find them when necessary.\nAs you can see in the code below, it looks very similar to single-machine model\nparallel training. The main difference is replacing Tensor.to(device) with\nRPC functions.",
            "markdown"
        ],
        [
            "class RNNModel(nn.Module):\n    def __init__(self, ps, ntoken, ninp, nhid, nlayers, dropout=0.5):\n        super(RNNModel, self).__init__()\n\n        # setup embedding table remotely\n        self.emb_table_rref = rpc.remote(ps, EmbeddingTable, args=(ntoken, ninp, dropout))\n        # setup LSTM locally\n        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n        # setup decoder remotely\n        self.decoder_rref = rpc.remote(ps, Decoder, args=(ntoken, nhid, dropout))\n\n    def forward(self, input, hidden):\n        # pass input to the remote embedding table and fetch emb tensor back\n        emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input)\n        output, hidden = self.rnn(emb, hidden)\n        # pass output to the rremote decoder and get the decoded output back\n        decoded = _remote_method(Decoder.forward, self.decoder_rref, output)\n        return decoded, hidden",
            "code"
        ],
        [
            "Before introducing the distributed optimizer, let\u2019s add a helper function to\ngenerate a list of RRefs of model parameters, which will be consumed by the\ndistributed optimizer. In local training, applications could call\nModule.parameters() to grab references to all parameter tensors, and pass it\nto the local optimizer for subsequent updates. However, the same API does not\nwork in distributed training scenarios as some parameters live on remote\nmachines. Therefore, instead of taking a list of parameter Tensors, the\ndistributed optimizer takes a list of RRefs, one RRef per model\nparameter for both local and remote model parameters. The helper function is\npretty simple, just call Module.parameters() and creates a local RRef on\neach of the parameters.",
            "markdown"
        ],
        [
            "def _parameter_rrefs(module):\n    param_rrefs = []\n    for param in module.parameters():\n        param_rrefs.append(RRef(param))\n    return param_rrefs",
            "code"
        ],
        [
            "Then, as the RNNModel contains three sub-modules, we need to call\n_parameter_rrefs three times, and wrap that into another helper function.",
            "markdown"
        ],
        [
            "class RNNModel(nn.Module):\n    ...\n    def parameter_rrefs(self):\n        remote_params = []\n        # get RRefs of embedding table\n        remote_params.extend(_remote_method(_parameter_rrefs, self.emb_table_rref))\n        # create RRefs for local parameters\n        remote_params.extend(_parameter_rrefs(self.rnn))\n        # get RRefs of decoder\n        remote_params.extend(_remote_method(_parameter_rrefs, self.decoder_rref))\n        return remote_params",
            "code"
        ],
        [
            "Now, we are ready to implement the training loop. After initializing model\narguments, we create the RNNModel and the DistributedOptimizer. The\ndistributed optimizer will take a list of parameter RRefs, find all distinct\nowner workers, and create the given local optimizer (i.e., SGD in this case,\nyou can use other local optimizers as well) on each of the owner worker using\nthe given arguments (i.e., lr=0.05).",
            "markdown"
        ],
        [
            "In the training loop, it first creates a distributed autograd context, which\nwill help the distributed autograd engine to find gradients and involved RPC\nsend/recv functions. The design details of the distributed autograd engine can\nbe found in its .\nThen, it kicks off the forward pass as if it is a local\nmodel, and run the distributed backward pass. For the distributed backward, you\nonly need to specify a list of roots, in this case, it is the loss Tensor.\nThe distributed autograd engine will traverse the distributed graph\nautomatically and write gradients properly. Next, it runs the step\nfunction on the distributed optimizer, which will reach out to all involved\nlocal optimizers to update model parameters. Compared to local training, one\nminor difference is that you don\u2019t need to run zero_grad() because each\nautograd context has dedicated space to store gradients, and as we create a\ncontext per iteration, those gradients from different iterations will not\naccumulate to the same set of Tensors.",
            "markdown"
        ],
        [
            "def run_trainer():\n    batch = 5\n    ntoken = 10\n    ninp = 2\n\n    nhid = 3\n    nindices = 3\n    nlayers = 4\n    hidden = (\n        torch.randn(nlayers, nindices, nhid),\n        torch.randn(nlayers, nindices, nhid)\n    )\n\n    model = rnn.RNNModel('ps', ntoken, ninp, nhid, nlayers)\n\n    # setup distributed optimizer\n    opt = DistributedOptimizer(\n        optim.SGD,\n        model.parameter_rrefs(),\n        lr=0.05,\n    )\n\n    criterion = torch.nn.CrossEntropyLoss()\n\n    def get_next_batch():\n        for _ in range(5):\n            data = torch.LongTensor(batch, nindices) % ntoken\n            target = torch.LongTensor(batch, ntoken) % nindices\n            yield data, target\n\n    # train for 10 iterations\n    for epoch in range(10):\n        for data, target in get_next_batch():\n            # create distributed autograd context\n            with dist_autograd.context() as context_id:\n                hidden[0].detach_()\n                hidden[1].detach_()\n                output, hidden = model(data, hidden)\n                loss = criterion(output, target)\n                # run distributed backward pass\n                dist_autograd.backward(context_id, [loss])\n                # run distributed optimizer\n                opt.step(context_id)\n                # not necessary to zero grads since they are\n                # accumulated into the distributed autograd context\n                # which is reset every iteration.\n        print(\"Training epoch {}\".format(epoch))",
            "code"
        ],
        [
            "Finally, let\u2019s add some glue code to launch the parameter server and the trainer\nprocesses.",
            "markdown"
        ],
        [
            "def run_worker(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 1:\n        rpc.init_rpc(\"trainer\", rank=rank, world_size=world_size)\n        _run_trainer()\n    else:\n        rpc.init_rpc(\"ps\", rank=rank, world_size=world_size)\n        # parameter server do nothing\n        pass\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\n\nif __name__==\"__main__\":\n    world_size = 2\n    mp.spawn(run_worker, args=(world_size, ), nprocs=world_size, join=True)",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Implementing a Parameter Server Using Distributed RPC Framework": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            " View and edit this tutorial in .",
            "markdown"
        ],
        [
            "Prerequisites:",
            "markdown"
        ],
        [
            "This tutorial walks through a simple example of implementing a parameter server using PyTorch\u2019s . The parameter server framework is a paradigm in which a set of servers store parameters, such as large embedding tables, and several trainers query the parameter servers in order to retrieve the most up to date parameters. These trainers can run a training loop locally and occasionally synchronize with the parameter server to get the latest parameters. For more reading on the parameter server approach, check out .",
            "markdown"
        ],
        [
            "Using the Distributed RPC Framework, we\u2019ll build an example where multiple trainers use RPC to communicate with the same parameter server and use  to access states on the remote parameter server instance. Each trainer will launch its dedicated backward pass in a distributed fashion through stitching of the autograd graph across multiple nodes using distributed autograd.",
            "markdown"
        ],
        [
            "<strong>Note</strong>: This tutorial covers the use of the Distributed RPC Framework, which is useful for splitting a model onto multiple machines, or for implementing a parameter-server training strategy where network trainers fetch parameters hosted on a different machine. If instead you are looking for replicating your model across many GPUs, please see the . There is also another  that covers reinforcement learning and RNN use cases.",
            "markdown"
        ],
        [
            "Let\u2019s start with the familiar: importing our required modules and defining a simple ConvNet that will train on the MNIST dataset. The below network is largely adopted from the network defined in the .",
            "markdown"
        ],
        [
            "import argparse\nimport os\nimport time\nfrom threading import Lock\n\nimport torch\nimport torch.distributed.autograd as dist_autograd\nimport torch.distributed.rpc as rpc\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.distributed.optim import DistributedOptimizer\nfrom torchvision import datasets, transforms\n\n# --------- MNIST Network to train, from pytorch/examples -----\n\nclass Net(nn.Module):\n    def __init__(self, num_gpus=0):\n        super(Net, self).__init__()\n        print(f\"Using {num_gpus} GPUs to train\")\n        self.num_gpus = num_gpus\n        device = torch.device(\n            \"cuda:0\" if torch.cuda.is_available() and self.num_gpus &gt; 0 else \"cpu\")\n        print(f\"Putting first 2 convs on {str(device)}\")\n        # Put conv layers on the first cuda device, or CPU if no cuda device\n        self.conv1 = nn.Conv2d(1, 32, 3, 1).to(device)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1).to(device)\n        # Put rest of the network on the 2nd cuda device, if there is one\n        if \"cuda\" in str(device) and num_gpus &gt; 1:\n            device = torch.device(\"cuda:1\")\n\n        print(f\"Putting rest of layers on {str(device)}\")\n        self.dropout1 = nn.Dropout2d(0.25).to(device)\n        self.dropout2 = nn.Dropout2d(0.5).to(device)\n        self.fc1 = nn.Linear(9216, 128).to(device)\n        self.fc2 = nn.Linear(128, 10).to(device)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        # Move tensor to next device if necessary\n        next_device = next(self.fc1.parameters()).device\n        x = x.to(next_device)\n\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output",
            "code"
        ],
        [
            "Next, let\u2019s define some helper functions that will be useful for the rest of our script. The following uses  and  in order to define a function that invokes a given method on an object living on a remote node. Below, our handle to the remote object is given by the rref argument, and we run it on its owning node: rref.owner(). On the caller node, we run this command synchronously through the use of rpc_sync, meaning that we will block until a response is received.",
            "markdown"
        ],
        [
            "# --------- Helper Methods --------------------\n\n# On the local node, call a method with first arg as the value held by the\n# RRef. Other args are passed in as arguments to the function called.\n# Useful for calling instance methods. method could be any matching function, including\n# class methods.\ndef call_method(method, rref, *args, **kwargs):\n    return method(rref.local_value(), *args, **kwargs)\n\n# Given an RRef, return the result of calling the passed in method on the value\n# held by the RRef. This call is done on the remote node that owns\n# the RRef and passes along the given argument.\n# Example: If the value held by the RRef is of type Foo, then\n# remote_method(Foo.bar, rref, arg1, arg2) is equivalent to calling\n# &lt;foo_instance&gt;.bar(arg1, arg2) on the remote node and getting the result\n# back.\n\ndef remote_method(method, rref, *args, **kwargs):\n    args = [method, rref] + list(args)\n    return rpc.rpc_sync(rref.owner(), call_method, args=args, kwargs=kwargs)",
            "code"
        ],
        [
            "Now, we\u2019re ready to define our parameter server. We will subclass nn.Module and save a handle to our network defined above. We\u2019ll also save an input device which will be the device our input is transferred to before invoking the model.",
            "markdown"
        ],
        [
            "# --------- Parameter Server --------------------\nclass ParameterServer(nn.Module):\n    def __init__(self, num_gpus=0):\n        super().__init__()\n        model = Net(num_gpus=num_gpus)\n        self.model = model\n        self.input_device = torch.device(\n            \"cuda:0\" if torch.cuda.is_available() and num_gpus &gt; 0 else \"cpu\")",
            "code"
        ],
        [
            "Next, we\u2019ll define our forward pass. Note that regardless of the device of the model output, we move the output to CPU, as the Distributed RPC Framework currently only supports sending CPU tensors over RPC. We have intentionally disabled sending CUDA tensors over RPC due to the potential for different devices (CPU/GPU) on on the caller/callee, but may support this in future releases.",
            "markdown"
        ],
        [
            "class ParameterServer(nn.Module):\n...\n    def forward(self, inp):\n        inp = inp.to(self.input_device)\n        out = self.model(inp)\n        # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors.\n        # Tensors must be moved in and out of GPU memory due to this.\n        out = out.to(\"cpu\")\n        return out",
            "code"
        ],
        [
            "Next, we\u2019ll define a few miscellaneous functions useful for training and verification purposes. The first, get_dist_gradients, will take in a Distributed Autograd context ID and call into the dist_autograd.get_gradients API in order to retrieve gradients computed by distributed autograd. More information can be found in the . Note that we also iterate through the resulting dictionary and convert each tensor to a CPU tensor, as the framework currently only supports sending tensors over RPC. Next, get_param_rrefs will iterate through our model parameters and wrap them as a (local) . This method will be invoked over RPC by trainer nodes and will return a list of the parameters to be optimized. This is required as input to the , which requires all parameters it must optimize as a list of RRefs.",
            "markdown"
        ],
        [
            "# Use dist autograd to retrieve gradients accumulated for this model.\n# Primarily used for verification.\ndef get_dist_gradients(self, cid):\n    grads = dist_autograd.get_gradients(cid)\n    # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors.\n    # Tensors must be moved in and out of GPU memory due to this.\n    cpu_grads = {}\n    for k, v in grads.items():\n        k_cpu, v_cpu = k.to(\"cpu\"), v.to(\"cpu\")\n        cpu_grads[k_cpu] = v_cpu\n    return cpu_grads\n\n# Wrap local parameters in a RRef. Needed for building the\n# DistributedOptimizer which optimizes paramters remotely.\ndef get_param_rrefs(self):\n    param_rrefs = [rpc.RRef(param) for param in self.model.parameters()]\n    return param_rrefs",
            "code"
        ],
        [
            "Finally, we\u2019ll create methods to initialize our parameter server. Note that there will only be one instance of a parameter server across all processes, and all trainers will talk to the same parameter server and update the same stored model. As seen in run_parameter_server, the server itself does not take any independent actions; it waits for requests from trainers (which are yet to be defined) and responds to them by running the requested function.",
            "markdown"
        ],
        [
            "# The global parameter server instance.\nparam_server = None\n# A lock to ensure we only have one parameter server.\nglobal_lock = Lock()\n\n\ndef get_parameter_server(num_gpus=0):\n    \"\"\"\n    Returns a singleton parameter server to all trainer processes\n    \"\"\"\n    global param_server\n    # Ensure that we get only one handle to the ParameterServer.\n    with global_lock:\n        if not param_server:\n            # construct it once\n            param_server = ParameterServer(num_gpus=num_gpus)\n        return param_server\n\ndef run_parameter_server(rank, world_size):\n    # The parameter server just acts as a host for the model and responds to\n    # requests from trainers.\n    # rpc.shutdown() will wait for all workers to complete by default, which\n    # in this case means that the parameter server will wait for all trainers\n    # to complete, and then exit.\n    print(\"PS master initializing RPC\")\n    rpc.init_rpc(name=\"parameter_server\", rank=rank, world_size=world_size)\n    print(\"RPC initialized! Running parameter server...\")\n    rpc.shutdown()\n    print(\"RPC shutdown on parameter server.\")",
            "code"
        ],
        [
            "Note that above, rpc.shutdown() will not immediately shut down the Parameter Server. Instead, it will wait for all workers (trainers in this case) to also call into rpc.shutdown(). This gives us the guarantee that the parameter server will not go offline before all trainers (yet to be define) have completed their training process.",
            "markdown"
        ],
        [
            "Next, we\u2019ll define our TrainerNet class. This will also be a subclass of nn.Module, and our __init__ method will use the rpc.remote API to obtain an RRef, or Remote Reference, to our parameter server. Note that here we are not copying the parameter server to our local process, instead, we can think of self.param_server_rref as a distributed shared pointer to the parameter server that lives on a separate process.",
            "markdown"
        ],
        [
            "# --------- Trainers --------------------\n\n# nn.Module corresponding to the network trained by this trainer. The\n# forward() method simply invokes the network on the given parameter\n# server.\nclass TrainerNet(nn.Module):\n    def __init__(self, num_gpus=0):\n        super().__init__()\n        self.num_gpus = num_gpus\n        self.param_server_rref = rpc.remote(\n            \"parameter_server\", get_parameter_server, args=(num_gpus,))",
            "code"
        ],
        [
            "Next, we\u2019ll define a method called get_global_param_rrefs. To motivate the need for this method, it is worth it to read through the documentation on , specifically the API signature.  The optimizer must be passed a list of RRefs corresponding to the remote parameters to be optimized, so here we obtain the necessary RRefs. Since the only remote worker that a given TrainerNet interacts with is the ParameterServer, we simply invoke a remote_method on the ParameterServer. We use the get_param_rrefs method which we defined in the ParameterServer class. This method will return a list of RRefs to the parameters that need to be optimized. Note that in this case our TrainerNet does not define its own paramaters; if it did, we would need to wrap each parameter in an RRef as well and include it into our input to DistributedOptimizer.",
            "markdown"
        ],
        [
            "class TrainerNet(nn.Module):\n...\n    def get_global_param_rrefs(self):\n        remote_params = remote_method(\n            ParameterServer.get_param_rrefs,\n            self.param_server_rref)\n        return remote_params",
            "code"
        ],
        [
            "Now, we\u2019re ready to define our forward method, which will invoke (synchronous) RPC to run the forward pass of the network defined on the ParameterServer. Note that we pass in self.param_server_rref, which is a remote handle to our ParameterServer, to our RPC call. This call will send an RPC to the node on which our ParameterServer is running, invoke the forward pass, and return the Tensor corresponding to the model\u2019s output.",
            "markdown"
        ],
        [
            "class TrainerNet(nn.Module):\n...\n    def forward(self, x):\n        model_output = remote_method(\n            ParameterServer.forward, self.param_server_rref, x)\n        return model_output",
            "code"
        ],
        [
            "With our trainer fully defined, it\u2019s now time to write our neural network training loop that will create our network and optimizer, run some inputs through the network and compute the loss. The training loop looks a lot like that of a local training program, with some modifications due to the nature of our network being distributed across machines.",
            "markdown"
        ],
        [
            "Below, we initialize our TrainerNet and build a DistributedOptimizer. Note that as mentioned above, we must pass in all of the global (across all nodes participating in distributed training) parameters that we want to be optimized. In addition, we pass in the local optimizer to be used, in this case, SGD. Note that we can configure the underlying optimizer algorithm in the same way as creating a local optimizer - all arguments for optimizer.SGD will be forwarded properly. As an example, we pass in a custom learning rate that will be used as the learning rate for all local optimizers.",
            "markdown"
        ],
        [
            "def run_training_loop(rank, num_gpus, train_loader, test_loader):\n    # Runs the typical nueral network forward + backward + optimizer step, but\n    # in a distributed fashion.\n    net = TrainerNet(num_gpus=num_gpus)\n    # Build DistributedOptimizer.\n    param_rrefs = net.get_global_param_rrefs()\n    opt = DistributedOptimizer(optim.SGD, param_rrefs, lr=0.03)",
            "code"
        ],
        [
            "Next, we define our main training loop. We loop through iterables given by PyTorch\u2019s . Before writing our typical forward/backward/optimizer loop, we first wrap the logic within a . Note that this is needed to record RPCs invoked in the model\u2019s forward pass, so that an appropriate graph can be constructed which includes all participating distributed workers in the backward pass. The distributed autograd context returns a context_id which serves as an identifier for accumulating and optimizing gradients corresponding to a particular iteration.",
            "markdown"
        ],
        [
            "As opposed to calling the typical loss.backward() which would kick off the backward pass on this local worker, we call dist_autograd.backward() and pass in our context_id as well as loss, which is the root at which we want the backward pass to begin. In addition, we pass this context_id into our optimizer call, which is required to be able to look up the corresponding gradients computed by this particular backwards pass across all nodes.",
            "markdown"
        ],
        [
            "def run_training_loop(rank, num_gpus, train_loader, test_loader):\n...\n    for i, (data, target) in enumerate(train_loader):\n        with dist_autograd.context() as cid:\n            model_output = net(data)\n            target = target.to(model_output.device)\n            loss = F.nll_loss(model_output, target)\n            if i % 5 == 0:\n                print(f\"Rank {rank} training batch {i} loss {loss.item()}\")\n            dist_autograd.backward(cid, [loss])\n            # Ensure that dist autograd ran successfully and gradients were\n            # returned.\n            assert remote_method(\n                ParameterServer.get_dist_gradients,\n                net.param_server_rref,\n                cid) != {}\n            opt.step(cid)\n\n     print(\"Training complete!\")\n     print(\"Getting accuracy....\")\n     get_accuracy(test_loader, net)",
            "code"
        ],
        [
            "The following simply computes the accuracy of our model after we\u2019re done training, much like a traditional local model. However, note that the net we pass into this function above is an instance of TrainerNet and therefore the forward pass invokes RPC in a transparent fashion.",
            "markdown"
        ],
        [
            "def get_accuracy(test_loader, model):\n    model.eval()\n    correct_sum = 0\n    # Use GPU to evaluate if possible\n    device = torch.device(\"cuda:0\" if model.num_gpus &gt; 0\n        and torch.cuda.is_available() else \"cpu\")\n    with torch.no_grad():\n        for i, (data, target) in enumerate(test_loader):\n            out = model(data, -1)\n            pred = out.argmax(dim=1, keepdim=True)\n            pred, target = pred.to(device), target.to(device)\n            correct = pred.eq(target.view_as(pred)).sum().item()\n            correct_sum += correct\n\n    print(f\"Accuracy {correct_sum / len(test_loader.dataset)}\")",
            "code"
        ],
        [
            "Next, similar to how we defined run_parameter_server as the main loop for our ParameterServer that is responsible for initializing RPC, let\u2019s define a similar loop for our trainers. The difference will be that our trainers must run the training loop we defined above:",
            "markdown"
        ],
        [
            "# Main loop for trainers.\ndef run_worker(rank, world_size, num_gpus, train_loader, test_loader):\n    print(f\"Worker rank {rank} initializing RPC\")\n    rpc.init_rpc(\n        name=f\"trainer_{rank}\",\n        rank=rank,\n        world_size=world_size)\n\n    print(f\"Worker {rank} done initializing RPC\")\n\n    run_training_loop(rank, num_gpus, train_loader, test_loader)\n    rpc.shutdown()",
            "code"
        ],
        [
            "Note that similar to run_parameter_server, rpc.shutdown() will by default wait for all workers, both trainers and ParameterServers, to call into rpc.shutdown() before this node exits. This ensures that nodes are terminated gracefully and no node goes offline while another is expecting it to be online.",
            "markdown"
        ],
        [
            "We\u2019ve now completed our trainer and parameter server specific code, and all that\u2019s left is to add code to launch trainers and parameter servers. First, we must take in various arguments that apply to our parameter server and trainers. world_size corresponds to the total number of nodes that will participate in training, and is the sum of all trainers and the parameter server. We also must pass in a unique rank for each individual process, from 0 (where we will run our single parameter server) to world_size - 1. master_addr and master_port are arguments that can be used to identify where the rank 0 process is running, and will be used by individual nodes to discover each other. To test this example out locally, simply pass in localhost and the same master_port to all instances spawned. Note that for demonstration purposes, this example supports only between 0-2 GPUs, although the pattern can be extended to make use of additional GPUs.",
            "markdown"
        ],
        [
            "if __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=\"Parameter-Server RPC based training\")\n    parser.add_argument(\n        \"--world_size\",\n        type=int,\n        default=4,\n        help=\"\"\"Total number of participating processes. Should be the sum of\n        master node and all training nodes.\"\"\")\n    parser.add_argument(\n        \"rank\",\n        type=int,\n        default=None,\n        help=\"Global rank of this process. Pass in 0 for master.\")\n    parser.add_argument(\n        \"num_gpus\",\n        type=int,\n        default=0,\n        help=\"\"\"Number of GPUs to use for training, Currently supports between 0\n         and 2 GPUs. Note that this argument will be passed to the parameter servers.\"\"\")\n    parser.add_argument(\n        \"--master_addr\",\n        type=str,\n        default=\"localhost\",\n        help=\"\"\"Address of master, will default to localhost if not provided.\n        Master must be able to accept network traffic on the address + port.\"\"\")\n    parser.add_argument(\n        \"--master_port\",\n        type=str,\n        default=\"29500\",\n        help=\"\"\"Port that master is listening on, will default to 29500 if not\n        provided. Master must be able to accept network traffic on the host and port.\"\"\")\n\n    args = parser.parse_args()\n    assert args.rank is not None, \"must provide rank argument.\"\n    assert args.num_gpus &lt;= 3, f\"Only 0-2 GPUs currently supported (got {args.num_gpus}).\"\n    os.environ['MASTER_ADDR'] = args.master_addr\n    os.environ[\"MASTER_PORT\"] = args.master_port",
            "code"
        ],
        [
            "Now, we\u2019ll create a process corresponding to either a parameter server or trainer depending on our command line arguments. We\u2019ll create a ParameterServer if our passed in rank is 0, and a TrainerNet otherwise. Note that we\u2019re using torch.multiprocessing to launch a subprocess corresponding to the function that we want to execute, and waiting on this process\u2019s completion from the main thread with p.join(). In the case of initializing our trainers, we also use PyTorch\u2019s  in order to specify train and test data loaders on the MNIST dataset.",
            "markdown"
        ],
        [
            "processes = []\nworld_size = args.world_size\nif args.rank == 0:\n    p = mp.Process(target=run_parameter_server, args=(0, world_size))\n    p.start()\n    processes.append(p)\nelse:\n    # Get data to train on\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=32, shuffle=True,)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\n            '../data',\n            train=False,\n            transform=transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.1307,), (0.3081,))\n                        ])),\n        batch_size=32,\n        shuffle=True,\n    )\n    # start training worker on this node\n    p = mp.Process(\n        target=run_worker,\n        args=(\n            args.rank,\n            world_size, args.num_gpus,\n            train_loader,\n            test_loader))\n    p.start()\n    processes.append(p)\n\nfor p in processes:\n    p.join()",
            "code"
        ],
        [
            "To run the example locally, run the following command worker for the server and each worker you wish to spawn, in separate terminal windows: python rpc_parameter_server.py --world_size=WORLD_SIZE --rank=RANK. For example, for a master node with world size of 2, the command would be python rpc_parameter_server.py --world_size=2 --rank=0. The trainer can then be launched with the command python rpc_parameter_server.py --world_size=2 --rank=1 in a separate window, and this will begin training with one server and a single trainer. Note that this tutorial assumes that training occurs using between 0 and 2 GPUs, and this argument can be configured by passing --num_gpus=N into the training script.",
            "markdown"
        ],
        [
            "You can pass in the command line arguments --master_addr=ADDRESS and --master_port=PORT to indicate the address and port that the master worker is listening on, for example, to test functionality where trainers and master nodes run on different machines.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Pipeline Parallelism Using RPC": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            " View and edit this tutorial in .",
            "markdown"
        ],
        [
            "Prerequisites:",
            "markdown"
        ],
        [
            "RRef helper functions:\n,\n, and",
            "markdown"
        ],
        [
            "This tutorial uses a Resnet50 model to demonstrate implementing distributed\npipeline parallelism with \nAPIs. This can be viewed as the distributed counterpart of the multi-GPU\npipeline parallelism discussed in\n.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This tutorial requires PyTorch v1.6.0 or above.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Full source code of this tutorial can be found at\n.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Pipeline Parallelism Using RPC->Basics": [
        [
            "The previous tutorial, \nshows how to use \nto implement distributed model parallelism for an RNN model. That tutorial uses\none GPU to host the EmbeddingTable, and the provided code works fine.\nHowever, if a model lives on multiple GPUs, it would require some extra steps to\nincrease the amortized utilization of all GPUs. Pipeline parallelism is one type\nof paradigm that can help in this case.",
            "markdown"
        ],
        [
            "In this tutorial, we use ResNet50 as an example model which is also used by\nthe \ntutorial. Similarly, the ResNet50 model is divided into two shards and\nthe input batch is partitioned into multiple splits and fed into the two model\nshards in a pipelined fashion. The difference is that, instead of parallelizing\nthe execution using CUDA streams, this tutorial invokes asynchronous RPCs. So,\nthe solution presented in this tutorial also works across machine boundaries.\nThe remainder of this tutorial presents the implementation in four steps.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Pipeline Parallelism Using RPC->Step 1: Partition ResNet50 Model": [
        [
            "This is the preparation step which implements ResNet50 in two model shards.\nThe code below is borrowed from the\n.\nThe ResNetBase module contains the common building blocks and attributes for\nthe two ResNet shards.",
            "markdown"
        ],
        [
            "import threading\n\nimport torch\nimport torch.nn as nn\n\nfrom torchvision.models.resnet import Bottleneck\n\nnum_classes = 1000\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass ResNetBase(nn.Module):\n    def __init__(self, block, inplanes, num_classes=1000,\n                groups=1, width_per_group=64, norm_layer=None):\n        super(ResNetBase, self).__init__()\n\n        self._lock = threading.Lock()\n        self._block = block\n        self._norm_layer = nn.BatchNorm2d\n        self.inplanes = inplanes\n        self.dilation = 1\n        self.groups = groups\n        self.base_width = width_per_group\n\n    def _make_layer(self, planes, blocks, stride=1):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if stride != 1 or self.inplanes != planes * self._block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * self._block.expansion, stride),\n                norm_layer(planes * self._block.expansion),\n            )\n\n        layers = []\n        layers.append(self._block(self.inplanes, planes, stride, downsample, self.groups,\n                                self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * self._block.expansion\n        for _ in range(1, blocks):\n            layers.append(self._block(self.inplanes, planes, groups=self.groups,\n                                    base_width=self.base_width, dilation=self.dilation,\n                                    norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def parameter_rrefs(self):\n        return [RRef(p) for p in self.parameters()]",
            "code"
        ],
        [
            "Now, we are ready to define the two model shards. For the constructor, we\nsimply split all ResNet50 layers into two parts and move each part into the\nprovided device. The forward functions of both shards take an RRef of\nthe input data, fetch the data locally, and then move it to the expected device.\nAfter applying all layers to the input, it moves the output to CPU and returns.\nIt is because the RPC API requires tensors to reside on CPU to avoid invalid\ndevice errors when the numbers of devices in the caller and the callee do not\nmatch.",
            "markdown"
        ],
        [
            "class ResNetShard1(ResNetBase):\n    def __init__(self, device, *args, **kwargs):\n        super(ResNetShard1, self).__init__(\n            Bottleneck, 64, num_classes=num_classes, *args, **kwargs)\n\n        self.device = device\n        self.seq = nn.Sequential(\n            nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False),\n            self._norm_layer(self.inplanes),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            self._make_layer(64, 3),\n            self._make_layer(128, 4, stride=2)\n        ).to(self.device)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x_rref):\n        x = x_rref.to_here().to(self.device)\n        with self._lock:\n            out =  self.seq(x)\n        return out.cpu()\n\n\nclass ResNetShard2(ResNetBase):\n    def __init__(self, device, *args, **kwargs):\n        super(ResNetShard2, self).__init__(\n            Bottleneck, 512, num_classes=num_classes, *args, **kwargs)\n\n        self.device = device\n        self.seq = nn.Sequential(\n            self._make_layer(256, 6, stride=2),\n            self._make_layer(512, 3, stride=2),\n            nn.AdaptiveAvgPool2d((1, 1)),\n        ).to(self.device)\n\n        self.fc =  nn.Linear(512 * self._block.expansion, num_classes).to(self.device)\n\n    def forward(self, x_rref):\n        x = x_rref.to_here().to(self.device)\n        with self._lock:\n            out = self.fc(torch.flatten(self.seq(x), 1))\n        return out.cpu()",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Pipeline Parallelism Using RPC->Step 2: Stitch ResNet50 Model Shards Into One Module": [
        [
            "Then, we create a DistResNet50 module to assemble the two shards and\nimplement the pipeline parallel logic. In the constructor, we use two\nrpc.remote calls to put the two shards on two different RPC workers\nrespectively and hold on to the RRef to the two model parts so that they\ncan be referenced in the forward pass.  The forward function\nsplits the input batch into multiple micro-batches, and feeds these\nmicro-batches to the two model parts in a pipelined fashion. It first uses an\nrpc.remote call to apply the first shard to a micro-batch and then forwards\nthe returned intermediate output RRef to the second model shard. After that,\nit collects the Future of all micro-outputs, and waits for all of them after\nthe loop. Note that both remote() and rpc_async() return immediately and\nrun asynchronously. Therefore, the entire loop is non-blocking, and will launch\nmultiple RPCs concurrently. The execution order of one micro-batch on two model\nparts are preserved by intermediate output y_rref. The execution order\nacross micro-batches does not matter. In the end, the forward function\nconcatenates outputs of all micro-batches into one single output tensor and\nreturns. The parameter_rrefs function is a helper to\nsimplify distributed optimizer construction, which will be used later.",
            "markdown"
        ],
        [
            "class DistResNet50(nn.Module):\n    def __init__(self, num_split, workers, *args, **kwargs):\n        super(DistResNet50, self).__init__()\n\n        self.num_split = num_split\n\n        # Put the first part of the ResNet50 on workers[0]\n        self.p1_rref = rpc.remote(\n            workers[0],\n            ResNetShard1,\n            args = (\"cuda:0\",) + args,\n            kwargs = kwargs\n        )\n\n        # Put the second part of the ResNet50 on workers[1]\n        self.p2_rref = rpc.remote(\n            workers[1],\n            ResNetShard2,\n            args = (\"cuda:1\",) + args,\n            kwargs = kwargs\n        )\n\n    def forward(self, xs):\n        out_futures = []\n        for x in iter(xs.split(self.num_split, dim=0)):\n            x_rref = RRef(x)\n            y_rref = self.p1_rref.remote().forward(x_rref)\n            z_fut = self.p2_rref.rpc_async().forward(y_rref)\n            out_futures.append(z_fut)\n\n        return torch.cat(torch.futures.wait_all(out_futures))\n\n    def parameter_rrefs(self):\n        remote_params = []\n        remote_params.extend(self.p1_rref.remote().parameter_rrefs().to_here())\n        remote_params.extend(self.p2_rref.remote().parameter_rrefs().to_here())\n        return remote_params",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Pipeline Parallelism Using RPC->Step 3: Define The Training Loop": [
        [
            "After defining the model, let us implement the training loop. We use a\ndedicated \u201cmaster\u201d worker to prepare random inputs and labels, and control the\ndistributed backward pass and distributed optimizer step. It first creates an\ninstance of the DistResNet50 module. It specifies the number of\nmicro-batches for each batch, and also provides the name of the two RPC workers\n(i.e., \u201cworker1\u201d, and \u201cworker2\u201d). Then it defines the loss function and creates\na DistributedOptimizer using the parameter_rrefs() helper to acquire a\nlist of parameter RRefs. Then, the main training loop is very similar to\nregular local training, except that it uses dist_autograd to launch\nbackward and provides the context_id for both backward and optimizer\nstep().",
            "markdown"
        ],
        [
            "import torch.distributed.autograd as dist_autograd\nimport torch.optim as optim\nfrom torch.distributed.optim import DistributedOptimizer\n\nnum_batches = 3\nbatch_size = 120\nimage_w = 128\nimage_h = 128\n\n\ndef run_master(num_split):\n    # put the two model parts on worker1 and worker2 respectively\n    model = DistResNet50(num_split, [\"worker1\", \"worker2\"])\n    loss_fn = nn.MSELoss()\n    opt = DistributedOptimizer(\n        optim.SGD,\n        model.parameter_rrefs(),\n        lr=0.05,\n    )\n\n    one_hot_indices = torch.LongTensor(batch_size) \\\n                        .random_(0, num_classes) \\\n                        .view(batch_size, 1)\n\n    for i in range(num_batches):\n        print(f\"Processing batch {i}\")\n        # generate random inputs and labels\n        inputs = torch.randn(batch_size, 3, image_w, image_h)\n        labels = torch.zeros(batch_size, num_classes) \\\n                    .scatter_(1, one_hot_indices, 1)\n\n        with dist_autograd.context() as context_id:\n            outputs = model(inputs)\n            dist_autograd.backward(context_id, [loss_fn(outputs, labels)])\n            opt.step(context_id)",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Pipeline Parallelism Using RPC->Step 4: Launch RPC Processes": [
        [
            "Finally, the code below shows the target function for all processes. The main\nlogic is defined in run_master. The workers passively waiting for\ncommands from the master, and hence simply runs init_rpc and shutdown,\nwhere the shutdown by default will block until all RPC participants finish.",
            "markdown"
        ],
        [
            "import os\nimport time\n\nimport torch.multiprocessing as mp\n\n\ndef run_worker(rank, world_size, num_split):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=128)\n\n    if rank == 0:\n        rpc.init_rpc(\n            \"master\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=options\n        )\n        run_master(num_split)\n    else:\n        rpc.init_rpc(\n            f\"worker{rank}\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=options\n        )\n        pass\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\n\nif __name__==\"__main__\":\n    world_size = 3\n    for num_split in [1, 2, 4, 8]:\n        tik = time.time()\n        mp.spawn(run_worker, args=(world_size, num_split), nprocs=world_size, join=True)\n        tok = time.time()\n        print(f\"number of splits = {num_split}, execution time = {tok - tik}\")",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Implementing Batch RPC Processing Using Asynchronous Executions": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            " View and edit this tutorial in .",
            "markdown"
        ],
        [
            "Prerequisites:",
            "markdown"
        ],
        [
            "This tutorial demonstrates how to build batch-processing RPC applications with\nthe \ndecorator, which helps to speed up training by reducing the number of blocked\nRPC threads and consolidating CUDA operations on the callee. This shares the\nsame idea as .",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This tutorial requires PyTorch v1.6.0 or above.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Implementing Batch RPC Processing Using Asynchronous Executions->Basics": [
        [
            "Previous tutorials have shown the steps to build distributed training\napplications using ,\nbut they didn\u2019t elaborate on what happens on the callee side when processing an\nRPC request. As of PyTorch v1.5, each RPC request will block one thread on the\ncallee to execute the function in that request until that function returns.\nThis works for many use cases, but there is one caveat. If the user function\nblocks on IO, e.g., with nested RPC invocation, or signaling, e.g., waiting for\na different RPC request to unblock, the RPC thread on the callee will have to\nidle waiting until the IO finishes or the signaling event occurs. As a result,\nRPC callees are likely to use more threads than necessary. The cause of this\nproblem is that RPC treats user functions as black boxes, and knows very little\nabout what happens in the function. To allow user functions to yield and free\nRPC threads, more hints need to be provided to the RPC system.",
            "markdown"
        ],
        [
            "Since v1.6.0, PyTorch addresses this problem by introducing two new concepts:",
            "markdown"
        ],
        [
            "A  type\nthat encapsulates an asynchronous execution, which also supports installing\ncallback functions.",
            "markdown"
        ],
        [
            "An \ndecorator that allows applications to tell the callee that the target function\nwill return a future and can pause and yield multiple times during execution.",
            "markdown"
        ],
        [
            "With these two tools, the application code can break a user function into\nmultiple smaller functions, chain them together as callbacks on Future\nobjects, and return the Future that contains the final result. On the callee\nside, when getting the Future object, it installs subsequent RPC response\npreparation and communication as callbacks as well, which will be triggered\nwhen the final result is ready. In this way, the callee no longer needs to block\none thread and wait until the final return value is ready. Please refer to the\nAPI doc of\n\nfor simple examples.",
            "markdown"
        ],
        [
            "Besides reducing the number of idle threads on the callee, these tools also help\nto make batch RPC processing easier and faster. The following two sections of\nthis tutorial demonstrate how to build distributed batch-updating parameter\nserver and batch-processing reinforcement learning applications using the\n\ndecorator.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Implementing Batch RPC Processing Using Asynchronous Executions->Batch-Updating Parameter Server": [
        [
            "Consider a synchronized parameter server training application with one parameter\nserver (PS) and multiple trainers. In this application, the PS holds the\nparameters and waits for all trainers to report gradients. In every iteration,\nit waits until receiving gradients from all trainers and then updates all\nparameters in one shot. The code below shows the implementation of the PS class.\nThe update_and_fetch_model method is decorated using\n@rpc.functions.async_execution and will be called by trainers. Each\ninvocation returns a Future object that will be populated with the updated\nmodel. Invocations launched by most trainers just accumulate gradients to the\n.grad field, return immediately, and yield the RPC thread on the PS. The\nlast arriving trainer will trigger the optimizer step and consume all previously\nreported gradients. Then it sets the future_model with the updated model,\nwhich in turn notifies all previous requests from other trainers through the\nFuture object and sends out the updated model to all trainers.",
            "markdown"
        ],
        [
            "import threading\nimport torchvision\nimport torch\nimport torch.distributed.rpc as rpc\nfrom torch import optim\n\nnum_classes, batch_update_size = 30, 5\n\nclass BatchUpdateParameterServer(object):\n    def __init__(self, batch_update_size=batch_update_size):\n        self.model = torchvision.models.resnet50(num_classes=num_classes)\n        self.lock = threading.Lock()\n        self.future_model = torch.futures.Future()\n        self.batch_update_size = batch_update_size\n        self.curr_update_size = 0\n        self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n        for p in self.model.parameters():\n            p.grad = torch.zeros_like(p)\n\n    def get_model(self):\n        return self.model\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def update_and_fetch_model(ps_rref, grads):\n        # Using the RRef to retrieve the local PS instance\n        self = ps_rref.local_value()\n        with self.lock:\n            self.curr_update_size += 1\n            # accumulate gradients into .grad field\n            for p, g in zip(self.model.parameters(), grads):\n                p.grad += g\n\n            # Save the current future_model and return it to make sure the\n            # returned Future object holds the correct model even if another\n            # thread modifies future_model before this thread returns.\n            fut = self.future_model\n\n            if self.curr_update_size &gt;= self.batch_update_size:\n                # update the model\n                for p in self.model.parameters():\n                    p.grad /= self.batch_update_size\n                self.curr_update_size = 0\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n                # by settiing the result on the Future object, all previous\n                # requests expecting this updated model will be notified and\n                # the their responses will be sent accordingly.\n                fut.set_result(self.model)\n                self.future_model = torch.futures.Future()\n\n        return fut",
            "code"
        ],
        [
            "For the trainers, they are all initialized using the same set of\nparameters from the PS. In every iteration, each trainer first runs the forward\nand the backward passes to generate gradients locally. Then, each trainer\nreports its gradients to the PS using RPC, and fetches back the updated\nparameters through the return value of the same RPC request. In the trainer\u2019s\nimplementation, whether the target function is marked with\n@rpc.functions.async_execution or not makes no difference. The\ntrainer simply calls update_and_fetch_model using rpc_sync which will\nblock on the trainer until the updated model is returned.",
            "markdown"
        ],
        [
            "batch_size, image_w, image_h  = 20, 64, 64\n\nclass Trainer(object):\n    def __init__(self, ps_rref):\n        self.ps_rref, self.loss_fn = ps_rref, torch.nn.MSELoss()\n        self.one_hot_indices = torch.LongTensor(batch_size) \\\n                                    .random_(0, num_classes) \\\n                                    .view(batch_size, 1)\n\n    def get_next_batch(self):\n        for _ in range(6):\n            inputs = torch.randn(batch_size, 3, image_w, image_h)\n            labels = torch.zeros(batch_size, num_classes) \\\n                        .scatter_(1, self.one_hot_indices, 1)\n            yield inputs.cuda(), labels.cuda()\n\n    def train(self):\n        name = rpc.get_worker_info().name\n        # get initial model parameters\n        m = self.ps_rref.rpc_sync().get_model().cuda()\n        # start training\n        for inputs, labels in self.get_next_batch():\n            self.loss_fn(m(inputs), labels).backward()\n            m = rpc.rpc_sync(\n                self.ps_rref.owner(),\n                BatchUpdateParameterServer.update_and_fetch_model,\n                args=(self.ps_rref, [p.grad for p in m.cpu().parameters()]),\n            ).cuda()",
            "code"
        ],
        [
            "We skip the code that launches multiple processes in this tutorial and please\nrefer to the \nrepo for the full implementation. Note that, it is possible to implement batch\nprocessing without the\n\ndecorator. However, that would require either blocking more RPC threads on\nthe PS or use another round of RPC to fetch updated models, where the latter\nwould add both more code complexity and more communication overhead.",
            "markdown"
        ],
        [
            "This section uses a simple parameter sever training example to show how to\nimplement batch RPC applications using the\n\ndecorator. In the next section, we re-implement the reinforcement learning\nexample in the previous\n\ntutorial using batch processing, and demonstrate its impact on the training\nspeed.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Implementing Batch RPC Processing Using Asynchronous Executions->Batch-Processing CartPole Solver": [
        [
            "This section uses CartPole-v1 from  as\nan example to show the performance impact of batch processing RPC. Please note\nthat since the goal is to demonstrate the usage of\n\ninstead of building the best CartPole solver or solving most different RL\nproblems, we use very simple policies and reward calculation strategies and\nfocus on the multi-observer single-agent batch RPC implementation. We use a\nsimilar Policy model as the previous tutorial which is shown below. Compared\nto the previous tutorial, the difference is that its constructor takes an\nadditional batch argument which controls the dim parameter for\nF.softmax because with batching, the x argument in the forward\nfunction contains states from multiple observers and hence the dimension needs\nto change properly. Everything else stays intact.",
            "markdown"
        ],
        [
            "import argparse\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nparser = argparse.ArgumentParser(description='PyTorch RPC Batch RL example')\nparser.add_argument('--gamma', type=float, default=1.0, metavar='G',\n                    help='discount factor (default: 1.0)')\nparser.add_argument('--seed', type=int, default=543, metavar='N',\n                    help='random seed (default: 543)')\nparser.add_argument('--num-episode', type=int, default=10, metavar='E',\n                    help='number of episodes (default: 10)')\nargs = parser.parse_args()\n\ntorch.manual_seed(args.seed)\n\nclass Policy(nn.Module):\n    def __init__(self, batch=True):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.dropout = nn.Dropout(p=0.6)\n        self.affine2 = nn.Linear(128, 2)\n        self.dim = 2 if batch else 1\n\n    def forward(self, x):\n        x = self.affine1(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        action_scores = self.affine2(x)\n        return F.softmax(action_scores, dim=self.dim)",
            "code"
        ],
        [
            "The constructor of the Observer adjusts accordingly as well. It also takes a\nbatch argument, which governs which Agent function it uses to select\nactions. In batch mode, it calls select_action_batch function on Agent\nwhich will be presented shortly, and this function will be decorated with\n.",
            "markdown"
        ],
        [
            "import gym\nimport torch.distributed.rpc as rpc\n\nclass Observer:\n    def __init__(self, batch=True):\n        self.id = rpc.get_worker_info().id - 1\n        self.env = gym.make('CartPole-v1')\n        self.env.seed(args.seed)\n        self.select_action = Agent.select_action_batch if batch else Agent.select_action",
            "code"
        ],
        [
            "Compared to the previous tutorial\n,\nobservers behave a little differently. Instead of exiting when the environment\nis stopped, it always runs n_steps iterations in every episode. When the\nenvironment returns, the observer simply resets the environment and start over\nagain. With this design, the agent will receive a fixed number of states from\nevery observer and hence can pack them into a fixed-size tensor. In every\nstep, the Observer uses RPC to send its state to the Agent and fetches\nthe action through the return value. At the end of every episode, it returns the\nrewards of all steps to Agent. Note that this run_episode function will\nbe called by the Agent using RPC. So the rpc_sync call in this function\nwill be a nested RPC invocation. We could mark this function as @rpc.functions.async_execution\ntoo to avoid blocking one thread on the Observer. However, as the bottleneck\nis the Agent instead of the Observer, it should be OK to block one\nthread on the Observer process.",
            "markdown"
        ],
        [
            "import torch\n\nclass Observer:\n    ...\n\n    def run_episode(self, agent_rref, n_steps):\n        state, ep_reward = self.env.reset(), NUM_STEPS\n        rewards = torch.zeros(n_steps)\n        start_step = 0\n        for step in range(n_steps):\n            state = torch.from_numpy(state).float().unsqueeze(0)\n            # send the state to the agent to get an action\n            action = rpc.rpc_sync(\n                agent_rref.owner(),\n                self.select_action,\n                args=(agent_rref, self.id, state)\n            )\n\n            # apply the action to the environment, and get the reward\n            state, reward, done, _ = self.env.step(action)\n            rewards[step] = reward\n\n            if done or step + 1 &gt;= n_steps:\n                curr_rewards = rewards[start_step:(step + 1)]\n                R = 0\n                for i in range(curr_rewards.numel() -1, -1, -1):\n                    R = curr_rewards[i] + args.gamma * R\n                    curr_rewards[i] = R\n                state = self.env.reset()\n                if start_step == 0:\n                    ep_reward = min(ep_reward, step - start_step + 1)\n                start_step = step + 1\n\n        return [rewards, ep_reward]",
            "code"
        ],
        [
            "The constructor of the Agent also takes a batch argument, which controls\nhow action probs are batched. In batch mode, the saved_log_probs contains a\nlist of tensors, where each tensor contains action robs from all observers in\none step. Without batching, the saved_log_probs is a dictionary where the\nkey is the observer id and the value is a list of action probs for that\nobserver.",
            "markdown"
        ],
        [
            "import threading\nfrom torch.distributed.rpc import RRef\n\nclass Agent:\n    def __init__(self, world_size, batch=True):\n        self.ob_rrefs = []\n        self.agent_rref = RRef(self)\n        self.rewards = {}\n        self.policy = Policy(batch).cuda()\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n        self.running_reward = 0\n\n        for ob_rank in range(1, world_size):\n            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))\n            self.ob_rrefs.append(rpc.remote(ob_info, Observer, args=(batch,)))\n            self.rewards[ob_info.id] = []\n\n        self.states = torch.zeros(len(self.ob_rrefs), 1, 4)\n        self.batch = batch\n        self.saved_log_probs = [] if batch else {k:[] for k in range(len(self.ob_rrefs))}\n        self.future_actions = torch.futures.Future()\n        self.lock = threading.Lock()\n        self.pending_states = len(self.ob_rrefs)",
            "code"
        ],
        [
            "The non-batching select_acion simply runs the state throw the policy, saves\nthe action prob, and returns the action to the observer right away.",
            "markdown"
        ],
        [
            "from torch.distributions import Categorical\n\nclass Agent:\n    ...\n\n    @staticmethod\n    def select_action(agent_rref, ob_id, state):\n        self = agent_rref.local_value()\n        probs = self.policy(state.cuda())\n        m = Categorical(probs)\n        action = m.sample()\n        self.saved_log_probs[ob_id].append(m.log_prob(action))\n        return action.item()",
            "code"
        ],
        [
            "With batching, the state is stored in a 2D tensor self.states, using the\nobserver id as the row id. Then, it chains a Future by installing a callback\nfunction to the batch-generated self.future_actions Future object, which\nwill be populated with the specific row indexed using the id of that observer.\nThe last arriving observer runs all batched states through the policy in one\nshot and set  self.future_actions accordingly. When this occurs, all the\ncallback functions installed on self.future_actions will be triggered and\ntheir return values will be used to populate the chained Future object,\nwhich in turn notifies the Agent to prepare and communicate responses for\nall previous RPC requests from other observers.",
            "markdown"
        ],
        [
            "class Agent:\n    ...\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def select_action_batch(agent_rref, ob_id, state):\n        self = agent_rref.local_value()\n        self.states[ob_id].copy_(state)\n        future_action = self.future_actions.then(\n            lambda future_actions: future_actions.wait()[ob_id].item()\n        )\n\n        with self.lock:\n            self.pending_states -= 1\n            if self.pending_states == 0:\n                self.pending_states = len(self.ob_rrefs)\n                probs = self.policy(self.states.cuda())\n                m = Categorical(probs)\n                actions = m.sample()\n                self.saved_log_probs.append(m.log_prob(actions).t()[0])\n                future_actions = self.future_actions\n                self.future_actions = torch.futures.Future()\n                future_actions.set_result(actions.cpu())\n        return future_action",
            "code"
        ],
        [
            "Now let\u2019s define how different RPC functions are stitched together. The Agent\ncontrols the execution of every episode. It first uses rpc_async to kick off\nthe episode on all observers and block on the returned futures which will be\npopulated with observer rewards. Note that the code below uses the RRef helper\nob_rref.rpc_async() to launch the run_episode function on the owner\nof the ob_rref RRef with the provided arguments.\nIt then converts the saved action probs and returned observer rewards into\nexpected data format, and launch the training step. Finally, it resets all\nstates and returns the reward of the current episode. This function is the entry\npoint to run one episode.",
            "markdown"
        ],
        [
            "class Agent:\n    ...\n\n    def run_episode(self, n_steps=0):\n        futs = []\n        for ob_rref in self.ob_rrefs:\n            # make async RPC to kick off an episode on all observers\n            futs.append(ob_rref.rpc_async().run_episode(self.agent_rref, n_steps))\n\n        # wait until all obervers have finished this episode\n        rets = torch.futures.wait_all(futs)\n        rewards = torch.stack([ret[0] for ret in rets]).cuda().t()\n        ep_rewards = sum([ret[1] for ret in rets]) / len(rets)\n\n        # stack saved probs into one tensor\n        if self.batch:\n            probs = torch.stack(self.saved_log_probs)\n        else:\n            probs = [torch.stack(self.saved_log_probs[i]) for i in range(len(rets))]\n            probs = torch.stack(probs)\n\n        policy_loss = -probs * rewards / len(rets)\n        policy_loss.sum().backward()\n        self.optimizer.step()\n        self.optimizer.zero_grad()\n\n        # reset variables\n        self.saved_log_probs = [] if self.batch else {k:[] for k in range(len(self.ob_rrefs))}\n        self.states = torch.zeros(len(self.ob_rrefs), 1, 4)\n\n        # calculate running rewards\n        self.running_reward = 0.5 * ep_rewards + 0.5 * self.running_reward\n        return ep_rewards, self.running_reward",
            "code"
        ],
        [
            "The rest of the code is normal processes launching and logging which are\nsimilar to other RPC tutorials. In this tutorial, all observers passively\nwaiting for commands from the agent. Please refer to the\n\nrepo for the full implementation.",
            "markdown"
        ],
        [
            "def run_worker(rank, world_size, n_episode, batch, print_log=True):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 0:\n        # rank0 is the agent\n        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)\n\n        agent = Agent(world_size, batch)\n        for i_episode in range(n_episode):\n            last_reward, running_reward = agent.run_episode(n_steps=NUM_STEPS)\n\n            if print_log:\n                print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n                    i_episode, last_reward, running_reward))\n    else:\n        # other ranks are the observer\n        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)\n        # observers passively waiting for instructions from agents\n    rpc.shutdown()\n\n\ndef main():\n    for world_size in range(2, 12):\n        delays = []\n        for batch in [True, False]:\n            tik = time.time()\n            mp.spawn(\n                run_worker,\n                args=(world_size, args.num_episode, batch),\n                nprocs=world_size,\n                join=True\n            )\n            tok = time.time()\n            delays.append(tok - tik)\n\n        print(f\"{world_size}, {delays[0]}, {delays[1]}\")\n\n\nif __name__ == '__main__':\n    main()",
            "code"
        ],
        [
            "Batch RPC helps to consolidate the action inference into less CUDA operations,\nand hence reduces the amortized overhead. The above main function runs the\nsame code on both batch and no-batch modes using different numbers of observers,\nranging from 1 to 10. The figure below plots the execution time of different\nworld sizes using default argument values. The results confirmed our expectation\nthat batch processing helped to speed up training.\n\n<img alt=\"\" src=\"../_images/batch.png\"/>",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Implementing Batch RPC Processing Using Asynchronous Executions->Learn More": [],
    "torch->Parallel and Distributed Training->Combining Distributed DataParallel with Distributed RPC Framework": [
        [
            "<strong>Authors</strong>:  and ",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            " View and edit this tutorial in .",
            "markdown"
        ],
        [
            "This tutorial uses a simple example to demonstrate how you can combine\n (DDP)\nwith the \nto combine distributed data parallelism with distributed model parallelism to\ntrain a simple model. Source code of the example can be found .",
            "markdown"
        ],
        [
            "Previous tutorials,\n\nand ,\ndescribed how to perform distributed data parallel and distributed model\nparallel training respectively. Although, there are several training paradigms\nwhere you might want to combine these two techniques. For example:",
            "markdown"
        ],
        [
            "If we have a model with a sparse part (large embedding table) and a dense\npart (FC layers), we might want to put the embedding table on a parameter\nserver and replicate the FC layer across multiple trainers using .\nThe \ncan be used to perform embedding lookups on the parameter server.",
            "markdown"
        ],
        [
            "Enable hybrid parallelism as described in the  paper.\nWe can use the \nto pipeline stages of the model across multiple workers and replicate each\nstage (if needed) using .\n\n\n<br/>",
            "markdown"
        ],
        [
            "In this tutorial we will cover case 1 mentioned above. We have a total of 4\nworkers in our setup as follows:",
            "markdown"
        ],
        [
            "1 Master, which is responsible for creating an embedding table\n(nn.EmbeddingBag) on the parameter server. The master also drives the\ntraining loop on the two trainers.",
            "markdown"
        ],
        [
            "1 Parameter Server, which basically holds the embedding table in memory and\nresponds to RPCs from the Master and Trainers.",
            "markdown"
        ],
        [
            "2 Trainers, which store an FC layer (nn.Linear) which is replicated amongst\nthemselves using .\nThe trainers are also responsible for executing the forward pass, backward\npass and optimizer step.\n\n\n<br/>",
            "markdown"
        ],
        [
            "The entire training process is executed as follows:",
            "markdown"
        ],
        [
            "The master creates a \nthat holds an embedding table on the Parameter Server.",
            "markdown"
        ],
        [
            "The master, then kicks off the training loop on the trainers and passes the\nremote module to the trainers.",
            "markdown"
        ],
        [
            "The trainers create a HybridModel which first performs an embedding lookup\nusing the remote module provided by the master and then executes the\nFC layer which is wrapped inside DDP.",
            "markdown"
        ],
        [
            "The trainer executes the forward pass of the model and uses the loss to\nexecute the backward pass using .",
            "markdown"
        ],
        [
            "As part of the backward pass, the gradients for the FC layer are computed\nfirst and synced to all trainers via allreduce in DDP.",
            "markdown"
        ],
        [
            "Next, Distributed Autograd propagates the gradients to the parameter server,\nwhere the gradients for the embedding table are updated.",
            "markdown"
        ],
        [
            "Finally, the  is used to update all the parameters.",
            "markdown"
        ],
        [
            "Attention",
            "markdown"
        ],
        [
            "You should always use \nfor the backward pass if you\u2019re combining DDP and RPC.",
            "markdown"
        ],
        [
            "Now, let\u2019s go through each part in detail. Firstly, we need to setup all of our\nworkers before we can perform any training. We create 4 processes such that\nranks 0 and 1 are our trainers, rank 2 is the master and rank 3 is the\nparameter server.",
            "markdown"
        ],
        [
            "We initialize the RPC framework on all 4 workers using the TCP init_method.\nOnce RPC initialization is done, the master creates a remote module that holds an \nlayer on the Parameter Server using .\nThe master then loops through each trainer and kicks off the training loop by\ncalling _run_trainer on each trainer using .\nFinally, the master waits for all training to finish before exiting.",
            "markdown"
        ],
        [
            "The trainers first initialize a ProcessGroup for DDP with world_size=2\n(for two trainers) using .\nNext, they initialize the RPC framework using the TCP init_method. Note that\nthe ports are different in RPC initialization and ProcessGroup initialization.\nThis is to avoid port conflicts between initialization of both frameworks.\nOnce the initialization is done, the trainers just wait for the _run_trainer\nRPC from the master.",
            "markdown"
        ],
        [
            "The parameter server just initializes the RPC framework and waits for RPCs from\nthe trainers and master.",
            "markdown"
        ],
        [
            "def run_worker(rank, world_size):\n    r\"\"\"\n    A wrapper function that initializes RPC, calls the function, and shuts down\n    RPC.\n    \"\"\"\n\n    # We need to use different port numbers in TCP init_method for init_rpc and\n    # init_process_group to avoid port conflicts.\n    rpc_backend_options = TensorPipeRpcBackendOptions()\n    rpc_backend_options.init_method = \"tcp://localhost:29501\"\n\n    # Rank 2 is master, 3 is ps and 0 and 1 are trainers.\n    if rank == 2:\n        rpc.init_rpc(\n            \"master\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=rpc_backend_options,\n        )\n\n        remote_emb_module = RemoteModule(\n            \"ps\",\n            torch.nn.EmbeddingBag,\n            args=(NUM_EMBEDDINGS, EMBEDDING_DIM),\n            kwargs={\"mode\": \"sum\"},\n        )\n\n        # Run the training loop on trainers.\n        futs = []\n        for trainer_rank in [0, 1]:\n            trainer_name = \"trainer{}\".format(trainer_rank)\n            fut = rpc.rpc_async(\n                trainer_name, _run_trainer, args=(remote_emb_module, trainer_rank)\n            )\n            futs.append(fut)\n\n        # Wait for all training to finish.\n        for fut in futs:\n            fut.wait()\n    elif rank &lt;= 1:\n        # Initialize process group for Distributed DataParallel on trainers.\n        dist.init_process_group(\n            backend=\"gloo\", rank=rank, world_size=2, init_method=\"tcp://localhost:29500\"\n        )\n\n        # Initialize RPC.\n        trainer_name = \"trainer{}\".format(rank)\n        rpc.init_rpc(\n            trainer_name,\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=rpc_backend_options,\n        )\n\n        # Trainer just waits for RPCs from master.\n    else:\n        rpc.init_rpc(\n            \"ps\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=rpc_backend_options,\n        )\n        # parameter server do nothing\n        pass\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\n\nif __name__ == \"__main__\":\n    # 2 trainers, 1 parameter server, 1 master.\n    world_size = 4\n    mp.spawn(run_worker, args=(world_size,), nprocs=world_size, join=True)",
            "code"
        ],
        [
            "Before we discuss details of the Trainer, let\u2019s introduce the HybridModel that\nthe trainer uses. As described below, the HybridModel is initialized using a\nremote module that holds an embedding table (remote_emb_module) on the parameter server and the device\nto use for DDP. The initialization of the model wraps an\n\nlayer inside DDP to replicate and synchronize this layer across all trainers.",
            "markdown"
        ],
        [
            "The forward method of the model is pretty straightforward. It performs an\nembedding lookup on the parameter server using RemoteModule\u2019s forward\nand passes its output onto the FC layer.",
            "markdown"
        ],
        [
            "class HybridModel(torch.nn.Module):\n    r\"\"\"\n    The model consists of a sparse part and a dense part.\n    1) The dense part is an nn.Linear module that is replicated across all trainers using DistributedDataParallel.\n    2) The sparse part is a Remote Module that holds an nn.EmbeddingBag on the parameter server.\n    This remote model can get a Remote Reference to the embedding table on the parameter server.\n    \"\"\"\n\n    def __init__(self, remote_emb_module, device):\n        super(HybridModel, self).__init__()\n        self.remote_emb_module = remote_emb_module\n        self.fc = DDP(torch.nn.Linear(16, 8).cuda(device), device_ids=[device])\n        self.device = device\n\n    def forward(self, indices, offsets):\n        emb_lookup = self.remote_emb_module.forward(indices, offsets)\n        return self.fc(emb_lookup.cuda(self.device))",
            "code"
        ],
        [
            "Next, let\u2019s look at the setup on the Trainer. The trainer first creates the\nHybridModel described above using a remote module that holds the embedding table on the\nparameter server and its own rank.",
            "markdown"
        ],
        [
            "Now, we need to retrieve a list of RRefs to all the parameters that we would\nlike to optimize with .\nTo retrieve the parameters for the embedding table from the parameter server,\nwe can call RemoteModule\u2019s ,\nwhich basically walks through all the parameters for the embedding table and returns\na list of RRefs. The trainer calls this method on the parameter server via RPC\nto receive a list of RRefs to the desired parameters. Since the\nDistributedOptimizer always takes a list of RRefs to parameters that need to\nbe optimized, we need to create RRefs even for the local parameters for our\nFC layers. This is done by walking model.fc.parameters(), creating an RRef for\neach parameter and appending it to the list returned from remote_parameters().\nNote that we cannnot use model.parameters(),\nbecause it will recursively call model.remote_emb_module.parameters(),\nwhich is not supported by RemoteModule.",
            "markdown"
        ],
        [
            "Finally, we create our DistributedOptimizer using all the RRefs and define a\nCrossEntropyLoss function.",
            "markdown"
        ],
        [
            "def _run_trainer(remote_emb_module, rank):\n    r\"\"\"\n    Each trainer runs a forward pass which involves an embedding lookup on the\n    parameter server and running nn.Linear locally. During the backward pass,\n    DDP is responsible for aggregating the gradients for the dense part\n    (nn.Linear) and distributed autograd ensures gradients updates are\n    propagated to the parameter server.\n    \"\"\"\n\n    # Setup the model.\n    model = HybridModel(remote_emb_module, rank)\n\n    # Retrieve all model parameters as rrefs for DistributedOptimizer.\n\n    # Retrieve parameters for embedding table.\n    model_parameter_rrefs = model.remote_emb_module.remote_parameters()\n\n    # model.fc.parameters() only includes local parameters.\n    # NOTE: Cannot call model.parameters() here,\n    # because this will call remote_emb_module.parameters(),\n    # which supports remote_parameters() but not parameters().\n    for param in model.fc.parameters():\n        model_parameter_rrefs.append(RRef(param))\n\n    # Setup distributed optimizer\n    opt = DistributedOptimizer(\n        optim.SGD,\n        model_parameter_rrefs,\n        lr=0.05,\n    )\n\n    criterion = torch.nn.CrossEntropyLoss()",
            "code"
        ],
        [
            "Now we\u2019re ready to introduce the main training loop that is run on each trainer.\nget_next_batch is just a helper function to generate random inputs and\ntargets for training. We run the training loop for multiple epochs and for each\nbatch:",
            "markdown"
        ],
        [
            "Setup a \nfor Distributed Autograd.",
            "markdown"
        ],
        [
            "Run the forward pass of the model and retrieve its output.",
            "markdown"
        ],
        [
            "Compute the loss based on our outputs and targets using the loss function.",
            "markdown"
        ],
        [
            "Use Distributed Autograd to execute a distributed backward pass using the loss.",
            "markdown"
        ],
        [
            "Finally, run a Distributed Optimizer step to optimize all the parameters.",
            "markdown"
        ],
        [
            "    def get_next_batch(rank):\n        for _ in range(10):\n            num_indices = random.randint(20, 50)\n            indices = torch.LongTensor(num_indices).random_(0, NUM_EMBEDDINGS)\n\n            # Generate offsets.\n            offsets = []\n            start = 0\n            batch_size = 0\n            while start &lt; num_indices:\n                offsets.append(start)\n                start += random.randint(1, 10)\n                batch_size += 1\n\n            offsets_tensor = torch.LongTensor(offsets)\n            target = torch.LongTensor(batch_size).random_(8).cuda(rank)\n            yield indices, offsets_tensor, target\n\n    # Train for 100 epochs\n    for epoch in range(100):\n        # create distributed autograd context\n        for indices, offsets, target in get_next_batch(rank):\n            with dist_autograd.context() as context_id:\n                output = model(indices, offsets)\n                loss = criterion(output, target)\n\n                # Run distributed backward pass\n                dist_autograd.backward(context_id, [loss])\n\n                # Tun distributed optimizer\n                opt.step(context_id)\n\n                # Not necessary to zero grads as each iteration creates a different\n                # distributed autograd context which hosts different grads\n        print(\"Training done for epoch {}\".format(epoch))",
            "code"
        ],
        [
            "Source code for the entire example can be found .",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Pipeline Parallelism": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "This tutorial demonstrates how to train a large Transformer model across\nmultiple GPUs using pipeline parallelism. This tutorial is an extension of the\n tutorial\nand scales up the same model to demonstrate how pipeline parallelism can be\nused to train Transformer models.",
            "markdown"
        ],
        [
            "Prerequisites:\n<blockquote>",
            "markdown"
        ],
        [
            "</blockquote>",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Pipeline Parallelism->Define the model": [
        [
            "In this tutorial, we will split a Transformer model across two GPUs and use\npipeline parallelism to train the model. The model is exactly the same model\nused in the  tutorial,\nbut is split into two stages. The largest number of parameters belong to the\n layer.\nThe \nitself consists of nlayers of .\nAs a result, our focus is on nn.TransformerEncoder and we split the model\nsuch that half of the nn.TransformerEncoderLayer are on one GPU and the\nother half are on another. To do this, we pull out the Encoder and\nDecoder sections into seperate modules and then build an nn.Sequential\nrepresenting the original Transformer module.",
            "markdown"
        ],
        [
            "import sys\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tempfile\nfrom torch.nn import TransformerEncoder, \n\nif sys.platform == 'win32':\n    print('Windows platform is not supported for pipeline parallelism')\n    sys.exit(0)\nif () &lt; 2:\n    print('Need at least two GPU devices for this tutorial')\n    sys.exit(0)\n\nclass Encoder():\n    def __init__(self, ntoken, ninp, dropout=0.5):\n        super(, self).__init__()\n        self.pos_encoder = (ninp, dropout)\n        self.encoder = (ntoken, ninp)\n        self.ninp = ninp\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        # Need (S, N) format for encoder.\n        src = src.t()\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        return self.pos_encoder(src)\n\nclass Decoder():\n    def __init__(self, ntoken, ninp):\n        super(, self).__init__()\n        self.decoder = (ninp, ntoken)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, inp):\n        # Need batch dimension first for output of pipeline.\n        return self.decoder(inp).permute(1, 0, 2)",
            "code"
        ],
        [
            "PositionalEncoding module injects some information about the\nrelative or absolute position of the tokens in the sequence. The\npositional encodings have the same dimension as the embeddings so that\nthe two can be summed. Here, we use sine and cosine functions of\ndifferent frequencies.",
            "markdown"
        ],
        [
            "class PositionalEncoding():\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(, self).__init__()\n        self.dropout = (p=dropout)\n\n        pe = (max_len, d_model)\n        position = (0, max_len, dtype=).unsqueeze(1)\n        div_term = ((0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = (position * div_term)\n        pe[:, 1::2] = (position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Pipeline Parallelism->Load and batch data": [
        [
            "The training process uses Wikitext-2 dataset from torchtext.\nTo access torchtext datasets, please install torchdata following instructions at .",
            "markdown"
        ],
        [
            "The vocab object is built based on the train dataset and is used to numericalize\ntokens into tensors. Starting from sequential data, the batchify()\nfunction arranges the dataset into columns, trimming off any tokens remaining\nafter the data has been divided into batches of size batch_size.\nFor instance, with the alphabet as the sequence (total length of 26)\nand a batch size of 4, we would divide the alphabet into 4 sequences of\nlength 6:\n\n\\[\\begin{bmatrix}\n\\text{A} &amp; \\text{B} &amp; \\text{C} &amp; \\ldots &amp; \\text{X} &amp; \\text{Y} &amp; \\text{Z}\n\\end{bmatrix}\n\\Rightarrow\n\\begin{bmatrix}\n\\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n\\end{bmatrix}\n\n\\]",
            "markdown"
        ],
        [
            "These columns are treated as independent by the model, which means that\nthe dependence of G and F can not be learned, but allows more\nefficient batch processing.",
            "markdown"
        ],
        [
            "import torch\nfrom torchtext.datasets import \nfrom torchtext.data.utils import \nfrom torchtext.vocab import \n\ntrain_iter = (split='train')\ntokenizer = ('basic_english')\n = (map(tokenizer, train_iter), specials=[\"&lt;unk&gt;\"])\n([\"&lt;unk&gt;\"])\n\ndef data_process(raw_text_iter):\n  data = [((tokenizer(item)), dtype=) for item in raw_text_iter]\n  return (tuple(filter(lambda t: t.numel() &gt; 0, data)))\n\ntrain_iter, val_iter, test_iter = ()\n = data_process(train_iter)\n = data_process(val_iter)\n = data_process(test_iter)\n\ndevice = (\"cuda\")\n\ndef batchify(data, bsz):\n    # Divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\nbatch_size = 20\neval_batch_size = 10\n = batchify(, batch_size)\n = batchify(, eval_batch_size)\n = batchify(, eval_batch_size)",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Pipeline Parallelism->Load and batch data->Functions to generate input and target sequence": [
        [
            "get_batch() function generates the input and target sequence for\nthe transformer model. It subdivides the source data into chunks of\nlength bptt. For the language modeling task, the model needs the\nfollowing words as Target. For example, with a bptt value of 2,\nwe\u2019d get the following two Variables for i = 0:\n<img alt=\"../_images/transformer_input_target.png\" src=\"../_images/transformer_input_target.png\"/>",
            "markdown"
        ],
        [
            "It should be noted that the chunks are along dimension 0, consistent\nwith the S dimension in the Transformer model. The batch dimension\nN is along dimension 1.",
            "markdown"
        ],
        [
            "bptt = 25\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].view(-1)\n    # Need batch dimension first for pipeline parallelism.\n    return data.t(), target",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Pipeline Parallelism->Model scale and Pipe initialization": [
        [
            "To demonstrate training large Transformer models using pipeline parallelism,\nwe scale up the Transformer layers appropriately. We use an embedding\ndimension of 4096, hidden size of 4096, 16 attention heads and 12 total\ntransformer layers (nn.TransformerEncoderLayer). This creates a model with\n<strong>~1.4 billion</strong> parameters.",
            "markdown"
        ],
        [
            "We need to initialize the \nsince Pipe depends on the RPC framework via \nwhich allows for future expansion to cross host pipelining. We need to\ninitialize the RPC framework with only a single worker since we\u2019re using a\nsingle process to drive multiple GPUs.",
            "markdown"
        ],
        [
            "The pipeline is then initialized with 8 transformer layers on one GPU and 8\ntransformer layers on the other GPU.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "For efficiency purposes we ensure that the nn.Sequential passed to\nPipe only consists of two elements (corresponding to two GPUs), this\nallows the Pipe to work with only two partitions and avoid any\ncross-partition overheads.",
            "markdown"
        ],
        [
            "ntokens = len() # the size of vocabulary\nemsize = 4096 # embedding dimension\nnhid = 4096 # the dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 12 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 16 # the number of heads in the multiheadattention models\ndropout = 0.2 # the dropout value\n\nfrom torch.distributed import rpc\ntmpfile = tempfile.NamedTemporaryFile()\n(\n    name=\"worker\",\n    rank=0,\n    world_size=1,\n    rpc_backend_options=(\n        init_method=\"file://{}\".format(tmpfile.name),\n        # Specifying _transports and _channels is a workaround and we no longer\n        # will have to specify _transports and _channels for PyTorch\n        # versions &gt;= 1.8.1\n        _transports=[\"ibv\", \"uv\"],\n        _channels=[\"cuda_ipc\", \"cuda_basic\"],\n    )\n)\n\nnum_gpus = 2\npartition_len = ((nlayers - 1) // num_gpus) + 1\n\n# Add encoder in the beginning.\ntmp_list = [(ntokens, emsize, dropout).cuda(0)]\nmodule_list = []\n\n# Add all the necessary transformer blocks.\nfor i in range(nlayers):\n     = (emsize, nhead, nhid, dropout)\n    if i != 0 and i % (partition_len) == 0:\n        module_list.append((*tmp_list))\n        tmp_list = []\n    device = i // (partition_len)\n    tmp_list.append((device))\n\n# Add decoder in the end.\ntmp_list.append((ntokens, emsize).cuda(num_gpus - 1))\nmodule_list.append((*tmp_list))\n\nfrom torch.distributed.pipeline.sync import \n\n# Build the pipeline.\nchunks = 8\n = ((*module_list), chunks = chunks)\n\n\ndef get_total_params(module: ):\n    total_params = 0\n    for param in module.parameters():\n        total_params += param.numel()\n    return total_params\n\nprint ('Total parameters in model: {:,}'.format(get_total_params()))",
            "code"
        ],
        [
            "Total parameters in model: 1,444,261,998",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Pipeline Parallelism->Run the model": [
        [
            "is applied to track the loss and\n\nimplements stochastic gradient descent method as the optimizer. The initial\nlearning rate is set to 5.0.  is\napplied to adjust the learn rate through epochs. During the\ntraining, we use\n\nfunction to scale all the gradient together to prevent exploding.",
            "markdown"
        ],
        [
            " = ()\nlr = 5.0 # learning rate\n = ((), lr=lr)\n = (, 1.0, gamma=0.95)\n\nimport time\ndef train():\n    () # Turn on the train mode\n    total_loss = 0.\n    start_time = time.time()\n    ntokens = len()\n\n    # Train only for 50 batches to keep script execution time low.\n    nbatches = min(50 * bptt, .size(0) - 1)\n\n    for batch, i in enumerate(range(0, nbatches, bptt)):\n        data, targets = get_batch(, i)\n        ()\n        # Since the Pipe is only within a single host and process the ``RRef``\n        # returned by forward method is local to this node and can simply\n        # retrieved via ``RRef.local_value()``.\n        output = (data).local_value()\n        # Need to move targets to the device where the output of the\n        # pipeline resides.\n        loss = (output.view(-1, ntokens), targets.cuda(1))\n        loss.backward()\n        ((), 0.5)\n        .step()\n\n        total_loss += loss.item()\n        log_interval = 10\n        if batch % log_interval == 0 and batch &gt; 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n                  'lr {:02.2f} | ms/batch {:5.2f} | '\n                  'loss {:5.2f} | ppl {:8.2f}'.format(\n                    epoch, batch, nbatches // bptt, .get_lr()[0],\n                    elapsed * 1000 / log_interval,\n                    cur_loss, math.exp(cur_loss)))\n            total_loss = 0\n            start_time = time.time()\n\ndef evaluate(eval_model, data_source):\n    eval_model.eval() # Turn on the evaluation mode\n    total_loss = 0.\n    ntokens = len()\n    # Evaluate only for 50 batches to keep script execution time low.\n    nbatches = min(50 * bptt, data_source.size(0) - 1)\n    with ():\n        for i in range(0, nbatches, bptt):\n            data, targets = get_batch(data_source, i)\n            output = eval_model(data).local_value()\n            output_flat = output.view(-1, ntokens)\n            # Need to move targets to the device where the output of the\n            # pipeline resides.\n            total_loss += len(data) * (output_flat, targets.cuda(1)).item()\n    return total_loss / (len(data_source) - 1)",
            "code"
        ],
        [
            "Loop over epochs. Save the model if the validation loss is the best\nwe\u2019ve seen so far. Adjust the learning rate after each epoch.",
            "markdown"
        ],
        [
            "best_val_loss = float(\"inf\")\nepochs = 3 # The number of epochs\n = None\n\nfor epoch in range(1, epochs + 1):\n    epoch_start_time = time.time()\n    train()\n    val_loss = evaluate(, )\n    print('-' * 89)\n    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                     val_loss, math.exp(val_loss)))\n    print('-' * 89)\n\n    if val_loss &lt; best_val_loss:\n        best_val_loss = val_loss\n         = \n\n    .step()",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:389: UserWarning:\n\nTo get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n\n| epoch   1 |    10/   50 batches | lr 5.00 | ms/batch 1816.43 | loss 48.60 | ppl 1277713776368723165184.00\n| epoch   1 |    20/   50 batches | lr 5.00 | ms/batch 1692.90 | loss 36.95 | ppl 11126355217385184.00\n| epoch   1 |    30/   50 batches | lr 5.00 | ms/batch 1653.26 | loss 39.14 | ppl 99769255234084784.00\n| epoch   1 |    40/   50 batches | lr 5.00 | ms/batch 1618.83 | loss 43.02 | ppl 4838079630300874752.00\n-----------------------------------------------------------------------------------------\n| end of epoch   1 | time: 92.53s | valid loss  0.92 | valid ppl     2.52\n-----------------------------------------------------------------------------------------\n| epoch   2 |    10/   50 batches | lr 4.51 | ms/batch 1816.67 | loss 37.51 | ppl 19522289362883428.00\n| epoch   2 |    20/   50 batches | lr 4.51 | ms/batch 1656.04 | loss 30.85 | ppl 24890930028738.51\n| epoch   2 |    30/   50 batches | lr 4.51 | ms/batch 1647.42 | loss 28.68 | ppl 2846180117003.57\n| epoch   2 |    40/   50 batches | lr 4.51 | ms/batch 1636.76 | loss 22.94 | ppl 9142782978.44\n-----------------------------------------------------------------------------------------\n| end of epoch   2 | time: 92.37s | valid loss  0.35 | valid ppl     1.43\n-----------------------------------------------------------------------------------------\n| epoch   3 |    10/   50 batches | lr 4.29 | ms/batch 1798.89 | loss 14.40 | ppl 1800868.66\n| epoch   3 |    20/   50 batches | lr 4.29 | ms/batch 1646.24 | loss 10.98 | ppl 58598.56\n| epoch   3 |    30/   50 batches | lr 4.29 | ms/batch 1645.90 | loss 10.66 | ppl 42600.28\n| epoch   3 |    40/   50 batches | lr 4.29 | ms/batch 1645.18 | loss 10.81 | ppl 49456.98\n-----------------------------------------------------------------------------------------\n| end of epoch   3 | time: 92.29s | valid loss  0.21 | valid ppl     1.23\n-----------------------------------------------------------------------------------------",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Pipeline Parallelism->Evaluate the model with the test dataset": [
        [
            "Apply the best model to check the result with the test dataset.",
            "markdown"
        ],
        [
            "test_loss = evaluate(, )\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n    test_loss, math.exp(test_loss)))\nprint('=' * 89)",
            "code"
        ],
        [
            "=========================================================================================\n| End of training | test loss  0.18 | test ppl     1.20\n=========================================================================================",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 5 minutes  11.429 seconds)",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "This tutorial demonstrates how to train a large Transformer model across\nmultiple GPUs using  and\n. This tutorial is an extension of the\n tutorial\nand scales up the same model to demonstrate how Distributed Data Parallel and\nPipeline Parallelism can be used to train Transformer models.",
            "markdown"
        ],
        [
            "Prerequisites:\n<blockquote>",
            "markdown"
        ],
        [
            "</blockquote>",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Define the model": [
        [
            "PositionalEncoding module injects some information about the\nrelative or absolute position of the tokens in the sequence. The\npositional encodings have the same dimension as the embeddings so that\nthe two can be summed. Here, we use sine and cosine functions of\ndifferent frequencies.",
            "markdown"
        ],
        [
            "import sys\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tempfile\nfrom torch.nn import TransformerEncoder, \n\nclass PositionalEncoding():\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = (p=dropout)\n\n        pe = (max_len, d_model)\n        position = (0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = ((0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = (position * div_term)\n        pe[:, 1::2] = (position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.pe = nn.Parameter(pe, requires_grad=False)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)",
            "code"
        ],
        [
            "In this tutorial, we will split a Transformer model across two GPUs and use\npipeline parallelism to train the model. In addition to this, we use\n\nto train two replicas of this pipeline. We have one process driving a pipe across\nGPUs 0 and 1 and another process driving a pipe across GPUs 2 and 3. Both these\nprocesses then use Distributed Data Parallel to train the two replicas. The\nmodel is exactly the same model used in the  tutorial,\nbut is split into two stages. The largest number of parameters belong to the\n layer.\nThe \nitself consists of nlayers of .\nAs a result, our focus is on nn.TransformerEncoder and we split the model\nsuch that half of the nn.TransformerEncoderLayer are on one GPU and the\nother half are on another. To do this, we pull out the Encoder and\nDecoder sections into seperate modules and then build an nn.Sequential\nrepresenting the original Transformer module.",
            "markdown"
        ],
        [
            "if sys.platform == 'win32':\n    print('Windows platform is not supported for pipeline parallelism')\n    sys.exit(0)\nif () &lt; 4:\n    print('Need at least four GPU devices for this tutorial')\n    sys.exit(0)\n\nclass Encoder():\n    def __init__(self, ntoken, ninp, dropout=0.5):\n        super(Encoder, self).__init__()\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        self.encoder = (ntoken, ninp)\n        self.ninp = ninp\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        # Need (S, N) format for encoder.\n        src = src.t()\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        return self.pos_encoder(src)\n\nclass Decoder():\n    def __init__(self, ntoken, ninp):\n        super(Decoder, self).__init__()\n        self.decoder = (ninp, ntoken)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, inp):\n        # Need batch dimension first for output of pipeline.\n        return self.decoder(inp).permute(1, 0, 2)",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Start multiple processes for training": [
        [
            "We start two processes where each process drives its own pipeline across two\nGPUs. run_worker is executed for each process.",
            "markdown"
        ],
        [
            "def run_worker(rank, world_size):",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Load and batch data": [
        [
            "The training process uses Wikitext-2 dataset from torchtext.\nTo access torchtext datasets, please install torchdata following instructions at .",
            "markdown"
        ],
        [
            "The vocab object is built based on the train dataset and is used to numericalize\ntokens into tensors. Starting from sequential data, the batchify()\nfunction arranges the dataset into columns, trimming off any tokens remaining\nafter the data has been divided into batches of size batch_size.\nFor instance, with the alphabet as the sequence (total length of 26)\nand a batch size of 4, we would divide the alphabet into 4 sequences of\nlength 6:\n\n\\[\\begin{bmatrix}\n\\text{A} &amp; \\text{B} &amp; \\text{C} &amp; \\ldots &amp; \\text{X} &amp; \\text{Y} &amp; \\text{Z}\n\\end{bmatrix}\n\\Rightarrow\n\\begin{bmatrix}\n\\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n\\end{bmatrix}\n\n\\]",
            "markdown"
        ],
        [
            "These columns are treated as independent by the model, which means that\nthe dependence of G and F can not be learned, but allows more\nefficient batch processing.",
            "markdown"
        ],
        [
            "# In 'run_worker'\n    def print_with_rank(msg):\n        print('[RANK {}]: {}'.format(rank, msg))\n\n    from torchtext.datasets import \n    from torchtext.data.utils import \n    from torchtext.vocab import \n\n    train_iter = (split='train')\n    tokenizer = ('basic_english')\n    vocab = (map(tokenizer, train_iter), specials=[\"&lt;unk&gt;\"])\n    vocab.set_default_index(vocab[\"&lt;unk&gt;\"])\n\n    def data_process(raw_text_iter):\n      data = [(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n      return (tuple(filter(lambda t: t.numel() &gt; 0, data)))\n\n    train_iter, val_iter, test_iter = ()\n    train_data = data_process(train_iter)\n    val_data = data_process(val_iter)\n    test_data = data_process(test_iter)\n\n    device = (2 * rank)\n\n    def batchify(data, bsz, rank, world_size, is_train=False):\n        # Divide the dataset into bsz parts.\n        nbatch = data.size(0) // bsz\n        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n        data = data.narrow(0, 0, nbatch * bsz)\n        # Evenly divide the data across the bsz batches.\n        data = data.view(bsz, -1).t().contiguous()\n        # Divide the data across the ranks only for training data.\n        if is_train:\n            data_per_rank = data.size(0) // world_size\n            data = data[rank * data_per_rank : (rank + 1) * data_per_rank]\n        return data.to(device)\n\n    batch_size = 20\n    eval_batch_size = 10\n    train_data = batchify(train_data, batch_size, rank, world_size, True)\n    val_data = batchify(val_data, eval_batch_size, rank, world_size)\n    test_data = batchify(test_data, eval_batch_size, rank, world_size)",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Load and batch data->Functions to generate input and target sequence": [
        [
            "get_batch() function generates the input and target sequence for\nthe transformer model. It subdivides the source data into chunks of\nlength bptt. For the language modeling task, the model needs the\nfollowing words as Target. For example, with a bptt value of 2,\nwe\u2019d get the following two Variables for i = 0:\n<img alt=\"../_images/transformer_input_target.png\" src=\"../_images/transformer_input_target.png\"/>",
            "markdown"
        ],
        [
            "It should be noted that the chunks are along dimension 0, consistent\nwith the S dimension in the Transformer model. The batch dimension\nN is along dimension 1.",
            "markdown"
        ],
        [
            "# In 'run_worker'\n    bptt = 35\n    def get_batch(source, i):\n        seq_len = min(bptt, len(source) - 1 - i)\n        data = source[i:i+seq_len]\n        target = source[i+1:i+1+seq_len].view(-1)\n        # Need batch dimension first for pipeline parallelism.\n        return data.t(), target",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Model scale and Pipe initialization": [
        [
            "To demonstrate training large Transformer models using pipeline parallelism,\nwe scale up the Transformer layers appropriately. We use an embedding\ndimension of 4096, hidden size of 4096, 16 attention heads and 8 total\ntransformer layers (nn.TransformerEncoderLayer). This creates a model with\n<strong>~1 billion</strong> parameters.",
            "markdown"
        ],
        [
            "We need to initialize the \nsince Pipe depends on the RPC framework via \nwhich allows for future expansion to cross host pipelining. We need to\ninitialize the RPC framework with only a single worker since we\u2019re using a\nsingle process to drive multiple GPUs.",
            "markdown"
        ],
        [
            "The pipeline is then initialized with 8 transformer layers on one GPU and 8\ntransformer layers on the other GPU. One pipe is setup across GPUs 0 and 1 and\nanother across GPUs 2 and 3. Both pipes are then replicated using DistributedDataParallel.",
            "markdown"
        ],
        [
            "# In 'run_worker'\n    ntokens = len(vocab) # the size of vocabulary\n    emsize = 4096 # embedding dimension\n    nhid = 4096 # the dimension of the feedforward network model in nn.TransformerEncoder\n    nlayers = 8 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n    nhead = 16 # the number of heads in the multiheadattention models\n    dropout = 0.2 # the dropout value\n\n    from torch.distributed import rpc\n    tmpfile = tempfile.NamedTemporaryFile()\n    (\n        name=\"worker\",\n        rank=0,\n        world_size=1,\n        rpc_backend_options=(\n            init_method=\"file://{}\".format(tmpfile.name),\n            # Specifying _transports and _channels is a workaround and we no longer\n            # will have to specify _transports and _channels for PyTorch\n            # versions &gt;= 1.8.1\n            _transports=[\"ibv\", \"uv\"],\n            _channels=[\"cuda_ipc\", \"cuda_basic\"],\n        )\n    )\n\n    # Num gpus for model parallelism.\n    num_gpus = 2\n    partition_len = ((nlayers - 1) // num_gpus) + 1\n\n    # Add encoder in the beginning.\n    tmp_list = [Encoder(ntokens, emsize, dropout).cuda(2 * rank)]\n    module_list = []\n\n    # Add all the necessary transformer blocks.\n    for i in range(nlayers):\n        transformer_block = (emsize, nhead, nhid, dropout)\n        if i != 0 and i % (partition_len) == 0:\n            module_list.append((*tmp_list))\n            tmp_list = []\n        device = i // (partition_len)\n        tmp_list.append(transformer_block.to(2 * rank + device))\n\n    # Add decoder in the end.\n    tmp_list.append(Decoder(ntokens, emsize).cuda(2 * rank + num_gpus - 1))\n    module_list.append((*tmp_list))\n\n    # Need to use 'checkpoint=never' since as of PyTorch 1.8, Pipe checkpointing\n    # doesn't work with DDP.\n    from torch.distributed.pipeline.sync import \n    chunks = 8\n    model = ((\n        *module_list), chunks = chunks, checkpoint=\"never\")\n\n    # Initialize process group and wrap model in DDP.\n    from torch.nn.parallel import \n    import torch.distributed as dist\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    (\n                backend=\"nccl\", rank=rank, world_size=world_size)\n    model = (model)\n\n    def get_total_params(module: ):\n        total_params = 0\n        for param in module.parameters():\n            total_params += param.numel()\n        return total_params\n\n    print_with_rank('Total parameters in model: {:,}'.format(get_total_params(model)))",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Run the model": [
        [
            "is applied to track the loss and\n\nimplements stochastic gradient descent method as the optimizer. The initial\nlearning rate is set to 5.0.  is\napplied to adjust the learn rate through epochs. During the\ntraining, we use\n\nfunction to scale all the gradient together to prevent exploding.",
            "markdown"
        ],
        [
            "# In 'run_worker'\n    criterion = ()\n    lr = 5.0 # learning rate\n    optimizer = (model.parameters(), lr=lr)\n    scheduler = (optimizer, 1.0, gamma=0.95)\n\n    import time\n    def train():\n        model.train() # Turn on the train mode\n        total_loss = 0.\n        start_time = time.time()\n        ntokens = len(vocab)\n\n        # Train only for 50 batches to keep script execution time low.\n        nbatches = min(50 * bptt, train_data.size(0) - 1)\n\n        for batch, i in enumerate(range(0, nbatches, bptt)):\n            data, targets = get_batch(train_data, i)\n            optimizer.zero_grad()\n            # Since the Pipe is only within a single host and process the ``RRef``\n            # returned by forward method is local to this node and can simply\n            # retrieved via ``RRef.local_value()``.\n            output = model(data).local_value()\n            # Need to move targets to the device where the output of the\n            # pipeline resides.\n            loss = criterion(output.view(-1, ntokens), targets.cuda(2 * rank + 1))\n            loss.backward()\n            (model.parameters(), 0.5)\n            optimizer.step()\n\n            total_loss += loss.item()\n            log_interval = 10\n            if batch % log_interval == 0 and batch &gt; 0:\n                cur_loss = total_loss / log_interval\n                elapsed = time.time() - start_time\n                print_with_rank('| epoch {:3d} | {:5d}/{:5d} batches | '\n                      'lr {:02.2f} | ms/batch {:5.2f} | '\n                      'loss {:5.2f} | ppl {:8.2f}'.format(\n                        epoch, batch, nbatches // bptt, scheduler.get_last_lr()[0],\n                        elapsed * 1000 / log_interval,\n                        cur_loss, math.exp(cur_loss)))\n                total_loss = 0\n                start_time = time.time()\n\n    def evaluate(eval_model, data_source):\n        eval_model.eval() # Turn on the evaluation mode\n        total_loss = 0.\n        ntokens = len(vocab)\n        # Evaluate only for 50 batches to keep script execution time low.\n        nbatches = min(50 * bptt, data_source.size(0) - 1)\n        with ():\n            for i in range(0, nbatches, bptt):\n                data, targets = get_batch(data_source, i)\n                output = eval_model(data).local_value()\n                output_flat = output.view(-1, ntokens)\n                # Need to move targets to the device where the output of the\n                # pipeline resides.\n                total_loss += len(data) * criterion(output_flat, targets.cuda(2 * rank + 1)).item()\n        return total_loss / (len(data_source) - 1)",
            "code"
        ],
        [
            "Loop over epochs. Save the model if the validation loss is the best\nwe\u2019ve seen so far. Adjust the learning rate after each epoch.",
            "markdown"
        ],
        [
            "# In 'run_worker'\n    best_val_loss = float(\"inf\")\n    epochs = 3 # The number of epochs\n    best_model = None\n\n    for epoch in range(1, epochs + 1):\n        epoch_start_time = time.time()\n        train()\n        val_loss = evaluate(model, val_data)\n        print_with_rank('-' * 89)\n        print_with_rank('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                         val_loss, math.exp(val_loss)))\n        print_with_rank('-' * 89)\n\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            best_model = model\n\n        scheduler.step()",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Evaluate the model with the test dataset": [
        [
            "Apply the best model to check the result with the test dataset.",
            "markdown"
        ],
        [
            "# In 'run_worker'\n    test_loss = evaluate(best_model, test_data)\n    print_with_rank('=' * 89)\n    print_with_rank('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n        test_loss, math.exp(test_loss)))\n    print_with_rank('=' * 89)\n\n# Main execution\nimport torch.multiprocessing as mp\n\nif __name__==\"__main__\":\n    world_size = 2\n    (run_worker, args=(world_size, ), nprocs=world_size, join=True)",
            "code"
        ]
    ],
    "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Output": [
        [
            "[RANK 0]: | epoch   1 |    10/   50 batches | lr 5.00 | ms/batch 778.97 | loss 43.31 | ppl 6432469059895903232.00\n[RANK 1]: | epoch   1 |    10/   50 batches | lr 5.00 | ms/batch 778.90 | loss 44.50 | ppl 21245447128217366528.00\n[RANK 0]: | epoch   1 |    20/   50 batches | lr 5.00 | ms/batch 699.89 | loss 44.50 | ppl 21176949187407757312.00\n[RANK 1]: | epoch   1 |    20/   50 batches | lr 5.00 | ms/batch 699.87 | loss 44.62 | ppl 23975861229620961280.00\n[RANK 0]: | epoch   1 |    30/   50 batches | lr 5.00 | ms/batch 698.86 | loss 41.62 | ppl 1193312915629888256.00\n[RANK 1]: | epoch   1 |    30/   50 batches | lr 5.00 | ms/batch 698.87 | loss 40.69 | ppl 471605759847546240.00\n[RANK 0]: | epoch   1 |    40/   50 batches | lr 5.00 | ms/batch 698.34 | loss 45.20 | ppl 42812308420836458496.00\n[RANK 1]: | epoch   1 |    40/   50 batches | lr 5.00 | ms/batch 698.33 | loss 45.68 | ppl 68839569686012223488.00\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 1]: | end of epoch   1 | time: 40.08s | valid loss  0.80 | valid ppl     2.22\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 0]: | end of epoch   1 | time: 40.09s | valid loss  0.80 | valid ppl     2.22\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 0]: | epoch   2 |    10/   50 batches | lr 4.75 | ms/batch 768.51 | loss 36.34 | ppl 6063529544668166.00\n[RANK 1]: | epoch   2 |    10/   50 batches | lr 4.75 | ms/batch 769.23 | loss 37.41 | ppl 17651211266236086.00\n[RANK 0]: | epoch   2 |    20/   50 batches | lr 4.75 | ms/batch 699.57 | loss 28.97 | ppl 3798441739584.11\n[RANK 1]: | epoch   2 |    20/   50 batches | lr 4.75 | ms/batch 699.56 | loss 29.28 | ppl 5203636967575.47\n[RANK 0]: | epoch   2 |    30/   50 batches | lr 4.75 | ms/batch 699.04 | loss 28.43 | ppl 2212498693571.25\n[RANK 1]: | epoch   2 |    30/   50 batches | lr 4.75 | ms/batch 699.05 | loss 28.33 | ppl 2015144761281.48\n[RANK 0]: | epoch   2 |    40/   50 batches | lr 4.75 | ms/batch 699.10 | loss 23.30 | ppl 13121380184.92\n[RANK 1]: | epoch   2 |    40/   50 batches | lr 4.75 | ms/batch 699.09 | loss 23.41 | ppl 14653799192.87\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 0]: | end of epoch   2 | time: 39.97s | valid loss  0.24 | valid ppl     1.27\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 1]: | end of epoch   2 | time: 39.98s | valid loss  0.24 | valid ppl     1.27\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 0]: | epoch   3 |    10/   50 batches | lr 4.51 | ms/batch 769.36 | loss 12.80 | ppl 361681.11\n[RANK 1]: | epoch   3 |    10/   50 batches | lr 4.51 | ms/batch 768.97 | loss 12.57 | ppl 287876.61\n[RANK 0]: | epoch   3 |    20/   50 batches | lr 4.51 | ms/batch 698.27 | loss 12.01 | ppl 164364.60\n[RANK 1]: | epoch   3 |    20/   50 batches | lr 4.51 | ms/batch 698.30 | loss 11.98 | ppl 159095.89\n[RANK 0]: | epoch   3 |    30/   50 batches | lr 4.51 | ms/batch 697.75 | loss 10.90 | ppl 54261.91\n[RANK 1]: | epoch   3 |    30/   50 batches | lr 4.51 | ms/batch 697.72 | loss 10.89 | ppl 53372.39\n[RANK 0]: | epoch   3 |    40/   50 batches | lr 4.51 | ms/batch 699.49 | loss 10.78 | ppl 47948.35\n[RANK 1]: | epoch   3 |    40/   50 batches | lr 4.51 | ms/batch 699.50 | loss 10.79 | ppl 48664.42\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 0]: | end of epoch   3 | time: 39.96s | valid loss  0.38 | valid ppl     1.46\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 1]: | end of epoch   3 | time: 39.96s | valid loss  0.38 | valid ppl     1.46\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 0]: =========================================================================================\n[RANK 0]: | End of training | test loss  0.33 | test ppl     1.39\n[RANK 0]: =========================================================================================\n[RANK 1]: =========================================================================================\n[RANK 1]: | End of training | test loss  0.33 | test ppl     1.39\n[RANK 1]: =========================================================================================",
            "code"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Training with Uneven Inputs Using the Join Context Manager": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            " View and edit this tutorial in .",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Join is introduced in PyTorch 1.10 as a prototype feature. This\nAPI is subject to change.",
            "markdown"
        ],
        [
            "In this tutorial, you will see:",
            "markdown"
        ],
        [
            "An overview of the  context manager.",
            "markdown"
        ],
        [
            "An example of how to use the context manager with DistributedDataParallel.",
            "markdown"
        ],
        [
            "An example of how to use the context manager with both\nDistributedDataParallel and ZeroRedundancyOptimizer.",
            "markdown"
        ],
        [
            "An example of passing in keyword arguments to the context manager.",
            "markdown"
        ],
        [
            "A dive into how the  context manager works.",
            "markdown"
        ],
        [
            "An example showing how to make a toy class compatible with the context\nmanager.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Training with Uneven Inputs Using the Join Context Manager->Requirements": [
        [
            "PyTorch 1.10+",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Training with Uneven Inputs Using the Join Context Manager->What is Join?": [
        [
            "In , you saw\nthe general skeleton for using  to perform data\nparallel training. This implicitly schedules all-reduces in each backward pass\nto synchronize gradients across ranks. Such  require participation\nfrom all ranks in the process group, so if a rank has fewer inputs, then the\nother ranks will hang or error (depending on the backend). More generally, this\nproblem persists for any class that performs per-iteration synchronous\ncollective communications.",
            "markdown"
        ],
        [
            "Join is a context manager to be used around your per-rank training loop to\nfacilitate training with uneven inputs. The context manager allows the ranks\nthat exhaust their inputs early (i.e. <em>join</em> early) to shadow the collective\ncommunications performed by those that have not yet joined. The ways in which\nthe communications are shadowed are specified by hooks.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Training with Uneven Inputs Using the Join Context Manager->Using Join with DistributedDataParallel": [
        [
            "PyTorch\u2019s  works out-of-the-box with the Join\ncontext manager. Here is an example usage:",
            "markdown"
        ],
        [
            "import os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.distributed.algorithms.join import Join\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nBACKEND = \"nccl\"\nWORLD_SIZE = 2\nNUM_INPUTS = 5\n\ndef worker(rank):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE)\n\n    model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])\n    # Rank 1 gets one more input than rank 0\n    inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)]\n\n    num_inputs = 0\n    with Join([model]):\n        for input in inputs:\n            num_inputs += 1\n            loss = model(input).sum()\n            loss.backward()\n\n    print(f\"Rank {rank} has exhausted all {num_inputs} of its inputs!\")\n\ndef main():\n    mp.spawn(worker, nprocs=WORLD_SIZE, join=True)\n\nif __name__ == \"__main__\":\n    main()",
            "code"
        ],
        [
            "This produces the following output (where the print() s from rank 0 and\nrank 1 may be arbitrarily ordered):",
            "markdown"
        ],
        [
            "Rank 0 has exhausted all 5 of its inputs!\nRank 1 has exhausted all 6 of its inputs!",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            " provided its own  context manager\nprior to the introduction of this generic Join context manager. In the\nabove example, using with Join([model]): is equivalent to using\nwith model.join():. One limitation of the existing\nDistributedDataParallel.join() is that it does not allow multiple\nparticipating classes, e.g. DistributedDataParallel and\n together.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Training with Uneven Inputs Using the Join Context Manager->Using Join with DistributedDataParallel and ZeroRedundancyOptimizer": [
        [
            "The Join context manager works not only with a single class but also with\nmultiple classes together. PyTorch\u2019s ZeroRedundancyOptimizer is also\ncompatible with the context manager, so here, we examine how to modify the\nprevious example to use both DistributedDataParallel and\nZeroRedundancyOptimizer:",
            "markdown"
        ],
        [
            "from torch.distributed.optim import ZeroRedundancyOptimizer as ZeRO\nfrom torch.optim import Adam\n\ndef worker(rank):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE)\n\n    model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])\n    optim = ZeRO(model.parameters(), Adam, lr=0.01)\n    # Rank 1 gets one more input than rank 0\n    inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)]\n\n    num_inputs = 0\n    # Pass both `model` and `optim` into `Join()`\n    with Join([model, optim]):\n        for input in inputs:\n            num_inputs += 1\n            loss = model(input).sum()\n            loss.backward()\n            optim.step()\n\n    print(f\"Rank {rank} has exhausted all {num_inputs} of its inputs!\")",
            "code"
        ],
        [
            "This will yield the same output as before. The notable change was\nadditionally passing in the ZeroRedundancyOptimizer instance into\nJoin().",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Training with Uneven Inputs Using the Join Context Manager->Passing Keyword Arguments": [
        [
            "Classes may provide keyword arguments that modify their behavior in the context\nmanager at run time. For example, DistributedDataParallel provides an\nargument divide_by_initial_world_size, which determines if gradients are\ndivided by the initial world size or by the effective world size (i.e. number\nof non-joined ranks). Such keyword arguments can be passed directly into the\ncontext manager.",
            "markdown"
        ],
        [
            "with Join([model, optim], divide_by_initial_world_size=False):\n    for input in inputs:\n        ...",
            "code"
        ],
        [
            "Warning",
            "markdown"
        ],
        [
            "The keyword arguments passed into the context manager are shared across\nall participating classes. This should not be a limitation since we do\nnot expect cases where multiple Joinable s need differing settings\nof the same argument. Nonetheless, this is something to keep in mind.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Training with Uneven Inputs Using the Join Context Manager->How Does Join Work?": [
        [
            "Now that we have seen some preliminary examples of how to use the Join\ncontext manager, let us delve deeper into how it works. This will provide a\ngreater insight into the full capability that it offers and prepare you to make\nyour own custom classes compatible. Here, we will go over the Join class as\nwell as the supporting classes Joinable and JoinHook.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Training with Uneven Inputs Using the Join Context Manager->How Does Join Work?->Joinable": [
        [
            "To begin, classes compatible with the Join context manager must inherit\nfrom the abstract base class Joinable. In particular, a Joinable must\nimplement:",
            "markdown"
        ],
        [
            "join_hook(self, **kwargs) -&gt; JoinHook",
            "markdown"
        ],
        [
            "This returns the JoinHook instance for the Joinable, determining how\njoined processes should shadow the per-iteration collective communications\nperformed by the Joinable.",
            "markdown"
        ],
        [
            "join_device(self) -&gt; torch.device",
            "markdown"
        ],
        [
            "This returns a device to be used by the Join context manager to perform\ncollective communications, e.g. torch.device(\"cuda:0\") or\ntorch.device(\"cpu\").",
            "markdown"
        ],
        [
            "join_process_group(self) -&gt; ProcessGroup",
            "markdown"
        ],
        [
            "This returns the process group to be used by the Join context manager to\nperform collective communications.",
            "markdown"
        ],
        [
            "In particular, the join_device and join_process_group are required\nattributes to ensure that the context manager can schedule collective\ncommunications between joined and non-joined processes. One usage is to count\nthe number of non-joined processes on each iteration using an all-reduce.\nAnother usage is for implementing the mechanism required for\nthrow_on_early_termination=True, which we will explain later below.",
            "markdown"
        ],
        [
            "DistributedDataParallel and ZeroRedundancyOptimizer already inherit\nfrom Joinable and implement the above methods, which is why we could\ndirectly use them in the previous examples.",
            "markdown"
        ],
        [
            "Joinable classes should make sure to call the Joinable constructor\nsince it initializes a JoinConfig instance, which is used internally by\nthe context manager to ensure correctness. This will be saved in each\nJoinable as a field _join_config.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Training with Uneven Inputs Using the Join Context Manager->How Does Join Work?->JoinHook": [
        [
            "Next, let us break down the JoinHook class. A JoinHook provides two\nentry points into a context manager:",
            "markdown"
        ],
        [
            "main_hook(self) -&gt; None",
            "markdown"
        ],
        [
            "This hook is called repeatedly by each joined rank while there exists a rank\nthat has not yet joined. It is meant to shadow the collective communications\nperformed by the Joinable in each training iteration (e.g. in one forward\npass, backward pass, and optimizer step).",
            "markdown"
        ],
        [
            "post_hook(self, is_last_joiner: bool) -&gt; None",
            "markdown"
        ],
        [
            "This hook is called once all ranks have joined. It is passed an additional\nbool argument is_last_joiner, which indicates if the rank was one of\nthe last to join. The argument may be useful for synchronization.",
            "markdown"
        ],
        [
            "To give concrete examples of what these hooks may look like, the provided\nZeroRedundancyOptimizer main hook performs an optimizer step per normal\nsince the joined rank is still responsible for updating and synchronizing its\nshard of the parameters, and the provided DistributedDataParallel post-hook\nbroadcasts the final updated model from one of the last joining ranks to ensure\nthat it is the same across all ranks.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Training with Uneven Inputs Using the Join Context Manager->How Does Join Work?->Join": [
        [
            "Finally, let us examine how these fit into the Join class itself.",
            "markdown"
        ],
        [
            "__init__(self, joinables: List[Joinable], enable: bool = True, throw_on_early_termination: bool = False)",
            "markdown"
        ],
        [
            "As we saw in the previous examples, the constructor takes in a list of the\nJoinable s that participate in the training loop. These should be the\nclasses that perform collective communications in each iteration.",
            "markdown"
        ],
        [
            "enable is a bool that can be set to False if you know that there\nwill not be uneven inputs, in which case the context manager becomes vacuous\nsimilar to contextlib.nullcontext(). This also may disable join-related\ncomputation in the participating Joinable s.",
            "markdown"
        ],
        [
            "throw_on_early_termination is a bool that can be set to True to\nhave each rank raise an exception the moment that uneven inputs are detected.\nThis is useful for cases that do not conform to the context manager\u2019s\nrequirements, which is most typically when there are collective communications\nfrom different classes that may be arbitrarily interleaved, such as when using\nDistributedDataParallel with a model that has SyncBatchNorm layers. In\nsuch cases, this argument should be set to True so that the application\nlogic can catch the exception and determine how to proceed.",
            "markdown"
        ],
        [
            "The core logic occurs in the __exit__() method, which loops while there\nexists a non-joined rank, calling each Joinable \u2018s main hook, and\nthen once all ranks have joined, calls their post hooks. Both the main hooks\nand post-hooks are iterated over in the order that the Joinable s are\npassed in.",
            "markdown"
        ],
        [
            "The context manager requires a heartbeat from non-joined processes. As such,\neach Joinable class should make a call to Join.notify_join_context()\nbefore its per-iteration collective communications. The context manager will\nensure that only the first Joinable passed in actually sends the\nheartbeat.",
            "markdown"
        ],
        [
            "Warning",
            "markdown"
        ],
        [
            "As mentioned above regarding throw_on_early_termination, the\nJoin context manager is not compatible with certain compositions of\nclasses. The Joinable \u2018s JoinHook s must be serializable since each\nhook is fully executed before proceeding to the next. In other words, two\nhooks cannot overlap. Moreover, currently, both the main hooks and post-\nhooks are iterated over in the same deterministic order. If this appears to\nbe a major limitation, we may modify the API to permit a customizable\nordering.",
            "markdown"
        ]
    ],
    "torch->Parallel and Distributed Training->Distributed Training with Uneven Inputs Using the Join Context Manager->Making a Toy Class Work with Join": [
        [
            "Since the previous section introduced several concepts, let us see them in\npractice with a toy example. Here, we will implement a class that counts the\nnumber of inputs that are seen across all ranks before its rank joins. This\nshould provide a basic idea of how you may make your own class compatible\nwith the Join context manager.",
            "markdown"
        ],
        [
            "Specifically, the following code has each rank print out (1) the number of\ninputs across all ranks that seen before it joins and (2) the total number\nof inputs across all ranks.",
            "markdown"
        ],
        [
            "import os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.distributed.algorithms.join import Join, Joinable, JoinHook\n\nBACKEND = \"nccl\"\nWORLD_SIZE = 2\nNUM_INPUTS = 5\n\nclass CounterJoinHook(JoinHook):\n    r\"\"\"\n    Join hook for :class:`Counter`.\n\n    Arguments:\n        counter (Counter): the :class:`Counter` object using this hook.\n        sync_max_count (bool): whether to sync the max count once all ranks\n            join.\n    \"\"\"\n    def __init__(\n        self,\n        counter,\n        sync_max_count\n    ):\n        self.counter = counter\n        self.sync_max_count = sync_max_count\n\n    def main_hook(self):\n        r\"\"\"\n        Shadows the counter's all-reduce by all-reducing a dim-1 zero tensor.\n        \"\"\"\n        t = torch.zeros(1, device=self.counter.device)\n        dist.all_reduce(t)\n\n    def post_hook(self, is_last_joiner: bool):\n        r\"\"\"\n        Synchronizes the max count across all :class:`Counter` s if\n        ``sync_max_count=True``.\n        \"\"\"\n        if not self.sync_max_count:\n            return\n        rank = dist.get_rank(self.counter.process_group)\n        common_rank = self.counter.find_common_rank(rank, is_last_joiner)\n        if rank == common_rank:\n            self.counter.max_count = self.counter.count.detach().clone()\n        dist.broadcast(self.counter.max_count, src=common_rank)\n\nclass Counter(Joinable):\n    r\"\"\"\n    Example :class:`Joinable` that counts the number of training iterations\n    that it participates in.\n    \"\"\"\n    def __init__(self, device, process_group):\n        super(Counter, self).__init__()\n        self.device = device\n        self.process_group = process_group\n        self.count = torch.tensor([0], device=device).float()\n        self.max_count = torch.tensor([0], device=device).float()\n\n    def __call__(self):\n        r\"\"\"\n        Counts the number of inputs processed on this iteration by all ranks\n        by all-reducing a dim-1 one tensor; increments its own internal count.\n        \"\"\"\n        Join.notify_join_context(self)\n        t = torch.ones(1, device=self.device).float()\n        dist.all_reduce(t)\n        self.count += t\n\n    def join_hook(self, **kwargs) -&gt; JoinHook:\n        r\"\"\"\n        Return a join hook that shadows the all-reduce in :meth:`__call__`.\n\n        This join hook supports the following keyword arguments:\n            sync_max_count (bool, optional): whether to synchronize the maximum\n                count across all ranks once all ranks join; default is ``False``.\n        \"\"\"\n        sync_max_count = kwargs.get(\"sync_max_count\", False)\n        return CounterJoinHook(self, sync_max_count)\n\n    @property\n    def join_device(self) -&gt; torch.device:\n        return self.device\n\n    @property\n    def join_process_group(self):\n        return self.process_group\n\n    def find_common_rank(self, rank, to_consider):\n        r\"\"\"\n        Returns the max rank of the ones to consider over the process group.\n        \"\"\"\n        common_rank = torch.tensor([rank if to_consider else -1], device=self.device)\n        dist.all_reduce(common_rank, op=dist.ReduceOp.MAX, group=self.process_group)\n        common_rank = common_rank.item()\n        return common_rank\n\ndef worker(rank):\n    assert torch.cuda.device_count() &gt;= WORLD_SIZE\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE)\n\n    counter = Counter(torch.device(f\"cuda:{rank}\"), dist.group.WORLD)\n    inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)]\n\n    with Join([counter], sync_max_count=True):\n        for _ in inputs:\n            counter()\n\n    print(f\"{int(counter.count.item())} inputs processed before rank {rank} joined!\")\n    print(f\"{int(counter.max_count.item())} inputs processed across all ranks!\")\n\ndef main():\n    mp.spawn(worker, nprocs=WORLD_SIZE, join=True)\n\nif __name__ == \"__main__\":\n    main()",
            "code"
        ],
        [
            "Since rank 0 sees 5 inputs and rank 1 sees 6, this yields the output:",
            "markdown"
        ],
        [
            "10 inputs processed before rank 0 joined!\n11 inputs processed across all ranks!\n11 inputs processed before rank 1 joined!\n11 inputs processed across all ranks!",
            "code"
        ],
        [
            "Some key points to highlight:",
            "markdown"
        ],
        [
            "A Counter instance performs a single all-reduce per iteration, so the\nmain hook performs a single all-reduce as well to shadow it.",
            "markdown"
        ],
        [
            "The Counter class makes a call to Join.notify_join_context() at the\nbeginning of its __call__() method since that is a place before its per-\niteration collective communications (i.e. its all-reduce).",
            "markdown"
        ],
        [
            "The is_last_joiner argument is used to determine the broadcast source in\nthe post-hooks.",
            "markdown"
        ],
        [
            "We pass in the sync_max_count keyword argument to the context manager,\nwhich is then forwarded to Counter \u2018s join hook.",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on iOS": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "<strong>Reviewed by</strong>: ",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on iOS->Introduction": [
        [
            "Semantic image segmentation is a computer vision task that uses semantic labels to mark specific regions of an input image. The PyTorch semantic image segmentation  can be used to label image regions with  including, for example, bicycle, bus, car, dog, and person. Image segmentation models can be very useful in applications such as autonomous driving and scene understanding.",
            "markdown"
        ],
        [
            "In this tutorial, we will provide a step-by-step guide on how to prepare and run the PyTorch DeepLabV3 model on iOS, taking you from the beginning of having a model you may want to use on iOS to the end of having a complete iOS app using the model. We will also cover practical and general tips on how to check if your next favorite pre-trained PyTorch models can run on iOS, and how to avoid pitfalls.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Before going through this tutorial, you should check out  and give the PyTorch iOS  example app a quick try. This tutorial will go beyond the image classification model, usually the first kind of model deployed on mobile. The complete code repo for this tutorial is available .",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on iOS->Learning Objectives": [
        [
            "In this tutorial, you will learn how to:",
            "markdown"
        ],
        [
            "Convert the DeepLabV3 model for iOS deployment.",
            "markdown"
        ],
        [
            "Get the output of the model for the example input image in Python and compare it to the output from the iOS app.",
            "markdown"
        ],
        [
            "Build a new iOS app or reuse an iOS example app to load the converted model.",
            "markdown"
        ],
        [
            "Prepare the input into the format that the model expects and process the model output.",
            "markdown"
        ],
        [
            "Complete the UI, refactor, build and run the app to see image segmentation in action.",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on iOS->Pre-requisites": [
        [
            "PyTorch 1.6 or 1.7",
            "markdown"
        ],
        [
            "torchvision 0.7 or 0.8",
            "markdown"
        ],
        [
            "Xcode 11 or 12",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on iOS->Steps->1. Convert the DeepLabV3 model for iOS deployment": [
        [
            "The first step to deploying a model on iOS is to convert the model into the  format.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Not all PyTorch models can be converted to TorchScript at this time because a model definition may use language features that are not in TorchScript, which is a subset of Python. See the  for more details.",
            "markdown"
        ],
        [
            "Simply run the script below to generate the scripted model <cite>deeplabv3_scripted.pt</cite>:",
            "markdown"
        ],
        [
            "import torch\n\n# use deeplabv3_resnet50 instead of deeplabv3_resnet101 to reduce the model size\nmodel = torch.hub.load('pytorch/vision:v0.8.0', 'deeplabv3_resnet50', pretrained=True)\nmodel.eval()\n\nscriptedm = torch.jit.script(model)\ntorch.jit.save(scriptedm, \"deeplabv3_scripted.pt\")",
            "code"
        ],
        [
            "The size of the generated <cite>deeplabv3_scripted.pt</cite> model file should be around 168MB. Ideally, a model should also be quantized for significant size reduction and faster inference before being deployed on an iOS app. To have a general understanding of quantization, see the  and the resource links there. We will cover in detail how to correctly apply a quantization workflow called Post Training  to the DeepLabV3 model in a future tutorial or recipe.",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on iOS->Steps->2. Get example input and output of the model in Python": [
        [
            "Now that we have a scripted PyTorch model, let\u2019s test with some example inputs to make sure the model works correctly on iOS. First, let\u2019s write a Python script that uses the model to make inferences and examine inputs and outputs. For this example of the DeepLabV3 model, we can reuse the code in Step 1 and in the . Add the following code snippet to the code above:",
            "markdown"
        ],
        [
            "from PIL import Image\nfrom torchvision import transforms\ninput_image = Image.open(\"deeplab.jpg\")\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\nwith torch.no_grad():\n    output = model(input_batch)['out'][0]\n\nprint(input_batch.shape)\nprint(output.shape)",
            "code"
        ],
        [
            "Download <cite>deeplab.jpg</cite> from  and run the script above to see the shapes of the input and output of the model:",
            "markdown"
        ],
        [
            "torch.Size([1, 3, 400, 400])\ntorch.Size([21, 400, 400])",
            "code"
        ],
        [
            "So if you provide the same image input <cite>deeplab.jpg</cite> of size 400x400 to the model on iOS, the output of the model should have the size [21, 400, 400]. You should also print out at least the beginning parts of the actual data of the input and output, to be used in Step 4 below to compare with the actual input and output of the model when running in the iOS app.",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on iOS->Steps->3. Build a new iOS app or reuse an example app and load the model": [
        [
            "First, follow Step 3 of the  to use our model in an Xcode project with PyTorch Mobile enabled. Because both the DeepLabV3 model used in this tutorial and the MobileNet v2 model used in the PyTorch HelloWorld iOS example are computer vision models, you may choose to start with the  as a template to reuse the code that loads the model and processes the input and output.",
            "markdown"
        ],
        [
            "Now let\u2019s add <cite>deeplabv3_scripted.pt</cite> and <cite>deeplab.jpg</cite> used in Step 2 to the Xcode project and modify <cite>ViewController.swift</cite> to resemble:",
            "markdown"
        ],
        [
            "class ViewController: UIViewController {\n    var image = UIImage(named: \"deeplab.jpg\")!\n\n    override func viewDidLoad() {\n        super.viewDidLoad()\n    }\n\n    private lazy var module: TorchModule = {\n        if let filePath = Bundle.main.path(forResource: \"deeplabv3_scripted\",\n              ofType: \"pt\"),\n            let module = TorchModule(fileAtPath: filePath) {\n            return module\n        } else {\n            fatalError(\"Can't load the model file!\")\n        }\n    }()\n}",
            "code"
        ],
        [
            "Then set a breakpoint at the line <cite>return module</cite> and build and run the app. The app should stop at the breakpoint, meaning that the scripted model in Step 1 has been successfully loaded on iOS.",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on iOS->Steps->4. Process the model input and output for model inference": [
        [
            "After the model loads in the previous step, let\u2019s verify that it works with expected inputs and can generate expected outputs. As the model input for the DeepLabV3 model is an image, the same as that of the MobileNet v2 in the HelloWorld example, we will reuse some of the code in the  file from HelloWorld for input processing. Replace the <cite>predictImage</cite> method implementation in <cite>TorchModule.mm</cite> with the following code:",
            "markdown"
        ],
        [
            "- (unsigned char*)predictImage:(void*)imageBuffer {\n    // 1. the example deeplab.jpg size is size 400x400 and there are 21 semantic classes\n    const int WIDTH = 400;\n    const int HEIGHT = 400;\n    const int CLASSNUM = 21;\n\n    at::Tensor tensor = torch::from_blob(imageBuffer, {1, 3, WIDTH, HEIGHT}, at::kFloat);\n    torch::autograd::AutoGradMode guard(false);\n    at::AutoNonVariableTypeMode non_var_type_mode(true);\n\n    // 2. convert the input tensor to an NSMutableArray for debugging\n    float* floatInput = tensor.data_ptr&lt;float&gt;();\n    if (!floatInput) {\n        return nil;\n    }\n    NSMutableArray* inputs = [[NSMutableArray alloc] init];\n    for (int i = 0; i &lt; 3 * WIDTH * HEIGHT; i++) {\n        [inputs addObject:@(floatInput[i])];\n    }\n\n    // 3. the output of the model is a dictionary of string and tensor, as\n    // specified at https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101\n    auto outputDict = _impl.forward({tensor}).toGenericDict();\n\n    // 4. convert the output to another NSMutableArray for easy debugging\n    auto outputTensor = outputDict.at(\"out\").toTensor();\n    float* floatBuffer = outputTensor.data_ptr&lt;float&gt;();\n    if (!floatBuffer) {\n      return nil;\n    }\n    NSMutableArray* results = [[NSMutableArray alloc] init];\n    for (int i = 0; i &lt; CLASSNUM * WIDTH * HEIGHT; i++) {\n      [results addObject:@(floatBuffer[i])];\n    }\n\n    return nil;\n}",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The model output is a dictionary for the DeepLabV3 model so we use <cite>toGenericDict</cite> to correctly extract the result. For other models, the model output may also be a single tensor or a tuple of tensors, among other things.",
            "markdown"
        ],
        [
            "With the code changes shown above, you can set breakpoints after the two for loops that populate <cite>inputs</cite> and <cite>results</cite> and compare them with the model input and output data you saw in Step 2 to see if they match. For the same inputs to the models running on iOS and Python, you should get the same outputs.",
            "markdown"
        ],
        [
            "All we have done so far is to confirm that the model of our interest can be scripted and run correctly in our iOS app as in Python. The steps we walked through so far for using a model in an iOS app consumes the bulk, if not most, of our app development time, similar to how data preprocessing is the heaviest lift for a typical machine learning project.",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on iOS->Steps->5. Complete the UI, refactor, build and run the app": [
        [
            "Now we are ready to complete the app and the UI to actually see the processed result as a new image. The output processing code should be like this, added to the end of the code snippet in Step 4 in <cite>TorchModule.mm</cite> - remember to first remove the line <cite>return nil;</cite> temporarily put there to make the code build and run:",
            "markdown"
        ],
        [
            "// see the 20 semantic classes link in Introduction\nconst int DOG = 12;\nconst int PERSON = 15;\nconst int SHEEP = 17;\n\nNSMutableData* data = [NSMutableData dataWithLength:\n    sizeof(unsigned char) * 3 * WIDTH * HEIGHT];\nunsigned char* buffer = (unsigned char*)[data mutableBytes];\n// go through each element in the output of size [WIDTH, HEIGHT] and\n// set different color for different classnum\nfor (int j = 0; j &lt; WIDTH; j++) {\n    for (int k = 0; k &lt; HEIGHT; k++) {\n        // maxi: the index of the 21 CLASSNUM with the max probability\n        int maxi = 0, maxj = 0, maxk = 0;\n        float maxnum = -100000.0;\n        for (int i = 0; i &lt; CLASSNUM; i++) {\n            if ([results[i * (WIDTH * HEIGHT) + j * WIDTH + k] floatValue] &gt; maxnum) {\n                maxnum = [results[i * (WIDTH * HEIGHT) + j * WIDTH + k] floatValue];\n                maxi = i; maxj = j; maxk = k;\n            }\n        }\n        int n = 3 * (maxj * width + maxk);\n        // color coding for person (red), dog (green), sheep (blue)\n        // black color for background and other classes\n        buffer[n] = 0; buffer[n+1] = 0; buffer[n+2] = 0;\n        if (maxi == PERSON) buffer[n] = 255;\n        else if (maxi == DOG) buffer[n+1] = 255;\n        else if (maxi == SHEEP) buffer[n+2] = 255;\n    }\n}\nreturn buffer;",
            "code"
        ],
        [
            "The implementation here is based on the understanding of the DeepLabV3 model which outputs a tensor of size [21, width, height] for an input image of width*height. Each element in the width*height output array is a value between 0 and 20 (for a total of 21 semantic labels described in Introduction) and the value is used to set a specific color. Color coding of the segmentation here is based on the class with the highest probability, and you can extend the color coding for all classes in your own dataset.",
            "markdown"
        ],
        [
            "After the output processing, you will also need to call a helper function to convert the RGB <cite>buffer</cite> to an <cite>UIImage</cite> instance to be shown on <cite>UIImageView</cite>. You can refer to the example code <cite>convertRGBBufferToUIImage</cite> defined in <cite>UIImageHelper.mm</cite> in the code repo.",
            "markdown"
        ],
        [
            "The UI for this app is also similar to that for HelloWorld, except that you do not need the <cite>UITextView</cite> to show the image classification result. You can also add two buttons <cite>Segment</cite> and <cite>Restart</cite> as shown in the code repo to run the model inference and to show back the original image after the segmentation result is shown.",
            "markdown"
        ],
        [
            "The last step before we can run the app is to connect all the pieces together. Modify the <cite>ViewController.swift</cite> file to use the <cite>predictImage</cite>, which is refactored and changed to <cite>segmentImage</cite> in the repo, and helper functions you built as shown in the example code in the repo in <cite>ViewController.swift</cite>. Connect the buttons to the actions and you should be good to go.",
            "markdown"
        ],
        [
            "Now when you run the app on an iOS simulator or an actual iOS device, you will see the following screens:",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on iOS->Recap": [
        [
            "In this tutorial, we described what it takes to convert a pre-trained PyTorch DeepLabV3 model for iOS and how to make sure the model can run successfully on iOS. Our focus was to help you understand the process of confirming that a model can indeed run on iOS. The complete code repo is available .",
            "markdown"
        ],
        [
            "More advanced topics such as quantization and using models via transfer learning or of your own on iOS will be covered soon in future demo apps and tutorials.",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on iOS->Learn More": [],
    "torch->Mobile->Image Segmentation DeepLabV3 on Android": [
        [
            "<strong>Author</strong>: ",
            "markdown"
        ],
        [
            "<strong>Reviewed by</strong>: ",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on Android->Introduction": [
        [
            "Semantic image segmentation is a computer vision task that uses semantic labels to mark specific regions of an input image. The PyTorch semantic image segmentation  can be used to label image regions with  including, for example, bicycle, bus, car, dog, and person. Image segmentation models can be very useful in applications such as autonomous driving and scene understanding.",
            "markdown"
        ],
        [
            "In this tutorial, we will provide a step-by-step guide on how to prepare and run the PyTorch DeepLabV3 model on Android, taking you from the beginning of having a model you may want to use on Android to the end of having a complete Android app using the model. We will also cover practical and general tips on how to check if your next favorable pre-trained PyTorch models can run on Android, and how to avoid pitfalls.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Before going through this tutorial, you should check out  and give the PyTorch Android  example app a quick try. This tutorial will go beyond the image classification model, usually the first kind of model deployed on mobile. The complete code repo for this tutorial is available .",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on Android->Learning Objectives": [
        [
            "In this tutorial, you will learn how to:",
            "markdown"
        ],
        [
            "Convert the DeepLabV3 model for Android deployment.",
            "markdown"
        ],
        [
            "Get the output of the model for the example input image in Python and compare it to the output from the Android app.",
            "markdown"
        ],
        [
            "Build a new Android app or reuse an Android example app to load the converted model.",
            "markdown"
        ],
        [
            "Prepare the input into the format that the model expects and process the model output.",
            "markdown"
        ],
        [
            "Complete the UI, refactor, build and run the app to see image segmentation in action.",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on Android->Pre-requisites": [
        [
            "PyTorch 1.6 or 1.7",
            "markdown"
        ],
        [
            "torchvision 0.7 or 0.8",
            "markdown"
        ],
        [
            "Android Studio 3.5.1 or above with NDK installed",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on Android->Steps->1. Convert the DeepLabV3 model for Android deployment": [
        [
            "The first step to deploying a model on Android is to convert the model into the  format.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Not all PyTorch models can be converted to TorchScript at this time because a model definition may use language features that are not in TorchScript, which is a subset of Python. See the  for more details.",
            "markdown"
        ],
        [
            "Simply run the script below to generate the scripted model <cite>deeplabv3_scripted.pt</cite>:",
            "markdown"
        ],
        [
            "import torch\n\n# use deeplabv3_resnet50 instead of resnet101 to reduce the model size\nmodel = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)\nmodel.eval()\n\nscriptedm = torch.jit.script(model)\ntorch.jit.save(scriptedm, \"deeplabv3_scripted.pt\")",
            "code"
        ],
        [
            "The size of the generated <cite>deeplabv3_scripted.pt</cite> model file should be around 168MB. Ideally, a model should also be quantized for significant size reduction and faster inference before being deployed on an Android app. To have a general understanding of quantization, see the  and the resource links there. We will cover in detail how to correctly apply a quantization workflow called Post Training  to the DeepLabV3 model in a future tutorial or recipe.",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on Android->Steps->2. Get example input and output of the model in Python": [
        [
            "Now that we have a scripted PyTorch model, let\u2019s test with some example inputs to make sure the model works correctly on Android. First, let\u2019s write a Python script that uses the model to make inferences and examine inputs and outputs. For this example of the DeepLabV3 model, we can reuse the code in Step 1 and in the . Add the following code snippet to the code above:",
            "markdown"
        ],
        [
            "from PIL import Image\nfrom torchvision import transforms\ninput_image = Image.open(\"deeplab.jpg\")\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\nwith torch.no_grad():\n    output = model(input_batch)['out'][0]\n\nprint(input_batch.shape)\nprint(output.shape)",
            "code"
        ],
        [
            "Download <cite>deeplab.jpg</cite> from , then run the script above and you will see the shapes of the input and output of the model:",
            "markdown"
        ],
        [
            "torch.Size([1, 3, 400, 400])\ntorch.Size([21, 400, 400])",
            "code"
        ],
        [
            "So if you provide the same image input <cite>deeplab.jpg</cite> of size 400x400 to the model on Android, the output of the model should have the size [21, 400, 400]. You should also print out at least the beginning parts of the actual data of the input and output, to be used in Step 4 below to compare with the actual input and output of the model when running in the Android app.",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on Android->Steps->3. Build a new Android app or reuse an example app and load the model": [
        [
            "First, follow Step 3 of the  to use our model in an Android Studio project with PyTorch Mobile enabled. Because both DeepLabV3 used in this tutorial and MobileNet v2 used in the PyTorch HelloWorld Android example are computer vision models, you can also get the  to make it easier to modify the code that loads the model and processes the input and output. The main goal in this step and Step 4 is to make sure the model <cite>deeplabv3_scripted.pt</cite> generated in Step 1 can indeed work correctly on Android.",
            "markdown"
        ],
        [
            "Now let\u2019s add <cite>deeplabv3_scripted.pt</cite> and <cite>deeplab.jpg</cite> used in Step 2 to the Android Studio project and modify the <cite>onCreate</cite> method in the <cite>MainActivity</cite> to resemble:",
            "markdown"
        ],
        [
            "Module module = null;\ntry {\n  module = Module.load(assetFilePath(this, \"deeplabv3_scripted.pt\"));\n} catch (IOException e) {\n  Log.e(\"ImageSegmentation\", \"Error loading model!\", e);\n  finish();\n}",
            "code"
        ],
        [
            "Then set a breakpoint at the line <cite>finish()</cite> and build and run the app. If the app doesn\u2019t stop at the breakpoint, it means  that the scripted model in Step 1 has been successfully loaded on Android.",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on Android->Steps->4. Process the model input and output for model inference": [
        [
            "After the model loads in the previous step, let\u2019s verify that it works with expected inputs and can generate expected outputs. As the model input for the DeepLabV3 model is an image the same as that of the MobileNet v2 in the HelloWorld example, we will reuse some of the code in the  file from HelloWorld for input processing. Replace the code snippet between  and 73 in <cite>MainActivity.java</cite> with the following code:",
            "markdown"
        ],
        [
            "final Tensor inputTensor = TensorImageUtils.bitmapToFloat32Tensor(bitmap,\n        TensorImageUtils.TORCHVISION_NORM_MEAN_RGB,\n        TensorImageUtils.TORCHVISION_NORM_STD_RGB);\nfinal float[] inputs = inputTensor.getDataAsFloatArray();\n\nMap&lt;String, IValue&gt; outTensors =\n    module.forward(IValue.from(inputTensor)).toDictStringKey();\n\n// the key \"out\" of the output tensor contains the semantic masks\n// see https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101\nfinal Tensor outputTensor = outTensors.get(\"out\").toTensor();\nfinal float[] outputs = outputTensor.getDataAsFloatArray();\n\nint width = bitmap.getWidth();\nint height = bitmap.getHeight();",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The model output is a dictionary for the DeepLabV3 model so we use <cite>toDictStringKey</cite> to correctly extract the result. For other models, the model output may also be a single tensor or a tuple of tensors, among other things.",
            "markdown"
        ],
        [
            "With the code changes shown above, you can set breakpoints after <cite>final float[] inputs</cite> and <cite>final float[] outputs</cite>, which populate the input tensor and output tensor data to float arrays for easy debugging. Run the app and when it stops at the breakpoints, compare the numbers in <cite>inputs</cite> and <cite>outputs</cite> with the model input and output data you see in Step 2 to see if they match. For the same inputs to the models running on Android and Python, you should get the same outputs.",
            "markdown"
        ],
        [
            "Warning",
            "markdown"
        ],
        [
            "You may see different model outputs with the same image input when running on an Android emulator due to some Android emulator\u2019s floating point implementation issue. So it is best to test the app on a real Android device.",
            "markdown"
        ],
        [
            "All we have done so far is to confirm that the model of our interest can be scripted and run correctly in our Android app as in Python. The steps we walked through so far for using a model in an iOS app consumes the bulk, if not most, of our app development time, similar to how data preprocessing is the heaviest lift for a typical machine learning project.",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on Android->Steps->5. Complete the UI, refactor, build and run the app": [
        [
            "Now we are ready to complete the app and the UI to actually see the processed result as a new image. The output processing code should be like this, added to the end of the code snippet in Step 4:",
            "markdown"
        ],
        [
            "int[] intValues = new int[width * height];\n// go through each element in the output of size [WIDTH, HEIGHT] and\n// set different color for different classnum\nfor (int j = 0; j &lt; width; j++) {\n    for (int k = 0; k &lt; height; k++) {\n        // maxi: the index of the 21 CLASSNUM with the max probability\n        int maxi = 0, maxj = 0, maxk = 0;\n        double maxnum = -100000.0;\n        for (int i=0; i &lt; CLASSNUM; i++) {\n            if (outputs[i*(width*height) + j*width + k] &gt; maxnum) {\n                maxnum = outputs[i*(width*height) + j*width + k];\n                maxi = i; maxj = j; maxk= k;\n            }\n        }\n        // color coding for person (red), dog (green), sheep (blue)\n        // black color for background and other classes\n        if (maxi == PERSON)\n            intValues[maxj*width + maxk] = 0xFFFF0000; // red\n        else if (maxi == DOG)\n            intValues[maxj*width + maxk] = 0xFF00FF00; // green\n        else if (maxi == SHEEP)\n            intValues[maxj*width + maxk] = 0xFF0000FF; // blue\n        else\n            intValues[maxj*width + maxk] = 0xFF000000; // black\n    }\n}",
            "code"
        ],
        [
            "The constants used in the code above are defined in the beginning of the class <cite>MainActivity</cite>:",
            "markdown"
        ],
        [
            "private static final int CLASSNUM = 21;\nprivate static final int DOG = 12;\nprivate static final int PERSON = 15;\nprivate static final int SHEEP = 17;",
            "code"
        ],
        [
            "The implementation here is based on the understanding of the DeepLabV3 model which outputs a tensor of size [21, width, height] for an input image of width*height. Each element in the width*height output array is a value between 0 and 20 (for a total of 21 semantic labels described in Introduction) and the value is used to set a specific color. Color coding of the segmentation here is based on the class with the highest probability, and you can extend the color coding for all classes in your own dataset.",
            "markdown"
        ],
        [
            "After the output processing, you will also need to call the code below to render the RGB <cite>intValues</cite> array to a bitmap instance <cite>outputBitmap</cite> before displaying it on an <cite>ImageView</cite>:",
            "markdown"
        ],
        [
            "Bitmap bmpSegmentation = Bitmap.createScaledBitmap(bitmap, width, height, true);\nBitmap outputBitmap = bmpSegmentation.copy(bmpSegmentation.getConfig(), true);\noutputBitmap.setPixels(intValues, 0, outputBitmap.getWidth(), 0, 0,\n    outputBitmap.getWidth(), outputBitmap.getHeight());\nimageView.setImageBitmap(outputBitmap);",
            "code"
        ],
        [
            "The UI for this app is also similar to that for HelloWorld, except that you do not need the <cite>TextView</cite> to show the image classification result. You can also add two buttons <cite>Segment</cite> and <cite>Restart</cite> as shown in the code repo to run the model inference and to show back the original image after the segmentation result is shown.",
            "markdown"
        ],
        [
            "Now when you run the app on an Android emulator or preferably an actual device, you will see screens like the following:",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on Android->Recap": [
        [
            "In this tutorial, we described what it takes to convert a pre-trained PyTorch DeepLabV3 model for Android and how to make sure the model can run successfully on Android. Our focus was to help you understand the process of confirming that a model can indeed run on Android. The complete code repo is available .",
            "markdown"
        ],
        [
            "More advanced topics such as quantization and using models via transfer learning or of your own on Android will be covered soon in future demo apps and tutorials.",
            "markdown"
        ]
    ],
    "torch->Mobile->Image Segmentation DeepLabV3 on Android->Learn More": [],
    "torch->Recommendation Systems->Introduction to TorchRec": [
        [
            "Tip",
            "markdown"
        ],
        [
            "To get the most of this tutorial, we suggest using this\n.\nThis will allow you to experiment with the information presented below.",
            "markdown"
        ],
        [
            "Follow along with the video below or on .\n\n<iframe allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/cjgj41dvSeQ\" width=\"560\"></iframe>",
            "markdown"
        ],
        [
            "When building recommendation systems, we frequently want to represent\nentities like products or pages with embeddings. For example, see Meta\nAI\u2019s , or DLRM. As the number of\nentities grow, the size of the embedding tables can exceed a single\nGPU\u2019s memory. A common practice is to shard the embedding table across\ndevices, a type of model parallelism. To that end, TorchRec introduces\nits primary API\ncalled ,\nor DMP. Like PyTorch\u2019s DistributedDataParallel, DMP wraps a model to\nenable distributed training.",
            "markdown"
        ]
    ],
    "torch->Recommendation Systems->Introduction to TorchRec->Installation": [
        [
            "Requirements: python &gt;= 3.7",
            "markdown"
        ],
        [
            "We highly recommend CUDA when using TorchRec (If using CUDA: cuda &gt;= 11.0).",
            "markdown"
        ],
        [
            "# install pytorch with cudatoolkit 11.3\nconda install pytorch cudatoolkit=11.3 -c pytorch-nightly -y\n# install TorchRec\npip3 install torchrec-nightly",
            "code"
        ]
    ],
    "torch->Recommendation Systems->Introduction to TorchRec->Overview": [
        [
            "This tutorial will cover three pieces of TorchRec: the nn.module , the  API, and\nthe datastructure .",
            "markdown"
        ]
    ],
    "torch->Recommendation Systems->Introduction to TorchRec->Overview->Distributed Setup": [
        [
            "We setup our environment with torch.distributed. For more info on\ndistributed, see this\n.",
            "markdown"
        ],
        [
            "Here, we use one rank (the colab process) corresponding to our 1 colab\nGPU.",
            "markdown"
        ],
        [
            "import os\nimport torch\nimport torchrec\nimport torch.distributed as dist\n\nos.environ[\"RANK\"] = \"0\"\nos.environ[\"WORLD_SIZE\"] = \"1\"\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"29500\"\n\n# Note - you will need a V100 or A100 to run tutorial as as!\n# If using an older GPU (such as colab free K80),\n# you will need to compile fbgemm with the appripriate CUDA architecture\n# or run with \"gloo\" on CPUs\ndist.init_process_group(backend=\"nccl\")",
            "code"
        ]
    ],
    "torch->Recommendation Systems->Introduction to TorchRec->Overview->From EmbeddingBag to EmbeddingBagCollection": [
        [
            "PyTorch represents embeddings through  and .\nEmbeddingBag is a pooled version of Embedding.",
            "markdown"
        ],
        [
            "TorchRec extends these modules by creating collections of embeddings. We\nwill use  to represent a group of EmbeddingBags.",
            "markdown"
        ],
        [
            "Here, we create an EmbeddingBagCollection (EBC) with two embedding bags.\nEach table, product_table and user_table, is represented by a 64\ndimension embedding of size 4096. Note how we initially allocate the EBC\non device \u201cmeta\u201d. This will tell EBC to not allocate memory yet.",
            "markdown"
        ],
        [
            "ebc = torchrec.EmbeddingBagCollection(\n    device=\"meta\",\n    tables=[\n        torchrec.EmbeddingBagConfig(\n            name=\"product_table\",\n            embedding_dim=64,\n            num_embeddings=4096,\n            feature_names=[\"product\"],\n            pooling=torchrec.PoolingType.SUM,\n        ),\n        torchrec.EmbeddingBagConfig(\n            name=\"user_table\",\n            embedding_dim=64,\n            num_embeddings=4096,\n            feature_names=[\"user\"],\n            pooling=torchrec.PoolingType.SUM,\n        )\n    ]\n)",
            "code"
        ]
    ],
    "torch->Recommendation Systems->Introduction to TorchRec->Overview->DistributedModelParallel": [
        [
            "Now, we\u2019re ready to wrap our model with  (DMP). Instantiating DMP will:",
            "markdown"
        ],
        [
            "Decide how to shard the model. DMP will collect the available\n\u2018sharders\u2019 and come up with a \u2018plan\u2019 of the optimal way to shard the\nembedding table(s) (i.e., the EmbeddingBagCollection).",
            "markdown"
        ],
        [
            "Actually shard the model. This includes allocating memory for each\nembedding table on the appropriate device(s).",
            "markdown"
        ],
        [
            "In this toy example, since we have two EmbeddingTables and one GPU,\nTorchRec will place both on the single GPU.",
            "markdown"
        ],
        [
            "model = torchrec.distributed.DistributedModelParallel(ebc, device=torch.device(\"cuda\"))\nprint(model)\nprint(model.plan)",
            "code"
        ]
    ],
    "torch->Recommendation Systems->Introduction to TorchRec->Overview->Query vanilla nn.EmbeddingBag with input and offsets": [
        [
            "We query  and \nwith input and offsets. Input is a 1-D tensor containing the\nlookup values. Offsets is a 1-D tensor where the sequence is a\ncumulative sum of the number of values to pool per example.",
            "markdown"
        ],
        [
            "Let\u2019s look at an example, recreating the product EmbeddingBag above:",
            "markdown"
        ],
        [
            "|------------|\n| product ID |\n|------------|\n| [101, 202] |\n| []         |\n| [303]      |\n|------------|",
            "code"
        ],
        [
            "product_eb = torch.nn.EmbeddingBag(4096, 64)\nproduct_eb(input=torch.tensor([101, 202, 303]), offsets=torch.tensor([0, 2, 2]))",
            "code"
        ]
    ],
    "torch->Recommendation Systems->Introduction to TorchRec->Overview->Representing minibatches with KeyedJaggedTensor": [
        [
            "We need an efficient representation of multiple examples of an arbitrary\nnumber of entity IDs per feature per example. In order to enable this\n\u201cjagged\u201d representation, we use the TorchRec datastructure\n (KJT).",
            "markdown"
        ],
        [
            "Let\u2019s take a look at how to lookup a collection of two embedding\nbags, \u201cproduct\u201d and \u201cuser\u201d. Assume the minibatch is made up of three\nexamples for three users. The first of which has two product IDs, the\nsecond with none, and the third with one product ID.",
            "markdown"
        ],
        [
            "|------------|------------|\n| product ID | user ID    |\n|------------|------------|\n| [101, 202] | [404]      |\n| []         | [505]      |\n| [303]      | [606]      |\n|------------|------------|",
            "code"
        ],
        [
            "The query should be:",
            "markdown"
        ],
        [
            "mb = torchrec.KeyedJaggedTensor(\n    keys = [\"product\", \"user\"],\n    values = torch.tensor([101, 202, 303, 404, 505, 606]).cuda(),\n    lengths = torch.tensor([2, 0, 1, 1, 1, 1], dtype=torch.int64).cuda(),\n)\n\nprint(mb.to(torch.device(\"cpu\")))",
            "code"
        ],
        [
            "Note that the KJT batch size is\nbatch_size = len(lengths)//len(keys). In the above example,\nbatch_size is 3.",
            "markdown"
        ]
    ],
    "torch->Recommendation Systems->Introduction to TorchRec->Overview->Putting it all together, querying our distributed model with a KJT minibatch": [
        [
            "Finally, we can query our model using our minibatch of products and\nusers.",
            "markdown"
        ],
        [
            "The resulting lookup will contain a KeyedTensor, where each key (or\nfeature) contains a 2D tensor of size 3x64 (batch_size x embedding_dim).",
            "markdown"
        ],
        [
            "pooled_embeddings = model(mb)\nprint(pooled_embeddings)",
            "code"
        ]
    ],
    "torch->Recommendation Systems->Introduction to TorchRec->More resources": [
        [
            "For more information, please see our\n\nexample, which includes multinode training on the criteo terabyte\ndataset, using Meta\u2019s .",
            "markdown"
        ]
    ],
    "torch->Recommendation Systems->Exploring TorchRec sharding": [
        [
            "This tutorial will mainly cover the sharding schemes of embedding tables\nvia EmbeddingPlanner and DistributedModelParallel API and\nexplore the benefits of different sharding schemes for the embedding\ntables by explicitly configuring them.",
            "markdown"
        ]
    ],
    "torch->Recommendation Systems->Exploring TorchRec sharding->Installation": [
        [
            "Requirements: - python &gt;= 3.7",
            "markdown"
        ],
        [
            "We highly recommend CUDA when using torchRec. If using CUDA: - cuda &gt;=\n11.0",
            "markdown"
        ],
        [
            "# install conda to make installying pytorch with cudatoolkit 11.3 easier.\n!sudo rm Miniconda3-py37_4.9.2-Linux-x86_64.sh Miniconda3-py37_4.9.2-Linux-x86_64.sh.*\n!sudo wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.9.2-Linux-x86_64.sh\n!sudo chmod +x Miniconda3-py37_4.9.2-Linux-x86_64.sh\n!sudo bash ./Miniconda3-py37_4.9.2-Linux-x86_64.sh -b -f -p /usr/local",
            "code"
        ],
        [
            "# install pytorch with cudatoolkit 11.3\n!sudo conda install pytorch cudatoolkit=11.3 -c pytorch-nightly -y",
            "code"
        ],
        [
            "Installing torchRec will also install\n, a collection of CUDA\nkernels and GPU enabled operations to run",
            "markdown"
        ],
        [
            "# install torchrec\n!pip3 install torchrec-nightly",
            "code"
        ],
        [
            "Install multiprocess which works with ipython to for multi-processing\nprogramming within colab",
            "markdown"
        ],
        [
            "!pip3 install multiprocess",
            "code"
        ],
        [
            "The following steps are needed for the Colab runtime to detect the added\nshared libraries. The runtime searches for shared libraries in /usr/lib,\nso we copy over the libraries which were installed in /usr/local/lib/.\n<strong>This is a very necessary step, only in the colab runtime</strong>.",
            "markdown"
        ],
        [
            "!sudo cp /usr/local/lib/lib* /usr/lib/",
            "code"
        ],
        [
            "<strong>Restart your runtime at this point for the newly installed packages\nto be seen.</strong> Run the step below immediately after restarting so that\npython knows where to look for packages. <strong>Always run this step after\nrestarting the runtime.</strong>",
            "markdown"
        ],
        [
            "import sys\nsys.path = ['', '/env/python', '/usr/local/lib/python37.zip', '/usr/local/lib/python3.7', '/usr/local/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/site-packages', './.local/lib/python3.7/site-packages']",
            "code"
        ]
    ],
    "torch->Recommendation Systems->Exploring TorchRec sharding->Distributed Setup": [
        [
            "Due to the notebook enviroment, we cannot run\n program here but we\ncan do multiprocessing inside the notebook to mimic the setup. Users\nshould be responsible for setting up their own\n launcher when using\nTorchrec. We setup our environment so that torch distributed based\ncommunication backend can work.",
            "markdown"
        ],
        [
            "import os\nimport torch\nimport torchrec\n\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"29500\"",
            "code"
        ]
    ],
    "torch->Recommendation Systems->Exploring TorchRec sharding->Constructing our embedding model": [
        [
            "Here we use TorchRec offering of\n\nto construct our embedding bag model with embedding tables.",
            "markdown"
        ],
        [
            "Here, we create an EmbeddingBagCollection (EBC) with four embedding\nbags. We have two types of tables: large tables and small tables\ndifferentiated by their row size difference: 4096 vs 1024. Each table is\nstill represented by 64 dimension embedding.",
            "markdown"
        ],
        [
            "We configure the ParameterConstraints data structure for the tables,\nwhich provides hints for the model parallel API to help decide the\nsharding and placement strategy for the tables. In TorchRec, we support\n* table-wise: place the entire table on one device; *\nrow-wise: shard the table evenly by row dimension and place one\nshard on each device of the communication world; * column-wise:\nshard the table evenly by embedding dimension, and place one shard on\neach device of the communication world; * table-row-wise: special\nsharding optimized for intra-host communication for available fast\nintra-machine device interconnect, e.g. NVLink; * data_parallel:\nreplicate the tables for every device;",
            "markdown"
        ],
        [
            "Note how we initially allocate the EBC on device \u201cmeta\u201d. This will tell\nEBC to not allocate memory yet.",
            "markdown"
        ],
        [
            "from torchrec.distributed.planner.types import ParameterConstraints\nfrom torchrec.distributed.embedding_types import EmbeddingComputeKernel\nfrom torchrec.distributed.types import ShardingType\nfrom typing import Dict\n\nlarge_table_cnt = 2\nsmall_table_cnt = 2\nlarge_tables=[\n  torchrec.EmbeddingBagConfig(\n    name=\"large_table_\" + str(i),\n    embedding_dim=64,\n    num_embeddings=4096,\n    feature_names=[\"large_table_feature_\" + str(i)],\n    pooling=torchrec.PoolingType.SUM,\n  ) for i in range(large_table_cnt)\n]\nsmall_tables=[\n  torchrec.EmbeddingBagConfig(\n    name=\"small_table_\" + str(i),\n    embedding_dim=64,\n    num_embeddings=1024,\n    feature_names=[\"small_table_feature_\" + str(i)],\n    pooling=torchrec.PoolingType.SUM,\n  ) for i in range(small_table_cnt)\n]\n\ndef gen_constraints(sharding_type: ShardingType = ShardingType.TABLE_WISE) -&gt; Dict[str, ParameterConstraints]:\n  large_table_constraints = {\n    \"large_table_\" + str(i): ParameterConstraints(\n      sharding_types=[sharding_type.value],\n    ) for i in range(large_table_cnt)\n  }\n  small_table_constraints = {\n    \"small_table_\" + str(i): ParameterConstraints(\n      sharding_types=[sharding_type.value],\n    ) for i in range(small_table_cnt)\n  }\n  constraints = {**large_table_constraints, **small_table_constraints}\n  return constraints",
            "code"
        ],
        [
            "ebc = torchrec.EmbeddingBagCollection(\n    device=\"cuda\",\n    tables=large_tables + small_tables\n)",
            "code"
        ]
    ],
    "torch->Recommendation Systems->Exploring TorchRec sharding->DistributedModelParallel in multiprocessing": [
        [
            "Now, we have a single process execution function for mimicking one\nrank\u2019s work during \nexecution.",
            "markdown"
        ],
        [
            "This code will shard the model collectively with other processes and\nallocate memories accordingly. It first sets up process groups and do\nembedding table placement using planner and generate sharded model using\nDistributedModelParallel.",
            "markdown"
        ],
        [
            "def single_rank_execution(\n    rank: int,\n    world_size: int,\n    constraints: Dict[str, ParameterConstraints],\n    module: torch.nn.Module,\n    backend: str,\n) -&gt; None:\n    import os\n    import torch\n    import torch.distributed as dist\n    from torchrec.distributed.embeddingbag import EmbeddingBagCollectionSharder\n    from torchrec.distributed.model_parallel import DistributedModelParallel\n    from torchrec.distributed.planner import EmbeddingShardingPlanner, Topology\n    from torchrec.distributed.types import ModuleSharder, ShardingEnv\n    from typing import cast\n\n    def init_distributed_single_host(\n        rank: int,\n        world_size: int,\n        backend: str,\n        # pyre-fixme[11]: Annotation `ProcessGroup` is not defined as a type.\n    ) -&gt; dist.ProcessGroup:\n        os.environ[\"RANK\"] = f\"{rank}\"\n        os.environ[\"WORLD_SIZE\"] = f\"{world_size}\"\n        dist.init_process_group(rank=rank, world_size=world_size, backend=backend)\n        return dist.group.WORLD\n\n    if backend == \"nccl\":\n        device = torch.device(f\"cuda:{rank}\")\n        torch.cuda.set_device(device)\n    else:\n        device = torch.device(\"cpu\")\n    topology = Topology(world_size=world_size, compute_device=\"cuda\")\n    pg = init_distributed_single_host(rank, world_size, backend)\n    planner = EmbeddingShardingPlanner(\n        topology=topology,\n        constraints=constraints,\n    )\n    sharders = [cast(ModuleSharder[torch.nn.Module], EmbeddingBagCollectionSharder())]\n    plan: ShardingPlan = planner.collective_plan(module, sharders, pg)\n\n    sharded_model = DistributedModelParallel(\n        module,\n        env=ShardingEnv.from_process_group(pg),\n        plan=plan,\n        sharders=sharders,\n        device=device,\n    )\n    print(f\"rank:{rank},sharding plan: {plan}\")\n    return sharded_model",
            "code"
        ]
    ],
    "torch->Recommendation Systems->Exploring TorchRec sharding->DistributedModelParallel in multiprocessing->Multiprocessing Execution": [
        [
            "Now let\u2019s execute the code in multi-processes representing multiple GPU\nranks.",
            "markdown"
        ],
        [
            "import multiprocess\n\ndef spmd_sharing_simulation(\n    sharding_type: ShardingType = ShardingType.TABLE_WISE,\n    world_size = 2,\n):\n  ctx = multiprocess.get_context(\"spawn\")\n  processes = []\n  for rank in range(world_size):\n      p = ctx.Process(\n          target=single_rank_execution,\n          args=(\n              rank,\n              world_size,\n              gen_constraints(sharding_type),\n              ebc,\n              \"nccl\"\n          ),\n      )\n      p.start()\n      processes.append(p)\n\n  for p in processes:\n      p.join()\n      assert 0 == p.exitcode",
            "code"
        ]
    ],
    "torch->Recommendation Systems->Exploring TorchRec sharding->DistributedModelParallel in multiprocessing->Table Wise Sharding": [
        [
            "Now let\u2019s execute the code in two processes for 2 GPUs. We can see in\nthe plan print that how our tables are sharded across GPUs. Each node\nwill have one large table and one small which shows our planner tries\nfor load balance for the embedding tables. Table-wise is the de-factor\ngo-to sharding schemes for many small-medium size tables for load\nbalancing over the devices.",
            "markdown"
        ],
        [
            "spmd_sharing_simulation(ShardingType.TABLE_WISE)",
            "code"
        ],
        [
            "rank:1,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:0/cuda:0)])), 'large_table_1': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 64], placement=rank:0/cuda:0)])), 'small_table_1': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 64], placement=rank:1/cuda:1)]))}}\nrank:0,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:0/cuda:0)])), 'large_table_1': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 64], placement=rank:0/cuda:0)])), 'small_table_1': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 64], placement=rank:1/cuda:1)]))}}",
            "code"
        ]
    ],
    "torch->Recommendation Systems->Exploring TorchRec sharding->DistributedModelParallel in multiprocessing->Explore other sharding modes": [
        [
            "We have initially explored what table-wise sharding would look like and\nhow it balances the tables placement. Now we explore sharding modes with\nfiner focus on load balance: row-wise. Row-wise is specifically\naddressing large tables which a single device cannot hold due to the\nmemory size increase from large embedding row numbers. It can address\nthe placement of the super large tables in your models. Users can see\nthat in the shard_sizes section in the printed plan log, the tables\nare halved by row dimension to be distributed onto two GPUs.",
            "markdown"
        ],
        [
            "spmd_sharing_simulation(ShardingType.ROW_WISE)",
            "code"
        ],
        [
            "rank:1,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[2048, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[2048, 0], shard_sizes=[2048, 64], placement=rank:1/cuda:1)])), 'large_table_1': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[2048, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[2048, 0], shard_sizes=[2048, 64], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[512, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[512, 0], shard_sizes=[512, 64], placement=rank:1/cuda:1)])), 'small_table_1': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[512, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[512, 0], shard_sizes=[512, 64], placement=rank:1/cuda:1)]))}}\nrank:0,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[2048, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[2048, 0], shard_sizes=[2048, 64], placement=rank:1/cuda:1)])), 'large_table_1': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[2048, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[2048, 0], shard_sizes=[2048, 64], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[512, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[512, 0], shard_sizes=[512, 64], placement=rank:1/cuda:1)])), 'small_table_1': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[512, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[512, 0], shard_sizes=[512, 64], placement=rank:1/cuda:1)]))}}",
            "code"
        ],
        [
            "Column-wise on the other hand, address the load imbalance problems for\ntables with large embedding dimensions. We will split the table\nvertically. Users can see that in the shard_sizes section in the\nprinted plan log, the tables are halved by embedding dimension to be\ndistributed onto two GPUs.",
            "markdown"
        ],
        [
            "spmd_sharing_simulation(ShardingType.COLUMN_WISE)",
            "code"
        ],
        [
            "rank:0,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[4096, 32], placement=rank:1/cuda:1)])), 'large_table_1': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[4096, 32], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[1024, 32], placement=rank:1/cuda:1)])), 'small_table_1': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[1024, 32], placement=rank:1/cuda:1)]))}}\nrank:1,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[4096, 32], placement=rank:1/cuda:1)])), 'large_table_1': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[4096, 32], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[1024, 32], placement=rank:1/cuda:1)])), 'small_table_1': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[1024, 32], placement=rank:1/cuda:1)]))}}",
            "code"
        ],
        [
            "For table-row-wise, unfortuately we cannot simulate it due to its\nnature of operating under multi-host setup. We will present a python\n example in the future\nto train models with table-row-wise.",
            "markdown"
        ],
        [
            "With data parallel, we will repeat the tables for all devices.",
            "markdown"
        ],
        [
            "spmd_sharing_simulation(ShardingType.DATA_PARALLEL)",
            "code"
        ],
        [
            "rank:0,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'large_table_1': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'small_table_0': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'small_table_1': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None)}}\nrank:1,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'large_table_1': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'small_table_0': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'small_table_1': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None)}}",
            "code"
        ]
    ],
    "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA": [
        [
            "Multimodal AI has recently become very popular owing to its ubiquitous\nnature, from use cases like image captioning and visual search to more\nrecent applications like image generation from text. <strong>TorchMultimodal\nis a library powered by Pytorch consisting of building blocks and end to\nend examples, aiming to enable and accelerate research in\nmultimodality</strong>.",
            "markdown"
        ],
        [
            "In this tutorial, we will demonstrate how to use a <strong>pretrained SoTA\nmodel called</strong>  <strong>from\nTorchMultimodal library to finetune on a multimodal task i.e. visual\nquestion answering</strong> (VQA). The model consists of two unimodal transformer\nbased encoders for text and image and a multimodal encoder to combine\nthe two embeddings. It is pretrained using contrastive, image text matching and\ntext, image and multimodal masking losses.",
            "markdown"
        ]
    ],
    "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Installation": [
        [
            "We will use TextVQA dataset and bert tokenizer from HuggingFace for this\ntutorial. So you need to install datasets and transformers in addition to TorchMultimodal.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "When running this tutorial in Google Colab, install the required packages by\ncreating a new cell and running the following commands:",
            "markdown"
        ],
        [
            "!pip install torchmultimodal-nightly\n!pip install datasets\n!pip install transformers",
            "code"
        ]
    ],
    "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps": [
        [
            "Download the HuggingFace dataset to a directory on your computer by running the following command:",
            "markdown"
        ],
        [
            "wget http://dl.fbaipublicfiles.com/pythia/data/vocab.tar.gz\ntar xf vocab.tar.gz",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "If you are running this tutorial in Google Colab, run these commands\nin a new cell and prepend these commands with an exclamation mark (!)",
            "markdown"
        ],
        [
            "For this tutorial, we treat VQA as a classification task where\nthe inputs are images and question (text) and the output is an answer class.\nSo we need to download the vocab file with answer classes and create the answer to\nlabel mapping.",
            "markdown"
        ],
        [
            "We also load the  containing 34602 training samples\n(images,questions and answers) from HuggingFace",
            "markdown"
        ],
        [
            "We see there are 3997 answer classes including a class representing\nunknown answers.",
            "markdown"
        ],
        [
            "with open(\"data/vocabs/answers_textvqa_more_than_1.txt\") as f:\n  vocab = f.readlines()\n\nanswer_to_idx = {}\nfor idx, entry in enumerate(vocab):\n  answer_to_idx[entry.strip(\"\\n\")] = idx\nprint(len(vocab))\nprint(vocab[:5])\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"textvqa\")",
            "code"
        ],
        [
            "3997\n['&lt;unk&gt;\\n', 'nokia\\n', 'ec\\n', 'virgin\\n', '2011\\n']\n\nDownloading builder script:   0%|          | 0.00/5.02k [00:00&lt;?, ?B/s]\nDownloading builder script: 100%|##########| 5.02k/5.02k [00:00&lt;00:00, 5.47MB/s]\n\nDownloading metadata:   0%|          | 0.00/13.3k [00:00&lt;?, ?B/s]\nDownloading metadata: 100%|##########| 13.3k/13.3k [00:00&lt;00:00, 12.9MB/s]\n\nDownloading readme:   0%|          | 0.00/13.2k [00:00&lt;?, ?B/s]\nDownloading readme: 100%|##########| 13.2k/13.2k [00:00&lt;00:00, 9.70MB/s]\nNo config specified, defaulting to: textvqa/textvqa\nDownloading and preparing dataset textvqa/textvqa to /var/lib/jenkins/.cache/huggingface/datasets/textvqa/textvqa/0.5.1/9b89037cc122c3b495b155a1bce4170851829843454e88f236bb8715d977c027...\n\nDownloading data files:   0%|          | 0/5 [00:00&lt;?, ?it/s]\n\nDownloading data:   0%|          | 0.00/21.6M [00:00&lt;?, ?B/s]\n\nDownloading data:   0%|          | 104k/21.6M [00:00&lt;00:21, 1.00MB/s]\n\nDownloading data:   1%|1         | 239k/21.6M [00:00&lt;00:21, 987kB/s]\n\nDownloading data:   2%|1         | 387k/21.6M [00:00&lt;00:17, 1.18MB/s]\n\nDownloading data:   3%|2         | 617k/21.6M [00:00&lt;00:14, 1.40MB/s]\n\nDownloading data:   4%|4         | 880k/21.6M [00:00&lt;00:11, 1.79MB/s]\n\nDownloading data:   5%|5         | 1.10M/21.6M [00:00&lt;00:12, 1.69MB/s]\n\nDownloading data:   6%|6         | 1.40M/21.6M [00:00&lt;00:09, 2.08MB/s]\n\nDownloading data:   7%|7         | 1.62M/21.6M [00:00&lt;00:09, 2.10MB/s]\n\nDownloading data:   9%|9         | 1.98M/21.6M [00:01&lt;00:08, 2.29MB/s]\n\nDownloading data:  11%|#1        | 2.46M/21.6M [00:01&lt;00:06, 2.97MB/s]\n\nDownloading data:  13%|#3        | 2.83M/21.6M [00:01&lt;00:06, 2.87MB/s]\n\nDownloading data:  15%|#5        | 3.29M/21.6M [00:01&lt;00:05, 3.30MB/s]\n\nDownloading data:  18%|#8        | 3.93M/21.6M [00:01&lt;00:04, 3.78MB/s]\n\nDownloading data:  21%|##        | 4.44M/21.6M [00:01&lt;00:04, 4.11MB/s]\n\nDownloading data:  24%|##4       | 5.23M/21.6M [00:01&lt;00:03, 5.16MB/s]\n\nDownloading data:  28%|##7       | 5.97M/21.6M [00:01&lt;00:03, 5.18MB/s]\n\nDownloading data:  32%|###1      | 6.82M/21.6M [00:01&lt;00:02, 6.06MB/s]\n\nDownloading data:  37%|###6      | 7.92M/21.6M [00:02&lt;00:02, 6.67MB/s]\n\nDownloading data:  41%|####1     | 8.98M/21.6M [00:02&lt;00:01, 7.69MB/s]\n\nDownloading data:  47%|####7     | 10.2M/21.6M [00:02&lt;00:01, 9.02MB/s]\n\nDownloading data:  54%|#####3    | 11.6M/21.6M [00:02&lt;00:01, 9.19MB/s]\n\nDownloading data:  62%|######1   | 13.3M/21.6M [00:02&lt;00:00, 11.4MB/s]\n\nDownloading data:  68%|######8   | 14.8M/21.6M [00:02&lt;00:00, 12.3MB/s]\n\nDownloading data:  77%|#######7  | 16.8M/21.6M [00:02&lt;00:00, 13.1MB/s]\n\nDownloading data:  86%|########6 | 18.6M/21.6M [00:02&lt;00:00, 14.6MB/s]\n\nDownloading data:  96%|#########6| 20.8M/21.6M [00:02&lt;00:00, 16.5MB/s]\nDownloading data: 100%|##########| 21.6M/21.6M [00:03&lt;00:00, 7.20MB/s]\n\nDownloading data files:  20%|##        | 1/5 [00:03&lt;00:14,  3.60s/it]\n\nDownloading data: 0.00B [00:00, ?B/s]\nDownloading data: 3.12MB [00:00, 63.1MB/s]\n\nDownloading data files:  40%|####      | 2/5 [00:04&lt;00:05,  1.76s/it]\n\nDownloading data: 0.00B [00:00, ?B/s]\nDownloading data: 2.77MB [00:00, 69.4MB/s]\n\nDownloading data files:  60%|######    | 3/5 [00:04&lt;00:02,  1.18s/it]\n\nDownloading data:   0%|          | 0.00/7.07G [00:00&lt;?, ?B/s]\n\nDownloading data:   0%|          | 88.1k/7.07G [00:00&lt;2:13:52, 880kB/s]\n\nDownloading data:   0%|          | 461k/7.07G [00:00&lt;46:10, 2.55MB/s]\n\nDownloading data:   0%|          | 1.92M/7.07G [00:00&lt;14:36, 8.07MB/s]\n\nDownloading data:   0%|          | 6.13M/7.07G [00:00&lt;05:28, 21.5MB/s]\n\nDownloading data:   0%|          | 12.8M/7.07G [00:00&lt;03:07, 37.7MB/s]\n\nDownloading data:   0%|          | 19.3M/7.07G [00:00&lt;02:30, 46.9MB/s]\n\nDownloading data:   0%|          | 25.4M/7.07G [00:00&lt;02:16, 51.6MB/s]\n\nDownloading data:   0%|          | 31.8M/7.07G [00:00&lt;02:06, 55.6MB/s]\n\nDownloading data:   1%|          | 38.1M/7.07G [00:00&lt;02:01, 58.1MB/s]\n\nDownloading data:   1%|          | 44.8M/7.07G [00:01&lt;01:55, 60.7MB/s]\n\nDownloading data:   1%|          | 51.3M/7.07G [00:01&lt;01:53, 62.1MB/s]\n\nDownloading data:   1%|          | 57.9M/7.07G [00:01&lt;01:50, 63.3MB/s]\n\nDownloading data:   1%|          | 64.3M/7.07G [00:01&lt;01:51, 62.7MB/s]\n\nDownloading data:   1%|          | 70.6M/7.07G [00:01&lt;01:51, 62.8MB/s]\n\nDownloading data:   1%|1         | 76.9M/7.07G [00:01&lt;01:51, 62.9MB/s]\n\nDownloading data:   1%|1         | 83.2M/7.07G [00:01&lt;01:50, 63.0MB/s]\n\nDownloading data:   1%|1         | 89.6M/7.07G [00:01&lt;01:50, 63.3MB/s]\n\nDownloading data:   1%|1         | 96.1M/7.07G [00:01&lt;01:49, 63.7MB/s]\n\nDownloading data:   1%|1         | 102M/7.07G [00:01&lt;01:49, 63.6MB/s]\n\nDownloading data:   2%|1         | 109M/7.07G [00:02&lt;01:49, 63.7MB/s]\n\nDownloading data:   2%|1         | 115M/7.07G [00:02&lt;01:49, 63.5MB/s]\n\nDownloading data:   2%|1         | 122M/7.07G [00:02&lt;01:49, 63.4MB/s]\n\nDownloading data:   2%|1         | 128M/7.07G [00:02&lt;01:49, 63.7MB/s]\n\nDownloading data:   2%|1         | 134M/7.07G [00:02&lt;01:48, 63.9MB/s]\n\nDownloading data:   2%|1         | 141M/7.07G [00:02&lt;01:47, 64.5MB/s]\n\nDownloading data:   2%|2         | 148M/7.07G [00:02&lt;01:47, 64.5MB/s]\n\nDownloading data:   2%|2         | 154M/7.07G [00:02&lt;01:47, 64.6MB/s]\n\nDownloading data:   2%|2         | 161M/7.07G [00:02&lt;01:46, 64.9MB/s]\n\nDownloading data:   2%|2         | 167M/7.07G [00:02&lt;01:46, 64.7MB/s]\n\nDownloading data:   2%|2         | 174M/7.07G [00:03&lt;01:46, 64.6MB/s]\n\nDownloading data:   3%|2         | 180M/7.07G [00:03&lt;01:47, 64.2MB/s]\n\nDownloading data:   3%|2         | 186M/7.07G [00:03&lt;01:49, 63.1MB/s]\n\nDownloading data:   3%|2         | 193M/7.07G [00:03&lt;01:49, 62.9MB/s]\n\nDownloading data:   3%|2         | 199M/7.07G [00:03&lt;01:49, 62.7MB/s]\n\nDownloading data:   3%|2         | 206M/7.07G [00:03&lt;01:48, 63.5MB/s]\n\nDownloading data:   3%|2         | 212M/7.07G [00:03&lt;01:47, 63.6MB/s]\n\nDownloading data:   3%|3         | 218M/7.07G [00:03&lt;01:46, 64.2MB/s]\n\nDownloading data:   3%|3         | 225M/7.07G [00:03&lt;01:47, 63.5MB/s]\n\nDownloading data:   3%|3         | 232M/7.07G [00:03&lt;01:46, 64.4MB/s]\n\nDownloading data:   3%|3         | 238M/7.07G [00:04&lt;01:46, 64.5MB/s]\n\nDownloading data:   3%|3         | 245M/7.07G [00:04&lt;01:45, 64.7MB/s]\n\nDownloading data:   4%|3         | 251M/7.07G [00:04&lt;01:45, 64.7MB/s]\n\nDownloading data:   4%|3         | 257M/7.07G [00:04&lt;01:45, 64.6MB/s]\n\nDownloading data:   4%|3         | 264M/7.07G [00:04&lt;01:44, 64.8MB/s]\n\nDownloading data:   4%|3         | 271M/7.07G [00:04&lt;01:44, 65.2MB/s]\n\nDownloading data:   4%|3         | 277M/7.07G [00:04&lt;01:43, 65.5MB/s]\n\nDownloading data:   4%|4         | 284M/7.07G [00:04&lt;01:43, 65.8MB/s]\n\nDownloading data:   4%|4         | 290M/7.07G [00:04&lt;01:42, 65.9MB/s]\n\nDownloading data:   4%|4         | 297M/7.07G [00:04&lt;01:43, 65.3MB/s]\n\nDownloading data:   4%|4         | 304M/7.07G [00:05&lt;01:45, 64.4MB/s]\n\nDownloading data:   4%|4         | 310M/7.07G [00:05&lt;01:44, 64.5MB/s]\n\nDownloading data:   4%|4         | 317M/7.07G [00:05&lt;01:43, 65.2MB/s]\n\nDownloading data:   5%|4         | 323M/7.07G [00:05&lt;01:42, 65.6MB/s]\n\nDownloading data:   5%|4         | 330M/7.07G [00:05&lt;01:42, 65.9MB/s]\n\nDownloading data:   5%|4         | 337M/7.07G [00:05&lt;01:41, 66.1MB/s]\n\nDownloading data:   5%|4         | 343M/7.07G [00:05&lt;01:41, 66.1MB/s]\n\nDownloading data:   5%|4         | 350M/7.07G [00:05&lt;01:41, 66.2MB/s]\n\nDownloading data:   5%|5         | 357M/7.07G [00:05&lt;01:41, 66.4MB/s]\n\nDownloading data:   5%|5         | 363M/7.07G [00:05&lt;01:40, 66.6MB/s]\n\nDownloading data:   5%|5         | 370M/7.07G [00:06&lt;01:40, 66.6MB/s]\n\nDownloading data:   5%|5         | 377M/7.07G [00:06&lt;01:40, 66.7MB/s]\n\nDownloading data:   5%|5         | 383M/7.07G [00:06&lt;01:41, 65.9MB/s]\n\nDownloading data:   6%|5         | 390M/7.07G [00:06&lt;01:41, 66.0MB/s]\n\nDownloading data:   6%|5         | 397M/7.07G [00:06&lt;01:40, 66.2MB/s]\n\nDownloading data:   6%|5         | 403M/7.07G [00:06&lt;01:40, 66.4MB/s]\n\nDownloading data:   6%|5         | 410M/7.07G [00:06&lt;01:40, 66.4MB/s]\n\nDownloading data:   6%|5         | 417M/7.07G [00:06&lt;01:40, 66.2MB/s]\n\nDownloading data:   6%|5         | 423M/7.07G [00:06&lt;01:40, 66.4MB/s]\n\nDownloading data:   6%|6         | 430M/7.07G [00:06&lt;01:39, 66.6MB/s]\n\nDownloading data:   6%|6         | 437M/7.07G [00:07&lt;01:39, 66.8MB/s]\n\nDownloading data:   6%|6         | 443M/7.07G [00:07&lt;01:39, 66.6MB/s]\n\nDownloading data:   6%|6         | 450M/7.07G [00:07&lt;01:39, 66.8MB/s]\n\nDownloading data:   6%|6         | 457M/7.07G [00:07&lt;01:38, 66.9MB/s]\n\nDownloading data:   7%|6         | 464M/7.07G [00:07&lt;01:38, 67.1MB/s]\n\nDownloading data:   7%|6         | 470M/7.07G [00:07&lt;01:38, 67.2MB/s]\n\nDownloading data:   7%|6         | 477M/7.07G [00:07&lt;01:38, 67.2MB/s]\n\nDownloading data:   7%|6         | 484M/7.07G [00:07&lt;01:38, 66.9MB/s]\n\nDownloading data:   7%|6         | 491M/7.07G [00:07&lt;01:38, 66.9MB/s]\n\nDownloading data:   7%|7         | 497M/7.07G [00:07&lt;01:38, 66.8MB/s]\n\nDownloading data:   7%|7         | 504M/7.07G [00:08&lt;01:39, 66.3MB/s]\n\nDownloading data:   7%|7         | 511M/7.07G [00:08&lt;01:39, 65.7MB/s]\n\nDownloading data:   7%|7         | 517M/7.07G [00:08&lt;01:39, 65.9MB/s]\n\nDownloading data:   7%|7         | 524M/7.07G [00:08&lt;01:38, 66.3MB/s]\n\nDownloading data:   8%|7         | 531M/7.07G [00:08&lt;01:38, 66.3MB/s]\n\nDownloading data:   8%|7         | 537M/7.07G [00:08&lt;01:38, 66.7MB/s]\n\nDownloading data:   8%|7         | 544M/7.07G [00:08&lt;01:37, 66.8MB/s]\n\nDownloading data:   8%|7         | 551M/7.07G [00:08&lt;01:37, 66.8MB/s]\n\nDownloading data:   8%|7         | 557M/7.07G [00:08&lt;01:37, 66.7MB/s]\n\nDownloading data:   8%|7         | 564M/7.07G [00:08&lt;01:37, 66.8MB/s]\n\nDownloading data:   8%|8         | 571M/7.07G [00:09&lt;01:37, 66.9MB/s]\n\nDownloading data:   8%|8         | 578M/7.07G [00:09&lt;01:37, 66.9MB/s]\n\nDownloading data:   8%|8         | 584M/7.07G [00:09&lt;01:37, 66.5MB/s]\n\nDownloading data:   8%|8         | 591M/7.07G [00:09&lt;01:39, 65.2MB/s]\n\nDownloading data:   8%|8         | 597M/7.07G [00:09&lt;01:39, 65.3MB/s]\n\nDownloading data:   9%|8         | 604M/7.07G [00:09&lt;01:38, 65.7MB/s]\n\nDownloading data:   9%|8         | 611M/7.07G [00:09&lt;01:37, 66.0MB/s]\n\nDownloading data:   9%|8         | 617M/7.07G [00:09&lt;01:38, 65.6MB/s]\n\nDownloading data:   9%|8         | 624M/7.07G [00:09&lt;01:39, 64.8MB/s]\n\nDownloading data:   9%|8         | 631M/7.07G [00:09&lt;01:38, 65.5MB/s]\n\nDownloading data:   9%|9         | 637M/7.07G [00:10&lt;01:37, 65.9MB/s]\n\nDownloading data:   9%|9         | 644M/7.07G [00:10&lt;01:37, 66.0MB/s]\n\nDownloading data:   9%|9         | 651M/7.07G [00:10&lt;02:26, 43.8MB/s]\n\nDownloading data:   9%|9         | 657M/7.07G [00:10&lt;02:13, 48.1MB/s]\n\nDownloading data:   9%|9         | 663M/7.07G [00:10&lt;02:04, 51.3MB/s]\n\nDownloading data:   9%|9         | 669M/7.07G [00:10&lt;01:57, 54.5MB/s]\n\nDownloading data:  10%|9         | 676M/7.07G [00:10&lt;01:51, 57.5MB/s]\n\nDownloading data:  10%|9         | 683M/7.07G [00:10&lt;01:46, 60.1MB/s]\n\nDownloading data:  10%|9         | 689M/7.07G [00:11&lt;01:44, 60.9MB/s]\n\nDownloading data:  10%|9         | 696M/7.07G [00:11&lt;01:41, 62.5MB/s]\n\nDownloading data:  10%|9         | 702M/7.07G [00:11&lt;01:40, 63.5MB/s]\n\nDownloading data:  10%|#         | 709M/7.07G [00:11&lt;01:39, 64.0MB/s]\n\nDownloading data:  10%|#         | 715M/7.07G [00:11&lt;01:38, 64.6MB/s]\n\nDownloading data:  10%|#         | 722M/7.07G [00:11&lt;01:37, 65.4MB/s]\n\nDownloading data:  10%|#         | 729M/7.07G [00:11&lt;01:39, 63.8MB/s]\n\nDownloading data:  10%|#         | 735M/7.07G [00:11&lt;01:41, 62.7MB/s]\n\nDownloading data:  10%|#         | 741M/7.07G [00:11&lt;01:40, 62.8MB/s]\n\nDownloading data:  11%|#         | 748M/7.07G [00:11&lt;01:40, 62.8MB/s]\n\nDownloading data:  11%|#         | 754M/7.07G [00:12&lt;01:41, 62.5MB/s]\n\nDownloading data:  11%|#         | 760M/7.07G [00:12&lt;01:41, 62.1MB/s]\n\nDownloading data:  11%|#         | 766M/7.07G [00:12&lt;01:41, 62.2MB/s]\n\nDownloading data:  11%|#         | 773M/7.07G [00:12&lt;01:41, 62.3MB/s]\n\nDownloading data:  11%|#1        | 779M/7.07G [00:12&lt;01:41, 62.1MB/s]\n\nDownloading data:  11%|#1        | 785M/7.07G [00:12&lt;01:41, 61.9MB/s]\n\nDownloading data:  11%|#1        | 791M/7.07G [00:12&lt;01:40, 62.2MB/s]\n\nDownloading data:  11%|#1        | 798M/7.07G [00:12&lt;01:40, 62.3MB/s]\n\nDownloading data:  11%|#1        | 804M/7.07G [00:12&lt;01:40, 62.6MB/s]\n\nDownloading data:  11%|#1        | 810M/7.07G [00:12&lt;01:53, 55.3MB/s]\n\nDownloading data:  12%|#1        | 816M/7.07G [00:13&lt;01:53, 55.3MB/s]\n\nDownloading data:  12%|#1        | 822M/7.07G [00:13&lt;01:52, 55.4MB/s]\n\nDownloading data:  12%|#1        | 827M/7.07G [00:13&lt;01:51, 56.0MB/s]\n\nDownloading data:  12%|#1        | 833M/7.07G [00:13&lt;01:50, 56.4MB/s]\n\nDownloading data:  12%|#1        | 839M/7.07G [00:13&lt;01:49, 56.7MB/s]\n\nDownloading data:  12%|#1        | 845M/7.07G [00:13&lt;01:49, 56.8MB/s]\n\nDownloading data:  12%|#2        | 850M/7.07G [00:13&lt;01:49, 56.8MB/s]\n\nDownloading data:  12%|#2        | 856M/7.07G [00:13&lt;01:49, 56.9MB/s]\n\nDownloading data:  12%|#2        | 862M/7.07G [00:13&lt;01:47, 57.6MB/s]\n\nDownloading data:  12%|#2        | 868M/7.07G [00:14&lt;01:46, 58.5MB/s]\n\nDownloading data:  12%|#2        | 874M/7.07G [00:14&lt;01:45, 58.9MB/s]\n\nDownloading data:  12%|#2        | 880M/7.07G [00:14&lt;01:44, 59.3MB/s]\n\nDownloading data:  13%|#2        | 886M/7.07G [00:14&lt;01:44, 59.3MB/s]\n\nDownloading data:  13%|#2        | 892M/7.07G [00:14&lt;01:44, 59.1MB/s]\n\nDownloading data:  13%|#2        | 898M/7.07G [00:14&lt;01:43, 59.7MB/s]\n\nDownloading data:  13%|#2        | 904M/7.07G [00:14&lt;01:42, 60.1MB/s]\n\nDownloading data:  13%|#2        | 910M/7.07G [00:14&lt;01:42, 59.9MB/s]\n\nDownloading data:  13%|#2        | 916M/7.07G [00:14&lt;01:42, 59.9MB/s]\n\nDownloading data:  13%|#3        | 922M/7.07G [00:14&lt;01:42, 60.0MB/s]\n\nDownloading data:  13%|#3        | 928M/7.07G [00:15&lt;01:42, 59.9MB/s]\n\nDownloading data:  13%|#3        | 934M/7.07G [00:15&lt;01:42, 59.9MB/s]\n\nDownloading data:  13%|#3        | 940M/7.07G [00:15&lt;01:41, 60.5MB/s]\n\nDownloading data:  13%|#3        | 946M/7.07G [00:15&lt;01:41, 60.4MB/s]\n\nDownloading data:  13%|#3        | 952M/7.07G [00:15&lt;01:40, 60.6MB/s]\n\nDownloading data:  14%|#3        | 959M/7.07G [00:15&lt;01:40, 60.9MB/s]\n\nDownloading data:  14%|#3        | 965M/7.07G [00:15&lt;01:40, 60.6MB/s]\n\nDownloading data:  14%|#3        | 971M/7.07G [00:15&lt;01:41, 60.4MB/s]\n\nDownloading data:  14%|#3        | 977M/7.07G [00:15&lt;01:40, 60.5MB/s]\n\nDownloading data:  14%|#3        | 983M/7.07G [00:15&lt;01:39, 61.0MB/s]\n\nDownloading data:  14%|#3        | 989M/7.07G [00:16&lt;01:40, 60.6MB/s]\n\nDownloading data:  14%|#4        | 995M/7.07G [00:16&lt;01:39, 60.9MB/s]\n\nDownloading data:  14%|#4        | 1.00G/7.07G [00:16&lt;01:39, 60.8MB/s]\n\nDownloading data:  14%|#4        | 1.01G/7.07G [00:16&lt;01:40, 60.4MB/s]\n\nDownloading data:  14%|#4        | 1.01G/7.07G [00:16&lt;01:40, 60.3MB/s]\n\nDownloading data:  14%|#4        | 1.02G/7.07G [00:16&lt;01:40, 60.2MB/s]\n\nDownloading data:  15%|#4        | 1.03G/7.07G [00:16&lt;01:41, 59.5MB/s]\n\nDownloading data:  15%|#4        | 1.03G/7.07G [00:16&lt;01:41, 59.7MB/s]\n\nDownloading data:  15%|#4        | 1.04G/7.07G [00:16&lt;01:41, 59.7MB/s]\n\nDownloading data:  15%|#4        | 1.04G/7.07G [00:16&lt;01:41, 59.1MB/s]\n\nDownloading data:  15%|#4        | 1.05G/7.07G [00:17&lt;01:41, 59.3MB/s]\n\nDownloading data:  15%|#4        | 1.06G/7.07G [00:17&lt;01:42, 58.7MB/s]\n\nDownloading data:  15%|#5        | 1.06G/7.07G [00:17&lt;01:41, 59.1MB/s]\n\nDownloading data:  15%|#5        | 1.07G/7.07G [00:17&lt;01:40, 59.7MB/s]\n\nDownloading data:  15%|#5        | 1.07G/7.07G [00:17&lt;01:40, 59.5MB/s]\n\nDownloading data:  15%|#5        | 1.08G/7.07G [00:17&lt;01:40, 59.8MB/s]\n\nDownloading data:  15%|#5        | 1.09G/7.07G [00:17&lt;01:40, 59.6MB/s]\n\nDownloading data:  15%|#5        | 1.09G/7.07G [00:17&lt;01:40, 59.6MB/s]\n\nDownloading data:  16%|#5        | 1.10G/7.07G [00:17&lt;01:40, 59.5MB/s]\n\nDownloading data:  16%|#5        | 1.10G/7.07G [00:17&lt;01:40, 59.7MB/s]\n\nDownloading data:  16%|#5        | 1.11G/7.07G [00:18&lt;01:54, 52.0MB/s]\n\nDownloading data:  16%|#5        | 1.12G/7.07G [00:18&lt;01:50, 54.0MB/s]\n\nDownloading data:  16%|#5        | 1.12G/7.07G [00:18&lt;01:47, 55.6MB/s]\n\nDownloading data:  16%|#5        | 1.13G/7.07G [00:18&lt;01:44, 56.9MB/s]\n\nDownloading data:  16%|#6        | 1.13G/7.07G [00:18&lt;01:39, 59.4MB/s]\n\nDownloading data:  16%|#6        | 1.14G/7.07G [00:18&lt;01:36, 61.6MB/s]\n\nDownloading data:  16%|#6        | 1.15G/7.07G [00:18&lt;01:34, 62.7MB/s]\n\nDownloading data:  16%|#6        | 1.15G/7.07G [00:18&lt;01:33, 63.4MB/s]\n\nDownloading data:  16%|#6        | 1.16G/7.07G [00:18&lt;01:32, 63.9MB/s]\n\nDownloading data:  16%|#6        | 1.17G/7.07G [00:18&lt;01:31, 64.7MB/s]\n\nDownloading data:  17%|#6        | 1.17G/7.07G [00:19&lt;01:30, 65.0MB/s]\n\nDownloading data:  17%|#6        | 1.18G/7.07G [00:19&lt;01:29, 65.6MB/s]\n\nDownloading data:  17%|#6        | 1.19G/7.07G [00:19&lt;01:29, 66.1MB/s]\n\nDownloading data:  17%|#6        | 1.19G/7.07G [00:19&lt;01:28, 66.3MB/s]\n\nDownloading data:  17%|#6        | 1.20G/7.07G [00:19&lt;01:28, 66.3MB/s]\n\nDownloading data:  17%|#7        | 1.21G/7.07G [00:19&lt;01:28, 66.5MB/s]\n\nDownloading data:  17%|#7        | 1.21G/7.07G [00:19&lt;01:27, 66.7MB/s]\n\nDownloading data:  17%|#7        | 1.22G/7.07G [00:19&lt;01:27, 66.7MB/s]\n\nDownloading data:  17%|#7        | 1.23G/7.07G [00:19&lt;01:27, 66.6MB/s]\n\nDownloading data:  17%|#7        | 1.23G/7.07G [00:19&lt;01:27, 66.6MB/s]\n\nDownloading data:  18%|#7        | 1.24G/7.07G [00:20&lt;01:27, 66.7MB/s]\n\nDownloading data:  18%|#7        | 1.25G/7.07G [00:20&lt;01:27, 66.5MB/s]\n\nDownloading data:  18%|#7        | 1.25G/7.07G [00:20&lt;01:27, 66.6MB/s]\n\nDownloading data:  18%|#7        | 1.26G/7.07G [00:20&lt;01:27, 66.7MB/s]\n\nDownloading data:  18%|#7        | 1.27G/7.07G [00:20&lt;01:27, 66.5MB/s]\n\nDownloading data:  18%|#8        | 1.27G/7.07G [00:20&lt;01:27, 66.5MB/s]\n\nDownloading data:  18%|#8        | 1.28G/7.07G [00:20&lt;01:27, 66.3MB/s]\n\nDownloading data:  18%|#8        | 1.29G/7.07G [00:20&lt;01:27, 66.5MB/s]\n\nDownloading data:  18%|#8        | 1.29G/7.07G [00:20&lt;01:27, 66.4MB/s]\n\nDownloading data:  18%|#8        | 1.30G/7.07G [00:20&lt;01:26, 66.5MB/s]\n\nDownloading data:  18%|#8        | 1.31G/7.07G [00:21&lt;01:26, 66.5MB/s]\n\nDownloading data:  19%|#8        | 1.31G/7.07G [00:21&lt;01:26, 66.4MB/s]\n\nDownloading data:  19%|#8        | 1.32G/7.07G [00:21&lt;01:26, 66.5MB/s]\n\nDownloading data:  19%|#8        | 1.33G/7.07G [00:21&lt;01:26, 66.6MB/s]\n\nDownloading data:  19%|#8        | 1.33G/7.07G [00:21&lt;01:26, 66.5MB/s]\n\nDownloading data:  19%|#8        | 1.34G/7.07G [00:21&lt;01:26, 66.5MB/s]\n\nDownloading data:  19%|#9        | 1.35G/7.07G [00:21&lt;01:26, 66.2MB/s]\n\nDownloading data:  19%|#9        | 1.35G/7.07G [00:21&lt;01:26, 66.3MB/s]\n\nDownloading data:  19%|#9        | 1.36G/7.07G [00:21&lt;01:25, 66.5MB/s]\n\nDownloading data:  19%|#9        | 1.37G/7.07G [00:21&lt;01:25, 66.6MB/s]\n\nDownloading data:  19%|#9        | 1.37G/7.07G [00:22&lt;01:25, 66.5MB/s]\n\nDownloading data:  20%|#9        | 1.38G/7.07G [00:22&lt;01:25, 66.5MB/s]\n\nDownloading data:  20%|#9        | 1.39G/7.07G [00:22&lt;01:25, 66.6MB/s]\n\nDownloading data:  20%|#9        | 1.39G/7.07G [00:22&lt;01:26, 65.8MB/s]\n\nDownloading data:  20%|#9        | 1.40G/7.07G [00:22&lt;01:27, 65.1MB/s]\n\nDownloading data:  20%|#9        | 1.41G/7.07G [00:22&lt;01:27, 64.5MB/s]\n\nDownloading data:  20%|#9        | 1.41G/7.07G [00:22&lt;01:29, 63.1MB/s]\n\nDownloading data:  20%|##        | 1.42G/7.07G [00:22&lt;01:30, 62.4MB/s]\n\nDownloading data:  20%|##        | 1.43G/7.07G [00:22&lt;01:29, 62.8MB/s]\n\nDownloading data:  20%|##        | 1.43G/7.07G [00:23&lt;01:30, 62.3MB/s]\n\nDownloading data:  20%|##        | 1.44G/7.07G [00:23&lt;01:30, 62.1MB/s]\n\nDownloading data:  20%|##        | 1.44G/7.07G [00:23&lt;01:30, 62.0MB/s]\n\nDownloading data:  21%|##        | 1.45G/7.07G [00:23&lt;01:30, 61.9MB/s]\n\nDownloading data:  21%|##        | 1.46G/7.07G [00:23&lt;01:30, 62.0MB/s]\n\nDownloading data:  21%|##        | 1.46G/7.07G [00:23&lt;01:30, 61.9MB/s]\n\nDownloading data:  21%|##        | 1.47G/7.07G [00:23&lt;01:30, 61.9MB/s]\n\nDownloading data:  21%|##        | 1.48G/7.07G [00:23&lt;01:29, 62.2MB/s]\n\nDownloading data:  21%|##        | 1.48G/7.07G [00:23&lt;01:30, 62.1MB/s]\n\nDownloading data:  21%|##1       | 1.49G/7.07G [00:23&lt;01:29, 62.1MB/s]\n\nDownloading data:  21%|##1       | 1.49G/7.07G [00:24&lt;01:29, 62.2MB/s]\n\nDownloading data:  21%|##1       | 1.50G/7.07G [00:24&lt;01:30, 61.7MB/s]\n\nDownloading data:  21%|##1       | 1.51G/7.07G [00:24&lt;01:30, 61.4MB/s]\n\nDownloading data:  21%|##1       | 1.51G/7.07G [00:24&lt;01:30, 61.6MB/s]\n\nDownloading data:  21%|##1       | 1.52G/7.07G [00:24&lt;01:29, 62.0MB/s]\n\nDownloading data:  22%|##1       | 1.53G/7.07G [00:24&lt;01:29, 62.3MB/s]\n\nDownloading data:  22%|##1       | 1.53G/7.07G [00:24&lt;01:28, 62.4MB/s]\n\nDownloading data:  22%|##1       | 1.54G/7.07G [00:24&lt;01:28, 62.5MB/s]\n\nDownloading data:  22%|##1       | 1.54G/7.07G [00:24&lt;01:29, 61.8MB/s]\n\nDownloading data:  22%|##1       | 1.55G/7.07G [00:24&lt;01:30, 61.1MB/s]\n\nDownloading data:  22%|##2       | 1.56G/7.07G [00:25&lt;01:29, 61.5MB/s]\n\nDownloading data:  22%|##2       | 1.56G/7.07G [00:25&lt;01:30, 60.6MB/s]\n\nDownloading data:  22%|##2       | 1.57G/7.07G [00:25&lt;01:31, 60.4MB/s]\n\nDownloading data:  22%|##2       | 1.58G/7.07G [00:25&lt;01:30, 61.0MB/s]\n\nDownloading data:  22%|##2       | 1.58G/7.07G [00:25&lt;01:29, 61.1MB/s]\n\nDownloading data:  22%|##2       | 1.59G/7.07G [00:25&lt;01:29, 61.4MB/s]\n\nDownloading data:  23%|##2       | 1.59G/7.07G [00:25&lt;01:27, 62.7MB/s]\n\nDownloading data:  23%|##2       | 1.60G/7.07G [00:25&lt;01:31, 60.0MB/s]\n\nDownloading data:  23%|##2       | 1.61G/7.07G [00:25&lt;01:30, 60.6MB/s]\n\nDownloading data:  23%|##2       | 1.61G/7.07G [00:25&lt;01:29, 61.1MB/s]\n\nDownloading data:  23%|##2       | 1.62G/7.07G [00:26&lt;01:29, 61.2MB/s]\n\nDownloading data:  23%|##2       | 1.63G/7.07G [00:26&lt;01:29, 60.9MB/s]\n\nDownloading data:  23%|##3       | 1.63G/7.07G [00:26&lt;01:29, 60.7MB/s]\n\nDownloading data:  23%|##3       | 1.64G/7.07G [00:26&lt;01:28, 61.1MB/s]\n\nDownloading data:  23%|##3       | 1.64G/7.07G [00:26&lt;01:28, 61.5MB/s]\n\nDownloading data:  23%|##3       | 1.65G/7.07G [00:26&lt;01:29, 60.9MB/s]\n\nDownloading data:  23%|##3       | 1.66G/7.07G [00:26&lt;01:28, 61.1MB/s]\n\nDownloading data:  24%|##3       | 1.66G/7.07G [00:26&lt;01:29, 60.6MB/s]\n\nDownloading data:  24%|##3       | 1.67G/7.07G [00:26&lt;01:29, 60.3MB/s]\n\nDownloading data:  24%|##3       | 1.67G/7.07G [00:26&lt;01:28, 61.0MB/s]\n\nDownloading data:  24%|##3       | 1.68G/7.07G [00:27&lt;01:28, 61.2MB/s]\n\nDownloading data:  24%|##3       | 1.69G/7.07G [00:27&lt;01:27, 61.6MB/s]\n\nDownloading data:  24%|##3       | 1.69G/7.07G [00:27&lt;01:28, 61.0MB/s]\n\nDownloading data:  24%|##4       | 1.70G/7.07G [00:27&lt;01:29, 60.1MB/s]\n\nDownloading data:  24%|##4       | 1.71G/7.07G [00:27&lt;01:29, 60.2MB/s]\n\nDownloading data:  24%|##4       | 1.71G/7.07G [00:27&lt;01:28, 60.3MB/s]\n\nDownloading data:  24%|##4       | 1.72G/7.07G [00:27&lt;01:27, 61.0MB/s]\n\nDownloading data:  24%|##4       | 1.72G/7.07G [00:27&lt;01:26, 61.6MB/s]\n\nDownloading data:  24%|##4       | 1.73G/7.07G [00:27&lt;01:27, 61.3MB/s]\n\nDownloading data:  25%|##4       | 1.74G/7.07G [00:27&lt;01:26, 61.5MB/s]\n\nDownloading data:  25%|##4       | 1.74G/7.07G [00:28&lt;01:26, 61.9MB/s]\n\nDownloading data:  25%|##4       | 1.75G/7.07G [00:28&lt;01:25, 62.1MB/s]\n\nDownloading data:  25%|##4       | 1.75G/7.07G [00:28&lt;01:26, 61.5MB/s]\n\nDownloading data:  25%|##4       | 1.76G/7.07G [00:28&lt;01:28, 60.1MB/s]\n\nDownloading data:  25%|##4       | 1.77G/7.07G [00:28&lt;01:27, 60.4MB/s]\n\nDownloading data:  25%|##5       | 1.77G/7.07G [00:28&lt;01:26, 61.1MB/s]\n\nDownloading data:  25%|##5       | 1.78G/7.07G [00:28&lt;01:26, 61.3MB/s]\n\nDownloading data:  25%|##5       | 1.79G/7.07G [00:28&lt;01:27, 60.1MB/s]\n\nDownloading data:  25%|##5       | 1.79G/7.07G [00:28&lt;01:26, 60.9MB/s]\n\nDownloading data:  25%|##5       | 1.80G/7.07G [00:28&lt;01:25, 61.4MB/s]\n\nDownloading data:  26%|##5       | 1.80G/7.07G [00:29&lt;01:26, 61.2MB/s]\n\nDownloading data:  26%|##5       | 1.81G/7.07G [00:29&lt;01:26, 60.6MB/s]\n\nDownloading data:  26%|##5       | 1.82G/7.07G [00:29&lt;01:27, 60.2MB/s]\n\nDownloading data:  26%|##5       | 1.82G/7.07G [00:29&lt;01:26, 60.6MB/s]\n\nDownloading data:  26%|##5       | 1.83G/7.07G [00:29&lt;01:25, 61.0MB/s]\n\nDownloading data:  26%|##5       | 1.84G/7.07G [00:29&lt;01:25, 61.4MB/s]\n\nDownloading data:  26%|##6       | 1.84G/7.07G [00:29&lt;01:25, 61.5MB/s]\n\nDownloading data:  26%|##6       | 1.85G/7.07G [00:29&lt;01:25, 60.8MB/s]\n\nDownloading data:  26%|##6       | 1.85G/7.07G [00:29&lt;01:25, 61.0MB/s]\n\nDownloading data:  26%|##6       | 1.86G/7.07G [00:29&lt;01:25, 60.8MB/s]\n\nDownloading data:  26%|##6       | 1.87G/7.07G [00:30&lt;01:26, 60.4MB/s]\n\nDownloading data:  26%|##6       | 1.87G/7.07G [00:30&lt;01:26, 59.9MB/s]\n\nDownloading data:  27%|##6       | 1.88G/7.07G [00:30&lt;01:26, 59.7MB/s]\n\nDownloading data:  27%|##6       | 1.88G/7.07G [00:30&lt;01:25, 60.4MB/s]\n\nDownloading data:  27%|##6       | 1.89G/7.07G [00:30&lt;01:24, 61.0MB/s]\n\nDownloading data:  27%|##6       | 1.90G/7.07G [00:30&lt;01:23, 61.9MB/s]\n\nDownloading data:  27%|##6       | 1.90G/7.07G [00:30&lt;01:22, 62.7MB/s]\n\nDownloading data:  27%|##7       | 1.91G/7.07G [00:30&lt;01:21, 63.5MB/s]\n\nDownloading data:  27%|##7       | 1.92G/7.07G [00:30&lt;01:20, 64.0MB/s]\n\nDownloading data:  27%|##7       | 1.92G/7.07G [00:31&lt;01:20, 64.2MB/s]\n\nDownloading data:  27%|##7       | 1.93G/7.07G [00:31&lt;01:20, 64.1MB/s]\n\nDownloading data:  27%|##7       | 1.94G/7.07G [00:31&lt;01:20, 63.6MB/s]\n\nDownloading data:  27%|##7       | 1.94G/7.07G [00:31&lt;01:21, 62.8MB/s]\n\nDownloading data:  28%|##7       | 1.95G/7.07G [00:31&lt;01:21, 63.0MB/s]\n\nDownloading data:  28%|##7       | 1.95G/7.07G [00:31&lt;01:27, 58.8MB/s]\n\nDownloading data:  28%|##7       | 1.96G/7.07G [00:31&lt;01:31, 55.6MB/s]\n\nDownloading data:  28%|##7       | 1.97G/7.07G [00:31&lt;01:36, 53.0MB/s]\n\nDownloading data:  28%|##7       | 1.97G/7.07G [00:31&lt;01:39, 51.2MB/s]\n\nDownloading data:  28%|##7       | 1.98G/7.07G [00:31&lt;01:41, 50.0MB/s]\n\nDownloading data:  28%|##8       | 1.98G/7.07G [00:32&lt;01:41, 50.3MB/s]\n\nDownloading data:  28%|##8       | 1.99G/7.07G [00:32&lt;01:42, 49.8MB/s]\n\nDownloading data:  28%|##8       | 1.99G/7.07G [00:32&lt;01:45, 48.2MB/s]\n\nDownloading data:  28%|##8       | 2.00G/7.07G [00:32&lt;01:45, 48.2MB/s]\n\nDownloading data:  28%|##8       | 2.00G/7.07G [00:32&lt;01:45, 48.2MB/s]\n\nDownloading data:  28%|##8       | 2.01G/7.07G [00:32&lt;01:45, 48.2MB/s]\n\nDownloading data:  28%|##8       | 2.01G/7.07G [00:32&lt;01:45, 48.0MB/s]\n\nDownloading data:  29%|##8       | 2.02G/7.07G [00:32&lt;01:45, 48.0MB/s]\n\nDownloading data:  29%|##8       | 2.02G/7.07G [00:32&lt;01:44, 48.4MB/s]\n\nDownloading data:  29%|##8       | 2.03G/7.07G [00:33&lt;01:43, 48.7MB/s]\n\nDownloading data:  29%|##8       | 2.03G/7.07G [00:33&lt;01:44, 48.5MB/s]\n\nDownloading data:  29%|##8       | 2.04G/7.07G [00:33&lt;01:44, 48.4MB/s]\n\nDownloading data:  29%|##8       | 2.04G/7.07G [00:33&lt;01:41, 49.5MB/s]\n\nDownloading data:  29%|##8       | 2.05G/7.07G [00:33&lt;01:42, 49.2MB/s]\n\nDownloading data:  29%|##8       | 2.05G/7.07G [00:33&lt;01:42, 49.0MB/s]\n\nDownloading data:  29%|##9       | 2.06G/7.07G [00:33&lt;01:43, 48.5MB/s]\n\nDownloading data:  29%|##9       | 2.06G/7.07G [00:33&lt;01:44, 48.1MB/s]\n\nDownloading data:  29%|##9       | 2.07G/7.07G [00:33&lt;02:29, 33.6MB/s]\n\nDownloading data:  29%|##9       | 2.07G/7.07G [00:34&lt;02:13, 37.5MB/s]\n\nDownloading data:  29%|##9       | 2.08G/7.07G [00:34&lt;02:04, 40.3MB/s]\n\nDownloading data:  29%|##9       | 2.08G/7.07G [00:34&lt;01:57, 42.4MB/s]\n\nDownloading data:  29%|##9       | 2.09G/7.07G [00:34&lt;02:37, 31.6MB/s]\n\nDownloading data:  30%|##9       | 2.09G/7.07G [00:34&lt;02:21, 35.3MB/s]\n\nDownloading data:  30%|##9       | 2.09G/7.07G [00:34&lt;02:09, 38.6MB/s]\n\nDownloading data:  30%|##9       | 2.10G/7.07G [00:34&lt;02:45, 30.1MB/s]\n\nDownloading data:  30%|##9       | 2.10G/7.07G [00:35&lt;02:25, 34.1MB/s]\n\nDownloading data:  30%|##9       | 2.11G/7.07G [00:35&lt;02:15, 36.7MB/s]\n\nDownloading data:  30%|##9       | 2.11G/7.07G [00:35&lt;02:05, 39.6MB/s]\n\nDownloading data:  30%|##9       | 2.12G/7.07G [00:35&lt;01:57, 42.2MB/s]\n\nDownloading data:  30%|###       | 2.12G/7.07G [00:35&lt;01:52, 44.0MB/s]\n\nDownloading data:  30%|###       | 2.13G/7.07G [00:35&lt;01:51, 44.5MB/s]\n\nDownloading data:  30%|###       | 2.13G/7.07G [00:35&lt;01:45, 46.8MB/s]\n\nDownloading data:  30%|###       | 2.14G/7.07G [00:35&lt;01:44, 47.3MB/s]\n\nDownloading data:  30%|###       | 2.14G/7.07G [00:35&lt;01:42, 47.9MB/s]\n\nDownloading data:  30%|###       | 2.15G/7.07G [00:35&lt;01:41, 48.3MB/s]\n\nDownloading data:  30%|###       | 2.15G/7.07G [00:36&lt;01:41, 48.6MB/s]\n\nDownloading data:  31%|###       | 2.16G/7.07G [00:36&lt;01:40, 48.9MB/s]\n\nDownloading data:  31%|###       | 2.16G/7.07G [00:36&lt;01:40, 48.8MB/s]\n\nDownloading data:  31%|###       | 2.17G/7.07G [00:36&lt;01:40, 48.9MB/s]\n\nDownloading data:  31%|###       | 2.17G/7.07G [00:36&lt;01:39, 49.2MB/s]\n\nDownloading data:  31%|###       | 2.18G/7.07G [00:36&lt;01:42, 47.5MB/s]\n\nDownloading data:  31%|###       | 2.18G/7.07G [00:36&lt;01:42, 47.8MB/s]\n\nDownloading data:  31%|###       | 2.19G/7.07G [00:36&lt;01:41, 48.3MB/s]\n\nDownloading data:  31%|###1      | 2.19G/7.07G [00:36&lt;01:41, 48.1MB/s]\n\nDownloading data:  31%|###1      | 2.20G/7.07G [00:36&lt;01:40, 48.6MB/s]\n\nDownloading data:  31%|###1      | 2.20G/7.07G [00:37&lt;01:40, 48.7MB/s]\n\nDownloading data:  31%|###1      | 2.21G/7.07G [00:37&lt;01:39, 48.8MB/s]\n\nDownloading data:  31%|###1      | 2.21G/7.07G [00:37&lt;01:39, 48.9MB/s]\n\nDownloading data:  31%|###1      | 2.22G/7.07G [00:37&lt;01:39, 48.8MB/s]\n\nDownloading data:  31%|###1      | 2.22G/7.07G [00:37&lt;01:38, 49.1MB/s]\n\nDownloading data:  31%|###1      | 2.23G/7.07G [00:37&lt;01:39, 48.7MB/s]\n\nDownloading data:  32%|###1      | 2.23G/7.07G [00:37&lt;01:37, 49.6MB/s]\n\nDownloading data:  32%|###1      | 2.24G/7.07G [00:37&lt;01:37, 49.5MB/s]\n\nDownloading data:  32%|###1      | 2.24G/7.07G [00:37&lt;01:37, 49.3MB/s]\n\nDownloading data:  32%|###1      | 2.25G/7.07G [00:38&lt;01:41, 47.6MB/s]\n\nDownloading data:  32%|###1      | 2.25G/7.07G [00:38&lt;01:40, 48.0MB/s]\n\nDownloading data:  32%|###1      | 2.26G/7.07G [00:38&lt;01:39, 48.4MB/s]\n\nDownloading data:  32%|###1      | 2.26G/7.07G [00:38&lt;01:39, 48.4MB/s]\n\nDownloading data:  32%|###2      | 2.27G/7.07G [00:38&lt;01:38, 48.6MB/s]\n\nDownloading data:  32%|###2      | 2.27G/7.07G [00:38&lt;01:38, 48.7MB/s]\n\nDownloading data:  32%|###2      | 2.28G/7.07G [00:38&lt;01:38, 48.8MB/s]\n\nDownloading data:  32%|###2      | 2.28G/7.07G [00:38&lt;01:38, 48.7MB/s]\n\nDownloading data:  32%|###2      | 2.29G/7.07G [00:38&lt;01:39, 48.3MB/s]\n\nDownloading data:  32%|###2      | 2.29G/7.07G [00:38&lt;01:36, 49.4MB/s]\n\nDownloading data:  32%|###2      | 2.30G/7.07G [00:39&lt;01:36, 49.5MB/s]\n\nDownloading data:  33%|###2      | 2.30G/7.07G [00:39&lt;01:36, 49.5MB/s]\n\nDownloading data:  33%|###2      | 2.31G/7.07G [00:39&lt;01:36, 49.5MB/s]\n\nDownloading data:  33%|###2      | 2.31G/7.07G [00:39&lt;01:36, 49.4MB/s]\n\nDownloading data:  33%|###2      | 2.32G/7.07G [00:39&lt;01:37, 48.8MB/s]\n\nDownloading data:  33%|###2      | 2.32G/7.07G [00:39&lt;01:39, 47.8MB/s]\n\nDownloading data:  33%|###2      | 2.33G/7.07G [00:39&lt;01:40, 47.3MB/s]\n\nDownloading data:  33%|###2      | 2.33G/7.07G [00:39&lt;01:37, 48.7MB/s]\n\nDownloading data:  33%|###3      | 2.34G/7.07G [00:39&lt;01:37, 48.8MB/s]\n\nDownloading data:  33%|###3      | 2.34G/7.07G [00:39&lt;01:36, 49.0MB/s]\n\nDownloading data:  33%|###3      | 2.35G/7.07G [00:40&lt;01:36, 49.2MB/s]\n\nDownloading data:  33%|###3      | 2.35G/7.07G [00:40&lt;01:35, 49.5MB/s]\n\nDownloading data:  33%|###3      | 2.36G/7.07G [00:40&lt;01:35, 49.4MB/s]\n\nDownloading data:  33%|###3      | 2.36G/7.07G [00:40&lt;01:35, 49.3MB/s]\n\nDownloading data:  33%|###3      | 2.37G/7.07G [00:40&lt;01:35, 49.5MB/s]\n\nDownloading data:  34%|###3      | 2.37G/7.07G [00:40&lt;01:34, 49.8MB/s]\n\nDownloading data:  34%|###3      | 2.38G/7.07G [00:40&lt;01:34, 49.9MB/s]\n\nDownloading data:  34%|###3      | 2.38G/7.07G [00:40&lt;01:34, 49.5MB/s]\n\nDownloading data:  34%|###3      | 2.39G/7.07G [00:40&lt;01:34, 49.7MB/s]\n\nDownloading data:  34%|###3      | 2.39G/7.07G [00:40&lt;01:34, 49.5MB/s]\n\nDownloading data:  34%|###3      | 2.40G/7.07G [00:41&lt;01:34, 49.7MB/s]\n\nDownloading data:  34%|###3      | 2.40G/7.07G [00:41&lt;01:33, 49.8MB/s]\n\nDownloading data:  34%|###4      | 2.41G/7.07G [00:41&lt;01:34, 49.5MB/s]\n\nDownloading data:  34%|###4      | 2.41G/7.07G [00:41&lt;01:33, 50.0MB/s]\n\nDownloading data:  34%|###4      | 2.42G/7.07G [00:41&lt;01:33, 49.9MB/s]\n\nDownloading data:  34%|###4      | 2.42G/7.07G [00:41&lt;01:33, 49.8MB/s]\n\nDownloading data:  34%|###4      | 2.43G/7.07G [00:41&lt;01:33, 49.8MB/s]\n\nDownloading data:  34%|###4      | 2.43G/7.07G [00:41&lt;01:37, 47.7MB/s]\n\nDownloading data:  34%|###4      | 2.44G/7.07G [00:41&lt;01:29, 51.6MB/s]\n\nDownloading data:  35%|###4      | 2.44G/7.07G [00:41&lt;01:29, 51.6MB/s]\n\nDownloading data:  35%|###4      | 2.45G/7.07G [00:42&lt;01:29, 51.6MB/s]\n\nDownloading data:  35%|###4      | 2.45G/7.07G [00:42&lt;01:29, 51.6MB/s]\n\nDownloading data:  35%|###4      | 2.46G/7.07G [00:42&lt;01:30, 51.2MB/s]\n\nDownloading data:  35%|###4      | 2.46G/7.07G [00:42&lt;01:31, 50.5MB/s]\n\nDownloading data:  35%|###4      | 2.47G/7.07G [00:42&lt;01:29, 51.2MB/s]\n\nDownloading data:  35%|###4      | 2.47G/7.07G [00:42&lt;01:30, 51.0MB/s]\n\nDownloading data:  35%|###5      | 2.48G/7.07G [00:42&lt;01:30, 50.9MB/s]\n\nDownloading data:  35%|###5      | 2.48G/7.07G [00:42&lt;01:29, 51.5MB/s]\n\nDownloading data:  35%|###5      | 2.49G/7.07G [00:42&lt;01:32, 49.7MB/s]\n\nDownloading data:  35%|###5      | 2.50G/7.07G [00:42&lt;01:26, 53.0MB/s]\n\nDownloading data:  35%|###5      | 2.50G/7.07G [00:43&lt;01:26, 53.2MB/s]\n\nDownloading data:  35%|###5      | 2.51G/7.07G [00:43&lt;01:25, 53.3MB/s]\n\nDownloading data:  36%|###5      | 2.51G/7.07G [00:43&lt;01:26, 53.0MB/s]\n\nDownloading data:  36%|###5      | 2.52G/7.07G [00:43&lt;01:25, 53.0MB/s]\n\nDownloading data:  36%|###5      | 2.52G/7.07G [00:43&lt;01:25, 52.9MB/s]\n\nDownloading data:  36%|###5      | 2.53G/7.07G [00:43&lt;01:25, 53.0MB/s]\n\nDownloading data:  36%|###5      | 2.53G/7.07G [00:43&lt;01:25, 53.0MB/s]\n\nDownloading data:  36%|###5      | 2.54G/7.07G [00:43&lt;01:25, 52.8MB/s]\n\nDownloading data:  36%|###5      | 2.54G/7.07G [00:43&lt;01:24, 53.5MB/s]\n\nDownloading data:  36%|###6      | 2.55G/7.07G [00:43&lt;01:24, 53.6MB/s]\n\nDownloading data:  36%|###6      | 2.55G/7.07G [00:44&lt;01:24, 53.6MB/s]\n\nDownloading data:  36%|###6      | 2.56G/7.07G [00:44&lt;01:23, 53.8MB/s]\n\nDownloading data:  36%|###6      | 2.57G/7.07G [00:44&lt;01:22, 54.7MB/s]\n\nDownloading data:  36%|###6      | 2.57G/7.07G [00:44&lt;01:21, 54.9MB/s]\n\nDownloading data:  36%|###6      | 2.58G/7.07G [00:44&lt;01:21, 55.2MB/s]\n\nDownloading data:  37%|###6      | 2.58G/7.07G [00:44&lt;01:22, 54.5MB/s]\n\nDownloading data:  37%|###6      | 2.59G/7.07G [00:44&lt;01:21, 54.9MB/s]\n\nDownloading data:  37%|###6      | 2.59G/7.07G [00:44&lt;01:20, 55.4MB/s]\n\nDownloading data:  37%|###6      | 2.60G/7.07G [00:44&lt;01:19, 56.0MB/s]\n\nDownloading data:  37%|###6      | 2.60G/7.07G [00:44&lt;01:18, 56.8MB/s]\n\nDownloading data:  37%|###6      | 2.61G/7.07G [00:45&lt;01:18, 56.8MB/s]\n\nDownloading data:  37%|###6      | 2.62G/7.07G [00:45&lt;01:20, 55.2MB/s]\n\nDownloading data:  37%|###7      | 2.62G/7.07G [00:45&lt;01:19, 55.9MB/s]\n\nDownloading data:  37%|###7      | 2.63G/7.07G [00:45&lt;01:19, 56.3MB/s]\n\nDownloading data:  37%|###7      | 2.63G/7.07G [00:45&lt;01:17, 57.0MB/s]\n\nDownloading data:  37%|###7      | 2.64G/7.07G [00:45&lt;01:16, 57.7MB/s]\n\nDownloading data:  37%|###7      | 2.65G/7.07G [00:45&lt;01:17, 56.9MB/s]\n\nDownloading data:  37%|###7      | 2.65G/7.07G [00:45&lt;01:17, 56.8MB/s]\n\nDownloading data:  38%|###7      | 2.66G/7.07G [00:45&lt;01:17, 57.3MB/s]\n\nDownloading data:  38%|###7      | 2.66G/7.07G [00:45&lt;01:16, 57.6MB/s]\n\nDownloading data:  38%|###7      | 2.67G/7.07G [00:46&lt;01:16, 57.5MB/s]\n\nDownloading data:  38%|###7      | 2.67G/7.07G [00:46&lt;01:16, 57.2MB/s]\n\nDownloading data:  38%|###7      | 2.68G/7.07G [00:46&lt;01:17, 56.9MB/s]\n\nDownloading data:  38%|###7      | 2.69G/7.07G [00:46&lt;01:15, 58.0MB/s]\n\nDownloading data:  38%|###8      | 2.69G/7.07G [00:46&lt;01:14, 58.5MB/s]\n\nDownloading data:  38%|###8      | 2.70G/7.07G [00:46&lt;01:14, 58.7MB/s]\n\nDownloading data:  38%|###8      | 2.70G/7.07G [00:46&lt;01:14, 58.7MB/s]\n\nDownloading data:  38%|###8      | 2.71G/7.07G [00:46&lt;01:14, 58.4MB/s]\n\nDownloading data:  38%|###8      | 2.72G/7.07G [00:46&lt;01:14, 58.2MB/s]\n\nDownloading data:  38%|###8      | 2.72G/7.07G [00:47&lt;01:14, 58.4MB/s]\n\nDownloading data:  39%|###8      | 2.73G/7.07G [00:47&lt;01:14, 58.4MB/s]\n\nDownloading data:  39%|###8      | 2.73G/7.07G [00:47&lt;01:14, 58.0MB/s]\n\nDownloading data:  39%|###8      | 2.74G/7.07G [00:47&lt;01:14, 58.3MB/s]\n\nDownloading data:  39%|###8      | 2.74G/7.07G [00:47&lt;01:12, 59.4MB/s]\n\nDownloading data:  39%|###8      | 2.75G/7.07G [00:47&lt;01:12, 59.9MB/s]\n\nDownloading data:  39%|###8      | 2.76G/7.07G [00:47&lt;01:11, 60.4MB/s]\n\nDownloading data:  39%|###9      | 2.76G/7.07G [00:47&lt;01:10, 61.1MB/s]\n\nDownloading data:  39%|###9      | 2.77G/7.07G [00:47&lt;01:09, 61.5MB/s]\n\nDownloading data:  39%|###9      | 2.78G/7.07G [00:47&lt;01:09, 61.9MB/s]\n\nDownloading data:  39%|###9      | 2.78G/7.07G [00:48&lt;01:22, 51.7MB/s]\n\nDownloading data:  39%|###9      | 2.79G/7.07G [00:48&lt;01:16, 55.9MB/s]\n\nDownloading data:  40%|###9      | 2.80G/7.07G [00:48&lt;01:13, 58.3MB/s]\n\nDownloading data:  40%|###9      | 2.80G/7.07G [00:48&lt;01:11, 59.7MB/s]\n\nDownloading data:  40%|###9      | 2.81G/7.07G [00:48&lt;01:10, 60.6MB/s]\n\nDownloading data:  40%|###9      | 2.81G/7.07G [00:48&lt;01:08, 62.3MB/s]\n\nDownloading data:  40%|###9      | 2.82G/7.07G [00:48&lt;01:07, 63.1MB/s]\n\nDownloading data:  40%|###9      | 2.83G/7.07G [00:48&lt;01:06, 63.5MB/s]\n\nDownloading data:  40%|####      | 2.83G/7.07G [00:48&lt;01:05, 64.5MB/s]\n\nDownloading data:  40%|####      | 2.84G/7.07G [00:48&lt;01:05, 64.8MB/s]\n\nDownloading data:  40%|####      | 2.85G/7.07G [00:49&lt;01:03, 66.3MB/s]\n\nDownloading data:  40%|####      | 2.85G/7.07G [00:49&lt;01:03, 66.5MB/s]\n\nDownloading data:  40%|####      | 2.86G/7.07G [00:49&lt;01:03, 66.8MB/s]\n\nDownloading data:  41%|####      | 2.87G/7.07G [00:49&lt;01:03, 66.0MB/s]\n\nDownloading data:  41%|####      | 2.87G/7.07G [00:49&lt;01:03, 66.6MB/s]\n\nDownloading data:  41%|####      | 2.88G/7.07G [00:49&lt;01:02, 66.9MB/s]\n\nDownloading data:  41%|####      | 2.89G/7.07G [00:49&lt;01:02, 67.1MB/s]\n\nDownloading data:  41%|####      | 2.90G/7.07G [00:49&lt;01:02, 66.9MB/s]\n\nDownloading data:  41%|####1     | 2.90G/7.07G [00:49&lt;01:04, 64.3MB/s]\n\nDownloading data:  41%|####1     | 2.91G/7.07G [00:50&lt;01:08, 60.8MB/s]\n\nDownloading data:  41%|####1     | 2.91G/7.07G [00:50&lt;01:11, 58.2MB/s]\n\nDownloading data:  41%|####1     | 2.92G/7.07G [00:50&lt;01:11, 58.1MB/s]\n\nDownloading data:  41%|####1     | 2.93G/7.07G [00:50&lt;01:11, 58.1MB/s]\n\nDownloading data:  41%|####1     | 2.93G/7.07G [00:50&lt;01:11, 58.0MB/s]\n\nDownloading data:  42%|####1     | 2.94G/7.07G [00:50&lt;01:11, 57.6MB/s]\n\nDownloading data:  42%|####1     | 2.94G/7.07G [00:50&lt;01:11, 58.0MB/s]\n\nDownloading data:  42%|####1     | 2.95G/7.07G [00:50&lt;01:13, 56.1MB/s]\n\nDownloading data:  42%|####1     | 2.96G/7.07G [00:50&lt;01:09, 59.1MB/s]\n\nDownloading data:  42%|####1     | 2.96G/7.07G [00:50&lt;01:09, 59.5MB/s]\n\nDownloading data:  42%|####1     | 2.97G/7.07G [00:51&lt;01:08, 59.7MB/s]\n\nDownloading data:  42%|####2     | 2.97G/7.07G [00:51&lt;01:08, 60.2MB/s]\n\nDownloading data:  42%|####2     | 2.98G/7.07G [00:51&lt;01:08, 60.0MB/s]\n\nDownloading data:  42%|####2     | 2.99G/7.07G [00:51&lt;01:08, 59.9MB/s]\n\nDownloading data:  42%|####2     | 2.99G/7.07G [00:51&lt;01:07, 60.4MB/s]\n\nDownloading data:  42%|####2     | 3.00G/7.07G [00:51&lt;01:07, 60.0MB/s]\n\nDownloading data:  42%|####2     | 3.00G/7.07G [00:51&lt;01:08, 59.7MB/s]\n\nDownloading data:  43%|####2     | 3.01G/7.07G [00:51&lt;01:07, 60.5MB/s]\n\nDownloading data:  43%|####2     | 3.02G/7.07G [00:51&lt;01:06, 61.1MB/s]\n\nDownloading data:  43%|####2     | 3.02G/7.07G [00:51&lt;01:06, 60.9MB/s]\n\nDownloading data:  43%|####2     | 3.03G/7.07G [00:52&lt;01:06, 60.6MB/s]\n\nDownloading data:  43%|####2     | 3.04G/7.07G [00:52&lt;01:05, 62.1MB/s]\n\nDownloading data:  43%|####3     | 3.04G/7.07G [00:52&lt;01:04, 62.6MB/s]\n\nDownloading data:  43%|####3     | 3.05G/7.07G [00:52&lt;01:04, 62.8MB/s]\n\nDownloading data:  43%|####3     | 3.05G/7.07G [00:52&lt;01:03, 63.0MB/s]\n\nDownloading data:  43%|####3     | 3.06G/7.07G [00:52&lt;01:04, 62.1MB/s]\n\nDownloading data:  43%|####3     | 3.07G/7.07G [00:52&lt;01:04, 62.4MB/s]\n\nDownloading data:  43%|####3     | 3.07G/7.07G [00:52&lt;01:02, 63.7MB/s]\n\nDownloading data:  44%|####3     | 3.08G/7.07G [00:52&lt;01:02, 64.2MB/s]\n\nDownloading data:  44%|####3     | 3.09G/7.07G [00:52&lt;01:01, 64.9MB/s]\n\nDownloading data:  44%|####3     | 3.09G/7.07G [00:53&lt;01:01, 64.5MB/s]\n\nDownloading data:  44%|####3     | 3.10G/7.07G [00:53&lt;01:01, 64.9MB/s]\n\nDownloading data:  44%|####3     | 3.11G/7.07G [00:53&lt;01:00, 65.5MB/s]\n\nDownloading data:  44%|####4     | 3.11G/7.07G [00:53&lt;01:00, 65.9MB/s]\n\nDownloading data:  44%|####4     | 3.12G/7.07G [00:53&lt;00:59, 65.9MB/s]\n\nDownloading data:  44%|####4     | 3.13G/7.07G [00:53&lt;00:59, 66.5MB/s]\n\nDownloading data:  44%|####4     | 3.13G/7.07G [00:53&lt;00:59, 66.3MB/s]\n\nDownloading data:  44%|####4     | 3.14G/7.07G [00:53&lt;00:59, 65.9MB/s]\n\nDownloading data:  45%|####4     | 3.15G/7.07G [00:53&lt;00:58, 66.9MB/s]\n\nDownloading data:  45%|####4     | 3.15G/7.07G [00:53&lt;00:59, 66.1MB/s]\n\nDownloading data:  45%|####4     | 3.16G/7.07G [00:54&lt;00:58, 67.3MB/s]\n\nDownloading data:  45%|####4     | 3.17G/7.07G [00:54&lt;00:57, 67.4MB/s]\n\nDownloading data:  45%|####4     | 3.18G/7.07G [00:54&lt;00:56, 68.6MB/s]\n\nDownloading data:  45%|####4     | 3.18G/7.07G [00:54&lt;00:57, 67.7MB/s]\n\nDownloading data:  45%|####5     | 3.19G/7.07G [00:54&lt;00:56, 68.2MB/s]\n\nDownloading data:  45%|####5     | 3.20G/7.07G [00:54&lt;00:56, 68.1MB/s]\n\nDownloading data:  45%|####5     | 3.20G/7.07G [00:54&lt;00:56, 68.5MB/s]\n\nDownloading data:  45%|####5     | 3.21G/7.07G [00:54&lt;00:56, 68.0MB/s]\n\nDownloading data:  45%|####5     | 3.22G/7.07G [00:54&lt;00:57, 67.0MB/s]\n\nDownloading data:  46%|####5     | 3.22G/7.07G [00:54&lt;00:56, 67.6MB/s]\n\nDownloading data:  46%|####5     | 3.23G/7.07G [00:55&lt;00:56, 67.5MB/s]\n\nDownloading data:  46%|####5     | 3.24G/7.07G [00:55&lt;00:56, 67.9MB/s]\n\nDownloading data:  46%|####5     | 3.24G/7.07G [00:55&lt;00:56, 68.2MB/s]\n\nDownloading data:  46%|####5     | 3.25G/7.07G [00:55&lt;00:55, 69.2MB/s]\n\nDownloading data:  46%|####6     | 3.26G/7.07G [00:55&lt;00:54, 69.8MB/s]\n\nDownloading data:  46%|####6     | 3.27G/7.07G [00:55&lt;00:54, 70.4MB/s]\n\nDownloading data:  46%|####6     | 3.27G/7.07G [00:55&lt;00:56, 67.4MB/s]\n\nDownloading data:  46%|####6     | 3.28G/7.07G [00:55&lt;00:56, 67.7MB/s]\n\nDownloading data:  46%|####6     | 3.29G/7.07G [00:55&lt;00:55, 67.6MB/s]\n\nDownloading data:  47%|####6     | 3.29G/7.07G [00:56&lt;00:55, 68.3MB/s]\n\nDownloading data:  47%|####6     | 3.30G/7.07G [00:56&lt;00:54, 69.3MB/s]\n\nDownloading data:  47%|####6     | 3.31G/7.07G [00:56&lt;00:54, 69.0MB/s]\n\nDownloading data:  47%|####6     | 3.31G/7.07G [00:56&lt;00:55, 67.6MB/s]\n\nDownloading data:  47%|####6     | 3.32G/7.07G [00:56&lt;00:55, 67.2MB/s]\n\nDownloading data:  47%|####7     | 3.33G/7.07G [00:56&lt;00:56, 66.8MB/s]\n\nDownloading data:  47%|####7     | 3.33G/7.07G [00:56&lt;00:55, 67.5MB/s]\n\nDownloading data:  47%|####7     | 3.34G/7.07G [00:56&lt;00:54, 69.0MB/s]\n\nDownloading data:  47%|####7     | 3.35G/7.07G [00:56&lt;00:53, 69.5MB/s]\n\nDownloading data:  47%|####7     | 3.36G/7.07G [00:56&lt;00:52, 70.2MB/s]\n\nDownloading data:  48%|####7     | 3.36G/7.07G [00:57&lt;00:52, 70.0MB/s]\n\nDownloading data:  48%|####7     | 3.37G/7.07G [00:57&lt;00:52, 70.5MB/s]\n\nDownloading data:  48%|####7     | 3.38G/7.07G [00:57&lt;00:59, 61.9MB/s]\n\nDownloading data:  48%|####7     | 3.38G/7.07G [00:57&lt;01:03, 58.2MB/s]\n\nDownloading data:  48%|####7     | 3.39G/7.07G [00:57&lt;01:04, 57.4MB/s]\n\nDownloading data:  48%|####8     | 3.40G/7.07G [00:57&lt;01:04, 56.9MB/s]\n\nDownloading data:  48%|####8     | 3.40G/7.07G [00:57&lt;01:07, 54.6MB/s]\n\nDownloading data:  48%|####8     | 3.41G/7.07G [00:57&lt;01:07, 53.9MB/s]\n\nDownloading data:  48%|####8     | 3.41G/7.07G [00:57&lt;01:06, 55.1MB/s]\n\nDownloading data:  48%|####8     | 3.42G/7.07G [00:58&lt;01:05, 55.6MB/s]\n\nDownloading data:  48%|####8     | 3.42G/7.07G [00:58&lt;01:05, 55.8MB/s]\n\nDownloading data:  48%|####8     | 3.43G/7.07G [00:58&lt;01:06, 54.8MB/s]\n\nDownloading data:  49%|####8     | 3.44G/7.07G [00:58&lt;01:04, 56.2MB/s]\n\nDownloading data:  49%|####8     | 3.44G/7.07G [00:58&lt;01:04, 56.7MB/s]\n\nDownloading data:  49%|####8     | 3.45G/7.07G [00:58&lt;01:03, 57.3MB/s]\n\nDownloading data:  49%|####8     | 3.45G/7.07G [00:58&lt;01:03, 56.8MB/s]\n\nDownloading data:  49%|####8     | 3.46G/7.07G [00:58&lt;01:03, 57.4MB/s]\n\nDownloading data:  49%|####8     | 3.46G/7.07G [00:58&lt;01:02, 57.4MB/s]\n\nDownloading data:  49%|####9     | 3.47G/7.07G [00:58&lt;01:02, 57.3MB/s]\n\nDownloading data:  49%|####9     | 3.48G/7.07G [00:59&lt;01:03, 57.1MB/s]\n\nDownloading data:  49%|####9     | 3.48G/7.07G [00:59&lt;01:02, 57.3MB/s]\n\nDownloading data:  49%|####9     | 3.49G/7.07G [00:59&lt;01:02, 57.4MB/s]\n\nDownloading data:  49%|####9     | 3.49G/7.07G [00:59&lt;01:02, 57.5MB/s]\n\nDownloading data:  49%|####9     | 3.50G/7.07G [00:59&lt;01:01, 57.9MB/s]\n\nDownloading data:  50%|####9     | 3.51G/7.07G [00:59&lt;01:01, 58.1MB/s]\n\nDownloading data:  50%|####9     | 3.51G/7.07G [00:59&lt;01:00, 58.5MB/s]\n\nDownloading data:  50%|####9     | 3.52G/7.07G [00:59&lt;01:00, 58.6MB/s]\n\nDownloading data:  50%|####9     | 3.52G/7.07G [00:59&lt;01:00, 58.8MB/s]\n\nDownloading data:  50%|####9     | 3.53G/7.07G [00:59&lt;01:00, 58.9MB/s]\n\nDownloading data:  50%|####9     | 3.53G/7.07G [01:00&lt;01:00, 58.9MB/s]\n\nDownloading data:  50%|#####     | 3.54G/7.07G [01:00&lt;01:00, 58.8MB/s]\n\nDownloading data:  50%|#####     | 3.55G/7.07G [01:00&lt;00:59, 59.5MB/s]\n\nDownloading data:  50%|#####     | 3.55G/7.07G [01:00&lt;00:58, 60.1MB/s]\n\nDownloading data:  50%|#####     | 3.56G/7.07G [01:00&lt;00:58, 60.0MB/s]\n\nDownloading data:  50%|#####     | 3.56G/7.07G [01:00&lt;00:58, 59.6MB/s]\n\nDownloading data:  50%|#####     | 3.57G/7.07G [01:00&lt;00:58, 60.0MB/s]\n\nDownloading data:  51%|#####     | 3.58G/7.07G [01:00&lt;00:58, 59.8MB/s]\n\nDownloading data:  51%|#####     | 3.58G/7.07G [01:00&lt;00:57, 60.4MB/s]\n\nDownloading data:  51%|#####     | 3.59G/7.07G [01:00&lt;00:58, 59.1MB/s]\n\nDownloading data:  51%|#####     | 3.60G/7.07G [01:01&lt;00:57, 60.9MB/s]\n\nDownloading data:  51%|#####     | 3.60G/7.07G [01:01&lt;00:57, 60.5MB/s]\n\nDownloading data:  51%|#####1    | 3.61G/7.07G [01:01&lt;00:56, 61.0MB/s]\n\nDownloading data:  51%|#####1    | 3.61G/7.07G [01:01&lt;01:00, 57.4MB/s]\n\nDownloading data:  51%|#####1    | 3.62G/7.07G [01:01&lt;00:57, 60.2MB/s]\n\nDownloading data:  51%|#####1    | 3.63G/7.07G [01:01&lt;00:56, 61.3MB/s]\n\nDownloading data:  51%|#####1    | 3.63G/7.07G [01:01&lt;00:55, 62.1MB/s]\n\nDownloading data:  51%|#####1    | 3.64G/7.07G [01:01&lt;00:55, 62.0MB/s]\n\nDownloading data:  52%|#####1    | 3.65G/7.07G [01:01&lt;00:55, 62.1MB/s]\n\nDownloading data:  52%|#####1    | 3.65G/7.07G [01:01&lt;00:54, 62.3MB/s]\n\nDownloading data:  52%|#####1    | 3.66G/7.07G [01:02&lt;00:54, 62.3MB/s]\n\nDownloading data:  52%|#####1    | 3.66G/7.07G [01:02&lt;00:54, 62.3MB/s]\n\nDownloading data:  52%|#####1    | 3.67G/7.07G [01:02&lt;00:54, 62.4MB/s]\n\nDownloading data:  52%|#####1    | 3.68G/7.07G [01:02&lt;00:54, 62.3MB/s]\n\nDownloading data:  52%|#####2    | 3.68G/7.07G [01:02&lt;00:54, 61.9MB/s]\n\nDownloading data:  52%|#####2    | 3.69G/7.07G [01:02&lt;00:54, 62.5MB/s]\n\nDownloading data:  52%|#####2    | 3.70G/7.07G [01:02&lt;00:53, 63.0MB/s]\n\nDownloading data:  52%|#####2    | 3.70G/7.07G [01:02&lt;00:53, 63.0MB/s]\n\nDownloading data:  52%|#####2    | 3.71G/7.07G [01:02&lt;00:53, 62.7MB/s]\n\nDownloading data:  53%|#####2    | 3.72G/7.07G [01:02&lt;00:54, 61.9MB/s]\n\nDownloading data:  53%|#####2    | 3.72G/7.07G [01:03&lt;00:53, 62.5MB/s]\n\nDownloading data:  53%|#####2    | 3.73G/7.07G [01:03&lt;00:53, 62.5MB/s]\n\nDownloading data:  53%|#####2    | 3.73G/7.07G [01:03&lt;00:53, 62.8MB/s]\n\nDownloading data:  53%|#####2    | 3.74G/7.07G [01:03&lt;00:53, 62.8MB/s]\n\nDownloading data:  53%|#####2    | 3.75G/7.07G [01:03&lt;00:53, 62.6MB/s]\n\nDownloading data:  53%|#####3    | 3.75G/7.07G [01:03&lt;00:53, 61.5MB/s]\n\nDownloading data:  53%|#####3    | 3.76G/7.07G [01:03&lt;00:52, 63.1MB/s]\n\nDownloading data:  53%|#####3    | 3.77G/7.07G [01:03&lt;00:52, 63.2MB/s]\n\nDownloading data:  53%|#####3    | 3.77G/7.07G [01:03&lt;00:52, 62.8MB/s]\n\nDownloading data:  53%|#####3    | 3.78G/7.07G [01:03&lt;00:52, 62.8MB/s]\n\nDownloading data:  54%|#####3    | 3.78G/7.07G [01:04&lt;00:52, 63.1MB/s]\n\nDownloading data:  54%|#####3    | 3.79G/7.07G [01:04&lt;00:52, 63.1MB/s]\n\nDownloading data:  54%|#####3    | 3.80G/7.07G [01:04&lt;00:51, 63.2MB/s]\n\nDownloading data:  54%|#####3    | 3.80G/7.07G [01:04&lt;00:52, 62.7MB/s]\n\nDownloading data:  54%|#####3    | 3.81G/7.07G [01:04&lt;00:51, 63.1MB/s]\n\nDownloading data:  54%|#####3    | 3.82G/7.07G [01:04&lt;00:51, 63.0MB/s]\n\nDownloading data:  54%|#####4    | 3.82G/7.07G [01:04&lt;00:50, 63.8MB/s]\n\nDownloading data:  54%|#####4    | 3.83G/7.07G [01:04&lt;00:52, 62.2MB/s]\n\nDownloading data:  54%|#####4    | 3.84G/7.07G [01:04&lt;00:51, 63.2MB/s]\n\nDownloading data:  54%|#####4    | 3.84G/7.07G [01:05&lt;00:51, 63.3MB/s]\n\nDownloading data:  54%|#####4    | 3.85G/7.07G [01:05&lt;00:50, 63.4MB/s]\n\nDownloading data:  55%|#####4    | 3.86G/7.07G [01:05&lt;00:50, 63.2MB/s]\n\nDownloading data:  55%|#####4    | 3.86G/7.07G [01:05&lt;00:51, 62.8MB/s]\n\nDownloading data:  55%|#####4    | 3.87G/7.07G [01:05&lt;00:51, 62.2MB/s]\n\nDownloading data:  55%|#####4    | 3.87G/7.07G [01:05&lt;00:50, 63.0MB/s]\n\nDownloading data:  55%|#####4    | 3.88G/7.07G [01:05&lt;00:51, 62.1MB/s]\n\nDownloading data:  55%|#####4    | 3.89G/7.07G [01:05&lt;00:50, 62.8MB/s]\n\nDownloading data:  55%|#####5    | 3.89G/7.07G [01:05&lt;00:50, 63.1MB/s]\n\nDownloading data:  55%|#####5    | 3.90G/7.07G [01:05&lt;00:49, 63.5MB/s]\n\nDownloading data:  55%|#####5    | 3.91G/7.07G [01:06&lt;00:49, 63.5MB/s]\n\nDownloading data:  55%|#####5    | 3.91G/7.07G [01:06&lt;00:49, 63.7MB/s]\n\nDownloading data:  55%|#####5    | 3.92G/7.07G [01:06&lt;00:50, 62.4MB/s]\n\nDownloading data:  56%|#####5    | 3.93G/7.07G [01:06&lt;00:49, 63.0MB/s]\n\nDownloading data:  56%|#####5    | 3.93G/7.07G [01:06&lt;00:49, 63.3MB/s]\n\nDownloading data:  56%|#####5    | 3.94G/7.07G [01:06&lt;00:49, 63.5MB/s]\n\nDownloading data:  56%|#####5    | 3.94G/7.07G [01:06&lt;00:49, 63.2MB/s]\n\nDownloading data:  56%|#####5    | 3.95G/7.07G [01:06&lt;00:49, 63.1MB/s]\n\nDownloading data:  56%|#####5    | 3.96G/7.07G [01:06&lt;00:51, 61.1MB/s]\n\nDownloading data:  56%|#####6    | 3.96G/7.07G [01:06&lt;00:49, 62.6MB/s]\n\nDownloading data:  56%|#####6    | 3.97G/7.07G [01:07&lt;00:48, 63.5MB/s]\n\nDownloading data:  56%|#####6    | 3.98G/7.07G [01:07&lt;00:48, 63.7MB/s]\n\nDownloading data:  56%|#####6    | 3.98G/7.07G [01:07&lt;00:49, 61.9MB/s]\n\nDownloading data:  56%|#####6    | 3.99G/7.07G [01:07&lt;00:49, 61.8MB/s]\n\nDownloading data:  57%|#####6    | 4.00G/7.07G [01:07&lt;00:48, 63.1MB/s]\n\nDownloading data:  57%|#####6    | 4.00G/7.07G [01:07&lt;00:49, 62.1MB/s]\n\nDownloading data:  57%|#####6    | 4.01G/7.07G [01:07&lt;00:49, 62.3MB/s]\n\nDownloading data:  57%|#####6    | 4.02G/7.07G [01:07&lt;00:48, 63.4MB/s]\n\nDownloading data:  57%|#####6    | 4.02G/7.07G [01:07&lt;00:49, 61.9MB/s]\n\nDownloading data:  57%|#####6    | 4.03G/7.07G [01:07&lt;00:48, 62.6MB/s]\n\nDownloading data:  57%|#####7    | 4.03G/7.07G [01:08&lt;00:48, 62.5MB/s]\n\nDownloading data:  57%|#####7    | 4.04G/7.07G [01:08&lt;00:48, 63.0MB/s]\n\nDownloading data:  57%|#####7    | 4.05G/7.07G [01:08&lt;00:48, 62.6MB/s]\n\nDownloading data:  57%|#####7    | 4.05G/7.07G [01:08&lt;00:48, 62.8MB/s]\n\nDownloading data:  57%|#####7    | 4.06G/7.07G [01:08&lt;00:48, 62.5MB/s]\n\nDownloading data:  57%|#####7    | 4.07G/7.07G [01:08&lt;00:48, 62.5MB/s]\n\nDownloading data:  58%|#####7    | 4.07G/7.07G [01:08&lt;00:48, 62.1MB/s]\n\nDownloading data:  58%|#####7    | 4.08G/7.07G [01:08&lt;00:48, 61.4MB/s]\n\nDownloading data:  58%|#####7    | 4.09G/7.07G [01:08&lt;00:47, 62.4MB/s]\n\nDownloading data:  58%|#####7    | 4.09G/7.07G [01:08&lt;00:47, 62.1MB/s]\n\nDownloading data:  58%|#####7    | 4.10G/7.07G [01:09&lt;00:48, 60.7MB/s]\n\nDownloading data:  58%|#####8    | 4.10G/7.07G [01:09&lt;00:47, 62.4MB/s]\n\nDownloading data:  58%|#####8    | 4.11G/7.07G [01:09&lt;00:47, 62.2MB/s]\n\nDownloading data:  58%|#####8    | 4.12G/7.07G [01:09&lt;00:47, 62.8MB/s]\n\nDownloading data:  58%|#####8    | 4.12G/7.07G [01:09&lt;00:47, 61.7MB/s]\n\nDownloading data:  58%|#####8    | 4.13G/7.07G [01:09&lt;00:48, 60.6MB/s]\n\nDownloading data:  58%|#####8    | 4.14G/7.07G [01:09&lt;00:46, 62.5MB/s]\n\nDownloading data:  59%|#####8    | 4.14G/7.07G [01:09&lt;00:46, 63.3MB/s]\n\nDownloading data:  59%|#####8    | 4.15G/7.07G [01:09&lt;00:46, 63.0MB/s]\n\nDownloading data:  59%|#####8    | 4.16G/7.07G [01:10&lt;00:46, 62.4MB/s]\n\nDownloading data:  59%|#####8    | 4.16G/7.07G [01:10&lt;00:46, 62.0MB/s]\n\nDownloading data:  59%|#####8    | 4.17G/7.07G [01:10&lt;00:46, 62.2MB/s]\n\nDownloading data:  59%|#####9    | 4.17G/7.07G [01:10&lt;00:46, 62.2MB/s]\n\nDownloading data:  59%|#####9    | 4.18G/7.07G [01:10&lt;00:46, 61.8MB/s]\n\nDownloading data:  59%|#####9    | 4.19G/7.07G [01:10&lt;00:46, 61.7MB/s]\n\nDownloading data:  59%|#####9    | 4.19G/7.07G [01:10&lt;00:46, 61.6MB/s]\n\nDownloading data:  59%|#####9    | 4.20G/7.07G [01:10&lt;00:46, 62.0MB/s]\n\nDownloading data:  59%|#####9    | 4.21G/7.07G [01:10&lt;00:46, 61.8MB/s]\n\nDownloading data:  60%|#####9    | 4.21G/7.07G [01:10&lt;00:46, 62.0MB/s]\n\nDownloading data:  60%|#####9    | 4.22G/7.07G [01:11&lt;00:46, 62.0MB/s]\n\nDownloading data:  60%|#####9    | 4.22G/7.07G [01:11&lt;00:46, 61.1MB/s]\n\nDownloading data:  60%|#####9    | 4.23G/7.07G [01:11&lt;00:45, 62.7MB/s]\n\nDownloading data:  60%|#####9    | 4.24G/7.07G [01:11&lt;00:45, 62.6MB/s]\n\nDownloading data:  60%|#####9    | 4.24G/7.07G [01:11&lt;00:45, 62.0MB/s]\n\nDownloading data:  60%|######    | 4.25G/7.07G [01:11&lt;00:45, 61.8MB/s]\n\nDownloading data:  60%|######    | 4.26G/7.07G [01:11&lt;00:45, 62.0MB/s]\n\nDownloading data:  60%|######    | 4.26G/7.07G [01:11&lt;00:45, 62.1MB/s]\n\nDownloading data:  60%|######    | 4.27G/7.07G [01:11&lt;00:45, 62.0MB/s]\n\nDownloading data:  60%|######    | 4.27G/7.07G [01:11&lt;00:45, 62.0MB/s]\n\nDownloading data:  61%|######    | 4.28G/7.07G [01:12&lt;00:45, 61.7MB/s]\n\nDownloading data:  61%|######    | 4.29G/7.07G [01:12&lt;00:44, 62.1MB/s]\n\nDownloading data:  61%|######    | 4.29G/7.07G [01:12&lt;00:44, 62.2MB/s]\n\nDownloading data:  61%|######    | 4.30G/7.07G [01:12&lt;00:45, 61.3MB/s]\n\nDownloading data:  61%|######    | 4.31G/7.07G [01:12&lt;00:45, 61.0MB/s]\n\nDownloading data:  61%|######    | 4.31G/7.07G [01:12&lt;00:45, 61.3MB/s]\n\nDownloading data:  61%|######1   | 4.32G/7.07G [01:12&lt;00:44, 61.6MB/s]\n\nDownloading data:  61%|######1   | 4.32G/7.07G [01:12&lt;00:45, 59.9MB/s]\n\nDownloading data:  61%|######1   | 4.33G/7.07G [01:12&lt;00:46, 58.5MB/s]\n\nDownloading data:  61%|######1   | 4.34G/7.07G [01:12&lt;00:46, 58.6MB/s]\n\nDownloading data:  61%|######1   | 4.34G/7.07G [01:13&lt;00:47, 57.6MB/s]\n\nDownloading data:  61%|######1   | 4.35G/7.07G [01:13&lt;00:47, 57.6MB/s]\n\nDownloading data:  62%|######1   | 4.35G/7.07G [01:13&lt;00:47, 57.2MB/s]\n\nDownloading data:  62%|######1   | 4.36G/7.07G [01:13&lt;00:47, 57.0MB/s]\n\nDownloading data:  62%|######1   | 4.36G/7.07G [01:13&lt;00:47, 57.3MB/s]\n\nDownloading data:  62%|######1   | 4.37G/7.07G [01:13&lt;00:46, 57.9MB/s]\n\nDownloading data:  62%|######1   | 4.38G/7.07G [01:13&lt;00:45, 58.9MB/s]\n\nDownloading data:  62%|######1   | 4.38G/7.07G [01:13&lt;00:45, 59.2MB/s]\n\nDownloading data:  62%|######2   | 4.39G/7.07G [01:13&lt;00:45, 59.2MB/s]\n\nDownloading data:  62%|######2   | 4.39G/7.07G [01:13&lt;00:44, 59.8MB/s]\n\nDownloading data:  62%|######2   | 4.40G/7.07G [01:14&lt;00:44, 60.2MB/s]\n\nDownloading data:  62%|######2   | 4.41G/7.07G [01:14&lt;00:44, 59.8MB/s]\n\nDownloading data:  62%|######2   | 4.41G/7.07G [01:14&lt;00:45, 59.1MB/s]\n\nDownloading data:  62%|######2   | 4.42G/7.07G [01:14&lt;00:45, 57.7MB/s]\n\nDownloading data:  63%|######2   | 4.42G/7.07G [01:14&lt;00:46, 56.6MB/s]\n\nDownloading data:  63%|######2   | 4.43G/7.07G [01:14&lt;00:47, 56.0MB/s]\n\nDownloading data:  63%|######2   | 4.44G/7.07G [01:14&lt;00:46, 56.3MB/s]\n\nDownloading data:  63%|######2   | 4.44G/7.07G [01:14&lt;00:46, 56.2MB/s]\n\nDownloading data:  63%|######2   | 4.45G/7.07G [01:14&lt;00:46, 56.0MB/s]\n\nDownloading data:  63%|######2   | 4.45G/7.07G [01:14&lt;00:46, 56.4MB/s]\n\nDownloading data:  63%|######3   | 4.46G/7.07G [01:15&lt;00:45, 57.6MB/s]\n\nDownloading data:  63%|######3   | 4.47G/7.07G [01:15&lt;00:44, 58.9MB/s]\n\nDownloading data:  63%|######3   | 4.47G/7.07G [01:15&lt;00:43, 59.9MB/s]\n\nDownloading data:  63%|######3   | 4.48G/7.07G [01:15&lt;00:43, 60.2MB/s]\n\nDownloading data:  63%|######3   | 4.48G/7.07G [01:15&lt;00:42, 60.9MB/s]\n\nDownloading data:  63%|######3   | 4.49G/7.07G [01:15&lt;00:41, 61.5MB/s]\n\nDownloading data:  64%|######3   | 4.50G/7.07G [01:15&lt;00:41, 61.7MB/s]\n\nDownloading data:  64%|######3   | 4.50G/7.07G [01:15&lt;00:41, 61.9MB/s]\n\nDownloading data:  64%|######3   | 4.51G/7.07G [01:15&lt;00:41, 62.1MB/s]\n\nDownloading data:  64%|######3   | 4.52G/7.07G [01:15&lt;00:40, 63.2MB/s]\n\nDownloading data:  64%|######3   | 4.52G/7.07G [01:16&lt;00:40, 63.6MB/s]\n\nDownloading data:  64%|######4   | 4.53G/7.07G [01:16&lt;00:40, 63.4MB/s]\n\nDownloading data:  64%|######4   | 4.53G/7.07G [01:16&lt;00:40, 63.1MB/s]\n\nDownloading data:  64%|######4   | 4.54G/7.07G [01:16&lt;00:40, 62.6MB/s]\n\nDownloading data:  64%|######4   | 4.55G/7.07G [01:16&lt;00:40, 63.0MB/s]\n\nDownloading data:  64%|######4   | 4.55G/7.07G [01:16&lt;00:39, 64.1MB/s]\n\nDownloading data:  64%|######4   | 4.56G/7.07G [01:16&lt;00:39, 64.2MB/s]\n\nDownloading data:  65%|######4   | 4.57G/7.07G [01:16&lt;00:38, 64.6MB/s]\n\nDownloading data:  65%|######4   | 4.57G/7.07G [01:16&lt;00:38, 65.0MB/s]\n\nDownloading data:  65%|######4   | 4.58G/7.07G [01:16&lt;00:38, 65.2MB/s]\n\nDownloading data:  65%|######4   | 4.59G/7.07G [01:17&lt;00:37, 65.6MB/s]\n\nDownloading data:  65%|######4   | 4.59G/7.07G [01:17&lt;00:37, 66.0MB/s]\n\nDownloading data:  65%|######5   | 4.60G/7.07G [01:17&lt;00:37, 66.0MB/s]\n\nDownloading data:  65%|######5   | 4.61G/7.07G [01:17&lt;00:37, 66.0MB/s]\n\nDownloading data:  65%|######5   | 4.61G/7.07G [01:17&lt;00:37, 66.0MB/s]\n\nDownloading data:  65%|######5   | 4.62G/7.07G [01:17&lt;00:37, 66.1MB/s]\n\nDownloading data:  65%|######5   | 4.63G/7.07G [01:17&lt;00:37, 65.7MB/s]\n\nDownloading data:  66%|######5   | 4.63G/7.07G [01:17&lt;00:37, 65.9MB/s]\n\nDownloading data:  66%|######5   | 4.64G/7.07G [01:17&lt;00:36, 66.1MB/s]\n\nDownloading data:  66%|######5   | 4.65G/7.07G [01:17&lt;00:36, 66.2MB/s]\n\nDownloading data:  66%|######5   | 4.65G/7.07G [01:18&lt;00:36, 66.4MB/s]\n\nDownloading data:  66%|######5   | 4.66G/7.07G [01:18&lt;00:36, 66.6MB/s]\n\nDownloading data:  66%|######5   | 4.67G/7.07G [01:18&lt;00:36, 66.4MB/s]\n\nDownloading data:  66%|######6   | 4.67G/7.07G [01:18&lt;00:36, 66.5MB/s]\n\nDownloading data:  66%|######6   | 4.68G/7.07G [01:18&lt;00:35, 66.5MB/s]\n\nDownloading data:  66%|######6   | 4.69G/7.07G [01:18&lt;00:35, 66.3MB/s]\n\nDownloading data:  66%|######6   | 4.69G/7.07G [01:18&lt;00:35, 66.6MB/s]\n\nDownloading data:  66%|######6   | 4.70G/7.07G [01:18&lt;00:35, 66.7MB/s]\n\nDownloading data:  67%|######6   | 4.71G/7.07G [01:18&lt;00:35, 66.5MB/s]\n\nDownloading data:  67%|######6   | 4.71G/7.07G [01:18&lt;00:35, 66.5MB/s]\n\nDownloading data:  67%|######6   | 4.72G/7.07G [01:19&lt;00:35, 66.5MB/s]\n\nDownloading data:  67%|######6   | 4.73G/7.07G [01:19&lt;00:35, 66.2MB/s]\n\nDownloading data:  67%|######6   | 4.73G/7.07G [01:19&lt;00:35, 66.3MB/s]\n\nDownloading data:  67%|######7   | 4.74G/7.07G [01:19&lt;00:35, 66.5MB/s]\n\nDownloading data:  67%|######7   | 4.75G/7.07G [01:19&lt;00:34, 66.7MB/s]\n\nDownloading data:  67%|######7   | 4.75G/7.07G [01:19&lt;00:34, 66.4MB/s]\n\nDownloading data:  67%|######7   | 4.76G/7.07G [01:19&lt;00:34, 66.3MB/s]\n\nDownloading data:  67%|######7   | 4.77G/7.07G [01:19&lt;00:34, 65.9MB/s]\n\nDownloading data:  67%|######7   | 4.77G/7.07G [01:19&lt;00:34, 65.9MB/s]\n\nDownloading data:  68%|######7   | 4.78G/7.07G [01:20&lt;00:34, 65.8MB/s]\n\nDownloading data:  68%|######7   | 4.79G/7.07G [01:20&lt;00:34, 65.9MB/s]\n\nDownloading data:  68%|######7   | 4.79G/7.07G [01:20&lt;00:34, 65.6MB/s]\n\nDownloading data:  68%|######7   | 4.80G/7.07G [01:20&lt;00:34, 66.0MB/s]\n\nDownloading data:  68%|######7   | 4.81G/7.07G [01:20&lt;00:34, 66.2MB/s]\n\nDownloading data:  68%|######8   | 4.81G/7.07G [01:20&lt;00:34, 66.5MB/s]\n\nDownloading data:  68%|######8   | 4.82G/7.07G [01:20&lt;00:33, 66.4MB/s]\n\nDownloading data:  68%|######8   | 4.83G/7.07G [01:20&lt;00:33, 66.2MB/s]\n\nDownloading data:  68%|######8   | 4.83G/7.07G [01:20&lt;00:33, 66.4MB/s]\n\nDownloading data:  68%|######8   | 4.84G/7.07G [01:20&lt;00:33, 66.6MB/s]\n\nDownloading data:  69%|######8   | 4.85G/7.07G [01:21&lt;00:33, 66.6MB/s]\n\nDownloading data:  69%|######8   | 4.85G/7.07G [01:21&lt;00:33, 66.6MB/s]\n\nDownloading data:  69%|######8   | 4.86G/7.07G [01:21&lt;00:33, 66.0MB/s]\n\nDownloading data:  69%|######8   | 4.87G/7.07G [01:21&lt;00:33, 66.1MB/s]\n\nDownloading data:  69%|######8   | 4.87G/7.07G [01:21&lt;00:33, 65.5MB/s]\n\nDownloading data:  69%|######8   | 4.88G/7.07G [01:21&lt;00:34, 63.6MB/s]\n\nDownloading data:  69%|######9   | 4.89G/7.07G [01:21&lt;00:34, 62.6MB/s]\n\nDownloading data:  69%|######9   | 4.89G/7.07G [01:21&lt;00:35, 61.8MB/s]\n\nDownloading data:  69%|######9   | 4.90G/7.07G [01:21&lt;00:35, 61.9MB/s]\n\nDownloading data:  69%|######9   | 4.90G/7.07G [01:21&lt;00:34, 63.4MB/s]\n\nDownloading data:  69%|######9   | 4.91G/7.07G [01:22&lt;00:33, 64.2MB/s]\n\nDownloading data:  70%|######9   | 4.92G/7.07G [01:22&lt;00:49, 43.8MB/s]\n\nDownloading data:  70%|######9   | 4.92G/7.07G [01:22&lt;00:44, 48.1MB/s]\n\nDownloading data:  70%|######9   | 4.93G/7.07G [01:22&lt;00:40, 52.9MB/s]\n\nDownloading data:  70%|######9   | 4.94G/7.07G [01:22&lt;00:37, 56.8MB/s]\n\nDownloading data:  70%|######9   | 4.94G/7.07G [01:22&lt;00:36, 58.8MB/s]\n\nDownloading data:  70%|######9   | 4.95G/7.07G [01:22&lt;00:35, 60.4MB/s]\n\nDownloading data:  70%|#######   | 4.96G/7.07G [01:22&lt;00:33, 62.3MB/s]\n\nDownloading data:  70%|#######   | 4.96G/7.07G [01:22&lt;00:32, 64.1MB/s]\n\nDownloading data:  70%|#######   | 4.97G/7.07G [01:23&lt;00:32, 65.4MB/s]\n\nDownloading data:  70%|#######   | 4.98G/7.07G [01:23&lt;00:31, 66.2MB/s]\n\nDownloading data:  70%|#######   | 4.98G/7.07G [01:23&lt;00:31, 66.6MB/s]\n\nDownloading data:  71%|#######   | 4.99G/7.07G [01:23&lt;00:31, 66.8MB/s]\n\nDownloading data:  71%|#######   | 5.00G/7.07G [01:23&lt;00:31, 65.6MB/s]\n\nDownloading data:  71%|#######   | 5.00G/7.07G [01:23&lt;00:31, 64.9MB/s]\n\nDownloading data:  71%|#######   | 5.01G/7.07G [01:23&lt;00:32, 64.3MB/s]\n\nDownloading data:  71%|#######   | 5.02G/7.07G [01:23&lt;00:32, 63.5MB/s]\n\nDownloading data:  71%|#######1  | 5.02G/7.07G [01:23&lt;00:32, 63.3MB/s]\n\nDownloading data:  71%|#######1  | 5.03G/7.07G [01:24&lt;00:31, 64.4MB/s]\n\nDownloading data:  71%|#######1  | 5.04G/7.07G [01:24&lt;00:31, 65.1MB/s]\n\nDownloading data:  71%|#######1  | 5.04G/7.07G [01:24&lt;00:31, 65.0MB/s]\n\nDownloading data:  71%|#######1  | 5.05G/7.07G [01:24&lt;00:30, 65.8MB/s]\n\nDownloading data:  72%|#######1  | 5.06G/7.07G [01:24&lt;00:30, 66.4MB/s]\n\nDownloading data:  72%|#######1  | 5.06G/7.07G [01:24&lt;00:30, 66.8MB/s]\n\nDownloading data:  72%|#######1  | 5.07G/7.07G [01:24&lt;00:29, 66.8MB/s]\n\nDownloading data:  72%|#######1  | 5.08G/7.07G [01:24&lt;00:29, 67.3MB/s]\n\nDownloading data:  72%|#######1  | 5.08G/7.07G [01:24&lt;00:29, 67.2MB/s]\n\nDownloading data:  72%|#######1  | 5.09G/7.07G [01:24&lt;00:30, 64.3MB/s]\n\nDownloading data:  72%|#######2  | 5.10G/7.07G [01:25&lt;00:31, 62.0MB/s]\n\nDownloading data:  72%|#######2  | 5.10G/7.07G [01:25&lt;00:33, 59.1MB/s]\n\nDownloading data:  72%|#######2  | 5.11G/7.07G [01:25&lt;00:33, 58.2MB/s]\n\nDownloading data:  72%|#######2  | 5.12G/7.07G [01:25&lt;00:33, 57.7MB/s]\n\nDownloading data:  72%|#######2  | 5.12G/7.07G [01:25&lt;00:34, 56.9MB/s]\n\nDownloading data:  73%|#######2  | 5.13G/7.07G [01:25&lt;00:33, 58.0MB/s]\n\nDownloading data:  73%|#######2  | 5.13G/7.07G [01:25&lt;00:33, 57.6MB/s]\n\nDownloading data:  73%|#######2  | 5.14G/7.07G [01:25&lt;00:33, 57.1MB/s]\n\nDownloading data:  73%|#######2  | 5.14G/7.07G [01:25&lt;00:34, 56.4MB/s]\n\nDownloading data:  73%|#######2  | 5.15G/7.07G [01:25&lt;00:33, 56.7MB/s]\n\nDownloading data:  73%|#######2  | 5.16G/7.07G [01:26&lt;00:33, 56.8MB/s]\n\nDownloading data:  73%|#######2  | 5.16G/7.07G [01:26&lt;00:33, 57.4MB/s]\n\nDownloading data:  73%|#######3  | 5.17G/7.07G [01:26&lt;00:33, 57.0MB/s]\n\nDownloading data:  73%|#######3  | 5.17G/7.07G [01:26&lt;00:33, 57.4MB/s]\n\nDownloading data:  73%|#######3  | 5.18G/7.07G [01:26&lt;00:32, 57.8MB/s]\n\nDownloading data:  73%|#######3  | 5.19G/7.07G [01:26&lt;00:32, 57.9MB/s]\n\nDownloading data:  73%|#######3  | 5.19G/7.07G [01:26&lt;00:32, 57.7MB/s]\n\nDownloading data:  73%|#######3  | 5.20G/7.07G [01:26&lt;00:32, 58.0MB/s]\n\nDownloading data:  74%|#######3  | 5.20G/7.07G [01:26&lt;00:32, 57.8MB/s]\n\nDownloading data:  74%|#######3  | 5.21G/7.07G [01:26&lt;00:32, 57.8MB/s]\n\nDownloading data:  74%|#######3  | 5.21G/7.07G [01:27&lt;00:31, 58.4MB/s]\n\nDownloading data:  74%|#######3  | 5.22G/7.07G [01:27&lt;00:31, 58.2MB/s]\n\nDownloading data:  74%|#######3  | 5.23G/7.07G [01:27&lt;00:31, 58.5MB/s]\n\nDownloading data:  74%|#######3  | 5.23G/7.07G [01:27&lt;00:31, 58.5MB/s]\n\nDownloading data:  74%|#######4  | 5.24G/7.07G [01:27&lt;00:31, 58.7MB/s]\n\nDownloading data:  74%|#######4  | 5.24G/7.07G [01:27&lt;00:31, 58.9MB/s]\n\nDownloading data:  74%|#######4  | 5.25G/7.07G [01:27&lt;00:40, 44.9MB/s]\n\nDownloading data:  74%|#######4  | 5.25G/7.07G [01:27&lt;00:41, 43.9MB/s]\n\nDownloading data:  74%|#######4  | 5.26G/7.07G [01:28&lt;00:36, 49.3MB/s]\n\nDownloading data:  74%|#######4  | 5.27G/7.07G [01:28&lt;00:32, 54.7MB/s]\n\nDownloading data:  75%|#######4  | 5.28G/7.07G [01:28&lt;00:30, 58.9MB/s]\n\nDownloading data:  75%|#######4  | 5.28G/7.07G [01:28&lt;00:28, 62.0MB/s]\n\nDownloading data:  75%|#######4  | 5.29G/7.07G [01:28&lt;00:27, 64.1MB/s]\n\nDownloading data:  75%|#######4  | 5.30G/7.07G [01:28&lt;00:27, 65.7MB/s]\n\nDownloading data:  75%|#######4  | 5.30G/7.07G [01:28&lt;00:26, 67.0MB/s]\n\nDownloading data:  75%|#######5  | 5.31G/7.07G [01:28&lt;00:25, 68.0MB/s]\n\nDownloading data:  75%|#######5  | 5.32G/7.07G [01:28&lt;00:26, 65.5MB/s]\n\nDownloading data:  75%|#######5  | 5.32G/7.07G [01:28&lt;00:27, 64.1MB/s]\n\nDownloading data:  75%|#######5  | 5.33G/7.07G [01:29&lt;00:27, 63.2MB/s]\n\nDownloading data:  75%|#######5  | 5.34G/7.07G [01:29&lt;00:28, 61.8MB/s]\n\nDownloading data:  76%|#######5  | 5.34G/7.07G [01:29&lt;00:34, 50.9MB/s]\n\nDownloading data:  76%|#######5  | 5.35G/7.07G [01:29&lt;00:33, 51.2MB/s]\n\nDownloading data:  76%|#######5  | 5.35G/7.07G [01:29&lt;00:32, 53.1MB/s]\n\nDownloading data:  76%|#######5  | 5.36G/7.07G [01:29&lt;00:30, 55.3MB/s]\n\nDownloading data:  76%|#######5  | 5.37G/7.07G [01:29&lt;00:30, 55.7MB/s]\n\nDownloading data:  76%|#######5  | 5.37G/7.07G [01:29&lt;00:29, 57.5MB/s]\n\nDownloading data:  76%|#######6  | 5.38G/7.07G [01:29&lt;00:29, 57.9MB/s]\n\nDownloading data:  76%|#######6  | 5.38G/7.07G [01:30&lt;00:28, 58.8MB/s]\n\nDownloading data:  76%|#######6  | 5.39G/7.07G [01:30&lt;00:28, 58.9MB/s]\n\nDownloading data:  76%|#######6  | 5.40G/7.07G [01:30&lt;00:28, 59.4MB/s]\n\nDownloading data:  76%|#######6  | 5.40G/7.07G [01:30&lt;00:27, 59.7MB/s]\n\nDownloading data:  76%|#######6  | 5.41G/7.07G [01:30&lt;00:27, 59.8MB/s]\n\nDownloading data:  77%|#######6  | 5.41G/7.07G [01:30&lt;00:27, 60.1MB/s]\n\nDownloading data:  77%|#######6  | 5.42G/7.07G [01:30&lt;00:27, 59.7MB/s]\n\nDownloading data:  77%|#######6  | 5.43G/7.07G [01:30&lt;00:27, 59.6MB/s]\n\nDownloading data:  77%|#######6  | 5.43G/7.07G [01:30&lt;00:27, 60.6MB/s]\n\nDownloading data:  77%|#######6  | 5.44G/7.07G [01:30&lt;00:27, 60.3MB/s]\n\nDownloading data:  77%|#######6  | 5.44G/7.07G [01:31&lt;00:27, 59.8MB/s]\n\nDownloading data:  77%|#######7  | 5.45G/7.07G [01:31&lt;00:27, 59.9MB/s]\n\nDownloading data:  77%|#######7  | 5.46G/7.07G [01:31&lt;00:27, 58.9MB/s]\n\nDownloading data:  77%|#######7  | 5.46G/7.07G [01:31&lt;00:27, 59.3MB/s]\n\nDownloading data:  77%|#######7  | 5.47G/7.07G [01:31&lt;00:26, 59.6MB/s]\n\nDownloading data:  77%|#######7  | 5.48G/7.07G [01:31&lt;00:26, 59.6MB/s]\n\nDownloading data:  78%|#######7  | 5.48G/7.07G [01:31&lt;00:26, 60.0MB/s]\n\nDownloading data:  78%|#######7  | 5.49G/7.07G [01:31&lt;00:26, 59.7MB/s]\n\nDownloading data:  78%|#######7  | 5.49G/7.07G [01:31&lt;00:26, 59.9MB/s]\n\nDownloading data:  78%|#######7  | 5.50G/7.07G [01:31&lt;00:26, 59.4MB/s]\n\nDownloading data:  78%|#######7  | 5.51G/7.07G [01:32&lt;00:26, 59.6MB/s]\n\nDownloading data:  78%|#######7  | 5.51G/7.07G [01:32&lt;00:25, 60.3MB/s]\n\nDownloading data:  78%|#######8  | 5.52G/7.07G [01:32&lt;00:25, 60.1MB/s]\n\nDownloading data:  78%|#######8  | 5.52G/7.07G [01:32&lt;00:25, 59.8MB/s]\n\nDownloading data:  78%|#######8  | 5.53G/7.07G [01:32&lt;00:25, 60.4MB/s]\n\nDownloading data:  78%|#######8  | 5.54G/7.07G [01:32&lt;00:25, 60.2MB/s]\n\nDownloading data:  78%|#######8  | 5.54G/7.07G [01:32&lt;00:25, 60.4MB/s]\n\nDownloading data:  78%|#######8  | 5.55G/7.07G [01:32&lt;00:25, 58.8MB/s]\n\nDownloading data:  79%|#######8  | 5.55G/7.07G [01:32&lt;00:25, 59.5MB/s]\n\nDownloading data:  79%|#######8  | 5.56G/7.07G [01:32&lt;00:25, 59.6MB/s]\n\nDownloading data:  79%|#######8  | 5.57G/7.07G [01:33&lt;00:25, 59.5MB/s]\n\nDownloading data:  79%|#######8  | 5.57G/7.07G [01:33&lt;00:25, 59.8MB/s]\n\nDownloading data:  79%|#######8  | 5.58G/7.07G [01:33&lt;00:27, 53.5MB/s]\n\nDownloading data:  79%|#######8  | 5.59G/7.07G [01:33&lt;00:25, 58.9MB/s]\n\nDownloading data:  79%|#######9  | 5.59G/7.07G [01:33&lt;00:23, 62.3MB/s]\n\nDownloading data:  79%|#######9  | 5.60G/7.07G [01:33&lt;00:23, 61.6MB/s]\n\nDownloading data:  79%|#######9  | 5.61G/7.07G [01:33&lt;00:23, 61.4MB/s]\n\nDownloading data:  79%|#######9  | 5.61G/7.07G [01:33&lt;00:23, 61.2MB/s]\n\nDownloading data:  79%|#######9  | 5.62G/7.07G [01:33&lt;00:23, 60.7MB/s]\n\nDownloading data:  80%|#######9  | 5.62G/7.07G [01:34&lt;00:23, 61.0MB/s]\n\nDownloading data:  80%|#######9  | 5.63G/7.07G [01:34&lt;00:23, 60.8MB/s]\n\nDownloading data:  80%|#######9  | 5.64G/7.07G [01:34&lt;00:23, 60.6MB/s]\n\nDownloading data:  80%|#######9  | 5.64G/7.07G [01:34&lt;00:23, 60.4MB/s]\n\nDownloading data:  80%|#######9  | 5.65G/7.07G [01:34&lt;00:23, 60.2MB/s]\n\nDownloading data:  80%|#######9  | 5.65G/7.07G [01:34&lt;00:23, 60.4MB/s]\n\nDownloading data:  80%|########  | 5.66G/7.07G [01:34&lt;00:23, 60.2MB/s]\n\nDownloading data:  80%|########  | 5.67G/7.07G [01:34&lt;00:23, 59.9MB/s]\n\nDownloading data:  80%|########  | 5.67G/7.07G [01:34&lt;00:23, 59.2MB/s]\n\nDownloading data:  80%|########  | 5.68G/7.07G [01:34&lt;00:23, 59.5MB/s]\n\nDownloading data:  80%|########  | 5.68G/7.07G [01:35&lt;00:23, 60.1MB/s]\n\nDownloading data:  80%|########  | 5.69G/7.07G [01:35&lt;00:23, 60.0MB/s]\n\nDownloading data:  81%|########  | 5.70G/7.07G [01:35&lt;00:22, 60.2MB/s]\n\nDownloading data:  81%|########  | 5.70G/7.07G [01:35&lt;00:22, 60.2MB/s]\n\nDownloading data:  81%|########  | 5.71G/7.07G [01:35&lt;00:22, 59.9MB/s]\n\nDownloading data:  81%|########  | 5.71G/7.07G [01:35&lt;00:22, 60.0MB/s]\n\nDownloading data:  81%|########  | 5.72G/7.07G [01:35&lt;00:22, 59.8MB/s]\n\nDownloading data:  81%|########  | 5.73G/7.07G [01:35&lt;00:22, 60.6MB/s]\n\nDownloading data:  81%|########1 | 5.73G/7.07G [01:35&lt;00:22, 59.3MB/s]\n\nDownloading data:  81%|########1 | 5.74G/7.07G [01:35&lt;00:21, 60.8MB/s]\n\nDownloading data:  81%|########1 | 5.75G/7.07G [01:36&lt;00:22, 59.1MB/s]\n\nDownloading data:  81%|########1 | 5.75G/7.07G [01:36&lt;00:22, 59.9MB/s]\n\nDownloading data:  81%|########1 | 5.76G/7.07G [01:36&lt;00:21, 60.2MB/s]\n\nDownloading data:  81%|########1 | 5.76G/7.07G [01:36&lt;00:21, 59.8MB/s]\n\nDownloading data:  82%|########1 | 5.77G/7.07G [01:36&lt;00:21, 60.6MB/s]\n\nDownloading data:  82%|########1 | 5.78G/7.07G [01:36&lt;00:21, 60.0MB/s]\n\nDownloading data:  82%|########1 | 5.78G/7.07G [01:36&lt;00:21, 60.0MB/s]\n\nDownloading data:  82%|########1 | 5.79G/7.07G [01:36&lt;00:21, 60.3MB/s]\n\nDownloading data:  82%|########1 | 5.79G/7.07G [01:36&lt;00:21, 59.7MB/s]\n\nDownloading data:  82%|########2 | 5.80G/7.07G [01:36&lt;00:21, 60.5MB/s]\n\nDownloading data:  82%|########2 | 5.81G/7.07G [01:37&lt;00:20, 60.7MB/s]\n\nDownloading data:  82%|########2 | 5.81G/7.07G [01:37&lt;00:20, 60.8MB/s]\n\nDownloading data:  82%|########2 | 5.82G/7.07G [01:37&lt;00:20, 60.9MB/s]\n\nDownloading data:  82%|########2 | 5.82G/7.07G [01:37&lt;00:20, 60.7MB/s]\n\nDownloading data:  82%|########2 | 5.83G/7.07G [01:37&lt;00:20, 60.7MB/s]\n\nDownloading data:  83%|########2 | 5.84G/7.07G [01:37&lt;00:20, 60.3MB/s]\n\nDownloading data:  83%|########2 | 5.84G/7.07G [01:37&lt;00:20, 61.2MB/s]\n\nDownloading data:  83%|########2 | 5.85G/7.07G [01:37&lt;00:19, 61.7MB/s]\n\nDownloading data:  83%|########2 | 5.86G/7.07G [01:37&lt;00:20, 60.8MB/s]\n\nDownloading data:  83%|########2 | 5.86G/7.07G [01:37&lt;00:19, 60.9MB/s]\n\nDownloading data:  83%|########2 | 5.87G/7.07G [01:38&lt;00:19, 61.2MB/s]\n\nDownloading data:  83%|########3 | 5.87G/7.07G [01:38&lt;00:19, 61.1MB/s]\n\nDownloading data:  83%|########3 | 5.88G/7.07G [01:38&lt;00:19, 61.5MB/s]\n\nDownloading data:  83%|########3 | 5.89G/7.07G [01:38&lt;00:19, 61.1MB/s]\n\nDownloading data:  83%|########3 | 5.89G/7.07G [01:38&lt;00:19, 61.7MB/s]\n\nDownloading data:  83%|########3 | 5.90G/7.07G [01:38&lt;00:18, 62.1MB/s]\n\nDownloading data:  83%|########3 | 5.91G/7.07G [01:38&lt;00:19, 61.0MB/s]\n\nDownloading data:  84%|########3 | 5.91G/7.07G [01:38&lt;00:18, 63.2MB/s]\n\nDownloading data:  84%|########3 | 5.92G/7.07G [01:38&lt;00:18, 62.8MB/s]\n\nDownloading data:  84%|########3 | 5.92G/7.07G [01:38&lt;00:18, 62.9MB/s]\n\nDownloading data:  84%|########3 | 5.93G/7.07G [01:39&lt;00:18, 61.9MB/s]\n\nDownloading data:  84%|########3 | 5.94G/7.07G [01:39&lt;00:18, 60.8MB/s]\n\nDownloading data:  84%|########4 | 5.94G/7.07G [01:39&lt;00:17, 63.4MB/s]\n\nDownloading data:  84%|########4 | 5.95G/7.07G [01:39&lt;00:17, 63.0MB/s]\n\nDownloading data:  84%|########4 | 5.96G/7.07G [01:39&lt;00:17, 63.8MB/s]\n\nDownloading data:  84%|########4 | 5.96G/7.07G [01:39&lt;00:17, 63.4MB/s]\n\nDownloading data:  84%|########4 | 5.97G/7.07G [01:39&lt;00:17, 63.6MB/s]\n\nDownloading data:  85%|########4 | 5.98G/7.07G [01:39&lt;00:17, 63.8MB/s]\n\nDownloading data:  85%|########4 | 5.98G/7.07G [01:39&lt;00:17, 64.0MB/s]\n\nDownloading data:  85%|########4 | 5.99G/7.07G [01:40&lt;00:16, 64.1MB/s]\n\nDownloading data:  85%|########4 | 6.00G/7.07G [01:40&lt;00:16, 64.7MB/s]\n\nDownloading data:  85%|########4 | 6.00G/7.07G [01:40&lt;00:16, 64.6MB/s]\n\nDownloading data:  85%|########4 | 6.01G/7.07G [01:40&lt;00:16, 64.1MB/s]\n\nDownloading data:  85%|########5 | 6.02G/7.07G [01:40&lt;00:27, 38.4MB/s]\n\nDownloading data:  85%|########5 | 6.02G/7.07G [01:40&lt;00:23, 44.2MB/s]\n\nDownloading data:  85%|########5 | 6.03G/7.07G [01:40&lt;00:23, 43.8MB/s]\n\nDownloading data:  85%|########5 | 6.03G/7.07G [01:40&lt;00:23, 45.0MB/s]\n\nDownloading data:  85%|########5 | 6.04G/7.07G [01:41&lt;00:21, 47.1MB/s]\n\nDownloading data:  85%|########5 | 6.04G/7.07G [01:41&lt;00:21, 47.2MB/s]\n\nDownloading data:  86%|########5 | 6.05G/7.07G [01:41&lt;00:21, 46.7MB/s]\n\nDownloading data:  86%|########5 | 6.05G/7.07G [01:41&lt;00:21, 47.4MB/s]\n\nDownloading data:  86%|########5 | 6.06G/7.07G [01:41&lt;00:21, 47.7MB/s]\n\nDownloading data:  86%|########5 | 6.06G/7.07G [01:41&lt;00:20, 48.5MB/s]\n\nDownloading data:  86%|########5 | 6.07G/7.07G [01:41&lt;00:20, 49.5MB/s]\n\nDownloading data:  86%|########5 | 6.07G/7.07G [01:41&lt;00:19, 49.9MB/s]\n\nDownloading data:  86%|########5 | 6.08G/7.07G [01:41&lt;00:19, 50.4MB/s]\n\nDownloading data:  86%|########6 | 6.08G/7.07G [01:41&lt;00:19, 49.8MB/s]\n\nDownloading data:  86%|########6 | 6.09G/7.07G [01:42&lt;00:19, 50.5MB/s]\n\nDownloading data:  86%|########6 | 6.09G/7.07G [01:42&lt;00:19, 51.1MB/s]\n\nDownloading data:  86%|########6 | 6.10G/7.07G [01:42&lt;00:19, 50.3MB/s]\n\nDownloading data:  86%|########6 | 6.10G/7.07G [01:42&lt;00:19, 50.2MB/s]\n\nDownloading data:  86%|########6 | 6.11G/7.07G [01:42&lt;00:18, 51.5MB/s]\n\nDownloading data:  86%|########6 | 6.12G/7.07G [01:42&lt;00:18, 52.1MB/s]\n\nDownloading data:  87%|########6 | 6.12G/7.07G [01:42&lt;00:18, 52.7MB/s]\n\nDownloading data:  87%|########6 | 6.13G/7.07G [01:42&lt;00:17, 53.4MB/s]\n\nDownloading data:  87%|########6 | 6.13G/7.07G [01:42&lt;00:17, 53.6MB/s]\n\nDownloading data:  87%|########6 | 6.14G/7.07G [01:43&lt;00:17, 54.6MB/s]\n\nDownloading data:  87%|########6 | 6.14G/7.07G [01:43&lt;00:17, 54.2MB/s]\n\nDownloading data:  87%|########6 | 6.15G/7.07G [01:43&lt;00:16, 55.3MB/s]\n\nDownloading data:  87%|########7 | 6.15G/7.07G [01:43&lt;00:16, 55.6MB/s]\n\nDownloading data:  87%|########7 | 6.16G/7.07G [01:43&lt;00:16, 56.4MB/s]\n\nDownloading data:  87%|########7 | 6.17G/7.07G [01:43&lt;00:15, 56.7MB/s]\n\nDownloading data:  87%|########7 | 6.17G/7.07G [01:43&lt;00:16, 55.3MB/s]\n\nDownloading data:  87%|########7 | 6.18G/7.07G [01:43&lt;00:16, 55.8MB/s]\n\nDownloading data:  87%|########7 | 6.18G/7.07G [01:43&lt;00:15, 56.5MB/s]\n\nDownloading data:  88%|########7 | 6.19G/7.07G [01:43&lt;00:15, 57.2MB/s]\n\nDownloading data:  88%|########7 | 6.19G/7.07G [01:44&lt;00:15, 57.1MB/s]\n\nDownloading data:  88%|########7 | 6.20G/7.07G [01:44&lt;00:15, 56.6MB/s]\n\nDownloading data:  88%|########7 | 6.21G/7.07G [01:44&lt;00:15, 57.0MB/s]\n\nDownloading data:  88%|########7 | 6.21G/7.07G [01:44&lt;00:14, 58.6MB/s]\n\nDownloading data:  88%|########7 | 6.22G/7.07G [01:44&lt;00:14, 58.5MB/s]\n\nDownloading data:  88%|########8 | 6.22G/7.07G [01:44&lt;00:14, 59.0MB/s]\n\nDownloading data:  88%|########8 | 6.23G/7.07G [01:44&lt;00:14, 59.0MB/s]\n\nDownloading data:  88%|########8 | 6.24G/7.07G [01:44&lt;00:14, 57.6MB/s]\n\nDownloading data:  88%|########8 | 6.24G/7.07G [01:44&lt;00:14, 58.3MB/s]\n\nDownloading data:  88%|########8 | 6.25G/7.07G [01:44&lt;00:14, 58.8MB/s]\n\nDownloading data:  88%|########8 | 6.25G/7.07G [01:45&lt;00:13, 59.5MB/s]\n\nDownloading data:  89%|########8 | 6.26G/7.07G [01:45&lt;00:13, 60.2MB/s]\n\nDownloading data:  89%|########8 | 6.27G/7.07G [01:45&lt;00:13, 60.2MB/s]\n\nDownloading data:  89%|########8 | 6.27G/7.07G [01:45&lt;00:13, 60.6MB/s]\n\nDownloading data:  89%|########8 | 6.28G/7.07G [01:45&lt;00:13, 60.1MB/s]\n\nDownloading data:  89%|########8 | 6.29G/7.07G [01:45&lt;00:12, 60.6MB/s]\n\nDownloading data:  89%|########8 | 6.29G/7.07G [01:45&lt;00:12, 61.2MB/s]\n\nDownloading data:  89%|########9 | 6.30G/7.07G [01:45&lt;00:12, 60.8MB/s]\n\nDownloading data:  89%|########9 | 6.30G/7.07G [01:45&lt;00:12, 61.3MB/s]\n\nDownloading data:  89%|########9 | 6.31G/7.07G [01:45&lt;00:12, 61.1MB/s]\n\nDownloading data:  89%|########9 | 6.32G/7.07G [01:46&lt;00:12, 61.2MB/s]\n\nDownloading data:  89%|########9 | 6.32G/7.07G [01:46&lt;00:12, 61.2MB/s]\n\nDownloading data:  89%|########9 | 6.33G/7.07G [01:46&lt;00:12, 61.4MB/s]\n\nDownloading data:  90%|########9 | 6.33G/7.07G [01:46&lt;00:12, 60.9MB/s]\n\nDownloading data:  90%|########9 | 6.34G/7.07G [01:46&lt;00:11, 61.7MB/s]\n\nDownloading data:  90%|########9 | 6.35G/7.07G [01:46&lt;00:11, 62.2MB/s]\n\nDownloading data:  90%|########9 | 6.35G/7.07G [01:46&lt;00:11, 62.1MB/s]\n\nDownloading data:  90%|########9 | 6.36G/7.07G [01:46&lt;00:11, 62.0MB/s]\n\nDownloading data:  90%|######### | 6.37G/7.07G [01:46&lt;00:11, 62.4MB/s]\n\nDownloading data:  90%|######### | 6.37G/7.07G [01:46&lt;00:11, 62.4MB/s]\n\nDownloading data:  90%|######### | 6.38G/7.07G [01:47&lt;00:11, 61.1MB/s]\n\nDownloading data:  90%|######### | 6.38G/7.07G [01:47&lt;00:11, 61.2MB/s]\n\nDownloading data:  90%|######### | 6.39G/7.07G [01:47&lt;00:11, 60.9MB/s]\n\nDownloading data:  90%|######### | 6.40G/7.07G [01:47&lt;00:10, 61.8MB/s]\n\nDownloading data:  91%|######### | 6.40G/7.07G [01:47&lt;00:10, 62.0MB/s]\n\nDownloading data:  91%|######### | 6.41G/7.07G [01:47&lt;00:10, 62.4MB/s]\n\nDownloading data:  91%|######### | 6.42G/7.07G [01:47&lt;00:10, 61.8MB/s]\n\nDownloading data:  91%|######### | 6.42G/7.07G [01:47&lt;00:10, 62.2MB/s]\n\nDownloading data:  91%|######### | 6.43G/7.07G [01:47&lt;00:10, 61.7MB/s]\n\nDownloading data:  91%|######### | 6.43G/7.07G [01:47&lt;00:10, 62.4MB/s]\n\nDownloading data:  91%|#########1| 6.44G/7.07G [01:48&lt;00:10, 62.0MB/s]\n\nDownloading data:  91%|#########1| 6.45G/7.07G [01:48&lt;00:10, 61.7MB/s]\n\nDownloading data:  91%|#########1| 6.45G/7.07G [01:48&lt;00:09, 62.6MB/s]\n\nDownloading data:  91%|#########1| 6.46G/7.07G [01:48&lt;00:09, 62.3MB/s]\n\nDownloading data:  91%|#########1| 6.47G/7.07G [01:48&lt;00:09, 62.4MB/s]\n\nDownloading data:  92%|#########1| 6.47G/7.07G [01:48&lt;00:09, 61.5MB/s]\n\nDownloading data:  92%|#########1| 6.48G/7.07G [01:48&lt;00:09, 62.0MB/s]\n\nDownloading data:  92%|#########1| 6.49G/7.07G [01:48&lt;00:09, 61.9MB/s]\n\nDownloading data:  92%|#########1| 6.49G/7.07G [01:48&lt;00:09, 61.7MB/s]\n\nDownloading data:  92%|#########1| 6.50G/7.07G [01:48&lt;00:09, 61.9MB/s]\n\nDownloading data:  92%|#########1| 6.50G/7.07G [01:49&lt;00:09, 61.8MB/s]\n\nDownloading data:  92%|#########2| 6.51G/7.07G [01:49&lt;00:09, 61.8MB/s]\n\nDownloading data:  92%|#########2| 6.52G/7.07G [01:49&lt;00:09, 61.7MB/s]\n\nDownloading data:  92%|#########2| 6.52G/7.07G [01:49&lt;00:08, 61.8MB/s]\n\nDownloading data:  92%|#########2| 6.53G/7.07G [01:49&lt;00:08, 63.1MB/s]\n\nDownloading data:  92%|#########2| 6.54G/7.07G [01:49&lt;00:08, 63.6MB/s]\n\nDownloading data:  93%|#########2| 6.54G/7.07G [01:49&lt;00:08, 64.3MB/s]\n\nDownloading data:  93%|#########2| 6.55G/7.07G [01:49&lt;00:08, 64.3MB/s]\n\nDownloading data:  93%|#########2| 6.56G/7.07G [01:49&lt;00:08, 64.6MB/s]\n\nDownloading data:  93%|#########2| 6.56G/7.07G [01:49&lt;00:07, 64.0MB/s]\n\nDownloading data:  93%|#########2| 6.57G/7.07G [01:50&lt;00:07, 65.1MB/s]\n\nDownloading data:  93%|#########2| 6.57G/7.07G [01:50&lt;00:07, 65.1MB/s]\n\nDownloading data:  93%|#########3| 6.58G/7.07G [01:50&lt;00:07, 65.2MB/s]\n\nDownloading data:  93%|#########3| 6.59G/7.07G [01:50&lt;00:07, 63.4MB/s]\n\nDownloading data:  93%|#########3| 6.60G/7.07G [01:50&lt;00:07, 66.5MB/s]\n\nDownloading data:  93%|#########3| 6.60G/7.07G [01:50&lt;00:07, 67.0MB/s]\n\nDownloading data:  93%|#########3| 6.61G/7.07G [01:50&lt;00:07, 65.3MB/s]\n\nDownloading data:  94%|#########3| 6.62G/7.07G [01:50&lt;00:07, 64.9MB/s]\n\nDownloading data:  94%|#########3| 6.62G/7.07G [01:50&lt;00:06, 65.4MB/s]\n\nDownloading data:  94%|#########3| 6.63G/7.07G [01:51&lt;00:06, 64.9MB/s]\n\nDownloading data:  94%|#########3| 6.64G/7.07G [01:51&lt;00:06, 64.5MB/s]\n\nDownloading data:  94%|#########3| 6.64G/7.07G [01:51&lt;00:06, 64.9MB/s]\n\nDownloading data:  94%|#########4| 6.65G/7.07G [01:51&lt;00:06, 64.8MB/s]\n\nDownloading data:  94%|#########4| 6.65G/7.07G [01:51&lt;00:06, 65.0MB/s]\n\nDownloading data:  94%|#########4| 6.66G/7.07G [01:51&lt;00:06, 65.1MB/s]\n\nDownloading data:  94%|#########4| 6.67G/7.07G [01:51&lt;00:06, 65.2MB/s]\n\nDownloading data:  94%|#########4| 6.67G/7.07G [01:51&lt;00:06, 64.7MB/s]\n\nDownloading data:  94%|#########4| 6.68G/7.07G [01:51&lt;00:06, 56.9MB/s]\n\nDownloading data:  95%|#########4| 6.69G/7.07G [01:51&lt;00:06, 58.1MB/s]\n\nDownloading data:  95%|#########4| 6.69G/7.07G [01:52&lt;00:06, 60.3MB/s]\n\nDownloading data:  95%|#########4| 6.70G/7.07G [01:52&lt;00:06, 61.5MB/s]\n\nDownloading data:  95%|#########4| 6.71G/7.07G [01:52&lt;00:05, 61.9MB/s]\n\nDownloading data:  95%|#########4| 6.71G/7.07G [01:52&lt;00:05, 63.3MB/s]\n\nDownloading data:  95%|#########5| 6.72G/7.07G [01:52&lt;00:05, 63.1MB/s]\n\nDownloading data:  95%|#########5| 6.73G/7.07G [01:52&lt;00:05, 64.4MB/s]\n\nDownloading data:  95%|#########5| 6.73G/7.07G [01:52&lt;00:05, 63.6MB/s]\n\nDownloading data:  95%|#########5| 6.74G/7.07G [01:52&lt;00:05, 64.7MB/s]\n\nDownloading data:  95%|#########5| 6.75G/7.07G [01:52&lt;00:04, 66.2MB/s]\n\nDownloading data:  95%|#########5| 6.75G/7.07G [01:52&lt;00:04, 65.7MB/s]\n\nDownloading data:  96%|#########5| 6.76G/7.07G [01:53&lt;00:04, 65.9MB/s]\n\nDownloading data:  96%|#########5| 6.77G/7.07G [01:53&lt;00:04, 65.7MB/s]\n\nDownloading data:  96%|#########5| 6.77G/7.07G [01:53&lt;00:04, 66.6MB/s]\n\nDownloading data:  96%|#########5| 6.78G/7.07G [01:53&lt;00:04, 65.5MB/s]\n\nDownloading data:  96%|#########5| 6.79G/7.07G [01:53&lt;00:04, 65.5MB/s]\n\nDownloading data:  96%|#########6| 6.79G/7.07G [01:53&lt;00:04, 65.5MB/s]\n\nDownloading data:  96%|#########6| 6.80G/7.07G [01:53&lt;00:04, 65.2MB/s]\n\nDownloading data:  96%|#########6| 6.81G/7.07G [01:53&lt;00:04, 65.3MB/s]\n\nDownloading data:  96%|#########6| 6.81G/7.07G [01:53&lt;00:04, 64.0MB/s]\n\nDownloading data:  96%|#########6| 6.82G/7.07G [01:53&lt;00:03, 63.8MB/s]\n\nDownloading data:  97%|#########6| 6.83G/7.07G [01:54&lt;00:03, 62.9MB/s]\n\nDownloading data:  97%|#########6| 6.83G/7.07G [01:54&lt;00:03, 64.1MB/s]\n\nDownloading data:  97%|#########6| 6.84G/7.07G [01:54&lt;00:03, 66.6MB/s]\n\nDownloading data:  97%|#########6| 6.85G/7.07G [01:54&lt;00:03, 65.9MB/s]\n\nDownloading data:  97%|#########6| 6.85G/7.07G [01:54&lt;00:05, 38.9MB/s]\n\nDownloading data:  97%|#########6| 6.86G/7.07G [01:54&lt;00:04, 44.4MB/s]\n\nDownloading data:  97%|#########7| 6.87G/7.07G [01:54&lt;00:04, 48.7MB/s]\n\nDownloading data:  97%|#########7| 6.87G/7.07G [01:55&lt;00:03, 52.3MB/s]\n\nDownloading data:  97%|#########7| 6.88G/7.07G [01:55&lt;00:03, 55.6MB/s]\n\nDownloading data:  97%|#########7| 6.88G/7.07G [01:55&lt;00:03, 58.4MB/s]\n\nDownloading data:  97%|#########7| 6.89G/7.07G [01:55&lt;00:03, 59.7MB/s]\n\nDownloading data:  98%|#########7| 6.90G/7.07G [01:55&lt;00:02, 61.5MB/s]\n\nDownloading data:  98%|#########7| 6.90G/7.07G [01:55&lt;00:02, 62.6MB/s]\n\nDownloading data:  98%|#########7| 6.91G/7.07G [01:55&lt;00:02, 63.6MB/s]\n\nDownloading data:  98%|#########7| 6.92G/7.07G [01:55&lt;00:02, 64.0MB/s]\n\nDownloading data:  98%|#########7| 6.92G/7.07G [01:55&lt;00:02, 64.1MB/s]\n\nDownloading data:  98%|#########7| 6.93G/7.07G [01:55&lt;00:02, 62.6MB/s]\n\nDownloading data:  98%|#########8| 6.94G/7.07G [01:56&lt;00:02, 66.0MB/s]\n\nDownloading data:  98%|#########8| 6.95G/7.07G [01:56&lt;00:01, 67.0MB/s]\n\nDownloading data:  98%|#########8| 6.95G/7.07G [01:56&lt;00:01, 66.3MB/s]\n\nDownloading data:  98%|#########8| 6.96G/7.07G [01:56&lt;00:01, 64.9MB/s]\n\nDownloading data:  98%|#########8| 6.97G/7.07G [01:56&lt;00:01, 64.5MB/s]\n\nDownloading data:  99%|#########8| 6.97G/7.07G [01:56&lt;00:01, 65.9MB/s]\n\nDownloading data:  99%|#########8| 6.98G/7.07G [01:56&lt;00:01, 56.6MB/s]\n\nDownloading data:  99%|#########8| 6.98G/7.07G [01:56&lt;00:01, 57.1MB/s]\n\nDownloading data:  99%|#########8| 6.99G/7.07G [01:56&lt;00:01, 58.9MB/s]\n\nDownloading data:  99%|#########8| 7.00G/7.07G [01:57&lt;00:01, 61.8MB/s]\n\nDownloading data:  99%|#########9| 7.00G/7.07G [01:57&lt;00:01, 62.6MB/s]\n\nDownloading data:  99%|#########9| 7.01G/7.07G [01:57&lt;00:00, 62.9MB/s]\n\nDownloading data:  99%|#########9| 7.02G/7.07G [01:57&lt;00:00, 64.0MB/s]\n\nDownloading data:  99%|#########9| 7.02G/7.07G [01:57&lt;00:00, 64.7MB/s]\n\nDownloading data:  99%|#########9| 7.03G/7.07G [01:57&lt;00:00, 64.6MB/s]\n\nDownloading data:  99%|#########9| 7.04G/7.07G [01:57&lt;00:00, 64.8MB/s]\n\nDownloading data: 100%|#########9| 7.04G/7.07G [01:57&lt;00:00, 65.8MB/s]\n\nDownloading data: 100%|#########9| 7.05G/7.07G [01:57&lt;00:00, 65.2MB/s]\n\nDownloading data: 100%|#########9| 7.06G/7.07G [01:57&lt;00:00, 65.5MB/s]\n\nDownloading data: 100%|#########9| 7.06G/7.07G [01:58&lt;00:00, 62.6MB/s]\n\nDownloading data: 100%|#########9| 7.07G/7.07G [01:58&lt;00:00, 64.0MB/s]\nDownloading data: 100%|##########| 7.07G/7.07G [01:58&lt;00:00, 59.9MB/s]\n\nDownloading data files:  80%|########  | 4/5 [02:03&lt;00:47, 47.67s/it]\n\nDownloading data:   0%|          | 0.00/970M [00:00&lt;?, ?B/s]\n\nDownloading data:   0%|          | 52.2k/970M [00:00&lt;34:19, 471kB/s]\n\nDownloading data:   0%|          | 191k/970M [00:00&lt;17:29, 925kB/s]\n\nDownloading data:   0%|          | 818k/970M [00:00&lt;05:05, 3.17MB/s]\n\nDownloading data:   0%|          | 3.21M/970M [00:00&lt;01:30, 10.7MB/s]\n\nDownloading data:   1%|          | 8.66M/970M [00:00&lt;00:37, 25.8MB/s]\n\nDownloading data:   2%|1         | 15.0M/970M [00:00&lt;00:25, 38.0MB/s]\n\nDownloading data:   2%|2         | 21.8M/970M [00:00&lt;00:19, 47.7MB/s]\n\nDownloading data:   3%|2         | 28.6M/970M [00:00&lt;00:17, 54.3MB/s]\n\nDownloading data:   4%|3         | 35.1M/970M [00:00&lt;00:16, 57.6MB/s]\n\nDownloading data:   4%|4         | 41.0M/970M [00:01&lt;00:17, 52.0MB/s]\n\nDownloading data:   5%|4         | 46.3M/970M [00:01&lt;00:19, 47.2MB/s]\n\nDownloading data:   5%|5         | 52.3M/970M [00:01&lt;00:18, 50.5MB/s]\n\nDownloading data:   6%|6         | 58.7M/970M [00:01&lt;00:16, 54.2MB/s]\n\nDownloading data:   7%|6         | 64.2M/970M [00:01&lt;00:20, 44.9MB/s]\n\nDownloading data:   7%|7         | 69.3M/970M [00:01&lt;00:19, 46.5MB/s]\n\nDownloading data:   8%|7         | 75.4M/970M [00:01&lt;00:17, 50.3MB/s]\n\nDownloading data:   8%|8         | 80.7M/970M [00:02&lt;00:23, 37.2MB/s]\n\nDownloading data:   9%|8         | 86.3M/970M [00:02&lt;00:21, 41.4MB/s]\n\nDownloading data:   9%|9         | 92.1M/970M [00:02&lt;00:31, 27.8MB/s]\n\nDownloading data:  10%|#         | 98.4M/970M [00:02&lt;00:25, 33.9MB/s]\n\nDownloading data:  11%|#         | 104M/970M [00:02&lt;00:22, 38.7MB/s]\n\nDownloading data:  11%|#1        | 109M/970M [00:02&lt;00:23, 36.0MB/s]\n\nDownloading data:  12%|#1        | 116M/970M [00:02&lt;00:20, 41.2MB/s]\n\nDownloading data:  13%|#2        | 122M/970M [00:03&lt;00:18, 46.8MB/s]\n\nDownloading data:  13%|#3        | 127M/970M [00:03&lt;00:18, 44.8MB/s]\n\nDownloading data:  14%|#3        | 134M/970M [00:03&lt;00:16, 50.3MB/s]\n\nDownloading data:  15%|#4        | 141M/970M [00:03&lt;00:15, 54.3MB/s]\n\nDownloading data:  15%|#5        | 148M/970M [00:03&lt;00:14, 58.5MB/s]\n\nDownloading data:  16%|#5        | 154M/970M [00:03&lt;00:13, 58.6MB/s]\n\nDownloading data:  17%|#6        | 160M/970M [00:03&lt;00:13, 60.4MB/s]\n\nDownloading data:  17%|#7        | 167M/970M [00:03&lt;00:13, 58.5MB/s]\n\nDownloading data:  18%|#7        | 173M/970M [00:03&lt;00:12, 61.3MB/s]\n\nDownloading data:  19%|#8        | 180M/970M [00:04&lt;00:12, 63.4MB/s]\n\nDownloading data:  19%|#9        | 187M/970M [00:04&lt;00:12, 64.2MB/s]\n\nDownloading data:  20%|#9        | 193M/970M [00:04&lt;00:12, 64.1MB/s]\n\nDownloading data:  21%|##        | 200M/970M [00:04&lt;00:11, 64.4MB/s]\n\nDownloading data:  21%|##1       | 206M/970M [00:04&lt;00:12, 63.4MB/s]\n\nDownloading data:  22%|##1       | 213M/970M [00:04&lt;00:11, 63.9MB/s]\n\nDownloading data:  23%|##2       | 219M/970M [00:04&lt;00:11, 64.7MB/s]\n\nDownloading data:  23%|##3       | 226M/970M [00:04&lt;00:11, 64.9MB/s]\n\nDownloading data:  24%|##3       | 233M/970M [00:04&lt;00:11, 65.5MB/s]\n\nDownloading data:  25%|##4       | 239M/970M [00:04&lt;00:12, 60.3MB/s]\n\nDownloading data:  25%|##5       | 246M/970M [00:05&lt;00:11, 61.8MB/s]\n\nDownloading data:  26%|##6       | 252M/970M [00:05&lt;00:11, 62.9MB/s]\n\nDownloading data:  27%|##6       | 259M/970M [00:05&lt;00:11, 63.9MB/s]\n\nDownloading data:  27%|##7       | 266M/970M [00:05&lt;00:10, 64.7MB/s]\n\nDownloading data:  28%|##8       | 272M/970M [00:05&lt;00:10, 65.4MB/s]\n\nDownloading data:  29%|##8       | 279M/970M [00:05&lt;00:10, 65.2MB/s]\n\nDownloading data:  29%|##9       | 285M/970M [00:05&lt;00:10, 65.4MB/s]\n\nDownloading data:  30%|###       | 292M/970M [00:05&lt;00:10, 65.4MB/s]\n\nDownloading data:  31%|###       | 299M/970M [00:05&lt;00:10, 65.7MB/s]\n\nDownloading data:  31%|###1      | 305M/970M [00:05&lt;00:10, 66.1MB/s]\n\nDownloading data:  32%|###2      | 312M/970M [00:06&lt;00:09, 66.2MB/s]\n\nDownloading data:  33%|###2      | 319M/970M [00:06&lt;00:09, 66.2MB/s]\n\nDownloading data:  34%|###3      | 325M/970M [00:06&lt;00:09, 65.8MB/s]\n\nDownloading data:  34%|###4      | 332M/970M [00:06&lt;00:09, 66.0MB/s]\n\nDownloading data:  35%|###4      | 339M/970M [00:06&lt;00:09, 66.9MB/s]\n\nDownloading data:  36%|###5      | 346M/970M [00:06&lt;00:09, 69.2MB/s]\n\nDownloading data:  36%|###6      | 354M/970M [00:06&lt;00:08, 70.6MB/s]\n\nDownloading data:  37%|###7      | 361M/970M [00:06&lt;00:08, 71.6MB/s]\n\nDownloading data:  38%|###7      | 368M/970M [00:06&lt;00:09, 62.7MB/s]\n\nDownloading data:  39%|###8      | 375M/970M [00:06&lt;00:09, 62.9MB/s]\n\nDownloading data:  39%|###9      | 382M/970M [00:07&lt;00:09, 64.4MB/s]\n\nDownloading data:  40%|####      | 388M/970M [00:07&lt;00:08, 64.7MB/s]\n\nDownloading data:  41%|####      | 395M/970M [00:07&lt;00:08, 65.4MB/s]\n\nDownloading data:  41%|####1     | 402M/970M [00:07&lt;00:08, 66.4MB/s]\n\nDownloading data:  42%|####2     | 409M/970M [00:07&lt;00:08, 66.1MB/s]\n\nDownloading data:  43%|####2     | 416M/970M [00:07&lt;00:08, 67.1MB/s]\n\nDownloading data:  44%|####3     | 422M/970M [00:07&lt;00:08, 66.1MB/s]\n\nDownloading data:  44%|####4     | 429M/970M [00:07&lt;00:08, 65.5MB/s]\n\nDownloading data:  45%|####4     | 436M/970M [00:07&lt;00:08, 65.1MB/s]\n\nDownloading data:  46%|####5     | 442M/970M [00:08&lt;00:08, 65.5MB/s]\n\nDownloading data:  46%|####6     | 449M/970M [00:08&lt;00:08, 63.3MB/s]\n\nDownloading data:  47%|####6     | 456M/970M [00:08&lt;00:07, 64.9MB/s]\n\nDownloading data:  48%|####7     | 463M/970M [00:08&lt;00:07, 65.0MB/s]\n\nDownloading data:  48%|####8     | 470M/970M [00:08&lt;00:07, 67.1MB/s]\n\nDownloading data:  49%|####9     | 477M/970M [00:08&lt;00:07, 67.6MB/s]\n\nDownloading data:  50%|####9     | 484M/970M [00:08&lt;00:07, 66.4MB/s]\n\nDownloading data:  51%|#####     | 491M/970M [00:08&lt;00:07, 68.4MB/s]\n\nDownloading data:  51%|#####1    | 498M/970M [00:08&lt;00:08, 58.8MB/s]\n\nDownloading data:  52%|#####1    | 505M/970M [00:09&lt;00:07, 60.7MB/s]\n\nDownloading data:  53%|#####2    | 512M/970M [00:09&lt;00:07, 63.4MB/s]\n\nDownloading data:  53%|#####3    | 518M/970M [00:09&lt;00:07, 62.8MB/s]\n\nDownloading data:  54%|#####4    | 525M/970M [00:09&lt;00:06, 64.4MB/s]\n\nDownloading data:  55%|#####4    | 532M/970M [00:09&lt;00:06, 64.6MB/s]\n\nDownloading data:  55%|#####5    | 538M/970M [00:09&lt;00:06, 64.1MB/s]\n\nDownloading data:  56%|#####6    | 544M/970M [00:09&lt;00:06, 61.0MB/s]\n\nDownloading data:  57%|#####6    | 551M/970M [00:09&lt;00:06, 62.2MB/s]\n\nDownloading data:  58%|#####7    | 558M/970M [00:09&lt;00:06, 64.2MB/s]\n\nDownloading data:  58%|#####8    | 564M/970M [00:09&lt;00:06, 62.9MB/s]\n\nDownloading data:  59%|#####8    | 571M/970M [00:10&lt;00:06, 60.5MB/s]\n\nDownloading data:  60%|#####9    | 578M/970M [00:10&lt;00:06, 62.6MB/s]\n\nDownloading data:  60%|######    | 584M/970M [00:10&lt;00:06, 60.3MB/s]\n\nDownloading data:  61%|######    | 591M/970M [00:10&lt;00:05, 63.6MB/s]\n\nDownloading data:  62%|######1   | 598M/970M [00:10&lt;00:05, 66.0MB/s]\n\nDownloading data:  62%|######2   | 605M/970M [00:10&lt;00:05, 64.9MB/s]\n\nDownloading data:  63%|######3   | 611M/970M [00:10&lt;00:05, 62.6MB/s]\n\nDownloading data:  64%|######3   | 618M/970M [00:10&lt;00:05, 60.8MB/s]\n\nDownloading data:  64%|######4   | 625M/970M [00:10&lt;00:05, 64.1MB/s]\n\nDownloading data:  65%|######5   | 631M/970M [00:11&lt;00:05, 63.6MB/s]\n\nDownloading data:  66%|######5   | 638M/970M [00:11&lt;00:05, 63.7MB/s]\n\nDownloading data:  66%|######6   | 644M/970M [00:11&lt;00:05, 61.5MB/s]\n\nDownloading data:  67%|######7   | 651M/970M [00:11&lt;00:04, 64.4MB/s]\n\nDownloading data:  68%|######7   | 658M/970M [00:11&lt;00:04, 65.3MB/s]\n\nDownloading data:  69%|######8   | 665M/970M [00:11&lt;00:04, 63.0MB/s]\n\nDownloading data:  69%|######9   | 672M/970M [00:11&lt;00:04, 64.9MB/s]\n\nDownloading data:  70%|######9   | 678M/970M [00:11&lt;00:04, 62.5MB/s]\n\nDownloading data:  71%|#######   | 684M/970M [00:11&lt;00:04, 61.2MB/s]\n\nDownloading data:  71%|#######1  | 691M/970M [00:11&lt;00:04, 62.6MB/s]\n\nDownloading data:  72%|#######1  | 698M/970M [00:12&lt;00:04, 64.3MB/s]\n\nDownloading data:  73%|#######2  | 704M/970M [00:12&lt;00:04, 63.8MB/s]\n\nDownloading data:  73%|#######3  | 711M/970M [00:12&lt;00:04, 63.3MB/s]\n\nDownloading data:  74%|#######3  | 717M/970M [00:12&lt;00:03, 63.4MB/s]\n\nDownloading data:  75%|#######4  | 723M/970M [00:12&lt;00:04, 61.6MB/s]\n\nDownloading data:  75%|#######5  | 730M/970M [00:12&lt;00:03, 61.1MB/s]\n\nDownloading data:  76%|#######5  | 736M/970M [00:12&lt;00:05, 45.8MB/s]\n\nDownloading data:  76%|#######6  | 742M/970M [00:12&lt;00:04, 49.0MB/s]\n\nDownloading data:  77%|#######7  | 748M/970M [00:12&lt;00:04, 52.8MB/s]\n\nDownloading data:  78%|#######7  | 754M/970M [00:13&lt;00:03, 55.9MB/s]\n\nDownloading data:  78%|#######8  | 761M/970M [00:13&lt;00:03, 58.1MB/s]\n\nDownloading data:  79%|#######9  | 768M/970M [00:13&lt;00:03, 61.4MB/s]\n\nDownloading data:  80%|#######9  | 774M/970M [00:13&lt;00:03, 61.3MB/s]\n\nDownloading data:  80%|########  | 780M/970M [00:13&lt;00:03, 60.7MB/s]\n\nDownloading data:  81%|########1 | 787M/970M [00:13&lt;00:03, 60.0MB/s]\n\nDownloading data:  82%|########1 | 794M/970M [00:13&lt;00:02, 63.5MB/s]\n\nDownloading data:  82%|########2 | 800M/970M [00:13&lt;00:02, 61.9MB/s]\n\nDownloading data:  83%|########3 | 807M/970M [00:13&lt;00:02, 62.6MB/s]\n\nDownloading data:  84%|########3 | 814M/970M [00:14&lt;00:02, 64.8MB/s]\n\nDownloading data:  85%|########4 | 820M/970M [00:14&lt;00:02, 64.2MB/s]\n\nDownloading data:  85%|########5 | 827M/970M [00:14&lt;00:02, 62.0MB/s]\n\nDownloading data:  86%|########5 | 834M/970M [00:14&lt;00:02, 65.0MB/s]\n\nDownloading data:  87%|########6 | 840M/970M [00:14&lt;00:02, 60.8MB/s]\n\nDownloading data:  87%|########7 | 847M/970M [00:14&lt;00:02, 61.3MB/s]\n\nDownloading data:  88%|########7 | 853M/970M [00:14&lt;00:02, 57.9MB/s]\n\nDownloading data:  89%|########8 | 860M/970M [00:14&lt;00:01, 62.5MB/s]\n\nDownloading data:  89%|########9 | 867M/970M [00:14&lt;00:01, 61.2MB/s]\n\nDownloading data:  90%|######### | 873M/970M [00:14&lt;00:01, 63.1MB/s]\n\nDownloading data:  91%|######### | 880M/970M [00:15&lt;00:01, 65.0MB/s]\n\nDownloading data:  92%|#########1| 888M/970M [00:15&lt;00:01, 68.0MB/s]\n\nDownloading data:  92%|#########2| 895M/970M [00:15&lt;00:01, 67.1MB/s]\n\nDownloading data:  93%|#########2| 902M/970M [00:15&lt;00:01, 67.2MB/s]\n\nDownloading data:  94%|#########3| 908M/970M [00:15&lt;00:00, 64.0MB/s]\n\nDownloading data:  94%|#########4| 915M/970M [00:15&lt;00:01, 54.1MB/s]\n\nDownloading data:  95%|#########4| 920M/970M [00:15&lt;00:01, 47.6MB/s]\n\nDownloading data:  95%|#########5| 926M/970M [00:15&lt;00:00, 48.3MB/s]\n\nDownloading data:  96%|#########6| 932M/970M [00:16&lt;00:00, 48.8MB/s]\n\nDownloading data:  97%|#########6| 937M/970M [00:16&lt;00:00, 50.8MB/s]\n\nDownloading data:  97%|#########7| 942M/970M [00:16&lt;00:00, 50.1MB/s]\n\nDownloading data:  98%|#########7| 947M/970M [00:16&lt;00:00, 49.6MB/s]\n\nDownloading data:  98%|#########8| 953M/970M [00:16&lt;00:00, 49.5MB/s]\n\nDownloading data:  99%|#########8| 958M/970M [00:16&lt;00:00, 49.0MB/s]\n\nDownloading data:  99%|#########9| 963M/970M [00:16&lt;00:00, 50.7MB/s]\n\nDownloading data: 100%|#########9| 968M/970M [00:16&lt;00:00, 51.1MB/s]\nDownloading data: 100%|##########| 970M/970M [00:16&lt;00:00, 57.7MB/s]\n\nDownloading data files: 100%|##########| 5/5 [02:21&lt;00:00, 36.83s/it]\nDownloading data files: 100%|##########| 5/5 [02:21&lt;00:00, 28.22s/it]\n\nExtracting data files:   0%|          | 0/5 [00:00&lt;?, ?it/s]\nExtracting data files:  80%|########  | 4/5 [00:28&lt;00:07,  7.04s/it]\nExtracting data files: 100%|##########| 5/5 [00:32&lt;00:00,  6.20s/it]\nExtracting data files: 100%|##########| 5/5 [00:32&lt;00:00,  6.40s/it]\n\nGenerating train split:   0%|          | 0/34602 [00:00&lt;?, ? examples/s]\nGenerating train split:   0%|          | 1/34602 [00:00&lt;6:28:41,  1.48 examples/s]\nGenerating train split:   1%|1         | 379/34602 [00:00&lt;00:51, 661.87 examples/s]\nGenerating train split:   2%|2         | 768/34602 [00:00&lt;00:25, 1305.96 examples/s]\nGenerating train split:   3%|3         | 1198/34602 [00:01&lt;00:20, 1624.78 examples/s]\nGenerating train split:   5%|4         | 1598/34602 [00:01&lt;00:15, 2116.36 examples/s]\nGenerating train split:   6%|5         | 2000/34602 [00:01&lt;00:13, 2447.35 examples/s]\nGenerating train split:   7%|6         | 2395/34602 [00:01&lt;00:11, 2803.76 examples/s]\nGenerating train split:   8%|8         | 2790/34602 [00:01&lt;00:10, 3091.92 examples/s]\nGenerating train split:  10%|9         | 3300/34602 [00:01&lt;00:09, 3197.93 examples/s]\nGenerating train split:  11%|#         | 3693/34602 [00:01&lt;00:09, 3378.51 examples/s]\nGenerating train split:  12%|#2        | 4216/34602 [00:01&lt;00:08, 3414.68 examples/s]\nGenerating train split:  13%|#3        | 4619/34602 [00:01&lt;00:08, 3566.97 examples/s]\nGenerating train split:  14%|#4        | 5000/34602 [00:02&lt;00:08, 3493.18 examples/s]\nGenerating train split:  16%|#5        | 5394/34602 [00:02&lt;00:08, 3609.95 examples/s]\nGenerating train split:  17%|#6        | 5795/34602 [00:02&lt;00:07, 3716.85 examples/s]\nGenerating train split:  18%|#8        | 6311/34602 [00:02&lt;00:07, 3608.36 examples/s]\nGenerating train split:  19%|#9        | 6701/34602 [00:02&lt;00:07, 3683.15 examples/s]\nGenerating train split:  21%|##        | 7221/34602 [00:02&lt;00:07, 3599.75 examples/s]\nGenerating train split:  22%|##2       | 7624/34602 [00:02&lt;00:07, 3706.21 examples/s]\nGenerating train split:  24%|##3       | 8198/34602 [00:02&lt;00:07, 3633.41 examples/s]\nGenerating train split:  25%|##4       | 8601/34602 [00:03&lt;00:06, 3728.54 examples/s]\nGenerating train split:  26%|##6       | 9000/34602 [00:03&lt;00:07, 3648.25 examples/s]\nGenerating train split:  27%|##7       | 9402/34602 [00:03&lt;00:06, 3743.27 examples/s]\nGenerating train split:  28%|##8       | 9797/34602 [00:03&lt;00:06, 3797.32 examples/s]\nGenerating train split:  30%|##9       | 10324/34602 [00:03&lt;00:06, 3689.01 examples/s]\nGenerating train split:  31%|###       | 10708/34602 [00:03&lt;00:06, 3724.76 examples/s]\nGenerating train split:  32%|###2      | 11217/34602 [00:03&lt;00:06, 3604.20 examples/s]\nGenerating train split:  34%|###3      | 11615/34602 [00:03&lt;00:06, 3696.31 examples/s]\nGenerating train split:  35%|###4      | 11995/34602 [00:03&lt;00:06, 3721.10 examples/s]\nGenerating train split:  36%|###6      | 12484/34602 [00:04&lt;00:06, 3552.63 examples/s]\nGenerating train split:  37%|###7      | 12888/34602 [00:04&lt;00:05, 3676.47 examples/s]\nGenerating train split:  39%|###8      | 13415/34602 [00:04&lt;00:05, 3614.38 examples/s]\nGenerating train split:  40%|###9      | 13824/34602 [00:04&lt;00:05, 3733.15 examples/s]\nGenerating train split:  41%|####1     | 14204/34602 [00:04&lt;00:05, 3628.00 examples/s]\nGenerating train split:  42%|####2     | 14605/34602 [00:04&lt;00:05, 3729.74 examples/s]\nGenerating train split:  43%|####3     | 14996/34602 [00:04&lt;00:05, 3774.74 examples/s]\nGenerating train split:  45%|####4     | 15482/34602 [00:04&lt;00:05, 3570.56 examples/s]\nGenerating train split:  46%|####5     | 15880/34602 [00:05&lt;00:05, 3674.60 examples/s]\nGenerating train split:  47%|####7     | 16420/34602 [00:05&lt;00:04, 3644.19 examples/s]\nGenerating train split:  49%|####8     | 16829/34602 [00:05&lt;00:04, 3754.54 examples/s]\nGenerating train split:  50%|#####     | 17412/34602 [00:05&lt;00:04, 3705.24 examples/s]\nGenerating train split:  51%|#####1    | 17815/34602 [00:05&lt;00:04, 3782.97 examples/s]\nGenerating train split:  53%|#####2    | 18324/34602 [00:05&lt;00:04, 3638.13 examples/s]\nGenerating train split:  54%|#####4    | 18724/34602 [00:05&lt;00:04, 3726.06 examples/s]\nGenerating train split:  55%|#####5    | 19165/34602 [00:05&lt;00:04, 3410.93 examples/s]\nGenerating train split:  57%|#####6    | 19641/34602 [00:06&lt;00:04, 3328.50 examples/s]\nGenerating train split:  58%|#####7    | 20040/34602 [00:06&lt;00:04, 3111.96 examples/s]\nGenerating train split:  59%|#####9    | 20507/34602 [00:06&lt;00:04, 3110.11 examples/s]\nGenerating train split:  61%|######    | 20986/34602 [00:06&lt;00:04, 3132.31 examples/s]\nGenerating train split:  62%|######1   | 21401/34602 [00:06&lt;00:04, 3018.20 examples/s]\nGenerating train split:  63%|######2   | 21732/34602 [00:06&lt;00:04, 3081.14 examples/s]\nGenerating train split:  64%|######4   | 22214/34602 [00:06&lt;00:03, 3120.59 examples/s]\nGenerating train split:  65%|######5   | 22582/34602 [00:07&lt;00:03, 3250.29 examples/s]\nGenerating train split:  66%|######6   | 22919/34602 [00:07&lt;00:03, 3278.15 examples/s]\nGenerating train split:  68%|######7   | 23401/34602 [00:07&lt;00:03, 3254.65 examples/s]\nGenerating train split:  69%|######8   | 23767/34602 [00:07&lt;00:03, 3352.62 examples/s]\nGenerating train split:  70%|#######   | 24252/34602 [00:07&lt;00:03, 3308.31 examples/s]\nGenerating train split:  71%|#######1  | 24637/34602 [00:07&lt;00:02, 3441.39 examples/s]\nGenerating train split:  72%|#######2  | 25000/34602 [00:07&lt;00:02, 3382.84 examples/s]\nGenerating train split:  73%|#######3  | 25379/34602 [00:07&lt;00:02, 3489.01 examples/s]\nGenerating train split:  74%|#######4  | 25761/34602 [00:07&lt;00:02, 3577.25 examples/s]\nGenerating train split:  76%|#######5  | 26288/34602 [00:08&lt;00:02, 3550.82 examples/s]\nGenerating train split:  77%|#######7  | 26687/34602 [00:08&lt;00:02, 3663.77 examples/s]\nGenerating train split:  79%|#######8  | 27214/34602 [00:08&lt;00:02, 3608.48 examples/s]\nGenerating train split:  80%|#######9  | 27617/34602 [00:08&lt;00:01, 3713.01 examples/s]\nGenerating train split:  81%|########  | 28003/34602 [00:08&lt;00:02, 2361.62 examples/s]\nGenerating train split:  82%|########1 | 28366/34602 [00:08&lt;00:02, 2605.37 examples/s]\nGenerating train split:  83%|########3 | 28754/34602 [00:09&lt;00:02, 2879.97 examples/s]\nGenerating train split:  85%|########4 | 29267/34602 [00:09&lt;00:01, 3048.78 examples/s]\nGenerating train split:  86%|########5 | 29658/34602 [00:09&lt;00:01, 3245.88 examples/s]\nGenerating train split:  87%|########7 | 30203/34602 [00:09&lt;00:01, 3315.18 examples/s]\nGenerating train split:  88%|########8 | 30600/34602 [00:09&lt;00:01, 3468.41 examples/s]\nGenerating train split:  90%|########9 | 30988/34602 [00:09&lt;00:01, 3570.03 examples/s]\nGenerating train split:  91%|#########1| 31525/34602 [00:09&lt;00:00, 3570.08 examples/s]\nGenerating train split:  92%|#########2| 31906/34602 [00:09&lt;00:00, 3628.23 examples/s]\nGenerating train split:  94%|#########3| 32359/34602 [00:10&lt;00:00, 3412.88 examples/s]\nGenerating train split:  95%|#########4| 32731/34602 [00:10&lt;00:00, 3485.80 examples/s]\nGenerating train split:  96%|#########5| 33216/34602 [00:10&lt;00:00, 3392.70 examples/s]\nGenerating train split:  97%|#########7| 33603/34602 [00:10&lt;00:00, 3509.93 examples/s]\nGenerating train split:  98%|#########8| 34000/34602 [00:10&lt;00:00, 3467.02 examples/s]\nGenerating train split:  99%|#########9| 34390/34602 [00:10&lt;00:00, 3578.57 examples/s]\n\n\nGenerating validation split:   0%|          | 0/5000 [00:00&lt;?, ? examples/s]\nGenerating validation split:   4%|3         | 199/5000 [00:00&lt;00:02, 1907.76 examples/s]\nGenerating validation split:  12%|#1        | 597/5000 [00:00&lt;00:01, 3101.72 examples/s]\nGenerating validation split:  20%|#9        | 984/5000 [00:00&lt;00:01, 3444.02 examples/s]\nGenerating validation split:  30%|##9       | 1499/5000 [00:00&lt;00:01, 3435.02 examples/s]\nGenerating validation split:  38%|###7      | 1894/5000 [00:00&lt;00:00, 3597.72 examples/s]\nGenerating validation split:  48%|####8     | 2407/5000 [00:00&lt;00:00, 3522.12 examples/s]\nGenerating validation split:  56%|#####6    | 2800/5000 [00:00&lt;00:00, 3634.53 examples/s]\nGenerating validation split:  66%|######6   | 3315/5000 [00:00&lt;00:00, 3553.00 examples/s]\nGenerating validation split:  74%|#######4  | 3705/5000 [00:01&lt;00:00, 3640.83 examples/s]\nGenerating validation split:  84%|########4 | 4217/5000 [00:01&lt;00:00, 3558.38 examples/s]\nGenerating validation split:  92%|#########1| 4585/5000 [00:01&lt;00:00, 3586.99 examples/s]\nGenerating validation split:  99%|#########9| 4958/5000 [00:01&lt;00:00, 3622.42 examples/s]\n\n\nGenerating test split:   0%|          | 0/5734 [00:00&lt;?, ? examples/s]\nGenerating test split:   5%|4         | 263/5734 [00:00&lt;00:02, 2617.06 examples/s]\nGenerating test split:  12%|#1        | 677/5734 [00:00&lt;00:01, 3506.36 examples/s]\nGenerating test split:  21%|##1       | 1210/5734 [00:00&lt;00:01, 3485.67 examples/s]\nGenerating test split:  27%|##7       | 1574/5734 [00:00&lt;00:01, 3534.72 examples/s]\nGenerating test split:  35%|###4      | 1981/5734 [00:00&lt;00:01, 3709.14 examples/s]\nGenerating test split:  44%|####4     | 2529/5734 [00:00&lt;00:00, 3682.74 examples/s]\nGenerating test split:  51%|#####1    | 2947/5734 [00:00&lt;00:00, 3819.82 examples/s]\nGenerating test split:  61%|######    | 3488/5734 [00:00&lt;00:00, 3737.10 examples/s]\nGenerating test split:  68%|######7   | 3896/5734 [00:01&lt;00:00, 3824.52 examples/s]\nGenerating test split:  77%|#######7  | 4418/5734 [00:01&lt;00:00, 3698.92 examples/s]\nGenerating test split:  84%|########4 | 4822/5734 [00:01&lt;00:00, 3783.61 examples/s]\nGenerating test split:  94%|#########4| 5404/5734 [00:01&lt;00:00, 3712.76 examples/s]\n\nDataset textvqa downloaded and prepared to /var/lib/jenkins/.cache/huggingface/datasets/textvqa/textvqa/0.5.1/9b89037cc122c3b495b155a1bce4170851829843454e88f236bb8715d977c027. Subsequent calls will reuse this data.\n\n  0%|          | 0/3 [00:00&lt;?, ?it/s]\n100%|##########| 3/3 [00:00&lt;00:00, 260.09it/s]",
            "code"
        ],
        [
            "Lets display a sample entry from the dataset:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "code"
        ],
        [
            "Question:  what year is shown in the photo?\nAnswers:  ['2011', '2011', '2011', '2011', '2011', '2011', '2011', '2011', '2011', '2011']",
            "code"
        ],
        [
            "3. Next, we write the transform function to convert the image and text into\nTensors consumable by our model - For images, we use the transforms from\ntorchvision to convert to Tensor and resize to uniform sizes - For text,\nwe tokenize (and pad) them using the BertTokenizer from HuggingFace -\nFor answers (i.e. labels), we take the most frequently occuring answer\nas the label to train with:",
            "markdown"
        ],
        [
            "import torch\nfrom torchvision import transforms\nfrom collections import defaultdict\nfrom transformers import BertTokenizer\nfrom functools import partial\n\ndef transform(tokenizer, input):\n  batch = {}\n  image_transform = ([(), ([224,224])])\n  image = image_transform(input[\"image\"][0].convert(\"RGB\"))\n  batch[\"image\"] = [image]\n\n  tokenized=tokenizer(input[\"question\"],return_tensors='pt',padding=\"max_length\",max_length=512)\n  batch.update(tokenized)\n\n\n  ans_to_count = defaultdict(int)\n  for ans in input[\"answers\"][0]:\n    ans_to_count[ans] += 1\n  max_value = max(ans_to_count, key=ans_to_count.get)\n  ans_idx = answer_to_idx.get(max_value,0)\n  batch[\"answers\"] = ([ans_idx])\n  return batch\n\ntokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\",padding=\"max_length\",max_length=512)\ntransform=partial(transform,tokenizer)\ndataset.set_transform(transform)",
            "code"
        ],
        [
            "Downloading (\u2026)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]\nDownloading (\u2026)solve/main/vocab.txt: 100%|##########| 232k/232k [00:00&lt;00:00, 2.68MB/s]\n\nDownloading (\u2026)okenizer_config.json:   0%|          | 0.00/28.0 [00:00&lt;?, ?B/s]\nDownloading (\u2026)okenizer_config.json: 100%|##########| 28.0/28.0 [00:00&lt;00:00, 22.7kB/s]\n\nDownloading (\u2026)lve/main/config.json:   0%|          | 0.00/570 [00:00&lt;?, ?B/s]\nDownloading (\u2026)lve/main/config.json: 100%|##########| 570/570 [00:00&lt;00:00, 623kB/s]",
            "code"
        ],
        [
            "4. Finally, we import the flava_model_for_classification from\ntorchmultimodal. It loads the pretrained flava checkpoint by default and\nincludes a classification head.",
            "markdown"
        ],
        [
            "The model forward function passes the image through the visual encoder\nand the question through the text encoder. The image and question\nembeddings are then passed through the multimodal encoder. The final\nembedding corresponding to the CLS token is passed through a MLP head\nwhich finally gives the probability distribution over each possible\nanswers.",
            "markdown"
        ],
        [
            "from torchmultimodal.models.flava.model import flava_model_for_classification\nmodel = flava_model_for_classification(num_classes=len(vocab))",
            "code"
        ],
        [
            "flava_for_pretraining_unified_text_encoder.pt: 0.00B [00:00, ?B/s]\nflava_for_pretraining_unified_text_encoder.pt:   0%|          | 8.19k/1.43G [00:00&lt;13:26:06, 29.6kB/s]\nflava_for_pretraining_unified_text_encoder.pt:   0%|          | 5.50M/1.43G [00:00&lt;01:16, 18.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   1%|          | 11.0M/1.43G [00:00&lt;00:46, 30.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   1%|1         | 14.8M/1.43G [00:00&lt;00:43, 32.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   1%|1         | 18.4M/1.43G [00:00&lt;00:41, 33.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   2%|1         | 22.6M/1.43G [00:00&lt;00:39, 36.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   2%|1         | 25.8M/1.43G [00:00&lt;00:40, 34.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   2%|2         | 30.3M/1.43G [00:00&lt;00:37, 37.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   3%|2         | 35.8M/1.43G [00:01&lt;00:32, 42.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   3%|2         | 40.5M/1.43G [00:01&lt;00:31, 44.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   3%|3         | 45.1M/1.43G [00:01&lt;00:30, 44.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   4%|3         | 50.6M/1.43G [00:01&lt;00:28, 47.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   4%|3         | 54.2M/1.43G [00:01&lt;00:31, 44.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   4%|4         | 59.7M/1.43G [00:01&lt;00:28, 47.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   5%|4         | 65.6M/1.43G [00:01&lt;00:26, 50.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   5%|4         | 70.7M/1.43G [00:01&lt;00:27, 50.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   5%|5         | 75.1M/1.43G [00:01&lt;00:28, 48.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   6%|5         | 79.4M/1.43G [00:01&lt;00:29, 46.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   6%|5         | 83.9M/1.43G [00:02&lt;00:29, 46.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   6%|6         | 87.3M/1.43G [00:02&lt;00:31, 42.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   6%|6         | 92.0M/1.43G [00:02&lt;00:35, 38.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   7%|6         | 96.8M/1.43G [00:02&lt;00:34, 38.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   7%|7         | 100M/1.43G [00:02&lt;00:35, 37.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   7%|7         | 105M/1.43G [00:02&lt;00:33, 40.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   8%|7         | 111M/1.43G [00:02&lt;00:28, 46.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   8%|8         | 115M/1.43G [00:02&lt;00:29, 45.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   8%|8         | 121M/1.43G [00:02&lt;00:27, 47.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   9%|8         | 125M/1.43G [00:03&lt;00:29, 44.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   9%|9         | 131M/1.43G [00:03&lt;00:26, 49.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   9%|9         | 135M/1.43G [00:03&lt;00:27, 46.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  10%|9         | 138M/1.43G [00:03&lt;00:31, 40.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  10%|#         | 144M/1.43G [00:03&lt;00:28, 44.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  10%|#         | 149M/1.43G [00:03&lt;00:28, 45.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  11%|#         | 152M/1.43G [00:03&lt;00:29, 43.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  11%|#         | 157M/1.43G [00:03&lt;00:29, 43.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  11%|#1        | 162M/1.43G [00:03&lt;00:27, 45.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  12%|#1        | 167M/1.43G [00:03&lt;00:26, 48.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  12%|#2        | 173M/1.43G [00:04&lt;00:24, 51.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  12%|#2        | 177M/1.43G [00:04&lt;00:25, 48.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  13%|#2        | 182M/1.43G [00:04&lt;00:26, 47.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  13%|#2        | 185M/1.43G [00:04&lt;00:28, 43.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  13%|#3        | 187M/1.43G [00:04&lt;00:34, 36.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  13%|#3        | 192M/1.43G [00:04&lt;00:31, 39.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  14%|#3        | 195M/1.43G [00:04&lt;00:33, 36.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  14%|#4        | 202M/1.43G [00:04&lt;00:27, 45.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  14%|#4        | 207M/1.43G [00:04&lt;00:25, 48.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  15%|#4        | 213M/1.43G [00:05&lt;00:24, 49.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  15%|#5        | 218M/1.43G [00:05&lt;00:23, 50.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  16%|#5        | 223M/1.43G [00:05&lt;00:24, 49.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  16%|#5        | 227M/1.43G [00:05&lt;00:28, 42.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  16%|#6        | 232M/1.43G [00:05&lt;00:27, 44.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  17%|#6        | 237M/1.43G [00:05&lt;00:26, 45.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  17%|#6        | 242M/1.43G [00:05&lt;00:24, 48.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  17%|#7        | 247M/1.43G [00:05&lt;00:25, 47.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  18%|#7        | 253M/1.43G [00:05&lt;00:23, 50.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  18%|#8        | 259M/1.43G [00:05&lt;00:21, 53.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  19%|#8        | 265M/1.43G [00:06&lt;00:21, 55.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  19%|#8        | 269M/1.43G [00:06&lt;00:23, 50.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  19%|#9        | 273M/1.43G [00:06&lt;00:24, 47.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  20%|#9        | 279M/1.43G [00:06&lt;00:22, 51.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  20%|#9        | 286M/1.43G [00:06&lt;00:20, 56.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  20%|##        | 290M/1.43G [00:06&lt;00:21, 52.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  21%|##        | 295M/1.43G [00:06&lt;00:21, 52.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  21%|##        | 299M/1.43G [00:06&lt;00:23, 47.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  21%|##1       | 304M/1.43G [00:06&lt;00:23, 48.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  22%|##1       | 310M/1.43G [00:06&lt;00:22, 50.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  22%|##1       | 311M/1.43G [00:07&lt;00:29, 38.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  22%|##1       | 312M/1.43G [00:07&lt;00:37, 30.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  22%|##1       | 315M/1.43G [00:07&lt;00:38, 29.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  22%|##2       | 320M/1.43G [00:07&lt;00:30, 36.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  23%|##2       | 326M/1.43G [00:07&lt;00:26, 42.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  23%|##3       | 331M/1.43G [00:07&lt;00:24, 44.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  23%|##3       | 336M/1.43G [00:07&lt;00:23, 47.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  24%|##3       | 341M/1.43G [00:07&lt;00:22, 48.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  24%|##4       | 346M/1.43G [00:07&lt;00:22, 47.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  25%|##4       | 351M/1.43G [00:07&lt;00:22, 48.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  25%|##4       | 356M/1.43G [00:08&lt;00:22, 47.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  25%|##5       | 361M/1.43G [00:08&lt;00:21, 50.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  26%|##5       | 367M/1.43G [00:08&lt;00:19, 54.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  26%|##6       | 373M/1.43G [00:08&lt;00:19, 53.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  26%|##6       | 378M/1.43G [00:08&lt;00:19, 52.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  27%|##6       | 381M/1.43G [00:08&lt;00:22, 46.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  27%|##7       | 387M/1.43G [00:08&lt;00:21, 48.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  27%|##7       | 391M/1.43G [00:08&lt;00:21, 47.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  28%|##7       | 397M/1.43G [00:08&lt;00:20, 51.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  28%|##8       | 402M/1.43G [00:08&lt;00:20, 50.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  28%|##8       | 408M/1.43G [00:09&lt;00:19, 52.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  29%|##8       | 413M/1.43G [00:09&lt;00:19, 53.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  29%|##9       | 419M/1.43G [00:09&lt;00:18, 55.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  30%|##9       | 425M/1.43G [00:09&lt;00:18, 54.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  30%|###       | 429M/1.43G [00:09&lt;00:19, 51.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  31%|###       | 437M/1.43G [00:09&lt;00:16, 59.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  31%|###       | 443M/1.43G [00:09&lt;00:16, 58.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  31%|###1      | 448M/1.43G [00:09&lt;00:17, 57.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  32%|###1      | 453M/1.43G [00:09&lt;00:18, 53.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  32%|###2      | 459M/1.43G [00:09&lt;00:17, 54.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  32%|###2      | 464M/1.43G [00:10&lt;00:17, 54.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  33%|###2      | 467M/1.43G [00:10&lt;00:20, 45.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  33%|###3      | 473M/1.43G [00:10&lt;00:18, 50.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  33%|###3      | 478M/1.43G [00:10&lt;00:18, 50.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  34%|###3      | 484M/1.43G [00:10&lt;00:18, 51.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  34%|###4      | 488M/1.43G [00:10&lt;00:18, 49.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  34%|###4      | 492M/1.43G [00:10&lt;00:22, 42.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  35%|###4      | 495M/1.43G [00:10&lt;00:23, 39.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  35%|###4      | 500M/1.43G [00:10&lt;00:21, 42.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  35%|###5      | 505M/1.43G [00:11&lt;00:20, 46.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  36%|###5      | 509M/1.43G [00:11&lt;00:21, 42.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  36%|###5      | 514M/1.43G [00:11&lt;00:19, 46.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  36%|###6      | 520M/1.43G [00:11&lt;00:17, 50.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  37%|###6      | 525M/1.43G [00:11&lt;00:18, 49.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  37%|###6      | 529M/1.43G [00:11&lt;00:20, 43.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  37%|###7      | 534M/1.43G [00:11&lt;00:19, 45.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  38%|###7      | 540M/1.43G [00:11&lt;00:17, 50.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  38%|###8      | 545M/1.43G [00:11&lt;00:18, 47.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  38%|###8      | 548M/1.43G [00:11&lt;00:19, 44.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  39%|###8      | 553M/1.43G [00:12&lt;00:18, 46.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  39%|###9      | 560M/1.43G [00:12&lt;00:16, 52.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  40%|###9      | 568M/1.43G [00:12&lt;00:14, 60.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  40%|####      | 575M/1.43G [00:12&lt;00:13, 63.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  41%|####      | 581M/1.43G [00:12&lt;00:13, 62.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  41%|####1     | 587M/1.43G [00:12&lt;00:13, 62.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  42%|####1     | 594M/1.43G [00:12&lt;00:12, 64.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  42%|####2     | 601M/1.43G [00:12&lt;00:12, 66.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  42%|####2     | 608M/1.43G [00:12&lt;00:12, 63.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  43%|####2     | 612M/1.43G [00:12&lt;00:14, 54.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  43%|####2     | 615M/1.43G [00:13&lt;00:16, 48.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  43%|####3     | 620M/1.43G [00:13&lt;00:16, 48.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  44%|####3     | 625M/1.43G [00:13&lt;00:16, 49.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  44%|####4     | 630M/1.43G [00:13&lt;00:16, 48.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  44%|####4     | 633M/1.43G [00:13&lt;00:17, 44.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  45%|####4     | 641M/1.43G [00:13&lt;00:14, 53.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  45%|####5     | 647M/1.43G [00:13&lt;00:13, 57.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  46%|####5     | 654M/1.43G [00:13&lt;00:13, 59.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  46%|####6     | 661M/1.43G [00:13&lt;00:12, 63.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  46%|####6     | 665M/1.43G [00:13&lt;00:13, 56.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  47%|####6     | 672M/1.43G [00:14&lt;00:12, 61.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  47%|####7     | 679M/1.43G [00:14&lt;00:12, 62.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  48%|####7     | 685M/1.43G [00:14&lt;00:12, 61.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  48%|####8     | 691M/1.43G [00:14&lt;00:12, 60.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  49%|####8     | 697M/1.43G [00:14&lt;00:12, 60.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  49%|####9     | 704M/1.43G [00:14&lt;00:11, 63.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  50%|####9     | 711M/1.43G [00:14&lt;00:11, 64.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  50%|#####     | 717M/1.43G [00:14&lt;00:11, 64.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  51%|#####     | 723M/1.43G [00:14&lt;00:11, 63.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  51%|#####     | 730M/1.43G [00:14&lt;00:11, 62.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  51%|#####1    | 735M/1.43G [00:15&lt;00:12, 57.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  52%|#####1    | 740M/1.43G [00:15&lt;00:12, 53.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  52%|#####2    | 746M/1.43G [00:15&lt;00:11, 57.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  53%|#####2    | 752M/1.43G [00:15&lt;00:11, 58.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  53%|#####3    | 759M/1.43G [00:15&lt;00:11, 59.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  53%|#####3    | 763M/1.43G [00:15&lt;00:12, 55.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  54%|#####3    | 768M/1.43G [00:15&lt;00:12, 52.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  54%|#####4    | 773M/1.43G [00:15&lt;00:12, 52.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  54%|#####4    | 779M/1.43G [00:15&lt;00:12, 53.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  55%|#####4    | 781M/1.43G [00:15&lt;00:15, 43.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  55%|#####4    | 786M/1.43G [00:16&lt;00:13, 46.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  55%|#####5    | 789M/1.43G [00:16&lt;00:17, 37.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  56%|#####5    | 795M/1.43G [00:16&lt;00:15, 41.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  56%|#####5    | 801M/1.43G [00:16&lt;00:13, 46.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  56%|#####6    | 806M/1.43G [00:16&lt;00:12, 49.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  57%|#####6    | 812M/1.43G [00:16&lt;00:11, 52.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  57%|#####7    | 818M/1.43G [00:16&lt;00:11, 54.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  58%|#####7    | 823M/1.43G [00:16&lt;00:11, 53.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  58%|#####7    | 829M/1.43G [00:16&lt;00:10, 55.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  58%|#####8    | 835M/1.43G [00:17&lt;00:10, 55.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  59%|#####8    | 839M/1.43G [00:17&lt;00:11, 50.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  59%|#####8    | 842M/1.43G [00:17&lt;00:13, 44.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  59%|#####9    | 846M/1.43G [00:17&lt;00:13, 42.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  59%|#####9    | 851M/1.43G [00:17&lt;00:13, 43.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  60%|#####9    | 855M/1.43G [00:17&lt;00:12, 44.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  60%|######    | 862M/1.43G [00:17&lt;00:11, 50.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  61%|######    | 868M/1.43G [00:17&lt;00:10, 53.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  61%|######1   | 874M/1.43G [00:17&lt;00:09, 56.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  62%|######1   | 880M/1.43G [00:17&lt;00:09, 56.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  62%|######1   | 883M/1.43G [00:18&lt;00:11, 48.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  62%|######2   | 887M/1.43G [00:18&lt;00:11, 46.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  62%|######2   | 893M/1.43G [00:18&lt;00:10, 49.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  63%|######2   | 896M/1.43G [00:18&lt;00:11, 44.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  63%|######2   | 901M/1.43G [00:18&lt;00:11, 45.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  63%|######3   | 907M/1.43G [00:18&lt;00:10, 48.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  64%|######3   | 913M/1.43G [00:18&lt;00:09, 52.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  64%|######4   | 918M/1.43G [00:18&lt;00:09, 52.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  64%|######4   | 920M/1.43G [00:18&lt;00:12, 42.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  65%|######4   | 924M/1.43G [00:18&lt;00:11, 42.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  65%|######4   | 930M/1.43G [00:19&lt;00:11, 45.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  65%|######5   | 933M/1.43G [00:19&lt;00:11, 43.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  66%|######5   | 938M/1.43G [00:19&lt;00:11, 43.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  66%|######5   | 943M/1.43G [00:19&lt;00:10, 44.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  66%|######6   | 946M/1.43G [00:19&lt;00:12, 39.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  66%|######6   | 949M/1.43G [00:19&lt;00:13, 36.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  67%|######6   | 953M/1.43G [00:19&lt;00:11, 39.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  67%|######6   | 958M/1.43G [00:19&lt;00:11, 41.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  67%|######7   | 963M/1.43G [00:19&lt;00:10, 45.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  68%|######7   | 968M/1.43G [00:19&lt;00:09, 46.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  68%|######7   | 972M/1.43G [00:20&lt;00:10, 42.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  68%|######8   | 978M/1.43G [00:20&lt;00:09, 47.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  69%|######8   | 983M/1.43G [00:20&lt;00:09, 47.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  69%|######9   | 988M/1.43G [00:20&lt;00:09, 48.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  69%|######9   | 993M/1.43G [00:20&lt;00:09, 48.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  70%|######9   | 997M/1.43G [00:20&lt;00:09, 45.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  70%|######9   | 1.00G/1.43G [00:20&lt;00:10, 42.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  70%|#######   | 1.00G/1.43G [00:20&lt;00:11, 38.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  70%|#######   | 1.01G/1.43G [00:20&lt;00:10, 39.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  71%|#######   | 1.01G/1.43G [00:20&lt;00:08, 48.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  71%|#######1  | 1.02G/1.43G [00:21&lt;00:07, 51.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  72%|#######1  | 1.03G/1.43G [00:21&lt;00:07, 51.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  72%|#######2  | 1.03G/1.43G [00:21&lt;00:07, 54.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  72%|#######2  | 1.04G/1.43G [00:21&lt;00:07, 50.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  73%|#######2  | 1.04G/1.43G [00:21&lt;00:07, 51.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  73%|#######3  | 1.05G/1.43G [00:21&lt;00:07, 52.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  73%|#######3  | 1.05G/1.43G [00:21&lt;00:07, 47.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  74%|#######3  | 1.06G/1.43G [00:21&lt;00:07, 47.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  74%|#######4  | 1.06G/1.43G [00:21&lt;00:07, 48.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  74%|#######4  | 1.07G/1.43G [00:22&lt;00:07, 48.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  75%|#######4  | 1.07G/1.43G [00:22&lt;00:08, 43.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  75%|#######4  | 1.07G/1.43G [00:22&lt;00:11, 31.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  75%|#######5  | 1.08G/1.43G [00:22&lt;00:09, 39.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  75%|#######5  | 1.08G/1.43G [00:22&lt;00:11, 30.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  75%|#######5  | 1.08G/1.43G [00:22&lt;00:15, 22.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  76%|#######5  | 1.08G/1.43G [00:22&lt;00:12, 28.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  76%|#######6  | 1.09G/1.43G [00:22&lt;00:09, 35.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  76%|#######6  | 1.09G/1.43G [00:22&lt;00:08, 39.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  77%|#######6  | 1.10G/1.43G [00:23&lt;00:08, 40.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  77%|#######6  | 1.10G/1.43G [00:23&lt;00:08, 38.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  77%|#######7  | 1.10G/1.43G [00:23&lt;00:09, 35.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  78%|#######7  | 1.11G/1.43G [00:23&lt;00:08, 39.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  78%|#######7  | 1.11G/1.43G [00:23&lt;00:07, 44.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  78%|#######8  | 1.12G/1.43G [00:23&lt;00:06, 45.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  78%|#######8  | 1.12G/1.43G [00:23&lt;00:07, 38.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  79%|#######8  | 1.13G/1.43G [00:23&lt;00:07, 38.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  79%|#######9  | 1.13G/1.43G [00:23&lt;00:06, 43.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  80%|#######9  | 1.14G/1.43G [00:23&lt;00:06, 47.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  80%|#######9  | 1.14G/1.43G [00:24&lt;00:05, 50.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  80%|########  | 1.15G/1.43G [00:24&lt;00:05, 50.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  81%|########  | 1.15G/1.43G [00:24&lt;00:05, 49.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  81%|########  | 1.16G/1.43G [00:24&lt;00:05, 48.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  81%|########1 | 1.16G/1.43G [00:24&lt;00:05, 51.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  82%|########1 | 1.17G/1.43G [00:24&lt;00:05, 51.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  82%|########2 | 1.17G/1.43G [00:24&lt;00:04, 52.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  82%|########2 | 1.18G/1.43G [00:24&lt;00:04, 51.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  83%|########2 | 1.18G/1.43G [00:24&lt;00:05, 48.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  83%|########2 | 1.19G/1.43G [00:24&lt;00:06, 38.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  83%|########3 | 1.19G/1.43G [00:25&lt;00:06, 39.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  83%|########3 | 1.19G/1.43G [00:25&lt;00:06, 38.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  84%|########3 | 1.20G/1.43G [00:25&lt;00:06, 37.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  84%|########3 | 1.20G/1.43G [00:25&lt;00:06, 34.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  84%|########4 | 1.20G/1.43G [00:25&lt;00:06, 33.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  84%|########4 | 1.21G/1.43G [00:25&lt;00:06, 32.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  85%|########4 | 1.21G/1.43G [00:25&lt;00:06, 32.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  85%|########4 | 1.21G/1.43G [00:25&lt;00:06, 35.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  85%|########5 | 1.22G/1.43G [00:25&lt;00:05, 37.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  85%|########5 | 1.22G/1.43G [00:25&lt;00:05, 37.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  86%|########5 | 1.23G/1.43G [00:26&lt;00:05, 40.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  86%|########5 | 1.23G/1.43G [00:26&lt;00:05, 34.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  86%|########6 | 1.23G/1.43G [00:26&lt;00:04, 42.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  87%|########6 | 1.24G/1.43G [00:26&lt;00:04, 45.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  87%|########7 | 1.25G/1.43G [00:26&lt;00:03, 48.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  87%|########7 | 1.25G/1.43G [00:26&lt;00:03, 47.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  88%|########7 | 1.25G/1.43G [00:26&lt;00:03, 46.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  88%|########8 | 1.26G/1.43G [00:26&lt;00:03, 49.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  88%|########8 | 1.26G/1.43G [00:26&lt;00:04, 40.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  89%|########8 | 1.27G/1.43G [00:26&lt;00:03, 43.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  89%|########8 | 1.27G/1.43G [00:27&lt;00:03, 42.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  89%|########9 | 1.28G/1.43G [00:27&lt;00:03, 44.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  90%|########9 | 1.28G/1.43G [00:27&lt;00:03, 49.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  90%|########9 | 1.29G/1.43G [00:27&lt;00:02, 48.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  90%|######### | 1.29G/1.43G [00:27&lt;00:02, 46.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  91%|######### | 1.30G/1.43G [00:27&lt;00:02, 52.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  91%|#########1| 1.30G/1.43G [00:27&lt;00:02, 52.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  92%|#########1| 1.31G/1.43G [00:27&lt;00:02, 56.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  92%|#########1| 1.32G/1.43G [00:27&lt;00:01, 58.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  92%|#########2| 1.32G/1.43G [00:27&lt;00:02, 53.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  93%|#########2| 1.33G/1.43G [00:28&lt;00:01, 53.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  93%|#########3| 1.33G/1.43G [00:28&lt;00:01, 53.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  93%|#########3| 1.34G/1.43G [00:28&lt;00:01, 52.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  94%|#########3| 1.34G/1.43G [00:28&lt;00:01, 54.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  94%|#########4| 1.35G/1.43G [00:28&lt;00:01, 49.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  94%|#########4| 1.35G/1.43G [00:28&lt;00:01, 47.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  95%|#########4| 1.36G/1.43G [00:28&lt;00:01, 51.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  95%|#########5| 1.36G/1.43G [00:28&lt;00:01, 56.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  96%|#########5| 1.37G/1.43G [00:28&lt;00:01, 55.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  96%|#########5| 1.37G/1.43G [00:28&lt;00:01, 48.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  96%|#########6| 1.37G/1.43G [00:29&lt;00:01, 39.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  96%|#########6| 1.38G/1.43G [00:29&lt;00:01, 42.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  97%|#########6| 1.38G/1.43G [00:29&lt;00:01, 41.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  97%|#########6| 1.39G/1.43G [00:29&lt;00:01, 42.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  97%|#########7| 1.39G/1.43G [00:29&lt;00:00, 43.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  98%|#########7| 1.40G/1.43G [00:29&lt;00:00, 49.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  98%|#########8| 1.40G/1.43G [00:29&lt;00:00, 47.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  98%|#########8| 1.41G/1.43G [00:29&lt;00:00, 51.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  99%|#########8| 1.41G/1.43G [00:29&lt;00:00, 49.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  99%|#########9| 1.42G/1.43G [00:30&lt;00:00, 50.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  99%|#########9| 1.42G/1.43G [00:30&lt;00:00, 45.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt: 100%|#########9| 1.43G/1.43G [00:30&lt;00:00, 40.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt: 100%|#########9| 1.43G/1.43G [00:30&lt;00:00, 35.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt: 100%|#########9| 1.43G/1.43G [00:30&lt;00:00, 23.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt: 1.43GB [00:30, 46.9MB/s]",
            "code"
        ],
        [
            "5. We put together the dataset and model in a toy training loop to\ndemonstrate how to train the model for 3 iterations:",
            "markdown"
        ],
        [
            "from torch import nn\nBATCH_SIZE = 2\nMAX_STEPS = 3\nfrom torch.utils.data import \n\n = (dataset[\"train\"], batch_size= BATCH_SIZE)\n = (())\n\n\nepochs = 1\nfor _ in range(epochs):\n  for idx, batch in enumerate():\n    ()\n    out = model(text = batch[\"input_ids\"], image = batch[\"image\"], labels = batch[\"answers\"])\n     = \n    ()\n    ()\n    print(f\"Loss at step {idx} = {}\")\n    if idx &gt; MAX_STEPS-1:\n      break",
            "code"
        ],
        [
            "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning:\n\nThe default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n\nLoss at step 0 = 8.314010620117188\nLoss at step 1 = 8.259458541870117\nLoss at step 2 = 8.271486282348633\nLoss at step 3 = 8.26636791229248",
            "code"
        ]
    ],
    "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Conclusion": [
        [
            "This tutorial introduced the basics around how to finetune on a\nmultimodal task using FLAVA from TorchMultimodal. Please also check out\nother examples from the library like\n\nwhich is a multimodal model for object detection and\n\nwhich is multitask model spanning image, video and 3d classification.",
            "markdown"
        ],
        [
            "<strong>Total running time of the script:</strong> ( 4 minutes  12.837 seconds)",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Prerequisites": [
        [
            "Before reading this tutorial, you should know a bit of Python. If you would like to refresh your memory, take a look at the .",
            "markdown"
        ],
        [
            "If you want to be able to run the examples in this tutorial, you should also have  and  installed on your computer.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Learner profile": [
        [
            "This tutorial is for people who have a basic understanding of linear algebra and arrays in NumPy and want to understand how n-dimensional (\\(n&gt;=2\\)) arrays are represented and can be manipulated. In particular, if you don\u2019t know how to apply common functions to n-dimensional arrays (without using for-loops), or if you want to understand axis and shape properties for n-dimensional arrays, this tutorial might be of help.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Learning Objectives": [
        [
            "After this tutorial, you should be able to:",
            "markdown"
        ],
        [
            "Understand the difference between one-, two- and n-dimensional arrays in NumPy;",
            "markdown"
        ],
        [
            "Understand how to apply some linear algebra operations to n-dimensional arrays without using for-loops;",
            "markdown"
        ],
        [
            "Understand axis and shape properties for n-dimensional arrays.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content": [
        [
            "In this tutorial, we will use a  from linear algebra, the Singular Value Decomposition, to generate a compressed approximation of an image. We\u2019ll use the face image from the  module:",
            "markdown"
        ],
        [
            "from scipy import misc\n\nimg = misc.face()",
            "code"
        ],
        [
            "/tmp/ipykernel_399/2202046956.py:3: DeprecationWarning: scipy.misc.face has been deprecated in SciPy v1.10.0; and will be completely removed in SciPy v1.12.0. Dataset methods have moved into the scipy.datasets module. Use scipy.datasets.face instead.\n  img = misc.face()",
            "code"
        ],
        [
            "<strong>Note</strong>: If you prefer, you can use your own image as you work through this tutorial. In order to transform your image into a NumPy array that can be manipulated, you can use the imread function from the  submodule. Alternatively, you can use the  function from the imageio library. Be aware that if you use your own image, you\u2019ll likely need to adapt the steps below. For more information on how images are treated when converted to NumPy arrays, see  from the scikit-image documentation.",
            "markdown"
        ],
        [
            "Now, img is a NumPy array, as we can see when using the type function:",
            "markdown"
        ],
        [
            "type(img)",
            "code"
        ],
        [
            "numpy.ndarray",
            "code"
        ],
        [
            "We can see the image using the  function &amp; the special iPython command, %matplotlib inline to display plots inline:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n%matplotlib inline",
            "code"
        ],
        [
            "plt.imshow(img)\nplt.show()\n\n\n\n\n<img alt=\"../_images/0802995ab45723028c194f181d4e07f2e90f6c49c7f49d85ee3de0f218782122.png\" src=\"../_images/0802995ab45723028c194f181d4e07f2e90f6c49c7f49d85ee3de0f218782122.png\"/>",
            "code"
        ]
    ],
    "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->Shape, axis and array properties": [
        [
            "Note that, in linear algebra, the dimension of a vector refers to the number of entries in an array. In NumPy, it instead defines the number of axes. For example, a 1D array is a vector such as [1, 2, 3], a 2D array is a matrix, and so forth.",
            "markdown"
        ],
        [
            "First, let\u2019s check for the shape of the data in our array. Since this image is two-dimensional (the pixels in the image form a rectangle), we might expect a two-dimensional array to represent it (a matrix). However, using the shape property of this NumPy array gives us a different result:",
            "markdown"
        ],
        [
            "img.shape",
            "code"
        ],
        [
            "(768, 1024, 3)",
            "code"
        ],
        [
            "The output is a  with three elements, which means that this is a three-dimensional array. In fact, since this is a color image, and we have used the imread function to read it, the data is organized in three 2D arrays, representing color channels (in this case, red, green and blue - RGB). You can see this by looking at the shape above: it indicates that we have an array of 3 matrices, each having shape 768x1024.",
            "markdown"
        ],
        [
            "Furthermore, using the ndim property of this array, we can see that",
            "markdown"
        ],
        [
            "img.ndim",
            "code"
        ],
        [
            "3",
            "code"
        ],
        [
            "NumPy refers to each dimension as an <em>axis</em>. Because of how imread works, the <em>first index in the 3rd axis</em> is the red pixel data for our image. We can access this by using the syntax",
            "markdown"
        ],
        [
            "img[:, :, 0]",
            "code"
        ],
        [
            "array([[121, 138, 153, ..., 119, 131, 139],\n       [ 89, 110, 130, ..., 118, 134, 146],\n       [ 73,  94, 115, ..., 117, 133, 144],\n       ...,\n       [ 87,  94, 107, ..., 120, 119, 119],\n       [ 85,  95, 112, ..., 121, 120, 120],\n       [ 85,  97, 111, ..., 120, 119, 118]], dtype=uint8)",
            "code"
        ],
        [
            "From the output above, we can see that every value in img[:, :, 0] is an integer value between 0 and 255, representing the level of red in each corresponding image pixel (keep in mind that this might be different if you\nuse your own image instead of ).",
            "markdown"
        ],
        [
            "As expected, this is a 768x1024 matrix:",
            "markdown"
        ],
        [
            "img[:, :, 0].shape",
            "code"
        ],
        [
            "(768, 1024)",
            "code"
        ],
        [
            "Since we are going to perform linear algebra operations on this data, it might be more interesting to have real numbers between 0 and 1 in each entry of the matrices to represent the RGB values. We can do that by setting",
            "markdown"
        ],
        [
            "img_array = img / 255",
            "code"
        ],
        [
            "This operation, dividing an array by a scalar, works because of NumPy\u2019s . (Note that in real-world applications, it would be better to use, for example, the  utility function from scikit-image).",
            "markdown"
        ],
        [
            "You can check that the above works by doing some tests; for example, inquiring\nabout maximum and minimum values for this array:",
            "markdown"
        ],
        [
            "img_array.max(), img_array.min()",
            "code"
        ],
        [
            "(1.0, 0.0)",
            "code"
        ],
        [
            "or checking the type of data in the array:",
            "markdown"
        ],
        [
            "img_array.dtype",
            "code"
        ],
        [
            "dtype('float64')",
            "code"
        ],
        [
            "Note that we can assign each color channel to a separate matrix using the slice syntax:",
            "markdown"
        ],
        [
            "red_array = img_array[:, :, 0]\ngreen_array = img_array[:, :, 1]\nblue_array = img_array[:, :, 2]",
            "code"
        ]
    ],
    "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->Operations on an axis": [
        [
            "It is possible to use methods from linear algebra to approximate an existing set of data. Here, we will use the  to try to rebuild an image that uses less singular value information than the original one, while still retaining some of its features.",
            "markdown"
        ],
        [
            "<strong>Note</strong>: We will use NumPy\u2019s linear algebra module, , to perform the operations in this tutorial. Most of the linear algebra functions in this module can also be found in , and users are encouraged to use the  module for real-world applications. However, some functions in the  module, such as the SVD function, only support 2D arrays. For more information on this, check the .",
            "markdown"
        ],
        [
            "To proceed, import the linear algebra submodule from NumPy:",
            "markdown"
        ],
        [
            "from numpy import linalg",
            "code"
        ],
        [
            "In order to extract information from a given matrix, we can use the SVD to obtain 3 arrays which can be multiplied to obtain the original matrix. From the theory of linear algebra, given a matrix \\(A\\), the following product can be computed:\n\n\\[U \\Sigma V^T = A\\]",
            "markdown"
        ],
        [
            "where \\(U\\) and \\(V^T\\) are square and \\(\\Sigma\\) is the same size as \\(A\\). \\(\\Sigma\\) is a diagonal matrix and contains the  of \\(A\\), organized from largest to smallest. These values are always non-negative and can be used as an indicator of the \u201cimportance\u201d of some features represented by the matrix \\(A\\).",
            "markdown"
        ],
        [
            "Let\u2019s see how this works in practice with just one matrix first. Note that according to ,\nit is possible to obtain a fairly reasonable grayscale version of our color image if we apply the formula\n\n\\[Y = 0.2126 R + 0.7152 G + 0.0722 B\\]",
            "markdown"
        ],
        [
            "where \\(Y\\) is the array representing the grayscale image, and \\(R\\), \\(G\\) and \\(B\\) are the red, green and blue channel arrays we had originally. Notice we can use the @ operator (the matrix multiplication operator for NumPy arrays, see ) for this:",
            "markdown"
        ],
        [
            "img_gray = img_array @ [0.2126, 0.7152, 0.0722]",
            "code"
        ],
        [
            "Now, img_gray has shape",
            "markdown"
        ],
        [
            "img_gray.shape",
            "code"
        ],
        [
            "(768, 1024)",
            "code"
        ],
        [
            "To see if this makes sense in our image, we should use a colormap from matplotlib corresponding to the color we wish to see in out image (otherwise, matplotlib will default to a colormap that does not correspond to the real data).",
            "markdown"
        ],
        [
            "In our case, we are approximating the grayscale portion of the image, so we will use the colormap gray:",
            "markdown"
        ],
        [
            "plt.imshow(img_gray, cmap=\"gray\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/7adf6740f755ce33a18fced959f7be249e3d4f46738c70cfd0eeff87ffe4ff20.png\" src=\"../_images/7adf6740f755ce33a18fced959f7be249e3d4f46738c70cfd0eeff87ffe4ff20.png\"/>",
            "code"
        ],
        [
            "Now, applying the  function to this matrix, we obtain the following decomposition:",
            "markdown"
        ],
        [
            "U, s, Vt = linalg.svd(img_gray)",
            "code"
        ],
        [
            "<strong>Note</strong> If you are using your own image, this command might take a while to run, depending on the size of your image and your hardware. Don\u2019t worry, this is normal! The SVD can be a pretty intensive computation.",
            "markdown"
        ],
        [
            "Let\u2019s check that this is what we expected:",
            "markdown"
        ],
        [
            "U.shape, s.shape, Vt.shape",
            "code"
        ],
        [
            "((768, 768), (768,), (1024, 1024))",
            "code"
        ],
        [
            "Note that s has a particular shape: it has only one dimension. This means that some linear algebra functions that expect 2d arrays might not work. For example, from the theory, one might expect s and Vt to be\ncompatible for multiplication. However, this is not true as s does not have a second axis. Executing",
            "markdown"
        ],
        [
            "s @ Vt",
            "code"
        ],
        [
            "results in a ValueError. This happens because having a one-dimensional array for s, in this case, is much more economic in practice than building a diagonal matrix with the same data. To reconstruct the original matrix, we can rebuild the diagonal matrix \\(\\Sigma\\) with the elements of s in its diagonal and with the appropriate dimensions for multiplying: in our case, \\(\\Sigma\\) should be 768x1024 since U is 768x768 and Vt is 1024x1024. In order to add the singular values to the diagonal of Sigma, we will use the  function from NumPy:",
            "markdown"
        ],
        [
            "import numpy as np\n\nSigma = np.zeros((U.shape[1], Vt.shape[0]))\nnp.fill_diagonal(Sigma, s)",
            "code"
        ],
        [
            "Now, we want to check if the reconstructed U @ Sigma @ Vt is close to the original img_gray matrix.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation": [
        [
            "The  module includes a norm function, which computes the norm of a vector or matrix represented in a NumPy array. For example, from the SVD explanation above, we would expect the norm of the difference between img_gray and the reconstructed SVD product to be small. As expected, you should see something like",
            "markdown"
        ],
        [
            "linalg.norm(img_gray - U @ Sigma @ Vt)",
            "code"
        ],
        [
            "1.4383100782204338e-12",
            "code"
        ],
        [
            "(The actual result of this operation might be different depending on your architecture and linear algebra setup. Regardless, you should see a small number.)",
            "markdown"
        ],
        [
            "We could also have used the  function to make sure the reconstructed product is, in fact, <em>close</em> to our original matrix (the difference between the two arrays is small):",
            "markdown"
        ],
        [
            "np.allclose(img_gray, U @ Sigma @ Vt)",
            "code"
        ],
        [
            "True",
            "code"
        ],
        [
            "To see if an approximation is reasonable, we can check the values in s:",
            "markdown"
        ],
        [
            "plt.plot(s)\nplt.show()\n\n\n\n\n<img alt=\"../_images/b3c5a64421a25e9b8f34f2d5e71f750a53e3ef7b513a82271d088e5c12a26308.png\" src=\"../_images/b3c5a64421a25e9b8f34f2d5e71f750a53e3ef7b513a82271d088e5c12a26308.png\"/>",
            "code"
        ],
        [
            "In the graph, we can see that although we have 768 singular values in s, most of those (after the 150th entry or so) are pretty small. So it might make sense to use only the information related to the first (say, 50) <em>singular values</em> to build a more economical approximation to our image.",
            "markdown"
        ],
        [
            "The idea is to consider all but the first k singular values in Sigma (which are the same as in s) as zeros, keeping U and Vt intact, and computing the product of these matrices as the approximation.",
            "markdown"
        ],
        [
            "For example, if we choose",
            "markdown"
        ],
        [
            "k = 10",
            "code"
        ],
        [
            "we can build the approximation by doing",
            "markdown"
        ],
        [
            "approx = U @ Sigma[:, :k] @ Vt[:k, :]",
            "code"
        ],
        [
            "Note that we had to use only the first k rows of Vt, since all other rows would be multiplied by the zeros corresponding to the singular values we eliminated from this approximation.",
            "markdown"
        ],
        [
            "plt.imshow(approx, cmap=\"gray\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/d1cf34104d6d0e0bdcfc9bf043db57d8d175cc23af1fb41a44dd536e3d9161df.png\" src=\"../_images/d1cf34104d6d0e0bdcfc9bf043db57d8d175cc23af1fb41a44dd536e3d9161df.png\"/>",
            "code"
        ],
        [
            "Now, you can go ahead and repeat this experiment with other values of k, and each of your experiments should give you a slightly better (or worse) image depending on the value you choose.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Applying to all colors": [
        [
            "Now we want to do the same kind of operation, but to all three colors. Our first instinct might be to repeat the same operation we did above to each color matrix individually. However, NumPy\u2019s <em>broadcasting</em> takes care of this\nfor us.",
            "markdown"
        ],
        [
            "If our array has more than two dimensions, then the SVD can be applied to all axes at once. However, the linear algebra functions in NumPy expect to see an array of the form (n, M, N), where the first axis n represents the number of MxN matrices in the stack.",
            "markdown"
        ],
        [
            "In our case,",
            "markdown"
        ],
        [
            "img_array.shape",
            "code"
        ],
        [
            "(768, 1024, 3)",
            "code"
        ],
        [
            "so we need to permutate the axis on this array to get a shape like (3, 768, 1024). Fortunately, the  function can do that for us:",
            "markdown"
        ],
        [
            "np.transpose(x, axes=(i, j, k))",
            "code"
        ],
        [
            "indicates that the axis will be reordered such that the final shape of the transposed array will be reordered according to the indices (i, j, k).",
            "markdown"
        ],
        [
            "Let\u2019s see how this goes for our array:",
            "markdown"
        ],
        [
            "img_array_transposed = np.transpose(img_array, (2, 0, 1))\nimg_array_transposed.shape",
            "code"
        ],
        [
            "(3, 768, 1024)",
            "code"
        ],
        [
            "Now we are ready to apply the SVD:",
            "markdown"
        ],
        [
            "U, s, Vt = linalg.svd(img_array_transposed)",
            "code"
        ],
        [
            "Finally, to obtain the full approximated image, we need to reassemble these matrices into the approximation. Now, note that",
            "markdown"
        ],
        [
            "U.shape, s.shape, Vt.shape",
            "code"
        ],
        [
            "((3, 768, 768), (3, 768), (3, 1024, 1024))",
            "code"
        ],
        [
            "To build the final approximation matrix, we must understand how multiplication across different axes works.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays": [
        [
            "If you have worked before with only one- or two-dimensional arrays in NumPy, you might use  and  (or the @ operator) interchangeably. However, for n-dimensional arrays, they work in very different ways. For more details, check the documentation on .",
            "markdown"
        ],
        [
            "Now, to build our approximation, we first need to make sure that our singular values are ready for multiplication, so we build our Sigma matrix similarly to what we did before. The Sigma array must have dimensions (3, 768, 1024). In order to add the singular values to the diagonal of Sigma, we will again use the  function, using each of the 3 rows in s as the diagonal for each of the 3 matrices in Sigma:",
            "markdown"
        ],
        [
            "Sigma = np.zeros((3, 768, 1024))\nfor j in range(3):\n    np.fill_diagonal(Sigma[j, :, :], s[j, :])",
            "code"
        ],
        [
            "Now, if we wish to rebuild the full SVD (with no approximation), we can do",
            "markdown"
        ],
        [
            "reconstructed = U @ Sigma @ Vt",
            "code"
        ],
        [
            "Note that",
            "markdown"
        ],
        [
            "reconstructed.shape",
            "code"
        ],
        [
            "(3, 768, 1024)",
            "code"
        ],
        [
            "The reconstructed image should be indistinguishable from the original one, except for differences due to floating point errors from the reconstruction. Recall that our original image consisted of floating point values in the range [0., 1.]. The accumulation of floating point error from the reconstruction can result in values slightly outside this original range:",
            "markdown"
        ],
        [
            "reconstructed.min(), reconstructed.max()",
            "code"
        ],
        [
            "(-4.870236158804886e-15, 1.0000000000000047)",
            "code"
        ],
        [
            "Since imshow expects values in the range, we can use clip to excise the floating point error:",
            "markdown"
        ],
        [
            "reconstructed = np.clip(reconstructed, 0, 1)\nplt.imshow(np.transpose(reconstructed, (1, 2, 0)))\nplt.show()\n\n\n\n\n<img alt=\"../_images/01d5082a5df7d6454455fe9df2a92b95e8eb97f54c36b0b3c73e01d0106c45b5.png\" src=\"../_images/01d5082a5df7d6454455fe9df2a92b95e8eb97f54c36b0b3c73e01d0106c45b5.png\"/>",
            "code"
        ],
        [
            "In fact, imshow peforms this clipping under-the-hood, so if you skip the first line in the previous code cell, you might see a warning message saying \"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\"",
            "markdown"
        ],
        [
            "Now, to do the approximation, we must choose only the first k singular values for each color channel. This can be done using the following syntax:",
            "markdown"
        ],
        [
            "approx_img = U @ Sigma[..., :k] @ Vt[..., :k, :]",
            "code"
        ],
        [
            "You can see that we have selected only the first k components of the last axis for Sigma (this means that we have used only the first k columns of each of the three matrices in the stack), and that we have selected only the first k components in the second-to-last axis of Vt (this means we have selected only the first k rows from every matrix in the stack Vt and all columns). If you are unfamiliar with the ellipsis syntax, it is a\nplaceholder for other axes. For more details, see the documentation on .",
            "markdown"
        ],
        [
            "Now,",
            "markdown"
        ],
        [
            "approx_img.shape",
            "code"
        ],
        [
            "(3, 768, 1024)",
            "code"
        ],
        [
            "which is not the right shape for showing the image. Finally, reordering the axes back to our original shape of (768, 1024, 3), we can see our approximation:",
            "markdown"
        ],
        [
            "plt.imshow(np.transpose(approx_img, (1, 2, 0)))\nplt.show()",
            "code"
        ],
        [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n<img alt=\"../_images/f548b24149bd161ccb169bb0651751737d236a8e5bce3a5eb5bc4b63a2cae891.png\" src=\"../_images/f548b24149bd161ccb169bb0651751737d236a8e5bce3a5eb5bc4b63a2cae891.png\"/>",
            "code"
        ],
        [
            "Even though the image is not as sharp, using a small number of k singular values (compared to the original set of 768 values), we can recover many of the distinguishing features from this image.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Final words": [
        [
            "Of course, this is not the best method to <em>approximate</em> an image. However, there is, in fact, a result in linear algebra that says that the approximation we built above is the best we can get to the original matrix in\nterms of the norm of the difference. For more information, see <em>G. H. Golub and C. F. Van Loan, Matrix Computations, Baltimore, MD, Johns Hopkins University Press, 1985</em>.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Further reading": [],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->What you\u2019ll learn": [
        [
            "You\u2019ll save your NumPy arrays as zipped files and human-readable\ncomma-delimited files i.e. *.csv. You will also learn to load both of these\nfile types back into NumPy workspaces.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->What you\u2019ll do": [
        [
            "You\u2019ll learn two ways of saving and reading files\u2013as compressed and as\ntext files\u2013that will serve most of your storage needs in NumPy.",
            "markdown"
        ],
        [
            "You\u2019ll create two 1D arrays and one 2D array",
            "markdown"
        ],
        [
            "You\u2019ll save these arrays to files",
            "markdown"
        ],
        [
            "You\u2019ll remove variables from your workspace",
            "markdown"
        ],
        [
            "You\u2019ll load the variables from your saved file",
            "markdown"
        ],
        [
            "You\u2019ll compare zipped binary files to human-readable delimited files",
            "markdown"
        ],
        [
            "You\u2019ll finish with the skills of saving, loading, and sharing NumPy arrays",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->What you\u2019ll need": [
        [
            "NumPy",
            "markdown"
        ],
        [
            "read-write access to your working directory",
            "markdown"
        ],
        [
            "Load the necessary functions using the following command.",
            "markdown"
        ],
        [
            "import numpy as np",
            "code"
        ],
        [
            "In this tutorial, you will use the following Python, IPython magic, and NumPy functions:",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->Create your arrays": [
        [
            "Now that you have imported the NumPy library, you can make a couple of\narrays; let\u2019s start with two 1D arrays, x and y, where y = x**2.You\nwill assign x to the integers from 0 to 9 using\n.",
            "markdown"
        ],
        [
            "x = np.arange(10)\ny = x ** 2\nprint(x)\nprint(y)",
            "code"
        ],
        [
            "[0 1 2 3 4 5 6 7 8 9]\n[ 0  1  4  9 16 25 36 49 64 81]",
            "code"
        ]
    ],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->Save your arrays with NumPy\u2019s": [
        [
            "Now you have two arrays in your workspace,",
            "markdown"
        ],
        [
            "x: [0 1 2 3 4 5 6 7 8 9]",
            "markdown"
        ],
        [
            "y: [ 0\u00a0 1\u00a0 4\u00a0 9 16 25 36 49 64 81]",
            "markdown"
        ],
        [
            "The first thing you will do is save them to a file as zipped arrays\nusing\n.\nYou will use two options to label the arrays in the file,",
            "markdown"
        ],
        [
            "x_axis = x: this option is assigning the name x_axis to the variable x",
            "markdown"
        ],
        [
            "y_axis = y: this option is assigning the name y_axis to the variable y",
            "markdown"
        ],
        [
            "np.savez(\"x_y-squared.npz\", x_axis=x, y_axis=y)",
            "code"
        ]
    ],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->Remove the saved arrays and load them back with NumPy\u2019s": [
        [
            "In your current working directory, you should have a new file with the\nname x_y-squared.npz. This file is a zipped binary of the two arrays,\nx and y. Let\u2019s clear the workspace and load the values back in. This\nx_y-squared.npz file contains two \nfiles. The NPY format is a . You cannot read\nthe numbers in a standard text editor or spreadsheet.",
            "markdown"
        ],
        [
            "remove x and y from the workspaec with ",
            "markdown"
        ],
        [
            "load the arrays into the workspace in a dictionary with ",
            "markdown"
        ],
        [
            "To see what variables are in the workspace, use the Jupyter/IPython\n\u201cmagic\u201d command\n.",
            "markdown"
        ],
        [
            "del x, y",
            "code"
        ],
        [
            "%whos",
            "code"
        ],
        [
            "Variable   Type      Data/Info\n------------------------------\nnp         module    &lt;module 'numpy' from '/ho&lt;...&gt;kages/numpy/__init__.py'&gt;",
            "code"
        ],
        [
            "load_xy = np.load(\"x_y-squared.npz\")\n\nprint(load_xy.files)",
            "code"
        ],
        [
            "['x_axis', 'y_axis']",
            "code"
        ],
        [
            "whos",
            "code"
        ],
        [
            "Variable   Type       Data/Info\n-------------------------------\nload_xy    NpzFile    &lt;numpy.lib.npyio.NpzFile &lt;...&gt;object at 0x7f7cfce98880&gt;\nnp         module     &lt;module 'numpy' from '/ho&lt;...&gt;kages/numpy/__init__.py'&gt;",
            "code"
        ]
    ],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->Reassign the NpzFile arrays to x and y": [
        [
            "You\u2019ve now created the dictionary with an NpzFile-type. The\nincluded files are x_axis and y_axis that you defined in your\nsavez command. You can reassign x and y to the load_xy files.",
            "markdown"
        ],
        [
            "x = load_xy[\"x_axis\"]\ny = load_xy[\"y_axis\"]\nprint(x)\nprint(y)",
            "code"
        ],
        [
            "[0 1 2 3 4 5 6 7 8 9]\n[ 0  1  4  9 16 25 36 49 64 81]",
            "code"
        ]
    ],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->Success": [
        [
            "You have created, saved, deleted, and loaded the variables x and y using savez and load. Nice work.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->Another option: saving to human-readable csv": [
        [
            "Let\u2019s consider another scenario, you want to share x and y with\nother people or other programs. You may need human-readable text file\nthat is easier to share. Next, you use the\n\nto save x and y in a comma separated value file, x_y-squared.csv.\nThe resulting csv is composed of ASCII characters. You can load the file\nback into NumPy or read it with other programs.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->Rearrange the data into a single 2D array": [
        [
            "First, you have to create a single 2D array from your two 1D arrays. The\ncsv-filetype is a spreadsheet-style dataset. The csv arranges numbers in\nrows\u2013separated by new lines\u2013and columns\u2013separated by commas. If the\ndata is more complex e.g. multiple 2D arrays or higher dimensional\narrays, it is better to use savez. Here, you use\ntwo NumPy functions to format the data:",
            "markdown"
        ],
        [
            ": this function appends arrays together into a 2D array",
            "markdown"
        ],
        [
            ": this function forces the 1D array into a 2D column vector with 10 rows and 1 column.",
            "markdown"
        ],
        [
            "array_out = np.block([x[:, np.newaxis], y[:, np.newaxis]])\nprint(\"the output array has shape \", array_out.shape, \" with values:\")\nprint(array_out)",
            "code"
        ],
        [
            "the output array has shape  (10, 2)  with values:\n[[ 0  0]\n [ 1  1]\n [ 2  4]\n [ 3  9]\n [ 4 16]\n [ 5 25]\n [ 6 36]\n [ 7 49]\n [ 8 64]\n [ 9 81]]",
            "code"
        ]
    ],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->Save the data to csv file using": [
        [
            "You use savetxt with a three options to make your file easier to read:",
            "markdown"
        ],
        [
            "X = array_out: this option tells savetxt to save your 2D array, array_out, to the file x_y-squared.csv",
            "markdown"
        ],
        [
            "header = 'x, y': this option writes a header before any data that labels the columns of the csv",
            "markdown"
        ],
        [
            "delimiter = ',': this option tells savetxt to place a comma between each column in the file",
            "markdown"
        ],
        [
            "np.savetxt(\"x_y-squared.csv\", X=array_out, header=\"x, y\", delimiter=\",\")",
            "code"
        ],
        [
            "Open the file, x_y-squared.csv, and you\u2019ll see the following:",
            "markdown"
        ],
        [
            "# x, y\n0.000000000000000000e+00,0.000000000000000000e+00\n1.000000000000000000e+00,1.000000000000000000e+00\n2.000000000000000000e+00,4.000000000000000000e+00\n3.000000000000000000e+00,9.000000000000000000e+00\n4.000000000000000000e+00,1.600000000000000000e+01\n5.000000000000000000e+00,2.500000000000000000e+01\n6.000000000000000000e+00,3.600000000000000000e+01\n7.000000000000000000e+00,4.900000000000000000e+01\n8.000000000000000000e+00,6.400000000000000000e+01\n9.000000000000000000e+00,8.100000000000000000e+01",
            "code"
        ]
    ],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->Our arrays as a csv file": [
        [
            "There are two features that you shoud notice here:",
            "markdown"
        ],
        [
            "NumPy uses # to ignore headings when using loadtxt. If you\u2019re using\n\nwith other csv files, you can skip header rows with skiprows = &lt;number_of_header_lines&gt;.",
            "markdown"
        ],
        [
            "The integers were written in scientific notation. <em>You can</em> specify\nthe format of the text using the savetxt option, , but it\nwill still be written with ASCII characters. In general, you cannot\npreserve the type of ASCII numbers as float or int.",
            "markdown"
        ],
        [
            "Now, delete x and y again and assign them to your columns in x-y_squared.csv.",
            "markdown"
        ],
        [
            "del x, y",
            "code"
        ],
        [
            "load_xy = np.loadtxt(\"x_y-squared.csv\", delimiter=\",\")",
            "code"
        ],
        [
            "load_xy.shape",
            "code"
        ],
        [
            "(10, 2)",
            "code"
        ],
        [
            "x = load_xy[:, 0]\ny = load_xy[:, 1]\nprint(x)\nprint(y)",
            "code"
        ],
        [
            "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n[ 0.  1.  4.  9. 16. 25. 36. 49. 64. 81.]",
            "code"
        ]
    ],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->Success, but remember your types": [
        [
            "When you saved the arrays to the csv file, you did not preserve the\nint type. When loading the arrays back into your workspace the default process will be to load the csv file as a 2D floating point array e.g. load_xy.dtype == 'float64' and load_xy.shape == (10, 2).",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Saving and sharing your NumPy arrays->Wrapping up": [
        [
            "In conclusion, you can create, save, and load arrays in NumPy. Saving arrays makes sharing your work and collaboration much easier. There are other ways Python can save data to files, such as , but savez and savetxt will serve most of your storage needs for future NumPy work and sharing with other people, respectively.",
            "markdown"
        ],
        [
            "<strong>Next steps</strong>: you can import data with missing values from  or learn more about general NumPy IO with .",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Masked Arrays->What you\u2019ll do": [
        [
            "Use the masked arrays module from NumPy to analyze COVID-19 data and deal with missing values.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Masked Arrays->What you\u2019ll learn": [
        [
            "You\u2019ll understand what are masked arrays and how they can be created",
            "markdown"
        ],
        [
            "You\u2019ll see how to access and modify data for masked arrays",
            "markdown"
        ],
        [
            "You\u2019ll be able to decide when the use of masked arrays is appropriate in some of your applications",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Masked Arrays->What you\u2019ll need": [
        [
            "Basic familiarity with Python. If you would like to refresh your memory, take a look at the .",
            "markdown"
        ],
        [
            "Basic familiarity with NumPy",
            "markdown"
        ],
        [
            "To run the plots on your computer, you need .",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Masked Arrays->What are masked arrays?": [
        [
            "Consider the following problem. You have a dataset with missing or invalid entries. If you\u2019re doing any kind of processing on this data, and want to <em>skip</em> or flag these unwanted entries without just deleting them, you may have to use conditionals or filter your data somehow. The  module provides some of the same functionality of  with added structure to ensure invalid entries are not used in computation.",
            "markdown"
        ],
        [
            "From the :\n<blockquote>",
            "markdown"
        ],
        [
            "A masked array is the combination of a standard  and a <strong>mask</strong>. A mask is either nomask, indicating that no value of the associated array is invalid, or an array of booleans that determines for each element of the associated array whether the value is valid or not. When an element of the mask is False, the corresponding element of the associated array is valid and is said to be unmasked. When an element of the mask is True, the corresponding element of the associated array is said to be masked (invalid).\n</blockquote>",
            "markdown"
        ],
        [
            "We can think of a  as a combination of:",
            "markdown"
        ],
        [
            "Data, as a regular numpy.ndarray of any shape or datatype;",
            "markdown"
        ],
        [
            "A boolean mask with the same shape as the data;",
            "markdown"
        ],
        [
            "A fill_value, a value that may be used to replace the invalid entries in order to return a standard numpy.ndarray.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Masked Arrays->When can they be useful?": [
        [
            "There are a few situations where masked arrays can be more useful than just eliminating the invalid entries of an array:",
            "markdown"
        ],
        [
            "When you want to preserve the values you masked for later processing, without copying the array;",
            "markdown"
        ],
        [
            "When you have to handle many arrays, each with their own mask. If the mask is part of the array, you avoid bugs and the code is possibly more compact;",
            "markdown"
        ],
        [
            "When you have different flags for missing or invalid values, and wish to preserve these flags without replacing them in the original dataset, but exclude them from computations;",
            "markdown"
        ],
        [
            "If you can\u2019t avoid or eliminate missing values, but don\u2019t want to deal with  values in your operations.",
            "markdown"
        ],
        [
            "Masked arrays are also a good idea since the numpy.ma module also comes with a specific implementation of most , which means that you can still apply fast vectorized functions and operations on masked data. The output is then a masked array. We\u2019ll see some examples of how this works in practice below.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Masked Arrays->Using masked arrays to see COVID-19 data": [
        [
            "From  it is possible to download a dataset with initial data about the COVID-19 outbreak in the beginning of 2020. We are going to look at a small subset of this data, contained in the file who_covid_19_sit_rep_time_series.csv. <em>(Note that this file has been replaced with a version without missing data sometime in late 2020.)</em>",
            "markdown"
        ],
        [
            "import numpy as np\nimport os\n\n# The os.getcwd() function returns the current folder; you can change\n# the filepath variable to point to the folder where you saved the .csv file\nfilepath = os.getcwd()\nfilename = os.path.join(filepath, \"who_covid_19_sit_rep_time_series.csv\")",
            "code"
        ],
        [
            "The data file contains data of different types and is organized as follows:",
            "markdown"
        ],
        [
            "The first row is a header line that (mostly) describes the data in each column that follow in the rows below, and beginning in the fourth column, the header is the date of the observation.",
            "markdown"
        ],
        [
            "The second through seventh row contain summary data that is of a different type than that which we are going to examine, so we will need to exclude that from the data with which we will work.",
            "markdown"
        ],
        [
            "The numerical data we wish to work with begins at column 4, row 8, and extends from there to the rightmost column and the lowermost row.",
            "markdown"
        ],
        [
            "Let\u2019s explore the data inside this file for the first 14 days of records. To gather data from the .csv file, we will use the  function, making sure we select only the columns with actual numbers instead of the first four columns which contain location data. We also skip the first 6\nrows of this file, since they contain other data we are not interested in. Separately, we will extract the information about dates and location for this data.",
            "markdown"
        ],
        [
            "# Note we are using skip_header and usecols to read only portions of the\n# data file into each variable.\n# Read just the dates for columns 4-18 from the first row\ndates = np.genfromtxt(\n    filename,\n    dtype=np.unicode_,\n    delimiter=\",\",\n    max_rows=1,\n    usecols=range(4, 18),\n    encoding=\"utf-8-sig\",\n)\n# Read the names of the geographic locations from the first two\n# columns, skipping the first six rows\nlocations = np.genfromtxt(\n    filename,\n    dtype=np.unicode_,\n    delimiter=\",\",\n    skip_header=6,\n    usecols=(0, 1),\n    encoding=\"utf-8-sig\",\n)\n# Read the numeric data from just the first 14 days\nnbcases = np.genfromtxt(\n    filename,\n    dtype=np.int_,\n    delimiter=\",\",\n    skip_header=6,\n    usecols=range(4, 18),\n    encoding=\"utf-8-sig\",\n)",
            "code"
        ],
        [
            "Included in the numpy.genfromtxt function call, we have selected the  for each subset of the data (either an integer - numpy.int_ - or a string of characters - numpy.unicode_). We have also used the encoding argument to select utf-8-sig as the encoding for the file (read more about encoding in the . You can read more about the numpy.genfromtxt function from the  or from the .",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Masked Arrays->Exploring the data": [
        [
            "First of all, we can plot the whole set of data we have and see what it looks like. In order to get a readable plot, we select only a few of the dates to show in our . Note also that in our plot command, we use nbcases.T (the transpose of the nbcases array) since this means we will plot each row of the file as a separate line. We choose to plot a dashed line (using the '--' line style). See the  documentation for more info on this.",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nselected_dates = [0, 3, 11, 13]\nplt.plot(dates, nbcases.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")",
            "code"
        ],
        [
            "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020')\n\n\n<img alt=\"../_images/61642c5f2f4ca1f6e8c5ff0631b418879f272fc201422517e443413c935adc67.png\" src=\"../_images/61642c5f2f4ca1f6e8c5ff0631b418879f272fc201422517e443413c935adc67.png\"/>",
            "code"
        ],
        [
            "The graph has a strange shape from January 24th to February 1st. It would be interesting to know where this data comes from. If we look at the locations array we extracted from the .csv file, we can see that we have two columns, where the first would contain regions and the second would contain the name of the country. However, only the first few rows contain data for the the first column (province names in China). Following that, we only have country names. So it would make sense to group all the data from China into a single row. For this, we\u2019ll select from the nbcases array only the rows for which the second entry of the locations array corresponds to China. Next, we\u2019ll use the  function to sum all the selected rows (axis=0). Note also that row 35 corresponds to the total counts for the whole country for each date. Since we want to calculate the sum ourselves from the provinces data, we have to remove that row first from both locations and nbcases:",
            "markdown"
        ],
        [
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "code"
        ],
        [
            "array([  247,   288,   556,   817,   -22,   -22,   -15,   -10,    -9,\n          -7,    -4, 11820, 14410, 17237])",
            "code"
        ],
        [
            "Something\u2019s wrong with this data - we are not supposed to have negative values in a cumulative data set. What\u2019s going on?",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Masked Arrays->Missing data": [
        [
            "Looking at the data, here\u2019s what we find: there is a period with <strong>missing data</strong>:",
            "markdown"
        ],
        [
            "nbcases",
            "code"
        ],
        [
            "array([[  258,   270,   375, ...,  7153,  9074, 11177],\n       [   14,    17,    26, ...,   520,   604,   683],\n       [   -1,     1,     1, ...,   422,   493,   566],\n       ...,\n       [   -1,    -1,    -1, ...,    -1,    -1,    -1],\n       [   -1,    -1,    -1, ...,    -1,    -1,    -1],\n       [   -1,    -1,    -1, ...,    -1,    -1,    -1]])",
            "code"
        ],
        [
            "All the -1 values we are seeing come from numpy.genfromtxt attempting to read missing data from the original .csv file. Obviously, we\ndon\u2019t want to compute missing data as -1 - we just want to skip this value so it doesn\u2019t interfere in our analysis. After importing the numpy.ma module, we\u2019ll create a new array, this time masking the invalid values:",
            "markdown"
        ],
        [
            "from numpy import ma\n\nnbcases_ma = ma.masked_values(nbcases, -1)",
            "code"
        ],
        [
            "If we look at the nbcases_ma masked array, this is what we have:",
            "markdown"
        ],
        [
            "nbcases_ma",
            "code"
        ],
        [
            "masked_array(\n  data=[[258, 270, 375, ..., 7153, 9074, 11177],\n        [14, 17, 26, ..., 520, 604, 683],\n        [--, 1, 1, ..., 422, 493, 566],\n        ...,\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --]],\n  mask=[[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [ True, False, False, ..., False, False, False],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n  fill_value=-1)",
            "code"
        ],
        [
            "We can see that this is a different kind of array. As mentioned in the introduction, it has three attributes (data, mask and fill_value).\nKeep in mind that the mask attribute has a True value for elements corresponding to <strong>invalid</strong> data (represented by two dashes in the data attribute).",
            "markdown"
        ],
        [
            "Let\u2019s try and see what the data looks like excluding the first row (data from the Hubei province in China) so we can look at the missing data more\nclosely:",
            "markdown"
        ],
        [
            "plt.plot(dates, nbcases_ma[1:].T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")",
            "code"
        ],
        [
            "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020')\n\n\n<img alt=\"../_images/6828fab516c9327c5f752f80a0b0d251c3f205860d47ac3c821a3ad7d1c76ce3.png\" src=\"../_images/6828fab516c9327c5f752f80a0b0d251c3f205860d47ac3c821a3ad7d1c76ce3.png\"/>",
            "code"
        ],
        [
            "Now that our data has been masked, let\u2019s try summing up all the cases in China:",
            "markdown"
        ],
        [
            "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "code"
        ],
        [
            "masked_array(data=[278, 309, 574, 835, 10, 10, 17, 22, 23, 25, 28, 11821,\n                   14411, 17238],\n             mask=[False, False, False, False, False, False, False, False,\n                   False, False, False, False, False, False],\n       fill_value=999999)",
            "code"
        ],
        [
            "Note that china_masked is a masked array, so it has a different data structure than a regular NumPy array. Now, we can access its data directly by using the .data attribute:",
            "markdown"
        ],
        [
            "china_total = china_masked.data\nchina_total",
            "code"
        ],
        [
            "array([  278,   309,   574,   835,    10,    10,    17,    22,    23,\n          25,    28, 11821, 14411, 17238])",
            "code"
        ],
        [
            "That is better: no more negative values. However, we can still see that for some days, the cumulative number of cases seems to go down (from 835 to 10, for example), which does not agree with the definition of \u201ccumulative data\u201d. If we look more closely at the data, we can see that in the period where there was missing data in mainland China, there was valid data for Hong Kong, Taiwan, Macau and \u201cUnspecified\u201d regions of China. Maybe we can remove those from the total sum of cases in China, to get a better understanding of the data.",
            "markdown"
        ],
        [
            "First, we\u2019ll identify the indices of locations in mainland China:",
            "markdown"
        ],
        [
            "china_mask = (\n    (locations[:, 1] == \"China\")\n    &amp; (locations[:, 0] != \"Hong Kong\")\n    &amp; (locations[:, 0] != \"Taiwan\")\n    &amp; (locations[:, 0] != \"Macau\")\n    &amp; (locations[:, 0] != \"Unspecified*\")\n)",
            "code"
        ],
        [
            "Now, china_mask is an array of boolean values (True or False); we can check that the indices are what we wanted with the  method for masked arrays:",
            "markdown"
        ],
        [
            "china_mask.nonzero()",
            "code"
        ],
        [
            "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n        17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 31, 33]),)",
            "code"
        ],
        [
            "Now we can correctly sum entries for mainland China:",
            "markdown"
        ],
        [
            "china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
            "code"
        ],
        [
            "masked_array(data=[278, 308, 440, 446, --, --, --, --, --, --, --, 11791,\n                   14380, 17205],\n             mask=[False, False, False, False,  True,  True,  True,  True,\n                    True,  True,  True, False, False, False],\n       fill_value=999999)",
            "code"
        ],
        [
            "We can replace the data with this information and plot a new graph, focusing on Mainland China:",
            "markdown"
        ],
        [
            "plt.plot(dates, china_total.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\")",
            "code"
        ],
        [
            "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China')\n\n\n<img alt=\"../_images/c73ad892cb741c9f3dc94135672ec45191a1d62af9283d1c9e99f90c125d3c1b.png\" src=\"../_images/c73ad892cb741c9f3dc94135672ec45191a1d62af9283d1c9e99f90c125d3c1b.png\"/>",
            "code"
        ],
        [
            "It\u2019s clear that masked arrays are the right solution here. We cannot represent the missing data without mischaracterizing the evolution of the curve.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Masked Arrays->Fitting Data": [
        [
            "One possibility we can think of is to interpolate the missing data to estimate the number of cases in late January. Observe that we can select the masked elements using the .mask attribute:",
            "markdown"
        ],
        [
            "china_total.mask\ninvalid = china_total[china_total.mask]\ninvalid",
            "code"
        ],
        [
            "masked_array(data=[--, --, --, --, --, --, --],\n             mask=[ True,  True,  True,  True,  True,  True,  True],\n       fill_value=999999,\n            dtype=int64)",
            "code"
        ],
        [
            "We can also access the valid entries by using the logical negation for this mask:",
            "markdown"
        ],
        [
            "valid = china_total[~china_total.mask]\nvalid",
            "code"
        ],
        [
            "masked_array(data=[278, 308, 440, 446, 11791, 14380, 17205],\n             mask=[False, False, False, False, False, False, False],\n       fill_value=999999)",
            "code"
        ],
        [
            "Now, if we want to create a very simple approximation for this data, we should take into account the valid entries around the invalid ones. So first let\u2019s select the dates for which the data is valid. Note that we can use the mask from the china_total masked array to index the dates array:",
            "markdown"
        ],
        [
            "dates[~china_total.mask]",
            "code"
        ],
        [
            "array(['1/21/20', '1/22/20', '1/23/20', '1/24/20', '2/1/20', '2/2/20',\n       '2/3/20'], dtype='&lt;U7')",
            "code"
        ],
        [
            "Finally, we can use the\n\npackage to create a cubic polynomial model that fits the data as best as possible:",
            "markdown"
        ],
        [
            "t = np.arange(len(china_total))\nmodel = np.polynomial.Polynomial.fit(t[~china_total.mask], valid, deg=3)\nplt.plot(t, china_total)\nplt.plot(t, model(t), \"--\")",
            "code"
        ],
        [
            "[&lt;matplotlib.lines.Line2D at 0x7fb70eec3a30&gt;]\n\n\n<img alt=\"../_images/7bb5f74f01083f9cef8de19d4ca821b3542f52c0aef5966bbe500f6f26854902.png\" src=\"../_images/7bb5f74f01083f9cef8de19d4ca821b3542f52c0aef5966bbe500f6f26854902.png\"/>",
            "code"
        ],
        [
            "This plot is not so readable since the lines seem to be over each other, so let\u2019s summarize in a more elaborate plot. We\u2019ll plot the real data when\navailable, and show the cubic fit for unavailable data, using this fit to compute an estimate to the observed number of cases on January 28th 2020, 7 days after the beginning of the records:",
            "markdown"
        ],
        [
            "plt.plot(t, china_total)\nplt.plot(t[china_total.mask], model(t)[china_total.mask], \"--\", color=\"orange\")\nplt.plot(7, model(7), \"r*\")\nplt.xticks([0, 7, 13], dates[[0, 7, 13]])\nplt.yticks([0, model(7), 10000, 17500])\nplt.legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"])\nplt.title(\n    \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\"\n    \"Cubic estimate for 7 days after start\"\n)",
            "code"
        ],
        [
            "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\nCubic estimate for 7 days after start')\n\n\n<img alt=\"../_images/a6f22fa34b666874eedfdbab1f4491a703ebfc1e441ced449b895c36529a7533.png\" src=\"../_images/a6f22fa34b666874eedfdbab1f4491a703ebfc1e441ced449b895c36529a7533.png\"/>",
            "code"
        ]
    ],
    "numpy->NumPy Features->Masked Arrays->In practice": [
        [
            "Adding -1 to missing data is not a problem with numpy.genfromtxt; in this particular case, substituting the missing value with 0 might have been fine, but we\u2019ll see later that this is far from a general solution. Also, it is possible to call the numpy.genfromtxt function using the usemask parameter. If usemask=True, numpy.genfromtxt automatically returns a masked array.",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Masked Arrays->Further reading": [
        [
            "Topics not covered in this tutorial can be found in the documentation:",
            "markdown"
        ],
        [
            " vs. ",
            "markdown"
        ]
    ],
    "numpy->NumPy Features->Masked Arrays->Further reading->Reference": [
        [
            "Ensheng Dong, Hongru Du, Lauren Gardner, <em>An interactive web-based dashboard to track COVID-19 in real time</em>, The Lancet Infectious Diseases, Volume 20, Issue 5, 2020, Pages 533-534, ISSN 1473-3099, .",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy": [
        [
            "<img alt=\"Scatter plot of MOS transistor count per microprocessor every two years as a demonstration of Moore's Law.\" src=\"../_images/01-mooreslaw-tutorial-intro.png\"/>",
            "markdown"
        ],
        [
            "<em>The number of transistors reported per a given chip plotted on a log scale in the y axis with the date of introduction on the linear scale x-axis. The blue data points are from a . The red line is an ordinary least squares prediction and the orange line is Moore\u2019s law.</em>",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->What you\u2019ll do": [
        [
            "In 1965, engineer Gordon Moore\n that\ntransistors on a chip would double every two years in the coming decade\n[,\n].\nYou\u2019ll compare Moore\u2019s prediction against actual transistor counts in\nthe 53 years following his prediction. You will determine the best-fit constants to describe the exponential growth of transistors on semiconductors compared to Moore\u2019s Law.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Skills you\u2019ll learn": [
        [
            "Load data from a  file",
            "markdown"
        ],
        [
            "Perform linear regression and predict exponential growth using ordinary least squares",
            "markdown"
        ],
        [
            "You\u2019ll compare exponential growth constants between models",
            "markdown"
        ],
        [
            "Share your analysis in a file:",
            "markdown"
        ],
        [
            "as NumPy zipped files *.npz",
            "markdown"
        ],
        [
            "as a *.csv file",
            "markdown"
        ],
        [
            "Assess the amazing progress semiconductor manufacturers have made in the last five decades",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->What you\u2019ll need": [
        [
            "<strong>1.</strong> These packages:",
            "markdown"
        ],
        [
            "NumPy",
            "markdown"
        ],
        [
            " ordinary linear regression",
            "markdown"
        ],
        [
            "imported with the following commands",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm",
            "code"
        ],
        [
            "<strong>2.</strong> Since this is an exponential growth law you need a little background in doing math with  and .",
            "markdown"
        ],
        [
            "You\u2019ll use these NumPy, Matplotlib, and statsmodels functions:",
            "markdown"
        ],
        [
            ": this function loads text into a NumPy array",
            "markdown"
        ],
        [
            ": this function takes the natural log of all elements in a NumPy array",
            "markdown"
        ],
        [
            ": this function takes the exponential of all elements in a NumPy array",
            "markdown"
        ],
        [
            ": this is a minimal function definition for creating a function model",
            "markdown"
        ],
        [
            ": this function will plot x-y data onto a figure with a linear x-axis and \\(\\log_{10}\\) y-axis\n: this function will plot x-y data on linear axes",
            "markdown"
        ],
        [
            ": find fitting parameters and standard errors using the statsmodels ordinary least squares model",
            "markdown"
        ],
        [
            "slicing arrays: view parts of the data loaded into the workspace, slice the arrays e.g. x[:10] for the first 10 values in the array, x",
            "markdown"
        ],
        [
            "boolean array indexing: to view parts of the data that match a given condition use boolean operations to index an array",
            "markdown"
        ],
        [
            ": to combine arrays into 2D arrays",
            "markdown"
        ],
        [
            ": to change a 1D vector to a row or column vector",
            "markdown"
        ],
        [
            " and : these two functions will save your arrays in zipped array format and text, respectively",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Building Moore\u2019s law as an exponential function": [
        [
            "Your empirical model assumes that the number of transistors per\nsemiconductor follows an exponential growth,",
            "markdown"
        ],
        [
            "\\(\\log(\\text{transistor_count})= f(\\text{year}) = A\\cdot \\text{year}+B,\\)",
            "markdown"
        ],
        [
            "where \\(A\\) and \\(B\\) are fitting constants. You use semiconductor\nmanufacturers\u2019 data to find the fitting constants.",
            "markdown"
        ],
        [
            "You determine these constants for Moore\u2019s law by specifying the\nrate for added transistors, 2, and giving an initial number of transistors for a given year.",
            "markdown"
        ],
        [
            "You state Moore\u2019s law in an exponential form as follows,",
            "markdown"
        ],
        [
            "\\(\\text{transistor_count}= e^{A_M\\cdot \\text{year} +B_M}.\\)",
            "markdown"
        ],
        [
            "Where \\(A_M\\) and \\(B_M\\) are constants that double the number of transistors every two years and start at 2250 transistors in 1971,",
            "markdown"
        ],
        [
            "\\(\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = 2 = \\dfrac{e^{B_M}e^{A_M \\text{year} + 2A_M}}{e^{B_M}e^{A_M \\text{year}}} = e^{2A_M} \\rightarrow A_M = \\frac{\\log(2)}{2}\\)",
            "markdown"
        ],
        [
            "\\(\\log(2250) = \\frac{\\log(2)}{2}\\cdot 1971 + B_M \\rightarrow B_M = \\log(2250)-\\frac{\\log(2)}{2}\\cdot 1971\\)",
            "markdown"
        ],
        [
            "so Moore\u2019s law stated as an exponential function is",
            "markdown"
        ],
        [
            "\\(\\log(\\text{transistor_count})= A_M\\cdot \\text{year}+B_M,\\)",
            "markdown"
        ],
        [
            "where",
            "markdown"
        ],
        [
            "\\(A_M=0.3466\\)",
            "markdown"
        ],
        [
            "\\(B_M=-675.4\\)",
            "markdown"
        ],
        [
            "Since the function represents Moore\u2019s law, define it as a Python\nfunction using",
            "markdown"
        ],
        [
            "A_M = np.log(2) / 2\nB_M = np.log(2250) - A_M * 1971\nMoores_law = lambda year: np.exp(B_M) * np.exp(A_M * year)",
            "code"
        ],
        [
            "In 1971, there were 2250 transistors on the Intel 4004 chip. Use\nMoores_law to check the number of semiconductors Gordon Moore would expect\nin 1973.",
            "markdown"
        ],
        [
            "ML_1971 = Moores_law(1971)\nML_1973 = Moores_law(1973)\nprint(\"In 1973, G. Moore expects {:.0f} transistors on Intels chips\".format(ML_1973))\nprint(\"This is x{:.2f} more transistors than 1971\".format(ML_1973 / ML_1971))",
            "code"
        ],
        [
            "In 1973, G. Moore expects 4500 transistors on Intels chips\nThis is x2.00 more transistors than 1971",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Loading historical manufacturing data to your workspace": [
        [
            "Now, make a prediction based upon the historical data for\nsemiconductors per chip. The \neach year is in the transistor_data.csv file. Before loading a *.csv\nfile into a NumPy array, its a good idea to inspect the structure of the\nfile first. Then, locate the columns of interest and save them to a\nvariable. Save two columns of the file to the array, data.",
            "markdown"
        ],
        [
            "Here, print out the first 10 rows of transistor_data.csv. The columns are",
            "markdown"
        ],
        [
            "! head transistor_data.csv",
            "code"
        ],
        [
            "Processor,MOS transistor count,Date of Introduction,Designer,MOSprocess,Area\nIntel 4004 (4-bit  16-pin),2250,1971,Intel,\"10,000\u00a0nm\",12\u00a0mm\u00b2\nIntel 8008 (8-bit  18-pin),3500,1972,Intel,\"10,000\u00a0nm\",14\u00a0mm\u00b2\nNEC \u03bcCOM-4 (4-bit  42-pin),2500,1973,NEC,\"7,500\u00a0nm\",?\nIntel 4040 (4-bit  16-pin),3000,1974,Intel,\"10,000\u00a0nm\",12\u00a0mm\u00b2\nMotorola 6800 (8-bit  40-pin),4100,1974,Motorola,\"6,000\u00a0nm\",16\u00a0mm\u00b2\nIntel 8080 (8-bit  40-pin),6000,1974,Intel,\"6,000\u00a0nm\",20\u00a0mm\u00b2\nTMS 1000 (4-bit  28-pin),8000,1974,Texas Instruments,\"8,000\u00a0nm\",11\u00a0mm\u00b2\nMOS Technology 6502 (8-bit  40-pin),4528,1975,MOS Technology,\"8,000\u00a0nm\",21\u00a0mm\u00b2\nIntersil IM6100 (12-bit  40-pin; clone of PDP-8),4000,1975,Intersil,,",
            "code"
        ],
        [
            "You don\u2019t need the columns that specify <strong>Processor</strong>, <strong>Designer</strong>,\n<strong>MOSprocess</strong>, or <strong>Area</strong>. That leaves the second and third columns,\n<strong>MOS transistor count</strong> and <strong>Date of Introduction</strong>, respectively.",
            "markdown"
        ],
        [
            "Next, you load these two columns into a NumPy array using\n.\nThe extra options below will put the data in the desired format:",
            "markdown"
        ],
        [
            "delimiter = ',': specify delimeter as a comma \u2018,\u2019 (this is the default behavior)",
            "markdown"
        ],
        [
            "usecols = [1,2]: import the second and third columns from the csv",
            "markdown"
        ],
        [
            "skiprows = 1: do not use the first row, because its a header row",
            "markdown"
        ],
        [
            "data = np.loadtxt(\"transistor_data.csv\", delimiter=\",\", usecols=[1, 2], skiprows=1)",
            "code"
        ],
        [
            "You loaded the entire history of semiconducting into a NumPy array named\ndata. The first column is the <strong>MOS transistor count</strong> and the second\ncolumn is the <strong>Date of Introduction</strong> in a four-digit year.",
            "markdown"
        ],
        [
            "Next, make the data easier to read and manage by assigning the two\ncolumns to variables, year and transistor_count. Print out the first\n10 values by slicing the year and transistor_count arrays with\n[:10]. Print these values out to check that you have the saved the\ndata to the correct variables.",
            "markdown"
        ],
        [
            "year = data[:, 1]  # grab the second column and assign\ntransistor_count = data[:, 0]  # grab the first column and assign\n\nprint(\"year:\\t\\t\", year[:10])\nprint(\"trans. cnt:\\t\", transistor_count[:10])",
            "code"
        ],
        [
            "year:\t\t [1971. 1972. 1973. 1974. 1974. 1974. 1974. 1975. 1975. 1975.]\ntrans. cnt:\t [2250. 3500. 2500. 3000. 4100. 6000. 8000. 4528. 4000. 5000.]",
            "code"
        ],
        [
            "You are creating a function that predicts the transistor count given a\nyear. You have an <em>independent variable</em>, year, and a <em>dependent\nvariable</em>, transistor_count. Transform the independent variable to\nlog-scale,",
            "markdown"
        ],
        [
            "\\(y_i = \\log(\\) transistor_count[i] \\(),\\)",
            "markdown"
        ],
        [
            "resulting in a linear equation,",
            "markdown"
        ],
        [
            "\\(y_i = A\\cdot \\text{year} +B\\).",
            "markdown"
        ],
        [
            "yi = np.log(transistor_count)",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors": [
        [
            "Your model assume that yi is a function of year. Now, find the best-fit model that minimizes the difference between \\(y_i\\) and \\(A\\cdot \\text{year} +B, \\) as such",
            "markdown"
        ],
        [
            "\\(\\min \\sum|y_i - (A\\cdot \\text{year}_i + B)|^2.\\)",
            "markdown"
        ],
        [
            "This  can be\nsuccinctly represented as arrays as such",
            "markdown"
        ],
        [
            "\\(\\sum|\\mathbf{y}-\\mathbf{Z} [A,~B]^T|^2,\\)",
            "markdown"
        ],
        [
            "where \\(\\mathbf{y}\\) are the observations of the log of the number of\ntransistors in a 1D array and \\(\\mathbf{Z}=[\\text{year}_i^1,~\\text{year}_i^0]\\) are the\npolynomial terms for \\(\\text{year}_i\\) in the first and second columns. By\ncreating this set of regressors in the \\(\\mathbf{Z}-\\)matrix you set\nup an ordinary least squares statistical model. Some clever\nNumPy array features will build \\(\\mathbf{Z}\\)",
            "markdown"
        ],
        [
            "year[:,np.newaxis] : takes the 1D array with shape (179,) and turns it into a 2D column vector with shape (179,1)",
            "markdown"
        ],
        [
            "**[1, 0] : stacks two columns, in the first column is year**1 and the second column is year**0 == 1",
            "markdown"
        ],
        [
            "Z = year[:, np.newaxis] ** [1, 0]",
            "code"
        ],
        [
            "Now that you have the created a matrix of regressors, \\(\\mathbf{Z},\\) and\nthe observations are in vector, \\(\\mathbf{y},\\) you can use these\nvariables to build the an ordinary least squares model with\n.",
            "markdown"
        ],
        [
            "model = sm.OLS(yi, Z)",
            "code"
        ],
        [
            "Now, you can view the fitting constants, \\(A\\) and \\(B\\), and their standard\nerrors.  Run the\n and print the\n to view results as such,",
            "markdown"
        ],
        [
            "results = model.fit()\nprint(results.summary())",
            "code"
        ],
        [
            "                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.949\nModel:                            OLS   Adj. R-squared:                  0.949\nMethod:                 Least Squares   F-statistic:                     3309.\nDate:                Fri, 17 Mar 2023   Prob (F-statistic):          1.75e-116\nTime:                        17:07:33   Log-Likelihood:                -273.43\nNo. Observations:                 179   AIC:                             550.9\nDf Residuals:                     177   BIC:                             557.2\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             0.3416      0.006     57.521      0.000       0.330       0.353\nconst       -666.3264     11.890    -56.042      0.000    -689.790    -642.862\n==============================================================================\nOmnibus:                      128.297   Durbin-Watson:                   1.600\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1184.322\nSkew:                          -2.637   Prob(JB):                    6.73e-258\nKurtosis:                      14.444   Cond. No.                     2.84e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.84e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
            "code"
        ],
        [
            "The <strong>OLS Regression Results</strong> summary gives a lot of information about\nthe regressors, \\(\\mathbf{Z},\\) and observations, \\(\\mathbf{y}.\\) The most\nimportant outputs for your current analysis are",
            "markdown"
        ],
        [
            "=================================\n                 coef    std err\n---------------------------------\nx1             0.3416      0.006\nconst       -666.3264     11.890\n=================================",
            "code"
        ],
        [
            "where x1 is slope, \\(A=0.3416\\), const is the intercept,\n\\(B=-666.364\\), and std error gives the precision of constants\n\\(A=0.342\\pm 0.006~\\dfrac{\\log(\\text{transistors}/\\text{chip})}{\\text{years}}\\) and \\(B=-666\\pm\n12~\\log(\\text{transistors}/\\text{chip}),\\) where the units are in\n\\(\\log(\\text{transistors}/\\text{chip})\\). You created an exponential growth model.\nTo get the constants, save them to an array AB with\nresults.params and assign \\(A\\) and \\(B\\) to x1 and constant.",
            "markdown"
        ],
        [
            "AB = results.params\nA = AB[0]\nB = AB[1]",
            "code"
        ],
        [
            "Did manufacturers double the transistor count every two years? You have\nthe final formula,",
            "markdown"
        ],
        [
            "\\(\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = xFactor =\n\\dfrac{e^{B}e^{A( \\text{year} + 2)}}{e^{B}e^{A \\text{year}}} = e^{2A}\\)",
            "markdown"
        ],
        [
            "where increase in number of transistors is \\(xFactor,\\) number of years is\n2, and \\(A\\) is the best fit slope on the semilog function. The error in\nyour\nprediction, \\(\\Delta(xFactor),\\) comes from the precision of your constant\n\\(A,\\) which you calculated as the standard error \\(\\Delta A= 0.006\\).",
            "markdown"
        ],
        [
            "\\(\\Delta (xFactor) = \\frac{\\partial}{\\partial A}(e^{2A})\\Delta A = 2Ae^{2A}\\Delta A\\)",
            "markdown"
        ],
        [
            "print(\"Rate of semiconductors added on a chip every 2 years:\")\nprint(\n    \"\\tx{:.2f} +/- {:.2f} semiconductors per chip\".format(\n        np.exp((A) * 2), 2 * A * np.exp(2 * A) * 0.006\n    )\n)",
            "code"
        ],
        [
            "Rate of semiconductors added on a chip every 2 years:\n\tx1.98 +/- 0.01 semiconductors per chip",
            "code"
        ],
        [
            "Based upon your least-squares regression model, the number of\nsemiconductors per chip increased by a factor of \\(1.98\\pm 0.01\\) every two\nyears. You have a model that predicts the number of semiconductors each\nyear. Now compare your model to the actual manufacturing reports.  Plot\nthe linear regression results and all of the transistor counts.",
            "markdown"
        ],
        [
            "Here, use\n\nto plot the number of transistors on a log-scale and the year on a\nlinear scale. You have defined a three arrays to get to a final model",
            "markdown"
        ],
        [
            "\\(y_i = \\log(\\text{transistor_count}),\\)",
            "markdown"
        ],
        [
            "\\(y_i = A \\cdot \\text{year} + B,\\)",
            "markdown"
        ],
        [
            "and",
            "markdown"
        ],
        [
            "\\(\\log(\\text{transistor_count}) = A\\cdot \\text{year} + B,\\)",
            "markdown"
        ],
        [
            "your variables, transistor_count, year, and yi all have the same\ndimensions, (179,). NumPy arrays need the same dimensions to make a\nplot. The predicted number of transistors is now",
            "markdown"
        ],
        [
            "\\(\\text{transistor_count}_{\\text{predicted}} = e^Be^{A\\cdot \\text{year}}\\).",
            "markdown"
        ],
        [
            "In the next plot, use the\n\nstyle sheet.\nThe style sheet replicates\nhttps://fivethirtyeight.com elements. Change the matplotlib style with\n.",
            "markdown"
        ],
        [
            "transistor_count_predicted = np.exp(B) * np.exp(A * year)\ntransistor_Moores_law = Moores_law(year)\nplt.style.use(\"fivethirtyeight\")\nplt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\")\nplt.semilogy(year, transistor_count_predicted, label=\"linear regression\")\n\n\nplt.plot(year, transistor_Moores_law, label=\"Moore's Law\")\nplt.title(\n    \"MOS transistor count per microprocessor\\n\"\n    + \"every two years \\n\"\n    + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2))\n)\nplt.xlabel(\"year introduced\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"# of transistors\\nper microprocessor\")",
            "code"
        ],
        [
            "Text(0, 0.5, '# of transistors\\nper microprocessor')\n\n\n<img alt=\"../_images/9407c374bb2daa0f7320ee0351424aa51d56711ed1ce5e5880db6bc269fbc956.png\" src=\"../_images/9407c374bb2daa0f7320ee0351424aa51d56711ed1ce5e5880db6bc269fbc956.png\"/>",
            "code"
        ],
        [
            "<em>A scatter plot of MOS transistor count per microprocessor every two years with a red line for the ordinary least squares prediction and an orange line for Moore\u2019s law.</em>",
            "markdown"
        ],
        [
            "The linear regression captures the increase in the number of transistors\nper semiconductors each year.  In 2015, semiconductor manufacturers\nclaimed they could not keep up with Moore\u2019s law anymore. Your analysis\nshows that since 1971, the average increase in transistor count was\nx1.98 every 2 years, but Gordon Moore predicted it would be x2\nevery 2 years. That is an amazing prediction.",
            "markdown"
        ],
        [
            "Consider the year 2017. Compare the data to your linear regression\nmodel and Gordon Moore\u2019s prediction. First, get the\ntransistor counts from the year 2017. You can do this with a Boolean\ncomparator,",
            "markdown"
        ],
        [
            "year == 2017.",
            "markdown"
        ],
        [
            "Then, make a prediction for 2017 with Moores_law defined above\nand plugging in your best fit constants into your function",
            "markdown"
        ],
        [
            "\\(\\text{transistor_count} = e^{B}e^{A\\cdot \\text{year}}\\).",
            "markdown"
        ],
        [
            "A great way to compare these measurements is to compare your prediction\nand Moore\u2019s prediction to the average transistor count and look at the\nrange of reported values for that year. Use the\n\noption,\n,\nto increase the transparency of the data. The more opaque the points\nappear, the more reported values lie on that measurement. The green \\(+\\)\nis the average reported transistor count for 2017. Plot your predictions\nfor $\\pm\\frac{1}{2}~years.",
            "markdown"
        ],
        [
            "transistor_count2017 = transistor_count[year == 2017]\nprint(\n    transistor_count2017.max(), transistor_count2017.min(), transistor_count2017.mean()\n)\ny = np.linspace(2016.5, 2017.5)\nyour_model2017 = np.exp(B) * np.exp(A * y)\nMoore_Model2017 = Moores_law(y)\n\nplt.plot(\n    2017 * np.ones(np.sum(year == 2017)),\n    transistor_count2017,\n    \"ro\",\n    label=\"2017\",\n    alpha=0.2,\n)\nplt.plot(2017, transistor_count2017.mean(), \"g+\", markersize=20, mew=6)\n\nplt.plot(y, your_model2017, label=\"Your prediction\")\nplt.plot(y, Moore_Model2017, label=\"Moores law\")\nplt.ylabel(\"# of transistors\\nper microprocessor\")\nplt.legend()",
            "code"
        ],
        [
            "19200000000.0 250000000.0 7050000000.0",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f83255cf040&gt;\n\n\n<img alt=\"../_images/397cfee4652c323437ff609ff34c7638ced66ba5e23cab7b83228681d94716a6.png\" src=\"../_images/397cfee4652c323437ff609ff34c7638ced66ba5e23cab7b83228681d94716a6.png\"/>",
            "code"
        ],
        [
            "The result is that your model is close to the mean, but Gordon\nMoore\u2019s prediction is closer to the maximum number of transistors per\nmicroprocessor produced in 2017. Even though semiconductor manufacturers\nthought that the growth would slow, once in 1975 and now again\napproaching 2025, manufacturers are still producing semiconductors every 2 years that\nnearly double the number of transistors.",
            "markdown"
        ],
        [
            "The linear regression model is much better at predicting the\naverage than extreme values because it satisfies the condition to\nminimize \\(\\sum |y_i - A\\cdot \\text{year}[i]+B|^2\\).",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Sharing your results as zipped arrays and a csv": [
        [
            "The last step, is to share your findings. You created\nnew arrays that represent a linear regression model and Gordon Moore\u2019s\nprediction. You started this process by importing a csv file into a NumPy\narray using np.loadtxt, to save your model use two approaches",
            "markdown"
        ],
        [
            ": save NumPy arrays for other Python sessions",
            "markdown"
        ],
        [
            ": save a csv file with the original data and your predicted data",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Sharing your results as zipped arrays and a csv->Zipping the arrays into a file": [
        [
            "Using np.savez, you can save thousands of arrays and give them names. The\nfunction np.load will load the arrays back into the workspace as a\ndictionary. You\u2019ll save a five arrays so the next user will have the year,\ntransistor count, predicted transistor count,  Gordon Moore\u2019s\npredicted count, and fitting constants. Add one more variable that other users can use to\nunderstand the model, notes.",
            "markdown"
        ],
        [
            "notes = \"the arrays in this file are the result of a linear regression model\\n\"\nnotes += \"the arrays include\\nyear: year of manufacture\\n\"\nnotes += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\"\nnotes += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B, A\n)\nnotes += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B_M, A_M\n)\nnotes += \"regression_csts: linear regression constants A and B for log(transistor_count)=A*year+B\"\nprint(notes)",
            "code"
        ],
        [
            "the arrays in this file are the result of a linear regression model\nthe arrays include\nyear: year of manufacture\ntransistor_count: number of transistors reported by manufacturers in a given year\ntransistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)\ntransistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)\nregression_csts: linear regression constants A and B for log(transistor_count)=A*year+B",
            "code"
        ],
        [
            "np.savez(\n    \"mooreslaw_regression.npz\",\n    notes=notes,\n    year=year,\n    transistor_count=transistor_count,\n    transistor_count_predicted=transistor_count_predicted,\n    transistor_Moores_law=transistor_Moores_law,\n    regression_csts=AB,\n)",
            "code"
        ],
        [
            "results = np.load(\"mooreslaw_regression.npz\")",
            "code"
        ],
        [
            "print(results[\"regression_csts\"][1])",
            "code"
        ],
        [
            "-666.3264063536255",
            "code"
        ],
        [
            "! ls",
            "code"
        ],
        [
            "air-quality-data.csv\nmooreslaw_regression.npz\nmooreslaw-tutorial.md\npairing.md\nsave-load-arrays.md\n_static\ntext_preprocessing.py\ntransistor_data.csv\ntutorial-air-quality-analysis.md\ntutorial-deep-learning-on-mnist.md\ntutorial-deep-reinforcement-learning-with-pong-from-pixels.md\ntutorial-ma.md\ntutorial-nlp-from-scratch\ntutorial-nlp-from-scratch.md\ntutorial-plotting-fractals\ntutorial-plotting-fractals.md\ntutorial-static_equilibrium.md\ntutorial-style-guide.md\ntutorial-svd.md\ntutorial-x-ray-image-processing\ntutorial-x-ray-image-processing.md\nwho_covid_19_sit_rep_time_series.csv\nx_y-squared.csv\nx_y-squared.npz",
            "code"
        ],
        [
            "The benefit of np.savez is you can save hundreds of arrays with\ndifferent shapes and types. Here, you saved 4 arrays that are double\nprecision floating point numbers shape = (179,), one array that was\ntext, and one array of double precision floating point numbers shape =\n(2,). This is the preferred method for saving NumPy arrays for use in\nanother analysis.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Sharing your results as zipped arrays and a csv->Creating your own comma separated value file": [
        [
            "If you want to share data and view the results in a table, then you have to\ncreate a text file. Save the data using np.savetxt. This\nfunction is more limited than np.savez. Delimited files, like csv\u2019s,\nneed 2D arrays.",
            "markdown"
        ],
        [
            "Prepare the data for export by creating a new 2D array whose columns\ncontain the data of interest.",
            "markdown"
        ],
        [
            "Use the header option to describe the data and the columns of\nthe file. Define another variable that contains file\ninformation as head.",
            "markdown"
        ],
        [
            "head = \"the columns in this file are the result of a linear regression model\\n\"\nhead += \"the columns include\\nyear: year of manufacture\\n\"\nhead += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\"\nhead += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B, A\n)\nhead += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B_M, A_M\n)\nhead += \"year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:\"\nprint(head)",
            "code"
        ],
        [
            "the columns in this file are the result of a linear regression model\nthe columns include\nyear: year of manufacture\ntransistor_count: number of transistors reported by manufacturers in a given year\ntransistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)\ntransistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)\nyear:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:",
            "code"
        ],
        [
            "Build a single 2D array to export to csv. Tabular data is inherently two\ndimensional. You need to organize your data to fit this 2D structure.\nUse year, transistor_count, transistor_count_predicted, and\ntransistor_Moores_law as the first through fourth columns,\nrespectively. Put the calculated constants in the header since they do\nnot fit the (179,) shape. The\n\nfunction appends arrays together to create a new, larger array. Arrange\nthe 1D vectors as columns using\n\ne.g.",
            "markdown"
        ],
        [
            "&gt;&gt;&gt; year.shape\n(179,)\n&gt;&gt;&gt; year[:,np.newaxis].shape\n(179,1)",
            "code"
        ],
        [
            "output = np.block(\n    [\n        year[:, np.newaxis],\n        transistor_count[:, np.newaxis],\n        transistor_count_predicted[:, np.newaxis],\n        transistor_Moores_law[:, np.newaxis],\n    ]\n)",
            "code"
        ],
        [
            "Creating the mooreslaw_regression.csv with np.savetxt, use three\noptions to create the desired file format:",
            "markdown"
        ],
        [
            "X = output : use output block to write the data into the file",
            "markdown"
        ],
        [
            "delimiter = ',' : use commas to separate columns in the file",
            "markdown"
        ],
        [
            "header = head : use the header head defined above",
            "markdown"
        ],
        [
            "np.savetxt(\"mooreslaw_regression.csv\", X=output, delimiter=\",\", header=head)",
            "code"
        ],
        [
            "! head mooreslaw_regression.csv",
            "code"
        ],
        [
            "# the columns in this file are the result of a linear regression model\n# the columns include\n# year: year of manufacture\n# transistor_count: number of transistors reported by manufacturers in a given year\n# transistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)\n# transistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)\n# year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:\n1.971000000000000000e+03,2.250000000000000000e+03,1.130514785642205879e+03,2.249999999999916326e+03\n1.972000000000000000e+03,3.500000000000000000e+03,1.590908400344028905e+03,3.181980515339620069e+03\n1.973000000000000000e+03,2.500000000000000000e+03,2.238793840141975579e+03,4.500000000000097316e+03",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Wrapping up": [
        [
            "In conclusion, you have compared historical data for semiconductor\nmanufacturers to Moore\u2019s law and created a linear regression model to\nfind the average number of transistors added to each microprocessor\nevery two years. Gordon Moore predicted the number of transistors would\ndouble every two years from 1965 through 1975, but the average growth\nhas maintained a consistent increase of \\(\\times 1.98 \\pm 0.01\\) every two\nyears from 1971 through 2019.  In 2015, Moore revised his prediction to\nsay Moore\u2019s law should hold until 2025.\n[].\nYou can share these results as a zipped NumPy array file,\nmooreslaw_regression.npz, or as another csv,\nmooreslaw_regression.csv.  The amazing progress in semiconductor\nmanufacturing has enabled new industries and computational power. This\nanalysis should give you a small insight into how incredible this growth\nhas been over the last half-century.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->References": [
        [
            ".",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Deep learning on MNIST": [
        [
            "This tutorial demonstrates how to build a simple  (with one hidden layer) and train it from scratch with NumPy to recognize handwritten digit images.",
            "markdown"
        ],
        [
            "Your deep learning model \u2014 one of the most basic artificial neural networks that resembles the original  \u2014 will learn to classify digits from 0 to 9 from the  dataset. The dataset contains 60,000 training and 10,000 test images and corresponding labels. Each training and test image is of size 784 (or 28x28 pixels) \u2014 this will be your input for the neural network.",
            "markdown"
        ],
        [
            "Based on the image inputs and their labels (), your neural network will be trained to learn their features using forward propagation and backpropagation ( differentiation). The final output of the network is a vector of 10 scores \u2014 one for each handwritten digit image. You will also evaluate how good your model is at classifying the images on the test set.",
            "markdown"
        ],
        [
            "<img alt=\"Diagram showing operations detailed in this tutorial (The input imageis passed into a Hidden layer that creates a weighted sum of outputs.The weighted sum is passed to the Non-linearity, then regularization andinto the output layer. The output layer creates a prediction which canthen be compared to existing data. The errors are used to calculate theloss function and update weights in the hidden layer and outputlayer.)\" src=\"../_images/tutorial-deep-learning-on-mnist.png\"/>",
            "markdown"
        ],
        [
            "This tutorial was adapted from the work by  (with the author\u2019s permission).",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Deep learning on MNIST->Prerequisites": [
        [
            "The reader should have some knowledge of Python, NumPy array manipulation, and linear algebra. In addition, you should be familiar with main concepts of .",
            "markdown"
        ],
        [
            "To refresh the memory, you can take the  and  tutorials.",
            "markdown"
        ],
        [
            "You are advised to read the  paper published in 2015 by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, who are regarded as some of the pioneers of the field. You should also consider reading Andrew Trask\u2019s , which teaches deep learning with NumPy.",
            "markdown"
        ],
        [
            "In addition to NumPy, you will be utilizing the following Python standard modules for data loading and processing:",
            "markdown"
        ],
        [
            " for URL handling",
            "markdown"
        ],
        [
            " for URL opening",
            "markdown"
        ],
        [
            " for gzip file decompression",
            "markdown"
        ],
        [
            " to work with the pickle file format",
            "markdown"
        ],
        [
            "as well as:",
            "markdown"
        ],
        [
            " for data visualization",
            "markdown"
        ],
        [
            "This tutorial can be run locally in an isolated environment, such as  or . You can use  to run each notebook cell. Don\u2019t forget to  and .",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Deep learning on MNIST->Table of contents": [
        [
            "Load the MNIST dataset",
            "markdown"
        ],
        [
            "Preprocess the dataset",
            "markdown"
        ],
        [
            "Build and train a small neural network from scratch",
            "markdown"
        ],
        [
            "Next steps",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Deep learning on MNIST->1. Load the MNIST dataset": [
        [
            "In this section, you will download the zipped MNIST dataset files originally stored in . Then, you will transform them into 4 files of NumPy array type using built-in Python modules. Finally, you will split the arrays into training and test sets.",
            "markdown"
        ],
        [
            "<strong>1.</strong> Define a variable to store the training/test image/label names of the MNIST dataset in a list:",
            "markdown"
        ],
        [
            "data_sources = {\n    \"training_images\": \"train-images-idx3-ubyte.gz\",  # 60,000 training images.\n    \"test_images\": \"t10k-images-idx3-ubyte.gz\",  # 10,000 test images.\n    \"training_labels\": \"train-labels-idx1-ubyte.gz\",  # 60,000 training labels.\n    \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",  # 10,000 test labels.\n}",
            "code"
        ],
        [
            "<strong>2.</strong> Load the data. First check if the data is stored locally; if not, then\ndownload it.",
            "markdown"
        ],
        [
            "import requests\nimport os\n\ndata_dir = \"../_data\"\nos.makedirs(data_dir, exist_ok=True)\n\nbase_url = \"https://github.com/rossbar/numpy-tutorial-data-mirror/blob/main/\"\n\nfor fname in data_sources.values():\n    fpath = os.path.join(data_dir, fname)\n    if not os.path.exists(fpath):\n        print(\"Downloading file: \" + fname)\n        resp = requests.get(base_url + fname, stream=True, **request_opts)\n        resp.raise_for_status()  # Ensure download was succesful\n        with open(fpath, \"wb\") as fh:\n            for chunk in resp.iter_content(chunk_size=128):\n                fh.write(chunk)",
            "code"
        ],
        [
            "Downloading file: train-images-idx3-ubyte.gz",
            "code"
        ],
        [
            "Downloading file: t10k-images-idx3-ubyte.gz",
            "code"
        ],
        [
            "Downloading file: train-labels-idx1-ubyte.gz",
            "code"
        ],
        [
            "Downloading file: t10k-labels-idx1-ubyte.gz",
            "code"
        ],
        [
            "<strong>3.</strong> Decompress the 4 files and create 4 , saving them into a dictionary. Each original image is of size 28x28 and neural networks normally expect a 1D vector input; therefore, you also need to reshape the images by multiplying 28 by 28 (784).",
            "markdown"
        ],
        [
            "import gzip\nimport numpy as np\n\nmnist_dataset = {}\n\n# Images\nfor key in (\"training_images\", \"test_images\"):\n    with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file:\n        mnist_dataset[key] = np.frombuffer(\n            mnist_file.read(), np.uint8, offset=16\n        ).reshape(-1, 28 * 28)\n# Labels\nfor key in (\"training_labels\", \"test_labels\"):\n    with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file:\n        mnist_dataset[key] = np.frombuffer(mnist_file.read(), np.uint8, offset=8)",
            "code"
        ],
        [
            "<strong>4.</strong> Split the data into training and test sets using the standard notation of x for data and y for labels, calling the training and test set images x_train and x_test, and the labels y_train and y_test:",
            "markdown"
        ],
        [
            "x_train, y_train, x_test, y_test = (\n    mnist_dataset[\"training_images\"],\n    mnist_dataset[\"training_labels\"],\n    mnist_dataset[\"test_images\"],\n    mnist_dataset[\"test_labels\"],\n)",
            "code"
        ],
        [
            "<strong>5.</strong> You can confirm that the shape of the image arrays is (60000, 784) and (10000, 784) for training and test sets, respectively, and the labels \u2014 (60000,) and (10000,):",
            "markdown"
        ],
        [
            "print(\n    \"The shape of training images: {} and training labels: {}\".format(\n        x_train.shape, y_train.shape\n    )\n)\nprint(\n    \"The shape of test images: {} and test labels: {}\".format(\n        x_test.shape, y_test.shape\n    )\n)",
            "code"
        ],
        [
            "The shape of training images: (60000, 784) and training labels: (60000,)\nThe shape of test images: (10000, 784) and test labels: (10000,)",
            "code"
        ],
        [
            "<strong>6.</strong> And you can inspect some images using Matplotlib:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\n# Take the 60,000th image (indexed at 59,999) from the training set,\n# reshape from (784, ) to (28, 28) to have a valid shape for displaying purposes.\nmnist_image = x_train[59999, :].reshape(28, 28)\n# Set the color mapping to grayscale to have a black background.\nplt.imshow(mnist_image, cmap=\"gray\")\n# Display the image.\nplt.show()\n\n\n\n\n<img alt=\"../_images/6e96473e0d5890e8b54a3796e049afb704ba9c4b0fcf60945de18f4cf0454c8f.png\" src=\"../_images/6e96473e0d5890e8b54a3796e049afb704ba9c4b0fcf60945de18f4cf0454c8f.png\"/>",
            "code"
        ],
        [
            "# Display 5 random images from the training set.\nnum_examples = 5\nseed = 147197952744\nrng = np.random.default_rng(seed)\n\nfig, axes = plt.subplots(1, num_examples)\nfor sample, ax in zip(rng.choice(x_train, size=num_examples, replace=False), axes):\n    ax.imshow(sample.reshape(28, 28), cmap=\"gray\")\n\n\n\n\n<img alt=\"../_images/8484ad8185328c820925db98e28702162d6c6d279eb7cd636dfc9e2d94b68215.png\" src=\"../_images/8484ad8185328c820925db98e28702162d6c6d279eb7cd636dfc9e2d94b68215.png\"/>",
            "code"
        ],
        [
            "<em>Above are five images taken from the MNIST training set. Various hand-drawn\nArabic numerals are shown, with exact values chosen randomly with each run of the code.</em>\n<blockquote>",
            "markdown"
        ],
        [
            "<strong>Note:</strong> You can also visualize a sample image as an array by printing x_train[59999]. Here, 59999 is your 60,000th training image sample (0 would be your first). Your output will be quite long and should contain an array of 8-bit integers:",
            "markdown"
        ],
        [
            "...\n         0,   0,  38,  48,  48,  22,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,  62,  97, 198, 243, 254, 254, 212,  27,   0,   0,   0,   0,\n...\n\n\n</blockquote>",
            "code"
        ],
        [
            "# Display the label of the 60,000th image (indexed at 59,999) from the training set.\ny_train[59999]",
            "code"
        ],
        [
            "8",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data": [
        [
            "Neural networks can work with inputs that are in a form of tensors (multidimensional arrays) of floating-point type. When preprocessing the data, you should consider the following processes:  and .",
            "markdown"
        ],
        [
            "Since the MNIST data is already vectorized and the arrays are of dtype uint8, your next challenge is to convert them to a floating-point format, such as float64 ():",
            "markdown"
        ],
        [
            "<em>Normalizing</em> the image data: a  procedure that can speed up the neural network training process by standardizing the .",
            "markdown"
        ],
        [
            "<em></em> of the image labels.",
            "markdown"
        ],
        [
            "In practice, you can use different types of floating-point precision depending on your goals and you can find more information about that in the  and  blog posts.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format": [
        [
            "The images data contain 8-bit integers encoded in the [0, 255] interval with color values between 0 and 255.",
            "markdown"
        ],
        [
            "You will normalize them into floating-point arrays in the [0, 1] interval by dividing them by 255.",
            "markdown"
        ],
        [
            "<strong>1.</strong> Check that the vectorized image data has type uint8:",
            "markdown"
        ],
        [
            "print(\"The data type of training images: {}\".format(x_train.dtype))\nprint(\"The data type of test images: {}\".format(x_test.dtype))",
            "code"
        ],
        [
            "The data type of training images: uint8\nThe data type of test images: uint8",
            "code"
        ],
        [
            "<strong>2.</strong> Normalize the arrays by dividing them by 255 (and thus promoting the data type from uint8 to float64) and then assign the train and test image data variables \u2014 x_train and x_test \u2014 to training_images and train_labels, respectively.\nTo reduce the model training and evaluation time in this example, only a subset\nof the training and test images will be used.\nBoth training_images and test_images will contain only 1,000 samples each out\nof the complete datasets of 60,000 and 10,000 images, respectively.\nThese values can be controlled by changing the  training_sample and\ntest_sample below, up to their maximum values of 60,000 and 10,000.",
            "markdown"
        ],
        [
            "training_sample, test_sample = 1000, 1000\ntraining_images = x_train[0:training_sample] / 255\ntest_images = x_test[0:test_sample] / 255",
            "code"
        ],
        [
            "<strong>3.</strong> Confirm that the image data has changed to the floating-point format:",
            "markdown"
        ],
        [
            "print(\"The data type of training images: {}\".format(training_images.dtype))\nprint(\"The data type of test images: {}\".format(test_images.dtype))",
            "code"
        ],
        [
            "The data type of training images: float64\nThe data type of test images: float64\n\n\n\n\n<blockquote>",
            "code"
        ],
        [
            "<strong>Note:</strong> You can also check that normalization was successful by printing training_images[0] in a notebook cell. Your long output should contain an array of floating-point numbers:",
            "markdown"
        ],
        [
            "...\n       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n...\n\n\n</blockquote>",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding": [
        [
            "You will use one-hot encoding to embed each digit label as an all-zero vector with np.zeros() and place 1 for a label index. As a result, your label data will be arrays with 1.0 (or 1.) in the position of each image label.",
            "markdown"
        ],
        [
            "Since there are 10 labels (from 0 to 9) in total, your arrays will look similar to this:",
            "markdown"
        ],
        [
            "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])",
            "code"
        ],
        [
            "<strong>1.</strong> Confirm that the image label data are integers with dtype uint8:",
            "markdown"
        ],
        [
            "print(\"The data type of training labels: {}\".format(y_train.dtype))\nprint(\"The data type of test labels: {}\".format(y_test.dtype))",
            "code"
        ],
        [
            "The data type of training labels: uint8\nThe data type of test labels: uint8",
            "code"
        ],
        [
            "<strong>2.</strong> Define a function that performs one-hot encoding on arrays:",
            "markdown"
        ],
        [
            "def one_hot_encoding(labels, dimension=10):\n    # Define a one-hot variable for an all-zero vector\n    # with 10 dimensions (number labels from 0 to 9).\n    one_hot_labels = labels[..., None] == np.arange(dimension)[None]\n    # Return one-hot encoded labels.\n    return one_hot_labels.astype(np.float64)",
            "code"
        ],
        [
            "<strong>3.</strong> Encode the labels and assign the values to new variables:",
            "markdown"
        ],
        [
            "training_labels = one_hot_encoding(y_train[:training_sample])\ntest_labels = one_hot_encoding(y_test[:test_sample])",
            "code"
        ],
        [
            "<strong>4.</strong> Check that the data type has changed to floating point:",
            "markdown"
        ],
        [
            "print(\"The data type of training labels: {}\".format(training_labels.dtype))\nprint(\"The data type of test labels: {}\".format(test_labels.dtype))",
            "code"
        ],
        [
            "The data type of training labels: float64\nThe data type of test labels: float64",
            "code"
        ],
        [
            "<strong>5.</strong> Examine a few encoded labels:",
            "markdown"
        ],
        [
            "print(training_labels[0])\nprint(training_labels[1])\nprint(training_labels[2])",
            "code"
        ],
        [
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]",
            "code"
        ],
        [
            "\u2026and compare to the originals:",
            "markdown"
        ],
        [
            "print(y_train[0])\nprint(y_train[1])\nprint(y_train[2])",
            "code"
        ],
        [
            "5\n0\n4",
            "code"
        ],
        [
            "You have finished preparing the dataset.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Deep learning on MNIST->3. Build and train a small neural network from scratch": [
        [
            "In this section you will familiarize yourself with some high-level concepts of the basic building blocks of a deep learning model. You can refer to the original  research publication for more information.",
            "markdown"
        ],
        [
            "Afterwards, you will construct the building blocks of a simple deep learning model in Python and NumPy and train it to learn to identify handwritten digits from the MNIST dataset with a certain level of accuracy.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Deep learning on MNIST->3. Build and train a small neural network from scratch->Neural network building blocks with NumPy": [
        [
            "<em>Layers</em>: These building blocks work as data filters \u2014 they process data and learn representations from inputs to better predict the target outputs.",
            "markdown"
        ],
        [
            "You will use 1 hidden layer in your model to pass the inputs forward (<em>forward propagation</em>) and propagate the gradients/error derivatives of a loss function backward (<em>backpropagation</em>). These are input, hidden and output layers.",
            "markdown"
        ],
        [
            "In the hidden (middle) and output (last) layers, the neural network model will compute the weighted sum of inputs. To compute this process, you will use NumPy\u2019s matrix multiplication function (the \u201cdot multiply\u201d or np.dot(layer, weights)).\n<blockquote>",
            "markdown"
        ],
        [
            "<strong>Note:</strong> For simplicity, the bias term is omitted in this example (there is no np.dot(layer, weights) + bias).\n</blockquote>",
            "markdown"
        ],
        [
            "<em>Weights</em>: These are important adjustable parameters that the neural network fine-tunes by forward and backward propagating the data. They are optimized through a process called . Before the model training starts, the weights are randomly initialized with NumPy\u2019s .",
            "markdown"
        ],
        [
            "The optimal weights should produce the highest prediction accuracy and the lowest error on the training and test sets.",
            "markdown"
        ],
        [
            "<em>Activation function</em>: Deep learning models are capable of determining non-linear relationships between inputs and outputs and these  are usually applied to the output of each layer.",
            "markdown"
        ],
        [
            "You will use a  to the hidden layer\u2019s output (for example, relu(np.dot(layer, weights)).",
            "markdown"
        ],
        [
            "<em>Regularization</em>: This  helps prevent the neural network model from .",
            "markdown"
        ],
        [
            "In this example, you will use a method called dropout \u2014  \u2014 that randomly sets a number of features in a layer to 0s. You will define it with NumPy\u2019s  method and apply it to the hidden layer of the network.",
            "markdown"
        ],
        [
            "<em>Loss function</em>: The computation determines the quality of predictions by comparing the image labels (the truth) with the predicted values in the final layer\u2019s output.",
            "markdown"
        ],
        [
            "For simplicity, you will use a basic total squared error using NumPy\u2019s np.sum() function (for example, np.sum((final_layer_output - image_labels) ** 2)).",
            "markdown"
        ],
        [
            "<em>Accuracy</em>: This metric measures the accuracy of the network\u2019s ability to predict on the data it hasn\u2019t seen.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Deep learning on MNIST->3. Build and train a small neural network from scratch->Model architecture and training summary": [
        [
            "Here is a summary of the neural network model architecture and the training process:",
            "markdown"
        ],
        [
            "<img alt=\"Diagram showing operations detailed in this tutorial (The input imageis passed into a Hidden layer that creates a weighted sum of outputs.The weighted sum is passed to the Non-linearity, then regularization andinto the output layer. The output layer creates a prediction which canthen be compared to existing data. The errors are used to calculate theloss function and update weights in the hidden layer and outputlayer.)\" src=\"../_images/tutorial-deep-learning-on-mnist.png\"/>",
            "markdown"
        ],
        [
            "<em>The input layer</em>:",
            "markdown"
        ],
        [
            "It is the input for the network \u2014 the previously preprocessed data that is loaded from training_images into layer_0.",
            "markdown"
        ],
        [
            "<em>The hidden (middle) layer</em>:",
            "markdown"
        ],
        [
            "layer_1 takes the output from the previous layer and performs matrix-multiplication of the input by weights (weights_1) with NumPy\u2019s np.dot()).",
            "markdown"
        ],
        [
            "Then, this output is passed through the ReLU activation function for non-linearity and then dropout is applied to help with overfitting.",
            "markdown"
        ],
        [
            "<em>The output (last) layer</em>:",
            "markdown"
        ],
        [
            "layer_2 ingests the output from layer_1 and repeats the same \u201cdot multiply\u201d process with weights_2.",
            "markdown"
        ],
        [
            "The final output returns 10 scores for each of the 0-9 digit labels. The network model ends with a size 10 layer \u2014 a 10-dimensional vector.",
            "markdown"
        ],
        [
            "<em>Forward propagation, backpropagation, training loop</em>:",
            "markdown"
        ],
        [
            "In the beginning of model training, your network randomly initializes the weights and feeds the input data forward through the hidden and output layers. This process is the forward pass or forward propagation.",
            "markdown"
        ],
        [
            "Then, the network propagates the \u201csignal\u201d from the loss function back through the hidden layer and adjusts the weights values with the help of the learning rate parameter (more on that later).\n\n\n<blockquote>",
            "markdown"
        ],
        [
            "<strong>Note:</strong> In more technical terms, you:",
            "markdown"
        ],
        [
            "Measure the error by comparing the real label of an image (the truth) with the prediction of the model.",
            "markdown"
        ],
        [
            "Differentiate the loss function.",
            "markdown"
        ],
        [
            "Ingest the  with the respect to the output, and backpropagate them with the respect to the inputs through the layer(s).",
            "markdown"
        ],
        [
            "Since the network contains tensor operations and weight matrices, backpropagation uses the .",
            "markdown"
        ],
        [
            "With each iteration (epoch) of the neural network training, this forward and backward propagation cycle adjusts the weights, which is reflected in the accuracy and error metrics. As you train the model, your goal is to minimize the error and maximize the accuracy on the training data, where the model learns from, as well as the test data, where you evaluate the model.\n</blockquote>",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Deep learning on MNIST->3. Build and train a small neural network from scratch->Compose the model and begin training and testing it": [
        [
            "Having covered the main deep learning concepts and the neural network architecture, let\u2019s write the code.",
            "markdown"
        ],
        [
            "<strong>1.</strong> We\u2019ll start by creating a new random number generator, providing a seed\nfor reproducibility:",
            "markdown"
        ],
        [
            "seed = 884736743\nrng = np.random.default_rng(seed)",
            "code"
        ],
        [
            "<strong>2.</strong> For the hidden layer, define the ReLU activation function for forward propagation and ReLU\u2019s derivative that will be used during backpropagation:",
            "markdown"
        ],
        [
            "# Define ReLU that returns the input if it's positive and 0 otherwise.\ndef relu(x):\n    return (x &gt;= 0) * x\n\n\n# Set up a derivative of the ReLU function that returns 1 for a positive input\n# and 0 otherwise.\ndef relu2deriv(output):\n    return output &gt;= 0",
            "code"
        ],
        [
            "<strong>3.</strong> Set certain default values of , such as:",
            "markdown"
        ],
        [
            ": learning_rate \u2014 helps limit the magnitude of weight updates to prevent them from overcorrecting.",
            "markdown"
        ],
        [
            "<em>Epochs (iterations)</em>: epochs \u2014 the number of complete passes \u2014 forward and backward propagations \u2014 of the data through the network. This parameter can positively or negatively affect the results. The higher the iterations, the longer the learning process may take. Because this is a computationally intensive task, we have chosen a very low number of epochs (20). To get meaningful results, you should choose a much larger number.",
            "markdown"
        ],
        [
            "<em>Size of the hidden (middle) layer in a network</em>: hidden_size \u2014 different sizes of the hidden layer can affect the results during training and testing.",
            "markdown"
        ],
        [
            "<em>Size of the input:</em> pixels_per_image \u2014 you have established that the image input is 784 (28x28) (in pixels).",
            "markdown"
        ],
        [
            "<em>Number of labels</em>: num_labels \u2014 indicates the output number for the output layer where the predictions occur for 10 (0 to 9) handwritten digit labels.",
            "markdown"
        ],
        [
            "learning_rate = 0.005\nepochs = 20\nhidden_size = 100\npixels_per_image = 784\nnum_labels = 10",
            "code"
        ],
        [
            "<strong>4.</strong> Initialize the weight vectors that will be used in the hidden and output layers with random values:",
            "markdown"
        ],
        [
            "weights_1 = 0.2 * rng.random((pixels_per_image, hidden_size)) - 0.1\nweights_2 = 0.2 * rng.random((hidden_size, num_labels)) - 0.1",
            "code"
        ],
        [
            "<strong>5.</strong> Set up the neural network\u2019s learning experiment with a training loop and start the training process.\nNote that the model is evaluated against the test set at each epoch to track\nits performance over the training epochs.",
            "markdown"
        ],
        [
            "Start the training process:",
            "markdown"
        ],
        [
            "# To store training and test set losses and accurate predictions\n# for visualization.\nstore_training_loss = []\nstore_training_accurate_pred = []\nstore_test_loss = []\nstore_test_accurate_pred = []\n\n# This is a training loop.\n# Run the learning experiment for a defined number of epochs (iterations).\nfor j in range(epochs):\n\n    #################\n    # Training step #\n    #################\n\n    # Set the initial loss/error and the number of accurate predictions to zero.\n    training_loss = 0.0\n    training_accurate_predictions = 0\n\n    # For all images in the training set, perform a forward pass\n    # and backpropagation and adjust the weights accordingly.\n    for i in range(len(training_images)):\n        # Forward propagation/forward pass:\n        # 1. The input layer:\n        #    Initialize the training image data as inputs.\n        layer_0 = training_images[i]\n        # 2. The hidden layer:\n        #    Take in the training image data into the middle layer by\n        #    matrix-multiplying it by randomly initialized weights.\n        layer_1 = np.dot(layer_0, weights_1)\n        # 3. Pass the hidden layer's output through the ReLU activation function.\n        layer_1 = relu(layer_1)\n        # 4. Define the dropout function for regularization.\n        dropout_mask = rng.integers(low=0, high=2, size=layer_1.shape)\n        # 5. Apply dropout to the hidden layer's output.\n        layer_1 *= dropout_mask * 2\n        # 6. The output layer:\n        #    Ingest the output of the middle layer into the the final layer\n        #    by matrix-multiplying it by randomly initialized weights.\n        #    Produce a 10-dimension vector with 10 scores.\n        layer_2 = np.dot(layer_1, weights_2)\n\n        # Backpropagation/backward pass:\n        # 1. Measure the training error (loss function) between the actual\n        #    image labels (the truth) and the prediction by the model.\n        training_loss += np.sum((training_labels[i] - layer_2) ** 2)\n        # 2. Increment the accurate prediction count.\n        training_accurate_predictions += int(\n            np.argmax(layer_2) == np.argmax(training_labels[i])\n        )\n        # 3. Differentiate the loss function/error.\n        layer_2_delta = training_labels[i] - layer_2\n        # 4. Propagate the gradients of the loss function back through the hidden layer.\n        layer_1_delta = np.dot(weights_2, layer_2_delta) * relu2deriv(layer_1)\n        # 5. Apply the dropout to the gradients.\n        layer_1_delta *= dropout_mask\n        # 6. Update the weights for the middle and input layers\n        #    by multiplying them by the learning rate and the gradients.\n        weights_1 += learning_rate * np.outer(layer_0, layer_1_delta)\n        weights_2 += learning_rate * np.outer(layer_1, layer_2_delta)\n\n    # Store training set losses and accurate predictions.\n    store_training_loss.append(training_loss)\n    store_training_accurate_pred.append(training_accurate_predictions)\n\n    ###################\n    # Evaluation step #\n    ###################\n\n    # Evaluate model performance on the test set at each epoch.\n\n    # Unlike the training step, the weights are not modified for each image\n    # (or batch). Therefore the model can be applied to the test images in a\n    # vectorized manner, eliminating the need to loop over each image\n    # individually:\n\n    results = relu(test_images @ weights_1) @ weights_2\n\n    # Measure the error between the actual label (truth) and prediction values.\n    test_loss = np.sum((test_labels - results) ** 2)\n\n    # Measure prediction accuracy on test set\n    test_accurate_predictions = np.sum(\n        np.argmax(results, axis=1) == np.argmax(test_labels, axis=1)\n    )\n\n    # Store test set losses and accurate predictions.\n    store_test_loss.append(test_loss)\n    store_test_accurate_pred.append(test_accurate_predictions)\n\n    # Summarize error and accuracy metrics at each epoch\n    print(\n        (\n            f\"Epoch: {j}\\n\"\n            f\"  Training set error: {training_loss / len(training_images):.3f}\\n\"\n            f\"  Training set accuracy: {training_accurate_predictions / len(training_images)}\\n\"\n            f\"  Test set error: {test_loss / len(test_images):.3f}\\n\"\n            f\"  Test set accuracy: {test_accurate_predictions / len(test_images)}\"\n        )\n    )",
            "code"
        ],
        [
            "Epoch: 0\n  Training set error: 0.898\n  Training set accuracy: 0.397\n  Test set error: 0.680\n  Test set accuracy: 0.582",
            "code"
        ],
        [
            "Epoch: 1\n  Training set error: 0.656\n  Training set accuracy: 0.633\n  Test set error: 0.607\n  Test set accuracy: 0.641",
            "code"
        ],
        [
            "Epoch: 2\n  Training set error: 0.592\n  Training set accuracy: 0.68\n  Test set error: 0.569\n  Test set accuracy: 0.679",
            "code"
        ],
        [
            "Epoch: 3\n  Training set error: 0.556\n  Training set accuracy: 0.7\n  Test set error: 0.541\n  Test set accuracy: 0.708",
            "code"
        ],
        [
            "Epoch: 4\n  Training set error: 0.534\n  Training set accuracy: 0.732\n  Test set error: 0.526\n  Test set accuracy: 0.729",
            "code"
        ],
        [
            "Epoch: 5\n  Training set error: 0.515\n  Training set accuracy: 0.715\n  Test set error: 0.500\n  Test set accuracy: 0.739",
            "code"
        ],
        [
            "Epoch: 6\n  Training set error: 0.495\n  Training set accuracy: 0.748\n  Test set error: 0.487\n  Test set accuracy: 0.753",
            "code"
        ],
        [
            "Epoch: 7\n  Training set error: 0.483\n  Training set accuracy: 0.769\n  Test set error: 0.486\n  Test set accuracy: 0.747",
            "code"
        ],
        [
            "Epoch: 8\n  Training set error: 0.473\n  Training set accuracy: 0.776\n  Test set error: 0.473\n  Test set accuracy: 0.752",
            "code"
        ],
        [
            "Epoch: 9\n  Training set error: 0.460\n  Training set accuracy: 0.788\n  Test set error: 0.462\n  Test set accuracy: 0.762",
            "code"
        ],
        [
            "Epoch: 10\n  Training set error: 0.465\n  Training set accuracy: 0.769\n  Test set error: 0.462\n  Test set accuracy: 0.767",
            "code"
        ],
        [
            "Epoch: 11\n  Training set error: 0.443\n  Training set accuracy: 0.801\n  Test set error: 0.456\n  Test set accuracy: 0.775",
            "code"
        ],
        [
            "Epoch: 12\n  Training set error: 0.448\n  Training set accuracy: 0.795\n  Test set error: 0.455\n  Test set accuracy: 0.772",
            "code"
        ],
        [
            "Epoch: 13\n  Training set error: 0.438\n  Training set accuracy: 0.787\n  Test set error: 0.453\n  Test set accuracy: 0.778",
            "code"
        ],
        [
            "Epoch: 14\n  Training set error: 0.446\n  Training set accuracy: 0.791\n  Test set error: 0.450\n  Test set accuracy: 0.779",
            "code"
        ],
        [
            "Epoch: 15\n  Training set error: 0.441\n  Training set accuracy: 0.788\n  Test set error: 0.452\n  Test set accuracy: 0.772",
            "code"
        ],
        [
            "Epoch: 16\n  Training set error: 0.437\n  Training set accuracy: 0.786\n  Test set error: 0.453\n  Test set accuracy: 0.772",
            "code"
        ],
        [
            "Epoch: 17\n  Training set error: 0.436\n  Training set accuracy: 0.794\n  Test set error: 0.449\n  Test set accuracy: 0.778",
            "code"
        ],
        [
            "Epoch: 18\n  Training set error: 0.433\n  Training set accuracy: 0.801\n  Test set error: 0.450\n  Test set accuracy: 0.774",
            "code"
        ],
        [
            "Epoch: 19\n  Training set error: 0.429\n  Training set accuracy: 0.785\n  Test set error: 0.436\n  Test set accuracy: 0.784",
            "code"
        ],
        [
            "The training process may take many minutes, depending on a number of factors, such as the processing power of the machine you are running the experiment on and the number of epochs. To reduce the waiting time, you can change the epoch (iteration) variable from 100 to a lower number, reset the runtime (which will reset the weights), and run the notebook cells again.",
            "markdown"
        ],
        [
            "After executing the cell above, you can visualize the training and test set errors and accuracy for an instance of this training process.",
            "markdown"
        ],
        [
            "# The training set metrics.\ny_training_error = [\n    store_training_loss[i] / float(len(training_images))\n    for i in range(len(store_training_loss))\n]\nx_training_error = range(1, len(store_training_loss) + 1)\ny_training_accuracy = [\n    store_training_accurate_pred[i] / float(len(training_images))\n    for i in range(len(store_training_accurate_pred))\n]\nx_training_accuracy = range(1, len(store_training_accurate_pred) + 1)\n\n# The test set metrics.\ny_test_error = [\n    store_test_loss[i] / float(len(test_images)) for i in range(len(store_test_loss))\n]\nx_test_error = range(1, len(store_test_loss) + 1)\ny_test_accuracy = [\n    store_training_accurate_pred[i] / float(len(training_images))\n    for i in range(len(store_training_accurate_pred))\n]\nx_test_accuracy = range(1, len(store_test_accurate_pred) + 1)\n\n# Display the plots.\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\naxes[0].set_title(\"Training set error, accuracy\")\naxes[0].plot(x_training_accuracy, y_training_accuracy, label=\"Training set accuracy\")\naxes[0].plot(x_training_error, y_training_error, label=\"Training set error\")\naxes[0].set_xlabel(\"Epochs\")\naxes[1].set_title(\"Test set error, accuracy\")\naxes[1].plot(x_test_accuracy, y_test_accuracy, label=\"Test set accuracy\")\naxes[1].plot(x_test_error, y_test_error, label=\"Test set error\")\naxes[1].set_xlabel(\"Epochs\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/a155f32ad6a3518292224002121b7167dd36f5c2c99a5ac7ac1d53357125a043.png\" src=\"../_images/a155f32ad6a3518292224002121b7167dd36f5c2c99a5ac7ac1d53357125a043.png\"/>",
            "code"
        ],
        [
            "<em>The training and testing error is shown above in the left and right\nplots, respectively. As the number of Epochs increases, the total error\ndecreases and the accuracy increases.</em>",
            "markdown"
        ],
        [
            "The accuracy rates that your model reaches during training and testing may be somewhat plausible but you may also find the error rates to be quite high.",
            "markdown"
        ],
        [
            "To reduce the error during training and testing, you can consider changing the simple loss function to, for example, categorical . Other possible solutions are discussed below.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Deep learning on MNIST->Next steps": [
        [
            "You have learned how to build and train a simple feed-forward neural network from scratch using just NumPy to classify handwritten MNIST digits.",
            "markdown"
        ],
        [
            "To further enhance and optimize your neural network model, you can consider one of a mixture of the following:",
            "markdown"
        ],
        [
            "Increase the training sample size from 1,000 to a higher number (up to 60,000).",
            "markdown"
        ],
        [
            "Use .",
            "markdown"
        ],
        [
            "Alter the architecture by introducing more hidden layers to make the network .",
            "markdown"
        ],
        [
            "Combine the  loss function with a  activation function in the last layer.",
            "markdown"
        ],
        [
            "Introduce convolutional layers: replace the feedforward network with a  architecture.",
            "markdown"
        ],
        [
            "Use a higher epoch size to train longer and add more regularization techniques, such as , to prevent .",
            "markdown"
        ],
        [
            "Introduce a  for an unbiased valuation of the model fit.",
            "markdown"
        ],
        [
            "Apply  for faster and more stable training.",
            "markdown"
        ],
        [
            "Tune other parameters, such as the learning rate and hidden layer size.",
            "markdown"
        ],
        [
            "Building a neural network from scratch with NumPy is a great way to learn more about NumPy and about deep learning. However, for real-world applications you should use specialized frameworks \u2014 such as , ,  or  \u2014 that provide NumPy-like APIs, have built-in  and GPU support, and are designed for high-performance numerical computing and machine learning.",
            "markdown"
        ],
        [
            "Finally, when developing a machine learning model, you should think about potential ethical issues and apply practices to avoid or mitigate those:",
            "markdown"
        ],
        [
            "Document a trained model with a Model Card - see the  by Margaret Mitchell et al..",
            "markdown"
        ],
        [
            "Document a dataset with a Datasheet - see the ) by Timnit Gebru et al..",
            "markdown"
        ],
        [
            "Consider the impact of your model - who is affected by it, who does it benefit - see  and  by Pratyusha Kalluri.",
            "markdown"
        ],
        [
            "For more resources, see  and the .",
            "markdown"
        ],
        [
            "(Credit to  for demonstrating how to download MNIST without the use of external libraries.)",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->X-ray image processing": [
        [
            "This tutorial demonstrates how to read and process X-ray images with NumPy,\nimageio, Matplotlib and SciPy. You will learn how to load medical images, focus\non certain parts, and visually compare them using the\n,\n,\n, and\n filters for edge\ndetection.",
            "markdown"
        ],
        [
            "X-ray image analysis can be part of your data analysis and\n\nwhen, for example, you\u2019re building an algorithm that helps\n\nas part of a \n.\nIn the healthcare industry, medical image processing and analysis is\nparticularly important when images are estimated to account for\n of all\nmedical data.",
            "markdown"
        ],
        [
            "You\u2019ll be working with radiology images from the\n\ndataset provided by the .\nChestX-ray8 contains over 100,000 de-identified X-ray images in the PNG format\nfrom more than 30,000 patients. You can find ChestX-ray8\u2019s files on NIH\u2019s public\nBox  in the /images\nfolder. (For more details, refer to the research\n\npublished at CVPR (a computer vision conference) in 2017.)",
            "markdown"
        ],
        [
            "For your convenience, a small number of PNG images have been saved to this\ntutorial\u2019s repository under tutorial-x-ray-image-processing/, since\nChestX-ray8 contains gigabytes of data and you may find it challenging to\ndownload it in batches.",
            "markdown"
        ],
        [
            "<img alt=\"A series of 9 x-ray images of the same region of a patient's chest is shown with different types of image processing filters applied to each image. Each x-ray shows different types of biological detail.\" src=\"../_images/tutorial-x-ray-image-processing.png\"/>",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->X-ray image processing->Prerequisites": [
        [
            "The reader should have some knowledge of Python, NumPy arrays, and Matplotlib.\nTo refresh the memory, you can take the\n and Matplotlib\n tutorials,\nand the NumPy .",
            "markdown"
        ],
        [
            "The following packages are used in this tutorial:",
            "markdown"
        ],
        [
            " for reading and writing image data. The\nhealthcare industry usually works with the\n format for medical imaging and\n should be\nwell-suited for reading that format. For simplicity, in this tutorial you\u2019ll be\nworking with PNG files.",
            "markdown"
        ],
        [
            " for data visualization.",
            "markdown"
        ],
        [
            " for multi-dimensional image processing via\n.",
            "markdown"
        ],
        [
            "This tutorial can be run locally in an isolated environment, such as\n or\n.\nYou can use  to run\neach notebook cell.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->X-ray image processing->Table of contents": [
        [
            "Examine an X-ray with imageio",
            "markdown"
        ],
        [
            "Combine images into a multi-dimensional array to demonstrate progression",
            "markdown"
        ],
        [
            "Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and\nCanny filters",
            "markdown"
        ],
        [
            "Apply masks to X-rays with np.where()",
            "markdown"
        ],
        [
            "Compare the results",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->X-ray image processing->Examine an X-ray with imageio": [
        [
            "Let\u2019s begin with a simple example using just one X-ray image from the\nChestX-ray8 dataset.",
            "markdown"
        ],
        [
            "The file \u2014 00000011_001.png \u2014 has been downloaded for you and saved in the\n/tutorial-x-ray-image-processing folder.",
            "markdown"
        ],
        [
            "<strong>1.</strong> Load the image with imageio:",
            "markdown"
        ],
        [
            "import os\nimport imageio\n\nDIR = \"tutorial-x-ray-image-processing\"\n\nxray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\"))",
            "code"
        ],
        [
            "<strong>2.</strong> Check that its shape is 1024x1024 pixels and that the array is made up of\n8-bit integers:",
            "markdown"
        ],
        [
            "print(xray_image.shape)\nprint(xray_image.dtype)",
            "code"
        ],
        [
            "(1024, 1024)\nuint8",
            "code"
        ],
        [
            "<strong>3.</strong> Import matplotlib and display the image in a grayscale colormap:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nplt.imshow(xray_image, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/5490847b3fbf08084e057be70bfdb737a09a1f14559a86ccf7408a5abd0812b0.png\" src=\"../_images/5490847b3fbf08084e057be70bfdb737a09a1f14559a86ccf7408a5abd0812b0.png\"/>",
            "code"
        ]
    ],
    "numpy->NumPy Applications->X-ray image processing->Combine images into a multidimensional array to demonstrate progression": [
        [
            "In the next example, instead of 1 image you\u2019ll use 9 X-ray 1024x1024-pixel\nimages from the ChestX-ray8 dataset that have been downloaded and extracted\nfrom one of the dataset files. They are numbered from ...000.png to\n...008.png and let\u2019s assume they belong to the same patient.",
            "markdown"
        ],
        [
            "<strong>1.</strong> Import NumPy, read in each of the X-rays, and create a three-dimensional\narray where the first dimension corresponds to image number:",
            "markdown"
        ],
        [
            "import numpy as np\nnum_imgs = 9\n\ncombined_xray_images_1 = np.array(\n    [imageio.v3.imread(os.path.join(DIR, f\"00000011_00{i}.png\")) for i in range(num_imgs)]\n)",
            "code"
        ],
        [
            "<strong>2.</strong> Check the shape of the new X-ray image array containing 9 stacked images:",
            "markdown"
        ],
        [
            "combined_xray_images_1.shape",
            "code"
        ],
        [
            "(9, 1024, 1024)",
            "code"
        ],
        [
            "Note that the shape in the first dimension matches num_imgs, so the\ncombined_xray_images_1 array can be interpreted as a stack of 2D images.",
            "markdown"
        ],
        [
            "<strong>3.</strong> You can now display the \u201chealth progress\u201d by plotting each of frames next\nto each other using Matplotlib:",
            "markdown"
        ],
        [
            "fig, axes = plt.subplots(nrows=1, ncols=num_imgs, figsize=(30, 30))\n\nfor img, ax in zip(combined_xray_images_1, axes):\n    ax.imshow(img, cmap='gray')\n    ax.axis('off')\n\n\n\n\n<img alt=\"../_images/c0ad74a0a105f09de501f2b79e2c983fed7f9bf2d17ba96732f2e0dd6e56d9f7.png\" src=\"../_images/c0ad74a0a105f09de501f2b79e2c983fed7f9bf2d17ba96732f2e0dd6e56d9f7.png\"/>",
            "code"
        ],
        [
            "<strong>4.</strong> In addition, it can be helpful to show the progress as an animation.\nLet\u2019s create a GIF file with imageio.mimwrite() and display the result in the\nnotebook:",
            "markdown"
        ],
        [
            "GIF_PATH = os.path.join(DIR, \"xray_image.gif\")\nimageio.mimwrite(GIF_PATH, combined_xray_images_1, format= \".gif\", fps=1)",
            "code"
        ],
        [
            "Which gives us:\n<img alt=\"An animated gif repeatedly cycles through a series of 8 x-rays, showing the same viewpoint of the patient's chest at different points in time. The patient's bones and internal organs can be visually compared from frame to frame.\" src=\"../_images/xray_image.gif\"/>",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters": [
        [
            "When processing biomedical data, it can be useful to emphasize the 2D\n to focus on particular\nfeatures in an image. To do that, using\n can be\nparticularly helpful when detecting the change of color pixel intensity.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Laplace filter with Gaussian second derivatives": [
        [
            "Let\u2019s start with an n-dimensional\n filter\n(\u201cLaplacian-Gaussian\u201d) that uses\n second\nderivatives. This Laplacian method focuses on pixels with rapid intensity change\nin values and is combined with Gaussian smoothing to\n. Let\u2019s examine\nhow it can be useful in analyzing 2D X-ray images.",
            "markdown"
        ],
        [
            "The implementation of the Laplacian-Gaussian filter is relatively\nstraightforward: 1) import the ndimage module from SciPy; and 2) call\n\nwith a sigma (scalar) parameter, which affects the standard deviations of the\nGaussian filter (you\u2019ll use 1 in the example below):",
            "markdown"
        ],
        [
            "from scipy import ndimage\n\nxray_image_laplace_gaussian = ndimage.gaussian_laplace(xray_image, sigma=1)",
            "code"
        ],
        [
            "Display the original X-ray and the one with the Laplacian-Gaussian filter:",
            "markdown"
        ],
        [
            "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Laplacian-Gaussian (edges)\")\naxes[1].imshow(xray_image_laplace_gaussian, cmap=\"gray\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/f9ee3ca99096fac8657226c65e072aea438d46085f1b0591e937158f05e4cf59.png\" src=\"../_images/f9ee3ca99096fac8657226c65e072aea438d46085f1b0591e937158f05e4cf59.png\"/>",
            "code"
        ]
    ],
    "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Gaussian gradient magnitude method": [
        [
            "Another method for edge detection that can be useful is the\n (gradient) filter.\nIt computes the multidimensional gradient magnitude with Gaussian derivatives\nand helps by remove\n\nimage components.",
            "markdown"
        ],
        [
            "<strong>1.</strong> Call \nwith a sigma (scalar) parameter (for standard deviations; you\u2019ll use 2 in the\nexample below):",
            "markdown"
        ],
        [
            "x_ray_image_gaussian_gradient = ndimage.gaussian_gradient_magnitude(xray_image, sigma=2)",
            "code"
        ],
        [
            "<strong>2.</strong> Display the original X-ray and the one with the Gaussian gradient filter:",
            "markdown"
        ],
        [
            "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Gaussian gradient (edges)\")\naxes[1].imshow(x_ray_image_gaussian_gradient, cmap=\"gray\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/219df502b351733336051b1eb47e5866d63e2487d77bf04ac960164fbb5ed167.png\" src=\"../_images/219df502b351733336051b1eb47e5866d63e2487d77bf04ac960164fbb5ed167.png\"/>",
            "code"
        ]
    ],
    "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Sobel-Feldman operator (the Sobel filter)": [
        [
            "To find regions of high spatial frequency (the edges or the edge maps) along the\nhorizontal and vertical axes of a 2D X-ray image, you can use the\n\ntechnique. The Sobel filter applies two 3x3 kernel matrices \u2014 one for each axis\n\u2014 onto the X-ray through a .\nThen, these two points (gradients) are combined using the\n to\nproduce a gradient magnitude.",
            "markdown"
        ],
        [
            "<strong>1.</strong> Use the Sobel filters \u2014 ()\n\u2014 on x- and y-axes of the X-ray. Then, calculate the distance between x and\ny (with the Sobel filters applied to them) using the\n and\nNumPy\u2019s \nto obtain the magnitude. Finally, normalize the rescaled image for the pixel\nvalues to be between 0 and 255.",
            "markdown"
        ],
        [
            "follows the output_channel = 255.0 * (input_channel - min_value) / (max_value - min_value)\n. Because you\u2019re\nusing a grayscale image, you need to normalize just one channel.",
            "markdown"
        ],
        [
            "x_sobel = ndimage.sobel(xray_image, axis=0)\ny_sobel = ndimage.sobel(xray_image, axis=1)\n\nxray_image_sobel = np.hypot(x_sobel, y_sobel)\n\nxray_image_sobel *= 255.0 / np.max(xray_image_sobel)",
            "code"
        ],
        [
            "<strong>2.</strong> Change the new image array data type to the 32-bit floating-point format\nfrom float16 to \nwith Matplotlib:",
            "markdown"
        ],
        [
            "print(\"The data type - before: \", xray_image_sobel.dtype)\n\nxray_image_sobel = xray_image_sobel.astype(\"float32\")\n\nprint(\"The data type - after: \", xray_image_sobel.dtype)",
            "code"
        ],
        [
            "The data type - before:  float16\nThe data type - after:  float32",
            "code"
        ],
        [
            "<strong>3.</strong> Display the original X-ray and the one with the Sobel \u201cedge\u201d filter\napplied. Note that both the grayscale and CMRmap colormaps are used to help\nemphasize the edges:",
            "markdown"
        ],
        [
            "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 15))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Sobel (edges) - grayscale\")\naxes[1].imshow(xray_image_sobel, cmap=\"gray\")\naxes[2].set_title(\"Sobel (edges) - CMRmap\")\naxes[2].imshow(xray_image_sobel, cmap=\"CMRmap\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/22e7655b5c372239084971ff47d3820119a4fc404d070d8e3d7dbb9b13ce9d16.png\" src=\"../_images/22e7655b5c372239084971ff47d3820119a4fc404d070d8e3d7dbb9b13ce9d16.png\"/>",
            "code"
        ]
    ],
    "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Canny filter": [
        [
            "You can also consider using another well-known filter for edge detection called\nthe .",
            "markdown"
        ],
        [
            "First, you apply a \nfilter to remove the noise in an image. In this example, you\u2019re using using the\n filter which\nsmoothens the X-ray through a \nprocess. Next, you apply the \non each of the 2 axes of the image to help detect some of the edges \u2014 this will\nresult in 2 gradient values. Similar to the Sobel filter, the Prewitt operator\nalso applies two 3x3 kernel matrices \u2014 one for each axis \u2014 onto the X-ray\nthrough a .\nIn the end, you compute the magnitude between the two gradients using the\n and\n\nthe images, as before.",
            "markdown"
        ],
        [
            "<strong>1.</strong> Use SciPy\u2019s Fourier filters \u2014 \n\u2014 with a small sigma value to remove some of the noise from the X-ray. Then,\ncalculate two gradients using .\nNext, measure the distance between the gradients using NumPy\u2019s np.hypot().\nFinally, \nthe rescaled image, as before.",
            "markdown"
        ],
        [
            "fourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05)\n\nx_prewitt = ndimage.prewitt(fourier_gaussian, axis=0)\ny_prewitt = ndimage.prewitt(fourier_gaussian, axis=1)\n\nxray_image_canny = np.hypot(x_prewitt, y_prewitt)\n\nxray_image_canny *= 255.0 / np.max(xray_image_canny)\n\nprint(\"The data type - \", xray_image_canny.dtype)",
            "code"
        ],
        [
            "The data type -  float64",
            "code"
        ],
        [
            "<strong>2.</strong> Plot the original X-ray image and the ones with the edges detected with\nthe help of the Canny filter technique. The edges can be emphasized using the\nprism, nipy_spectral, and terrain Matplotlib colormaps.",
            "markdown"
        ],
        [
            "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 15))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Canny (edges) - prism\")\naxes[1].imshow(xray_image_canny, cmap=\"prism\")\naxes[2].set_title(\"Canny (edges) - nipy_spectral\")\naxes[2].imshow(xray_image_canny, cmap=\"nipy_spectral\")\naxes[3].set_title(\"Canny (edges) - terrain\")\naxes[3].imshow(xray_image_canny, cmap=\"terrain\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/e7ee670442f8d404bfdcfdb117f2063747c3554aad2d41cd1dbc99b51c61c9ae.png\" src=\"../_images/e7ee670442f8d404bfdcfdb117f2063747c3554aad2d41cd1dbc99b51c61c9ae.png\"/>",
            "code"
        ]
    ],
    "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()": [
        [
            "To screen out only certain pixels in X-ray images to help detect particular\nfeatures, you can apply masks with NumPy\u2019s\n\nthat returns x when True and y when False.",
            "markdown"
        ],
        [
            "Identifying regions of interest \u2014 certain sets of pixels in an image \u2014 can be\nuseful and masks serve as boolean arrays of the same shape as the original\nimage.",
            "markdown"
        ],
        [
            "<strong>1.</strong> Retrieve some basics statistics about the pixel values in the original\nX-ray image you\u2019ve been working with:",
            "markdown"
        ],
        [
            "print(\"The data type of the X-ray image is: \", xray_image.dtype)\nprint(\"The minimum pixel value is: \", np.min(xray_image))\nprint(\"The maximum pixel value is: \", np.max(xray_image))\nprint(\"The average pixel value is: \", np.mean(xray_image))\nprint(\"The median pixel value is: \", np.median(xray_image))",
            "code"
        ],
        [
            "The data type of the X-ray image is:  uint8\nThe minimum pixel value is:  0\nThe maximum pixel value is:  255\nThe average pixel value is:  172.52233219146729\nThe median pixel value is:  195.0",
            "code"
        ],
        [
            "<strong>2.</strong> The array data type is uint8 and the minimum/maximum value results\nsuggest that all 256 colors (from 0 to 255) are used in the X-ray. Let\u2019s\nvisualize the <em>pixel intensity distribution</em> of the original raw X-ray image\nwith ndimage.histogram() and Matplotlib:",
            "markdown"
        ],
        [
            "pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "code"
        ],
        [
            "As the pixel intensity distribution suggests, there are many low (between around\n0 and 20) and very high (between around 200 and 240) pixel values.",
            "markdown"
        ],
        [
            "<strong>3.</strong> You can create different conditional masks with NumPy\u2019s np.where() \u2014\nfor example, let\u2019s have only those values of the image with the pixels exceeding\na certain threshold:",
            "markdown"
        ],
        [
            "# The threshold is \"greater than 150\"\n# Return the original image if true, `0` otherwise\nxray_image_mask_noisy = np.where(xray_image &gt; 150, xray_image, 0)\n\nplt.imshow(xray_image_mask_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/976c52ef7b0c1a1d25ff407f99a98e8290bc74a7eebe2564e88373b58fc3cee1.png\" src=\"../_images/976c52ef7b0c1a1d25ff407f99a98e8290bc74a7eebe2564e88373b58fc3cee1.png\"/>",
            "code"
        ],
        [
            "# The threshold is \"greater than 150\"\n# Return `1` if true, `0` otherwise\nxray_image_mask_less_noisy = np.where(xray_image &gt; 150, 1, 0)\n\nplt.imshow(xray_image_mask_less_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/2b2a2a73c09768a3e924a35f2e10a716c3171b763df137f3c1688aa222499701.png\" src=\"../_images/2b2a2a73c09768a3e924a35f2e10a716c3171b763df137f3c1688aa222499701.png\"/>",
            "code"
        ]
    ],
    "numpy->NumPy Applications->X-ray image processing->Compare the results": [
        [
            "Let\u2019s display some of the results of processed X-ray images you\u2019ve worked with\nso far:",
            "markdown"
        ],
        [
            "fig, axes = plt.subplots(nrows=1, ncols=9, figsize=(30, 30))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Laplace-Gaussian (edges)\")\naxes[1].imshow(xray_image_laplace_gaussian, cmap=\"gray\")\naxes[2].set_title(\"Gaussian gradient (edges)\")\naxes[2].imshow(x_ray_image_gaussian_gradient, cmap=\"gray\")\naxes[3].set_title(\"Sobel (edges) - grayscale\")\naxes[3].imshow(xray_image_sobel, cmap=\"gray\")\naxes[4].set_title(\"Sobel (edges) - hot\")\naxes[4].imshow(xray_image_sobel, cmap=\"hot\")\naxes[5].set_title(\"Canny (edges) - prism)\")\naxes[5].imshow(xray_image_canny, cmap=\"prism\")\naxes[6].set_title(\"Canny (edges) - nipy_spectral)\")\naxes[6].imshow(xray_image_canny, cmap=\"nipy_spectral\")\naxes[7].set_title(\"Mask (&gt; 150, noisy)\")\naxes[7].imshow(xray_image_mask_noisy, cmap=\"gray\")\naxes[8].set_title(\"Mask (&gt; 150, less noisy)\")\naxes[8].imshow(xray_image_mask_less_noisy, cmap=\"gray\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/a181aad23123a912880c01644de71ffbbac292fb5c3338ce09893b614bed1188.png\" src=\"../_images/a181aad23123a912880c01644de71ffbbac292fb5c3338ce09893b614bed1188.png\"/>",
            "code"
        ]
    ],
    "numpy->NumPy Applications->X-ray image processing->Next steps": [
        [
            "If you want to use your own samples, you can use\n\nor search for various other ones on the \ndatabase. Openi contains many biomedical images and it can be especially helpful\nif you have low bandwidth and/or are restricted by the amount of data you can\ndownload.",
            "markdown"
        ],
        [
            "To learn more about image processing in the context of biomedical image data or\nsimply edge detection, you may find the following material useful:",
            "markdown"
        ],
        [
            " with Scikit-Image and pydicom (Radiology Data Quest)",
            "markdown"
        ],
        [
            " (Scipy Lecture Notes)",
            "markdown"
        ],
        [
            " (presentation, DataCamp)",
            "markdown"
        ],
        [
            " (Maker Portal)",
            "markdown"
        ],
        [
            " with deep learning (a Kaggle-hosted Jupyter notebook)",
            "markdown"
        ],
        [
            " (lecture slides, CS6670: Computer Vision, Cornell University)",
            "markdown"
        ],
        [
            " and NumPy (Towards Data Science)",
            "markdown"
        ],
        [
            " with Scikit-Image (Data Carpentry)",
            "markdown"
        ],
        [
            " (lecture slides, 16-385 Computer Vision, Carnegie Mellon University)",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Static Equilibrium in NumPy": [
        [
            "When analyzing physical structures, it is crucial to understand the mechanics keeping them stable. Applied forces on a floor, a beam, or any other structure, create reaction forces and moments. These reactions are the structure resisting movement without breaking. In cases where structures do not move despite having forces applied to them,  states that both the acceleration and sum of forces in all directions in the system must be zero. You can represent and solve this concept with NumPy arrays.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Static Equilibrium in NumPy->What you\u2019ll do:": [
        [
            "In this tutorial, you will use NumPy to create vectors and moments using NumPy arrays",
            "markdown"
        ],
        [
            "Solve problems involving cables and floors holding up structures",
            "markdown"
        ],
        [
            "Write NumPy matrices to isolate unkowns",
            "markdown"
        ],
        [
            "Use NumPy functions to perform linear algebra operations",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Static Equilibrium in NumPy->What you\u2019ll learn:": [
        [
            "How to represent points, vectors, and moments with NumPy.",
            "markdown"
        ],
        [
            "How to find the ",
            "markdown"
        ],
        [
            "Using NumPy to compute matrix calculations",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Static Equilibrium in NumPy->What you\u2019ll need:": [
        [
            "NumPy",
            "markdown"
        ],
        [
            "imported with the following comands:",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt",
            "code"
        ],
        [
            "In this tutorial you will use the following NumPy tools:",
            "markdown"
        ],
        [
            " : this function determines the measure of vector magnitude",
            "markdown"
        ],
        [
            " : this function takes two matrices and produces the cross product",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Static Equilibrium in NumPy->Solving equilibrium with Newton\u2019s second law": [
        [
            "Your model consists of a beam under a sum of forces and moments. You can start analyzing this system with Newton\u2019s second law:\n\n\\[\\sum{\\text{force}} = \\text{mass} \\times \\text{acceleration}.\\]",
            "markdown"
        ],
        [
            "In order to simplify the examples looked at, assume they are static, with acceleration \\(=0\\). Due to our system existing in three dimensions, consider forces being applied in each of these dimensions. This means that you can represent these forces as vectors. You come to the same conclusion for , which result from forces being applied a certain distance away from an object\u2019s center of mass.",
            "markdown"
        ],
        [
            "Assume that the force \\(F\\) is represented as a three-dimensional vector\n\n\\[F = (F_x, F_y, F_z)\\]",
            "markdown"
        ],
        [
            "where each of the three components represent the magnitude of the force being applied in each corresponding direction. Assume also that each component in the vector\n\n\\[r = (r_x, r_y, r_z)\\]",
            "markdown"
        ],
        [
            "is the distance between the point where each component of the force is applied and the centroid of the system. Then, the moment can be computed by\n\n\\[r \\times F = (r_x, r_y, r_z) \\times (F_x, F_y, F_z).\\]",
            "markdown"
        ],
        [
            "Start with some simple examples of force vectors",
            "markdown"
        ],
        [
            "forceA = np.array([1, 0, 0])\nforceB = np.array([0, 1, 0])\nprint(\"Force A =\", forceA)\nprint(\"Force B =\", forceB)",
            "code"
        ],
        [
            "Force A = [1 0 0]\nForce B = [0 1 0]",
            "code"
        ],
        [
            "This defines forceA as being a vector with magnitude of 1 in the \\(x\\) direction and forceB as magnitude 1 in the \\(y\\) direction.",
            "markdown"
        ],
        [
            "It may be helpful to visualize these forces in order to better understand how they interact with each other.\nMatplotlib is a library with visualization tools that can be utilized for this purpose.\nQuiver plots will be used to demonstrate , but the library can also be used for .",
            "markdown"
        ],
        [
            "fig = plt.figure()\n\nd3 = fig.add_subplot(projection=\"3d\")\n\nd3.set_xlim(-1, 1)\nd3.set_ylim(-1, 1)\nd3.set_zlim(-1, 1)\n\nx, y, z = np.array([0, 0, 0])  # defining the point of application.  Make it the origin\n\nu, v, w = forceA  # breaking the force vector into individual components\nd3.quiver(x, y, z, u, v, w, color=\"r\", label=\"forceA\")\n\nu, v, w = forceB\nd3.quiver(x, y, z, u, v, w, color=\"b\", label=\"forceB\")\n\nplt.legend()\nplt.show()\n\n\n\n\n<img alt=\"../_images/76371f22e077e6a6f6ba349058d00c6213034bd336d0032bc8c19bae94a6b60c.png\" src=\"../_images/76371f22e077e6a6f6ba349058d00c6213034bd336d0032bc8c19bae94a6b60c.png\"/>",
            "code"
        ],
        [
            "There are two forces emanating from a single point. In order to simplify this problem, you can add them together to find the sum of forces. Note that both forceA and forceB are three-dimensional vectors, represented by NumPy as arrays with three components. Because NumPy is meant to simplify and optimize operations between vectors, you can easily compute the sum of these two vectors as follows:",
            "markdown"
        ],
        [
            "forceC = forceA + forceB\nprint(\"Force C =\", forceC)",
            "code"
        ],
        [
            "Force C = [1 1 0]",
            "code"
        ],
        [
            "Force C now acts as a single force that represents both A and B.\nYou can plot it to see the result.",
            "markdown"
        ],
        [
            "fig = plt.figure()\n\nd3 = fig.add_subplot(projection=\"3d\")\n\nd3.set_xlim(-1, 1)\nd3.set_ylim(-1, 1)\nd3.set_zlim(-1, 1)\n\nx, y, z = np.array([0, 0, 0])\n\nu, v, w = forceA\nd3.quiver(x, y, z, u, v, w, color=\"r\", label=\"forceA\")\nu, v, w = forceB\nd3.quiver(x, y, z, u, v, w, color=\"b\", label=\"forceB\")\nu, v, w = forceC\nd3.quiver(x, y, z, u, v, w, color=\"g\", label=\"forceC\")\n\nplt.legend()\nplt.show()\n\n\n\n\n<img alt=\"../_images/815f2a13f3ea03fdf90492e2ed93ccfe51644b6ade838d8fa0f67e8d5e60a981.png\" src=\"../_images/815f2a13f3ea03fdf90492e2ed93ccfe51644b6ade838d8fa0f67e8d5e60a981.png\"/>",
            "code"
        ],
        [
            "However, the goal is equilibrium.\nThis means that you want your sum of forces to be \\((0, 0, 0)\\) or else your object will experience acceleration.\nTherefore, there needs to be another force that counteracts the prior ones.",
            "markdown"
        ],
        [
            "You can write this problem as \\(A+B+R=0\\), with \\(R\\) being the reaction force that solves the problem.",
            "markdown"
        ],
        [
            "In this example this would mean:\n\n\\[(1, 0, 0) + (0, 1, 0) + (R_x, R_y, R_z) = (0, 0, 0)\\]",
            "markdown"
        ],
        [
            "Broken into \\(x\\), \\(y\\), and \\(z\\) components this gives you:\n\n\\[\\begin{split}\\begin{cases}\n1+0+R_x=0\\\\\n0+1+R_y=0\\\\\n0+0+R_z=0\n\\end{cases}\\end{split}\\]",
            "markdown"
        ],
        [
            "solving for \\(R_x\\), \\(R_y\\), and \\(R_z\\) gives you a vector \\(R\\) of \\((-1, -1, 0)\\).",
            "markdown"
        ],
        [
            "If plotted, the forces seen in prior examples should be nullified.\nOnly if there is no force remaining is the system considered to be in equilibrium.",
            "markdown"
        ],
        [
            "R = np.array([-1, -1, 0])\n\nfig = plt.figure()\n\nd3.set_xlim(-1, 1)\nd3.set_ylim(-1, 1)\nd3.set_zlim(-1, 1)\n\nd3 = fig.add_subplot(projection=\"3d\")\n\nx, y, z = np.array([0, 0, 0])\n\nu, v, w = forceA + forceB + R  # add them all together for sum of forces\nd3.quiver(x, y, z, u, v, w)\n\nplt.show()\n\n\n\n\n<img alt=\"../_images/716bd88df28c59c340b26ca7e38ba4b0092ef349670d4b541a6067a0ebeb1a7c.png\" src=\"../_images/716bd88df28c59c340b26ca7e38ba4b0092ef349670d4b541a6067a0ebeb1a7c.png\"/>",
            "code"
        ],
        [
            "The empty graph signifies that there are no outlying forces. This denotes a system in equilibrium.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Static Equilibrium in NumPy->Solving Equilibrium as a sum of moments": [
        [
            "Next let\u2019s move to a more complicated application.\nWhen forces are not all applied at the same point, moments are created.",
            "markdown"
        ],
        [
            "Similar to forces, these moments must all sum to zero, otherwise rotational acceleration will be experienced.  Similar to the sum of forces, this creates a linear equation for each of the three coordinate directions in space.",
            "markdown"
        ],
        [
            "A simple example of this would be from a force applied to a stationary pole secured in the ground.\nThe pole does not move, so it must apply a reaction force.\nThe pole also does not rotate, so it must also be creating a reaction moment.\nSolve for both the reaction force and moments.",
            "markdown"
        ],
        [
            "Lets say a 5N force is applied perpendicularly 2m above the base of the pole.",
            "markdown"
        ],
        [
            "f = 5  # Force in newtons\nL = 2  # Length of the pole\n\nR = 0 - f\nM = 0 - f * L\nprint(\"Reaction force =\", R)\nprint(\"Reaction moment =\", M)",
            "code"
        ],
        [
            "Reaction force = -5\nReaction moment = -10",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Determining Static Equilibrium in NumPy->Finding values with physical properties": [
        [
            "Let\u2019s say that instead of a force acting perpendicularly to the beam, a force was applied to our pole through a wire that was also attached to the ground.\nGiven the tension in this cord, all you need to solve this problem are the physical locations of these objects.",
            "markdown"
        ],
        [
            "<img alt=\"Image representing the problem\" src=\"../_images/static_eqbm-fig01.png\"/>",
            "markdown"
        ],
        [
            "In response to the forces acting upon the pole, the base generated reaction forces in the x and y directions, as well as a reaction moment.",
            "markdown"
        ],
        [
            "Denote the base of the pole as the origin.\nNow, say the cord is attached to the ground 3m in the x direction and attached to the pole 2m up, in the z direction.",
            "markdown"
        ],
        [
            "Define these points in space as NumPy arrays, and then use those arrays to find directional vectors.",
            "markdown"
        ],
        [
            "poleBase = np.array([0, 0, 0])\ncordBase = np.array([3, 0, 0])\ncordConnection = np.array([0, 0, 2])\n\npoleDirection = cordConnection - poleBase\nprint(\"Pole direction =\", poleDirection)\ncordDirection = cordBase - cordConnection\nprint(\"Cord direction =\", cordDirection)",
            "code"
        ],
        [
            "Pole direction = [0 0 2]\nCord direction = [ 3  0 -2]",
            "code"
        ],
        [
            "In order to use these vectors in relation to forces you need to convert them into unit vectors.\nUnit vectors have a magnitude of one, and convey only the direction of the forces.",
            "markdown"
        ],
        [
            "cordUnit = cordDirection / np.linalg.norm(cordDirection)\nprint(\"Cord unit vector =\", cordUnit)",
            "code"
        ],
        [
            "Cord unit vector = [ 0.83205029  0.         -0.5547002 ]",
            "code"
        ],
        [
            "You can then multiply this direction with the magnitude of the force in order to find the force vector.",
            "markdown"
        ],
        [
            "Let\u2019s say the cord has a tension of 5N:",
            "markdown"
        ],
        [
            "cordTension = 5\nforceCord = cordUnit * cordTension\nprint(\"Force from the cord =\", forceCord)",
            "code"
        ],
        [
            "Force from the cord = [ 4.16025147  0.         -2.77350098]",
            "code"
        ],
        [
            "In order to find the moment you need the cross product of the force vector and the radius.",
            "markdown"
        ],
        [
            "momentCord = np.cross(forceCord, poleDirection)\nprint(\"Moment from the cord =\", momentCord)",
            "code"
        ],
        [
            "Moment from the cord = [ 0.         -8.32050294  0.        ]",
            "code"
        ],
        [
            "Now all you need to do is find the reaction force and moment.",
            "markdown"
        ],
        [
            "equilibrium = np.array([0, 0, 0])\nR = equilibrium - forceCord\nM = equilibrium - momentCord\nprint(\"Reaction force =\", R)\nprint(\"Reaction moment =\", M)",
            "code"
        ],
        [
            "Reaction force = [-4.16025147  0.          2.77350098]\nReaction moment = [0.         8.32050294 0.        ]",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Determining Static Equilibrium in NumPy->Finding values with physical properties->Another Example": [
        [
            "Let\u2019s look at a slightly more complicated model.  In this example you will be observing a beam with two cables and an applied force.  This time you need to find both the tension in the cords and the reaction forces of the beam. <em>(Source: , Problem 4.106)</em>",
            "markdown"
        ],
        [
            "<img alt=\"image.png\" src=\"../_images/problem4.png\"/>",
            "markdown"
        ],
        [
            "Define distance <em>a</em> as 3 meters",
            "markdown"
        ],
        [
            "As before, start by defining the location of each relevant point as an array.",
            "markdown"
        ],
        [
            "A = np.array([0, 0, 0])\nB = np.array([0, 3, 0])\nC = np.array([0, 6, 0])\nD = np.array([1.5, 0, -3])\nE = np.array([1.5, 0, 3])\nF = np.array([-3, 0, 2])",
            "code"
        ],
        [
            "From these equations, you start by determining vector directions with unit vectors.",
            "markdown"
        ],
        [
            "AB = B - C\nAC = C - A\nBD = D - B\nBE = E - B\nCF = F - C\n\nUnitBD = BD / np.linalg.norm(BD)\nUnitBE = BE / np.linalg.norm(BE)\nUnitCF = CF / np.linalg.norm(CF)\n\nRadBD = np.cross(AB, UnitBD)\nRadBE = np.cross(AB, UnitBE)\nRadCF = np.cross(AC, UnitCF)",
            "code"
        ],
        [
            "This lets you represent the tension (T) and reaction (R) forces acting on the system as\n\n\\[\\begin{split}\\left[\n\\begin{array}\n~1/3 &amp; 1/3 &amp; 1 &amp; 0 &amp; 0\\\\\n-2/3 &amp; -2/3 &amp; 0 &amp; 1 &amp; 0\\\\\n-2/3 &amp; 2/3 &amp; 0 &amp; 0 &amp; 1\\\\\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n~T_{BD}\\\\\nT_{BE}\\\\\nR_{x}\\\\\nR_{y}\\\\\nR_{z}\\\\\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n~195\\\\\n390\\\\\n-130\\\\\n\\end{array}\n\\right]\\end{split}\\]",
            "markdown"
        ],
        [
            "and the moments as\n\n\\[\\begin{split}\\left[\n\\begin{array}\n~2 &amp; -2\\\\\n1 &amp; 1\\\\\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n~T_{BD}\\\\\nT_{BE}\\\\\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n~780\\\\\n1170\\\\\n\\end{array}\n\\right]\\end{split}\\]",
            "markdown"
        ],
        [
            "Where \\(T\\) is the tension in the respective cord and \\(R\\) is the reaction force in a respective direction. Then you just have six equations:",
            "markdown"
        ],
        [
            "\\(\\sum F_{x} = 0 = T_{BE}/3+T_{BD}/3-195+R_{x}\\)",
            "markdown"
        ],
        [
            "\\(\\sum F_{y} = 0 = (-\\frac{2}{3})T_{BE}-\\frac{2}{3}T_{BD}-390+R_{y}\\)",
            "markdown"
        ],
        [
            "\\(\\sum F_{z} = 0 = (-\\frac{2}{3})T_{BE}+\\frac{2}{3}T_{BD}+130+R_{z}\\)",
            "markdown"
        ],
        [
            "\\(\\sum M_{x} = 0 = 780+2T_{BE}-2T_{BD}\\)",
            "markdown"
        ],
        [
            "\\(\\sum M_{z} = 0 = 1170-T_{BE}-T_{BD}\\)",
            "markdown"
        ],
        [
            "You now have five unknowns with five equations, and can solve for:",
            "markdown"
        ],
        [
            "\\(\\ T_{BD} = 780N\\)",
            "markdown"
        ],
        [
            "\\(\\ T_{BE} = 390N\\)",
            "markdown"
        ],
        [
            "\\(\\ R_{x} = -195N\\)",
            "markdown"
        ],
        [
            "\\(\\ R_{y} = 1170N\\)",
            "markdown"
        ],
        [
            "\\(\\ R_{z} = 130N\\)",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Static Equilibrium in NumPy->Wrapping up": [
        [
            "You have learned how to use arrays to represent points, forces, and moments in three dimensional space. Each entry in an array can be used to represent a physical property broken into directional components. These can then be easily manipulated with NumPy functions.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Static Equilibrium in NumPy->Wrapping up->Additional Applications": [
        [
            "This same process can be applied to kinetic problems or in any number of dimensions. The examples done in this tutorial assumed three dimensional problems in static equilibrium. These methods can easily be used in more varied problems. More or less dimensions require larger or smaller arrays to represent. In systems experiencing acceleration, velocity and acceleration can be similarly be represented as vectors as well.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Determining Static Equilibrium in NumPy->Wrapping up->References": [],
    "numpy->NumPy Applications->Plotting Fractals": [
        [
            "<img alt=\"Fractal picture\" src=\"../_images/fractal.png\"/>",
            "markdown"
        ],
        [
            "Fractals are beautiful, compelling mathematical forms that can be oftentimes created from a relatively simple set of instructions. In nature they can be found in various places, such as coastlines, seashells, and ferns, and even were used in creating certain types of antennas. The mathematical idea of fractals was known for quite some time, but they really began to be truly appreciated in the 1970\u2019s as advancements in computer graphics and some accidental discoveries lead researchers like  to stumble upon the truly mystifying visualizations that fractals possess.",
            "markdown"
        ],
        [
            "Today we will learn how to plot these beautiful visualizations and will start to do a bit of exploring for ourselves as we gain familiarity of the mathematics behind fractals and will use the ever powerful NumPy universal functions to perform the necessary calculations efficiently.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Plotting Fractals->What you\u2019ll do": [
        [
            "Write a function for plotting various Julia sets",
            "markdown"
        ],
        [
            "Create a visualization of the Mandelbrot set",
            "markdown"
        ],
        [
            "Write a function that computes Newton fractals",
            "markdown"
        ],
        [
            "Experiment with variations of general fractal types",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Plotting Fractals->What you\u2019ll learn": [
        [
            "A better intuition for how fractals work mathematically",
            "markdown"
        ],
        [
            "A basic understanding about NumPy universal functions and Boolean Indexing",
            "markdown"
        ],
        [
            "The basics of working with complex numbers in NumPy",
            "markdown"
        ],
        [
            "How to create your own unique fractal visualizations",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Plotting Fractals->What you\u2019ll need": [
        [
            "make_axis_locatable function from mpl_toolkits API",
            "markdown"
        ],
        [
            "which can be imported as follows:",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable",
            "code"
        ],
        [
            "Some familiarity with Python, NumPy and matplotlib",
            "markdown"
        ],
        [
            "An idea of elementary mathematical functions, such as , ,  etc",
            "markdown"
        ],
        [
            "A very basic understanding of  would be useful",
            "markdown"
        ],
        [
            "Knowledge of  may be helpful",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Plotting Fractals->Warmup": [
        [
            "To gain some intuition for what fractals are, we will begin with an example.",
            "markdown"
        ],
        [
            "Consider the following equation:",
            "markdown"
        ],
        [
            "\\(f(z) = z^2 -1 \\)",
            "markdown"
        ],
        [
            "where z is a complex number (i.e of the form \\(a + bi\\) )",
            "markdown"
        ],
        [
            "For our convenience, we will write a Python function for it",
            "markdown"
        ],
        [
            "def f(z):\n    return np.square(z) - 1",
            "code"
        ],
        [
            "Note that the square function we used is an example of a <strong></strong>; we will come back to the significance of this decision shortly.",
            "markdown"
        ],
        [
            "To gain some intuition for the behaviour of the function, we can try plugging in some different values.",
            "markdown"
        ],
        [
            "For \\(z = 0\\), we would expect to get \\(-1\\):",
            "markdown"
        ],
        [
            "f(0)",
            "code"
        ],
        [
            "-1",
            "code"
        ],
        [
            "Since we used a universal function in our design, we can compute multiple inputs at the same time:",
            "markdown"
        ],
        [
            "z = [4, 1-0.2j, 1.6]\nf(z)",
            "code"
        ],
        [
            "array([15.  +0.j , -0.04-0.4j,  1.56+0.j ])",
            "code"
        ],
        [
            "Some values grow, some values shrink, some don\u2019t experience much change.",
            "markdown"
        ],
        [
            "To see the behaviour of the function on a larger scale, we can apply the function to a subset of the complex plane and plot the result. To create our subset (or mesh), we can make use of the  function.",
            "markdown"
        ],
        [
            "x, y = np.meshgrid(np.linspace(-10, 10, 20), np.linspace(-10, 10, 20))\nmesh = x + (1j * y)  # Make mesh of complex plane",
            "code"
        ],
        [
            "Now we will apply our function to each value contained in the mesh. Since we used a universal function in our design, this means that we can pass in the entire mesh all at once. This is extremely convenient for two reasons: It reduces the amount of code needed to be written and greatly increases the efficiency (as universal functions make use of system level C programming in their computations).",
            "markdown"
        ],
        [
            "Here we plot the absolute value (or modulus) of each element in the mesh after one \u201citeration\u201d of the function using a :",
            "markdown"
        ],
        [
            "output = np.abs(f(mesh))  # Take the absolute value of the output (for plotting)\n\nfig = plt.figure()\nax = plt.axes(projection='3d')\n\nax.scatter(x, y, output, alpha=0.2)\n\nax.set_xlabel('Real axis')\nax.set_ylabel('Imaginary axis')\nax.set_zlabel('Absolute value')\nax.set_title('One Iteration: $ f(z) = z^2 - 1$');\n\n\n\n\n<img alt=\"../_images/7a6c141a418bba32990998c7d201a565b436c3d4fa0f254d8fccd4499f26e895.png\" src=\"../_images/7a6c141a418bba32990998c7d201a565b436c3d4fa0f254d8fccd4499f26e895.png\"/>",
            "code"
        ],
        [
            "This gives us a rough idea of what one iteration of the function does. Certain areas (notably in the areas closest to \\((0,0i)\\)) remain rather small while other areas grow quite considerably. Note that we lose information about the output by taking the absolute value, but it is the only way for us to be able to make a plot.",
            "markdown"
        ],
        [
            "Let\u2019s see what happens when we apply 2 iterations to the mesh:",
            "markdown"
        ],
        [
            "output = np.abs(f(f(mesh)))\n\nax = plt.axes(projection='3d')\n\nax.scatter(x, y, output, alpha=0.2)\n\nax.set_xlabel('Real axis')\nax.set_ylabel('Imaginary axis')\nax.set_zlabel('Absolute value')\nax.set_title('Two Iterations: $ f(z) = z^2 - 1$');\n\n\n\n\n<img alt=\"../_images/7a222133db32eae7f71b9575aaf81b60e1c64824261ea8a992d3e03e3ef9be9b.png\" src=\"../_images/7a222133db32eae7f71b9575aaf81b60e1c64824261ea8a992d3e03e3ef9be9b.png\"/>",
            "code"
        ],
        [
            "Once again, we see that values around the origin remain small, and values with a larger absolute value (or modulus) \u201cexplode\u201d.",
            "markdown"
        ],
        [
            "From first impression, its behaviour appears to be normal, and may even seem mundane. Fractals tend to have more to them then what meets the eye; the exotic behavior shows itself when we begin applying more iterations.",
            "markdown"
        ],
        [
            "Consider three complex numbers:",
            "markdown"
        ],
        [
            "\\(z_1 = 0.4 + 0.4i \\),",
            "markdown"
        ],
        [
            "\\(z_2 = z_1 + 0.1\\),",
            "markdown"
        ],
        [
            "\\(z_3 = z_1 + 0.1i\\)",
            "markdown"
        ],
        [
            "Given the shape of our first two plots, we would expect that these values would remain near the origin as we apply iterations to them. Let us see what happens when we apply 10 iterations to each value:",
            "markdown"
        ],
        [
            "selected_values = np.array([0.4 + 0.4j, 0.41 + 0.4j, 0.4 + 0.41j])\nnum_iter = 9\n\noutputs = np.zeros((num_iter+1, selected_values.shape[0]), dtype=complex)\noutputs[0] = selected_values\n\nfor i in range(num_iter):\n    outputs[i+1] = f(outputs[i])  # Apply 10 iterations, save each output\n\nfig, axes = plt.subplots(1, selected_values.shape[0], figsize=(16, 6))\naxes[1].set_xlabel('Real axis')\naxes[0].set_ylabel('Imaginary axis')\n\nfor ax, data in zip(axes, outputs.T):\n    cycle = ax.scatter(data.real, data.imag, c=range(data.shape[0]), alpha=0.6)\n    ax.set_title(f'Mapping of iterations on {data[0]}')\n\nfig.colorbar(cycle, ax=axes, location=\"bottom\", label='Iteration');\n\n\n\n\n<img alt=\"../_images/0f42c59cef601c9805113d0bede6a59560a1d564c6ca29b224d80e8f39313a0e.png\" src=\"../_images/0f42c59cef601c9805113d0bede6a59560a1d564c6ca29b224d80e8f39313a0e.png\"/>",
            "code"
        ],
        [
            "To our surprise, the behaviour of the function did not come close to matching our hypothesis. This is a prime example of the chaotic behaviour fractals possess. In the first two plots, the value \u201cexploded\u201d on the last iteration, jumping way beyond the region that it was contained in previously. The third plot on the other hand remained bounded to a small region close to the origin, yielding completely different behaviour despite the tiny change in value.",
            "markdown"
        ],
        [
            "This leads us to an extremely important question: <strong>How many iterations can be applied to each value before they diverge (\u201cexplode\u201d)?</strong>",
            "markdown"
        ],
        [
            "As we saw from the first two plots, the further the values were from the origin, the faster they generally exploded. Although the behaviour is uncertain for smaller values (like \\(z_1, z_2, z_3\\)), we can assume that if a value surpasses a certain distance from the origin (say 2) that it is doomed to diverge. We will call this threshold the <strong>radius</strong>.",
            "markdown"
        ],
        [
            "This allows us to quantify the behaviour of the function for a particular value without having to perform as many computations. Once the radius is surpassed, we are allowed to stop iterating, which gives us a way of answering the question we posed. If we tally how many computations were applied before divergence, we gain insight into the behaviour of the function that would be hard to keep track of otherwise.",
            "markdown"
        ],
        [
            "Of course, we can do much better and design a function that performs the procedure on an entire mesh.",
            "markdown"
        ],
        [
            "def divergence_rate(mesh, num_iter=10, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(mesh.shape)  # Keep tally of the number of iterations\n\n    # Iterate on element if and only if |element| &lt; radius (Otherwise assume divergence)\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        diverge_len[conv_mask] += 1\n        z[conv_mask] = f(z[conv_mask])\n\n    return diverge_len",
            "code"
        ],
        [
            "The behaviour of this function may look confusing at first glance, so it will help to explain some of the notation.",
            "markdown"
        ],
        [
            "Our goal is to iterate over each value in the mesh and to tally the number of iterations before the value diverges. Since some values will diverge quicker than others, we need a procedure that only iterates over values that have an absolute value that is sufficiently small enough. We also want to stop tallying values once they surpass the radius. For this, we can use <strong></strong>, a NumPy feature that when paired with universal functions is unbeatable. Boolean Indexing allows for operations to be performed conditionally on a NumPy array without having to resort to looping over and checking for each array value individually.",
            "markdown"
        ],
        [
            "In our case, we use a loop to apply iterations to our function \\(f(z) = z^2 -1 \\) and keep tally. Using Boolean indexing, we only apply the iterations to values that have an absolute value less than 2.",
            "markdown"
        ],
        [
            "With that out of the way, we can go about plotting our first fractal! We will use the  function to create a colour-coded visualization of the tallies.",
            "markdown"
        ],
        [
            "x, y = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\nmesh = x + (1j * y)\n\noutput = divergence_rate(mesh)\n\nfig = plt.figure(figsize=(5, 5))\nax = plt.axes()\n\nax.set_title('$f(z) = z^2 -1$')\nax.set_xlabel('Real axis')\nax.set_ylabel('Imaginary axis')\n\nim = ax.imshow(output, extent=[-2, 2, -2, 2])\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\nplt.colorbar(im, cax=cax, label='Number of iterations');\n\n\n\n\n<img alt=\"../_images/2d354fd0ad367540f3138acc05ce34b19938b301fb66cec55761b453b1fe33ef.png\" src=\"../_images/2d354fd0ad367540f3138acc05ce34b19938b301fb66cec55761b453b1fe33ef.png\"/>",
            "code"
        ],
        [
            "What this stunning visual conveys is the complexity of the function\u2019s behaviour. The yellow region represents values that remain small, while the purple region represents the divergent values. The beautiful pattern that arises on the border of the converging and diverging values is even more fascinating when you realize that it is created from such a simple function.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Plotting Fractals->Julia set": [
        [
            "What we just explored was an example of a fractal visualization of a specific Julia Set.",
            "markdown"
        ],
        [
            "Consider the function \\(f(z) = z^2 + c\\) where \\(c\\) is a complex number. The <strong>filled-in Julia set</strong> of \\(c\\) is the set of all complex numbers z in which the function converges at \\(f(z)\\). Likewise, the boundary of the filled-in Julia set is what we call the <strong>Julia set</strong>. In our above visualization, we can see that the yellow region represents an approximation of the filled-in Julia set for \\(c = -1\\) and the greenish-yellow border would contain the Julia set.",
            "markdown"
        ],
        [
            "To gain access to a wider range of \u201cJulia fractals\u201d, we can write a function that allows for different values of \\(c\\) to be passed in:",
            "markdown"
        ],
        [
            "def julia(mesh, c=-1, num_iter=10, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        z[conv_mask] = np.square(z[conv_mask]) + c\n        diverge_len[conv_mask] += 1\n\n    return diverge_len",
            "code"
        ],
        [
            "To make our lives easier, we will create a couple meshes that we will reuse throughout the rest of the examples:",
            "markdown"
        ],
        [
            "x, y = np.meshgrid(np.linspace(-1, 1, 400), np.linspace(-1, 1, 400))\nsmall_mesh = x + (1j * y)\n\nx, y = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\nmesh = x + (1j * y)",
            "code"
        ],
        [
            "We will also write a function that we will use to create our fractal plots:",
            "markdown"
        ],
        [
            "def plot_fractal(fractal, title='Fractal', figsize=(6, 6), cmap='rainbow', extent=[-2, 2, -2, 2]):\n\n    plt.figure(figsize=figsize)\n    ax = plt.axes()\n\n    ax.set_title(f'${title}$')\n    ax.set_xlabel('Real axis')\n    ax.set_ylabel('Imaginary axis')\n\n    im = ax.imshow(fractal, extent=extent, cmap=cmap)\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n    plt.colorbar(im, cax=cax, label='Number of iterations')",
            "code"
        ],
        [
            "Using our newly defined functions, we can make a quick plot of the first fractal again:",
            "markdown"
        ],
        [
            "output = julia(mesh, num_iter=15)\nkwargs = {'title': 'f(z) = z^2 -1'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/6f5ebf5f94a5bdba07a281f894d39c1f30729b755d1481c4cc3727674555ee16.png\" src=\"../_images/6f5ebf5f94a5bdba07a281f894d39c1f30729b755d1481c4cc3727674555ee16.png\"/>",
            "code"
        ],
        [
            "We also can explore some different Julia sets by experimenting with different values of \\(c\\). It can be surprising how much influence it has on the shape of the fractal.",
            "markdown"
        ],
        [
            "For example, setting \\(c = \\frac{\\pi}{10}\\) gives us a very elegant cloud shape, while setting c = \\(-\\frac{3}{4} + 0.4i\\) yields a completely different pattern.",
            "markdown"
        ],
        [
            "output = julia(mesh, c=np.pi/10, num_iter=20)\nkwargs = {'title': 'f(z) = z^2 + \\dfrac{\\pi}{10}', 'cmap': 'plasma'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/5ea538e3ccf0cf0acc5f746184d99282ac5e09450cab6cde73c6106e5a8cef33.png\" src=\"../_images/5ea538e3ccf0cf0acc5f746184d99282ac5e09450cab6cde73c6106e5a8cef33.png\"/>",
            "code"
        ],
        [
            "output = julia(mesh, c=-0.75 + 0.4j, num_iter=20)\nkwargs = {'title': 'f(z) = z^2 - \\dfrac{3}{4} + 0.4i', 'cmap': 'Greens_r'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/bfb120678f65f1892658c8e3121fc93f5e95b61f9b4acd4ff184d041d5903eac.png\" src=\"../_images/bfb120678f65f1892658c8e3121fc93f5e95b61f9b4acd4ff184d041d5903eac.png\"/>",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Plotting Fractals->Mandelbrot set": [
        [
            "Closely related to the Julia set is the famous <strong>Mandelbrot set</strong>, which has a slightly different definition. Once again, we define \\(f(z) = z^2 + c\\) where \\(c\\) is a complex number, but this time our focus is on our choice of \\(c\\). We say that \\(c\\) is an element of the Mandelbrot set if f converges at \\(z = 0\\). An equivalent definition is to say that \\(c\\) is an element of the Mandelbrot set if \\(f(c)\\) can be iterated infinitely and not \u2018explode\u2019. We will tweak our Julia function slightly (and rename it appropriately) so that we can plot a visualization of the Mandelbrot set, which possesses an elegant fractal pattern.",
            "markdown"
        ],
        [
            "def mandelbrot(mesh, num_iter=10, radius=2):\n\n    c = mesh.copy()\n    z = np.zeros(mesh.shape, dtype=np.complex128)\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        z[conv_mask] = np.square(z[conv_mask]) + c[conv_mask]\n        diverge_len[conv_mask] += 1\n\n    return diverge_len",
            "code"
        ],
        [
            "output = mandelbrot(mesh, num_iter=50)\nkwargs = {'title': 'Mandelbrot \\ set', 'cmap': 'hot'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/20dd449f2a6134d4842d5337133c4150c22d59046f8a6076cf707296b245644c.png\" src=\"../_images/20dd449f2a6134d4842d5337133c4150c22d59046f8a6076cf707296b245644c.png\"/>",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Plotting Fractals->Generalizing the Julia set": [
        [
            "We can generalize our Julia function even further by giving it a parameter for which universal function we would like to pass in. This would allow us to plot fractals of the form \\(f(z) = g(z) + c\\) where g is a universal function selected by us.",
            "markdown"
        ],
        [
            "def general_julia(mesh, c=-1, f=np.square, num_iter=100, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        z[conv_mask] = f(z[conv_mask]) + c\n        diverge_len[conv_mask] += 1\n\n    return diverge_len",
            "code"
        ],
        [
            "One cool set of fractals that can be plotted using our general Julia function are ones of the form \\(f(z) = z^n + c\\) for some positive integer \\(n\\). A very cool pattern which emerges is that the number of regions that \u2018stick out\u2019 matches the degree in which we raise the function to while iterating over it.",
            "markdown"
        ],
        [
            "fig, axes = plt.subplots(2, 3, figsize=(8, 8))\nbase_degree = 2\n\nfor deg, ax in enumerate(axes.ravel()):\n    degree = base_degree + deg\n    power = lambda z: np.power(z, degree)  # Create power function for current degree\n\n    diverge_len = general_julia(mesh, f=power, num_iter=15)\n    ax.imshow(diverge_len, extent=[-2, 2, -2, 2], cmap='binary')\n    ax.set_title(f'$f(z) = z^{degree} -1$')\n\nfig.tight_layout();\n\n\n\n\n<img alt=\"../_images/73d82e6263ce699c9f5574fbe8c3a85d913e2de43ba2fe4b07e6bbfaec8cd810.png\" src=\"../_images/73d82e6263ce699c9f5574fbe8c3a85d913e2de43ba2fe4b07e6bbfaec8cd810.png\"/>",
            "code"
        ],
        [
            "Needless to say, there is a large amount of exploring that can be done by fiddling with the inputted function, value of \\(c\\), number of iterations, radius and even the density of the mesh and choice of colours.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Plotting Fractals->Generalizing the Julia set->Newton Fractals": [
        [
            "Newton fractals are a specific class of fractals, where iterations involve adding or subtracting the ratio of a function (often a polynomial) and its derivative to the input values. Mathematically, it can be expressed as:",
            "markdown"
        ],
        [
            "\\(z := z - \\frac{f(z)}{f'(z)}\\)",
            "markdown"
        ],
        [
            "We will define a general version of the fractal which will allow for different variations to be plotted by passing in our functions of choice.",
            "markdown"
        ],
        [
            "def newton_fractal(mesh, f, df, num_iter=10, r=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; r\n        pz = f(z[conv_mask])\n        dp = df(z[conv_mask])\n        z[conv_mask] = z[conv_mask] - pz/dp\n        diverge_len[conv_mask] += 1\n\n    return diverge_len",
            "code"
        ],
        [
            "Now we can experiment with some different functions. For polynomials, we can create our plots quite effortlessly using the , which has built in functionality for computing derivatives.",
            "markdown"
        ],
        [
            "For example, let\u2019s try a higher-degree polynomial:",
            "markdown"
        ],
        [
            "p = np.polynomial.Polynomial([-16, 0, 0, 0, 15, 0, 0, 0, 1])\np\n\n\n\n\n\n\\[x \\mapsto \\text{-16.0}\\color{LightGray}{ + \\text{0.0}\\,x}\\color{LightGray}{ + \\text{0.0}\\,x^{2}}\\color{LightGray}{ + \\text{0.0}\\,x^{3}} + \\text{15.0}\\,x^{4}\\color{LightGray}{ + \\text{0.0}\\,x^{5}}\\color{LightGray}{ + \\text{0.0}\\,x^{6}}\\color{LightGray}{ + \\text{0.0}\\,x^{7}} + \\text{1.0}\\,x^{8}\\]",
            "code"
        ],
        [
            "which has the derivative:",
            "markdown"
        ],
        [
            "p.deriv()\n\n\n\n\n\n\\[x \\mapsto \\color{LightGray}{\\text{0.0}}\\color{LightGray}{ + \\text{0.0}\\,x}\\color{LightGray}{ + \\text{0.0}\\,x^{2}} + \\text{60.0}\\,x^{3}\\color{LightGray}{ + \\text{0.0}\\,x^{4}}\\color{LightGray}{ + \\text{0.0}\\,x^{5}}\\color{LightGray}{ + \\text{0.0}\\,x^{6}} + \\text{8.0}\\,x^{7}\\]",
            "code"
        ],
        [
            "output = newton_fractal(mesh, p, p.deriv(), num_iter=15, r=2)\nkwargs = {'title': 'f(z) = z - \\dfrac{(z^8 + 15z^4 - 16)}{(8z^7 + 60z^3)}', 'cmap': 'copper'}\n\nplot_fractal(output, **kwargs)\n\n\n\n\n<img alt=\"../_images/a48a6fcf739b31c8b3af199ca2f5dc75c0eb09a8a51543af4a9c037a9cd59be7.png\" src=\"../_images/a48a6fcf739b31c8b3af199ca2f5dc75c0eb09a8a51543af4a9c037a9cd59be7.png\"/>",
            "code"
        ],
        [
            "Beautiful! Let\u2019s try another one:",
            "markdown"
        ],
        [
            "f(z) = \\(tan^2(z)\\)",
            "markdown"
        ],
        [
            "\\(\\frac{df}{dz} = 2 \\cdot tan(z) sec^2(z) =\\frac{2 \\cdot tan(z)}{cos^2(z)}\\)",
            "markdown"
        ],
        [
            "This makes \\(\\frac{f(z)}{f'(z)} =  tan^2(z) \\cdot \\frac{cos^2(z)}{2 \\cdot tan(z)} = \\frac{tan(z)\\cdot cos^2(z)}{2} = \\frac{sin(z)\\cdot cos(z)}{2}\\)",
            "markdown"
        ],
        [
            "def f_tan(z):\n    return np.square(np.tan(z))\n\n\ndef d_tan(z):\n    return 2*np.tan(z) / np.square(np.cos(z))",
            "code"
        ],
        [
            "output = newton_fractal(mesh, f_tan, d_tan, num_iter=15, r=50)\nkwargs = {'title': 'f(z) = z - \\dfrac{sin(z)cos(z)}{2}', 'cmap': 'binary'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/f3be0b5fb4d9cc46faffa70cb89005b898d8adbc4c2860cb2f113d61de3cee97.png\" src=\"../_images/f3be0b5fb4d9cc46faffa70cb89005b898d8adbc4c2860cb2f113d61de3cee97.png\"/>",
            "code"
        ],
        [
            "Note that you sometimes have to play with the radius in order to get a neat looking fractal.",
            "markdown"
        ],
        [
            "Finally, we can go a little bit wild with our function selection",
            "markdown"
        ],
        [
            "\\(f(z) = \\sum_{i=1}^{10} sin^i(z)\\)",
            "markdown"
        ],
        [
            "\\(\\frac{df}{dz} = \\sum_{i=1}^{10} i \\cdot sin^{i-1}(z) \\cdot cos(z)\\)",
            "markdown"
        ],
        [
            "def sin_sum(z, n=10):\n    total = np.zeros(z.size, dtype=z.dtype)\n    for i in range(1, n+1):\n        total += np.power(np.sin(z), i)\n    return total\n\n\ndef d_sin_sum(z, n=10):\n    total = np.zeros(z.size, dtype=z.dtype)\n    for i in range(1, n+1):\n        total += i * np.power(np.sin(z), i-1) * np.cos(z)\n    return total",
            "code"
        ],
        [
            "We will denote this one \u2018Wacky fractal\u2019, as its equation would not be fun to try and put in the title.",
            "markdown"
        ],
        [
            "output = newton_fractal(small_mesh, sin_sum, d_sin_sum, num_iter=10, r=1)\nkwargs = {'title': 'Wacky \\ fractal', 'figsize': (6, 6), 'extent': [-1, 1, -1, 1], 'cmap': 'terrain'}\n\nplot_fractal(output, **kwargs)\n\n\n\n\n<img alt=\"../_images/054b639a3d49da873297a98e8a06288bab3e08761da084c9251a4d4f24163e04.png\" src=\"../_images/054b639a3d49da873297a98e8a06288bab3e08761da084c9251a4d4f24163e04.png\"/>",
            "code"
        ],
        [
            "It is truly fascinating how distinct yet similar these fractals are with each other. This leads us to the final section.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Plotting Fractals->Creating your own fractals": [
        [
            "What makes fractals more exciting is how much there is to explore once you become familiar with the basics. Now we will wrap up our tutorial by exploring some of the different ways one can experiment in creating unique fractals. I encourage you to try some things out on your own (if you have not done so already).",
            "markdown"
        ],
        [
            "One of the first places to experiment would be with the function for the generalized Julia set, where we can try passing in different functions as parameters.",
            "markdown"
        ],
        [
            "Let\u2019s start by choosing",
            "markdown"
        ],
        [
            "\\(f(z) = tan(z^2)\\)",
            "markdown"
        ],
        [
            "def f(z):\n    return np.tan(np.square(z))",
            "code"
        ],
        [
            "output = general_julia(mesh, f=f, num_iter=15, radius=2.1)\nkwargs = {'title': 'f(z) = tan(z^2)', 'cmap': 'gist_stern'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/4f49f2bcbdd0b60c30e8d4aca75da994995a9d5cb1d4e929e9a1751e8c8bdb4f.png\" src=\"../_images/4f49f2bcbdd0b60c30e8d4aca75da994995a9d5cb1d4e929e9a1751e8c8bdb4f.png\"/>",
            "code"
        ],
        [
            "What happens if we compose our defined function inside of a sine function?",
            "markdown"
        ],
        [
            "Let\u2019s try defining",
            "markdown"
        ],
        [
            "\\(g(z) = sin(f(z)) = sin(tan(z^2))\\)",
            "markdown"
        ],
        [
            "def g(z):\n    return np.sin(f(z))",
            "code"
        ],
        [
            "output = general_julia(mesh, f=g, num_iter=15, radius=2.1)\nkwargs = {'title': 'g(z) = sin(tan(z^2))', 'cmap': 'plasma_r'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/5526a4060ed968ec299d39a61bbae12254e1e2536b7ad21d8175edd94edf1bfd.png\" src=\"../_images/5526a4060ed968ec299d39a61bbae12254e1e2536b7ad21d8175edd94edf1bfd.png\"/>",
            "code"
        ],
        [
            "Next, let\u2019s create a function that applies both f and g to the inputs each iteration and adds the result together:",
            "markdown"
        ],
        [
            "\\(h(z) = f(z) + g(z) = tan(z^2) + sin(tan(z^2))\\)",
            "markdown"
        ],
        [
            "def h(z):\n    return f(z) + g(z)",
            "code"
        ],
        [
            "output = general_julia(small_mesh, f=h, num_iter=10, radius=2.1)\nkwargs = {'title': 'h(z) = tan(z^2) + sin(tan(z^2))', 'figsize': (7, 7), 'extent': [-1, 1, -1, 1], 'cmap': 'jet'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/dbc0e399684e1cc4e4eeff0527f59ad37af23e35431abf58dcee0f7e2bdd370a.png\" src=\"../_images/dbc0e399684e1cc4e4eeff0527f59ad37af23e35431abf58dcee0f7e2bdd370a.png\"/>",
            "code"
        ],
        [
            "You can even create beautiful fractals through your own errors. Here is one that got created accidently by making a mistake in computing the derivative of a Newton fractal:",
            "markdown"
        ],
        [
            "def accident(z):\n    return z - (2 * np.power(np.tan(z), 2) / (np.sin(z) * np.cos(z)))",
            "code"
        ],
        [
            "output = general_julia(mesh, f=accident, num_iter=15, c=0, radius=np.pi)\nkwargs = {'title': 'Accidental \\ fractal', 'cmap': 'Blues'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/352104137d4eb764db51b8dd2d5e9fd7e3f16db6117e52b1033a7e9f92f71309.png\" src=\"../_images/352104137d4eb764db51b8dd2d5e9fd7e3f16db6117e52b1033a7e9f92f71309.png\"/>",
            "code"
        ],
        [
            "Needless to say, there are a nearly endless supply of interesting fractal creations that can be made just by playing around with various combinations of NumPy universal functions and by tinkering with the parameters.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Plotting Fractals->In conclusion": [
        [
            "We learned a lot about generating fractals today. We saw how complicated fractals requiring many iterations could be computed efficiently using universal functions. We also took advantage of boolean indexing, which allowed for less computations to be made without having to individually verify each value. Finally, we learned a lot about fractals themselves. As a recap:",
            "markdown"
        ],
        [
            "Fractal images are created by iterating a function over a set of values, and keeping tally of how long it takes for each value to pass a certain threshold",
            "markdown"
        ],
        [
            "The colours in the image correspond to the tally counts of the values",
            "markdown"
        ],
        [
            "The filled-in Julia set for \\(c\\) consists of all complex numbers z in which \\(f(z) = z^2 + c\\) converges",
            "markdown"
        ],
        [
            "The Julia set for \\(c\\) is the set of complex numbers that make up the boundary of the filled-in Julia set",
            "markdown"
        ],
        [
            "The Mandelbrot set is all values \\(c\\) in which \\(f(z) = z^2 + c\\) converges at 0",
            "markdown"
        ],
        [
            "Newton fractals use functions of the form \\(f(z) = z - \\frac{p(z)}{p'(z)}\\)",
            "markdown"
        ],
        [
            "The fractal images can vary as you adjust the number of iterations, radius of convergence, mesh size, colours, function choice and parameter choice",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Plotting Fractals->On your own": [
        [
            "Play around with the parameters of the generalized Julia set function, try playing with the constant value, number of iterations, function choice, radius, and colour choice.",
            "markdown"
        ],
        [
            "Visit the \u201cList of fractals by Hausdorff dimension\u201d Wikipedia page (linked in the Further reading section) and try writing a function for a fractal not mentioned in this tutorial.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Plotting Fractals->Further reading": [],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India": [
        [
            "<img alt=\"A grid showing the India Gate in smog above and clear air below\" src=\"../_images/11-delhi-aqi.jpg\"/>",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->What you\u2019ll do": [
        [
            "Calculate Air Quality Indices (AQI) and perform paired Student\u2019s t-test on them.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->What you\u2019ll learn": [
        [
            "You\u2019ll learn the concept of moving averages",
            "markdown"
        ],
        [
            "You\u2019ll learn how to calculate Air Quality Index (AQI)",
            "markdown"
        ],
        [
            "You\u2019ll learn how to perform a paired Student\u2019s t-test and find the t and p values",
            "markdown"
        ],
        [
            "You\u2019ll learn how to interpret these values",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->What you\u2019ll need": [
        [
            " installed in your environment",
            "markdown"
        ],
        [
            "Basic understanding of statistical terms like population, sample, mean, standard deviation etc.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->The problem of air pollution": [
        [
            "Air pollution is one of the most prominent types of pollution we face that has an immediate effect on our daily lives. The\nCOVID-19 pandemic resulted in lockdowns in different parts of the world; offering a rare opportunity to study the effect of\nhuman activity (or lack thereof) on air pollution. In this tutorial, we will study the air quality in Delhi, one of the\nworst affected cities by air pollution, before and during the lockdown from March to June 2020. For this, we will first compute\nthe Air Quality Index for each hour from the collected pollutant measurements. Next, we will sample these indices and perform\na  on them. It will statistically show us that the air quality improved due to the lockdown, supporting our intuition.",
            "markdown"
        ],
        [
            "Let\u2019s start by importing the necessary libraries into our environment.",
            "markdown"
        ],
        [
            "import numpy as np\nfrom numpy.random import default_rng\nfrom scipy import stats",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset": [
        [
            "We will use a condensed version of the  dataset. This dataset contains air quality data and AQI (Air Quality Index) at hourly and daily level of various stations across multiple cities in India. The condensed version available with this tutorial contains hourly pollutant measurements for Delhi\nfrom May 31, 2019 to June 30, 2020. It has measurements of the standard pollutants that are required for Air Quality Index calculation and a few other important ones:\nParticulate Matter (PM 2.5 and PM 10), nitrogen dioxide (NO2), ammonia (NH3), sulfur dioxide (SO2), carbon monoxide (CO), ozone (O3), oxides of nitrogen (NOx), nitric oxide (NO), benzene, toluene, and xylene.",
            "markdown"
        ],
        [
            "Let\u2019s print out the first few rows to have a glimpse of our dataset.",
            "markdown"
        ],
        [
            "! head air-quality-data.csv",
            "code"
        ],
        [
            "Datetime,PM2.5,PM10,NO2,NH3,SO2,CO,O3,NOx,NO,Benzene,Toluene,Xylene\n2019-05-31 00:00:00,103.26,305.46,94.71,31.43,30.16,3.0,18.06,178.31,152.73,13.65,83.47,2.54\n2019-05-31 01:00:00,104.47,309.14,74.66,34.08,27.02,1.69,18.65,106.5,79.98,11.35,76.79,2.91\n2019-05-31 02:00:00,90.0,314.02,48.11,32.6,18.12,0.83,28.27,48.45,25.27,5.66,32.91,1.59\n2019-05-31 03:00:00,78.01,356.14,45.45,30.21,16.78,0.79,27.47,44.22,21.5,3.6,21.41,0.78\n2019-05-31 04:00:00,80.19,372.9,45.23,28.68,16.41,0.76,26.92,44.06,22.15,4.5,23.39,0.62\n2019-05-31 05:00:00,83.59,389.97,39.49,27.71,17.42,0.76,28.71,39.33,21.04,3.25,23.59,0.56\n2019-05-31 06:00:00,79.04,371.64,39.61,26.87,16.91,0.84,29.26,43.11,24.37,3.12,15.27,0.46\n2019-05-31 07:00:00,77.32,361.88,42.63,27.26,17.86,0.96,27.07,48.22,28.81,3.32,14.42,0.41\n2019-05-31 08:00:00,84.3,377.77,42.49,28.41,20.19,0.98,33.05,48.22,27.76,3.4,14.53,0.4",
            "code"
        ],
        [
            "For the purpose of this tutorial, we are only concerned with standard pollutants required for calculating the AQI, viz., PM 2.5, PM 10, NO2, NH3, SO2, CO, and O3. So, we will only import these particular columns with . We\u2019ll then  and create two sets: pollutants_A with PM 2.5, PM 10, NO2, NH3, and SO2, and pollutants_B with CO and O3. The\ntwo sets will be processed slightly differently, as we\u2019ll see later on.",
            "markdown"
        ],
        [
            "pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)",
            "code"
        ],
        [
            "(9528, 5)\n(9528, 2)",
            "code"
        ],
        [
            "Our dataset might contain missing values, denoted by NaN, so let\u2019s do a quick check with .",
            "markdown"
        ],
        [
            "np.all(np.isfinite(pollutant_data))",
            "code"
        ],
        [
            "True",
            "code"
        ],
        [
            "With this, we have successfully imported the data and checked that it is complete. Let\u2019s move on to the AQI calculations!",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Calculating the Air Quality Index": [
        [
            "We will calculate the AQI using  adopted by the  of India.  To summarize the steps:",
            "markdown"
        ],
        [
            "Collect 24-hourly average concentration values for the standard pollutants; 8-hourly in case of CO and O3.",
            "markdown"
        ],
        [
            "Calculate the sub-indices for these pollutants with the formula:\n\n\\[\n    Ip = \\dfrac{\\text{IHi \u2013 ILo}}{\\text{BPHi \u2013 BPLo}}\\cdot{\\text{Cp \u2013 BPLo}} + \\text{ILo}\n    \\]",
            "markdown"
        ],
        [
            "Where,",
            "markdown"
        ],
        [
            "Ip = sub-index of pollutant p<br/>\nCp = averaged concentration of pollutant p<br/>\nBPHi = concentration breakpoint i.e. greater than or equal to Cp<br/>\nBPLo = concentration breakpoint i.e. less than or equal to Cp<br/>\nIHi = AQI value corresponding to BPHi<br/>\nILo = AQI value corresponding to BPLo",
            "markdown"
        ],
        [
            "The maximum sub-index at any given time is the Air Quality Index.",
            "markdown"
        ],
        [
            "The Air Quality Index is calculated with the help of breakpoint ranges as shown in the chart below.",
            "markdown"
        ],
        [
            "<img alt=\"Chart of the breakpoint ranges\" src=\"../_images/11-breakpoints.png\"/>",
            "markdown"
        ],
        [
            "Let\u2019s create two arrays to store the AQI ranges and breakpoints so that we can use them later for our calculations.",
            "markdown"
        ],
        [
            "AQI = np.array([0, 51, 101, 201, 301, 401, 501])\n\nbreakpoints = {\n    'PM2.5': np.array([0, 31, 61, 91, 121, 251]),\n    'PM10': np.array([0, 51, 101, 251, 351, 431]),\n    'NO2': np.array([0, 41, 81, 181, 281, 401]),\n    'NH3': np.array([0, 201, 401, 801, 1201, 1801]),\n    'SO2': np.array([0, 41, 81, 381, 801, 1601]),\n    'CO': np.array([0, 1.1, 2.1, 10.1, 17.1, 35]),\n    'O3': np.array([0, 51, 101, 169, 209, 749])\n}",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Calculating the Air Quality Index->Moving averages": [
        [
            "For the first step, we have to compute  for pollutants_A over a window of 24 hours and pollutants_B over a\nwindow of 8 hours. We will write a simple function moving_mean using  and  to achieve this.",
            "markdown"
        ],
        [
            "To make sure both the sets are of the same length, we will truncate the pollutants_B_8hr_avg according to the length of\npollutants_A_24hr_avg. This will also ensure we have concentrations for all the pollutants over the same period of time.",
            "markdown"
        ],
        [
            "def moving_mean(a, n):\n    ret = np.cumsum(a, dtype=float, axis=0)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\npollutants_A_24hr_avg = moving_mean(pollutants_A, 24)\npollutants_B_8hr_avg = moving_mean(pollutants_B, 8)[-(pollutants_A_24hr_avg.shape[0]):]",
            "code"
        ],
        [
            "Now, we can join both sets with  to form a single data set of all the averaged concentrations. Note that we have to join our arrays column-wise so we pass the\naxis=1 parameter.",
            "markdown"
        ],
        [
            "pollutants = np.concatenate((pollutants_A_24hr_avg, pollutants_B_8hr_avg), axis=1)",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Calculating the Air Quality Index->Sub-indices": [
        [
            "The subindices for each pollutant are calculated according to the linear relationship between the AQI and standard breakpoint ranges with the formula as above:\n\n\\[\nIp = \\dfrac{\\text{IHi \u2013 ILo}}{\\text{BPHi \u2013 BPLo}}\\cdot{\\text{Cp \u2013 BPLo}} + \\text{ILo}\n\\]",
            "markdown"
        ],
        [
            "The compute_indices function first fetches the correct upper and lower bounds of AQI categories and breakpoint concentrations for the input concentration and pollutant with the help of arrays AQI and breakpoints we created above. Then, it feeds these values into the formula to calculate the sub-index.",
            "markdown"
        ],
        [
            "def compute_indices(pol, con):\n    bp = breakpoints[pol]\n    \n    if pol == 'CO':\n        inc = 0.1\n    else:\n        inc = 1\n    \n    if bp[0] &lt;= con &lt; bp[1]:\n        Bl = bp[0]\n        Bh = bp[1] - inc\n        Ih = AQI[1] - inc\n        Il = AQI[0]\n\n    elif bp[1] &lt;= con &lt; bp[2]:\n        Bl = bp[1]\n        Bh = bp[2] - inc\n        Ih = AQI[2] - inc\n        Il = AQI[1]\n\n    elif bp[2] &lt;= con &lt; bp[3]:\n        Bl = bp[2]\n        Bh = bp[3] - inc\n        Ih = AQI[3] - inc\n        Il = AQI[2]\n\n    elif bp[3] &lt;= con &lt; bp[4]:\n        Bl = bp[3]\n        Bh = bp[4] - inc\n        Ih = AQI[4] - inc\n        Il = AQI[3]\n\n    elif bp[4] &lt;= con &lt; bp[5]:\n        Bl = bp[4]\n        Bh = bp[5] - inc\n        Ih = AQI[5] - inc\n        Il = AQI[4]\n\n    elif bp[5] &lt;= con:\n        Bl = bp[5]\n        Bh = bp[5] + bp[4] - (2 * inc)\n        Ih = AQI[6]\n        Il = AQI[5]\n\n    else:\n        print(\"Concentration out of range!\")\n        \n    return ((Ih - Il) / (Bh - Bl)) * (con - Bl) + Il",
            "code"
        ],
        [
            "We will use  to utilize the concept of vectorization. This simply means we don\u2019t have loop over each element of the pollutant array ourselves.  is one of the key advantages of NumPy.",
            "markdown"
        ],
        [
            "vcompute_indices = np.vectorize(compute_indices)",
            "code"
        ],
        [
            "By calling our vectorized function vcompute_indices for each pollutant, we get the sub-indices. To get back an array with the original shape, we use .",
            "markdown"
        ],
        [
            "sub_indices = np.stack((vcompute_indices('PM2.5', pollutants[..., 0]),\n                        vcompute_indices('PM10', pollutants[..., 1]),\n                        vcompute_indices('NO2', pollutants[..., 2]),\n                        vcompute_indices('NH3', pollutants[..., 3]),\n                        vcompute_indices('SO2', pollutants[..., 4]),\n                        vcompute_indices('CO', pollutants[..., 5]),\n                        vcompute_indices('O3', pollutants[..., 6])), axis=1)",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Calculating the Air Quality Index->Air quality indices": [
        [
            "Using , we find out the maximum sub-index for each period, which is our Air Quality Index!",
            "markdown"
        ],
        [
            "aqi_array = np.max(sub_indices, axis=1)",
            "code"
        ],
        [
            "With this, we have the AQI for every hour from June 1, 2019 to June 30, 2020. Note that even though we started out with\nthe data from 31st May, we truncated that during the moving averages step.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs": [
        [
            "Hypothesis testing is a form of descriptive statistics used to help us make decisions with the data. From the calculated AQI data, we want to find out if there was a statistically significant difference in average AQI before and after the lockdown was imposed. We will use the left-tailed,  to compute two test statistics- the  and the . We will then compare these with the corresponding critical values to make a decision.",
            "markdown"
        ],
        [
            "<img alt=\"Normal distribution plot showing area of rejection in one-tailed test (left tailed)\" src=\"../_images/11-one-tailed-test.svg\"/>",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Sampling": [
        [
            "We will now import the datetime column from our original dataset into a  array. We will use this array to index the AQI array and obtain subsets of the dataset.",
            "markdown"
        ],
        [
            "datetime = np.loadtxt(\"air-quality-data.csv\", dtype='M8[h]', delimiter=\",\",\n                         skiprows=1, usecols=(0, ))[-(pollutants_A_24hr_avg.shape[0]):]",
            "code"
        ],
        [
            "Since total lockdown commenced in Delhi from March 24, 2020, the after-lockdown subset is of the period March 24, 2020 to June 30, 2020. The before-lockdown subset is for the same length of time before 24th March.",
            "markdown"
        ],
        [
            "after_lock = aqi_array[np.where(datetime &gt;= np.datetime64('2020-03-24T00'))]\n\nbefore_lock = aqi_array[np.where(datetime &lt;= np.datetime64('2020-03-21T00'))][-(after_lock.shape[0]):]\n\nprint(after_lock.shape)\nprint(before_lock.shape)",
            "code"
        ],
        [
            "(2376,)\n(2376,)",
            "code"
        ],
        [
            "To make sure our samples are <em>approximately</em> normally distributed, we take samples of size n = 30. before_sample and after_sample are the set of random observations drawn before and after the total lockdown. We use  to generate the samples.",
            "markdown"
        ],
        [
            "rng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Defining the hypothesis": [
        [
            "Let us assume that there is no significant difference between the sample means before and after the lockdown. This will be the null hypothesis. The alternative hypothesis would be that there <em>is</em> a significant difference between the means and the AQI <em>improved</em>. Mathematically,",
            "markdown"
        ],
        [
            "\\(H_{0}: \\mu_\\text{after-before} = 0\\) <br/>\n\\(H_{a}: \\mu_\\text{after-before} &lt; 0\\)",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Calculating the test statistics": [
        [
            "We will use the t statistic to evaluate our hypothesis and even calculate the p value from it. The formula for the t statistic is:\n\n\\[\nt = \\frac{\\mu_\\text{after-before}}{\\sqrt{\\sigma^{2}/n}}\n\\]",
            "markdown"
        ],
        [
            "where,",
            "markdown"
        ],
        [
            "\\(\\mu_\\text{after-before}\\) = mean differences of samples <br/>\n\\(\\sigma^{2}\\) = variance of mean differences <br/>\n\\(n\\) = sample size",
            "markdown"
        ],
        [
            "def t_test(x, y):\n    diff = y - x\n    var = np.var(diff, ddof=1)\n    num = np.mean(diff)\n    denom = np.sqrt(var / len(x))\n    return np.divide(num, denom)\n\nt_value = t_test(before_sample, after_sample)",
            "code"
        ],
        [
            "For the p value, we will use SciPy\u2019s stats.distributions.t.cdf() function. It takes two arguments- the t statistic and the degrees of freedom (dof). The formula for dof is n - 1.",
            "markdown"
        ],
        [
            "dof = len(before_sample) - 1\n\np_value = stats.distributions.t.cdf(t_value, dof)\n\nprint(\"The t value is {} and the p value is {}.\".format(t_value, p_value))",
            "code"
        ],
        [
            "The t value is -5.877365898842091 and the p value is 1.115499699062372e-06.",
            "code"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->What do the t and p values mean?": [
        [
            "We will now compare the calculated test statistics with the critical test statistics. The critical t value is calculated by looking up the .",
            "markdown"
        ],
        [
            "<img alt=\"Table of selected t values at different confidence levels. T value for 29 dof at 95% confidence level is highlighted with a yellow square\" src=\"../_images/11-t-table.png\"/>",
            "markdown"
        ],
        [
            "From the table above, the critical value is 1.699 for 29 dof at a confidence level of 95%. Since we are using the left tailed test, our critical value is -1.699. Clearly, the calculated t value is less than the critical value so we can safely reject the null hypothesis.",
            "markdown"
        ],
        [
            "The critical p value, denoted by \\(\\alpha\\), is usually chosen to be 0.05, corresponding to a confidence level of 95%. If the calculated p value is less than \\(\\alpha\\), then the null hypothesis can be safely rejected. Clearly, our p value is much less than \\(\\alpha\\), so we can reject the null hypothesis.",
            "markdown"
        ],
        [
            "Note that this does not mean we can accept the alternative hypothesis. It only tells us that there is not enough evidence to reject \\(H_{a}\\). In other words, we fail to reject the alternative hypothesis so, it <em>may</em> be true.",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->In practice\u2026": [
        [
            "The  library is preferable to use for time-series data analysis.",
            "markdown"
        ],
        [
            "The SciPy stats module provides the  function which can be used to get the t statistic and p value.",
            "markdown"
        ],
        [
            "In real life, data are generally not normally distributed. There are tests for such non-normal data like the .",
            "markdown"
        ]
    ],
    "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Further reading": [
        [
            "There are a host of statistical tests you can choose according to the characteristics of the given data. Read more about them at\n.",
            "markdown"
        ],
        [
            "There are various versions of the  that you can adopt according to your needs.",
            "markdown"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels": [
        [
            "Caution",
            "markdown"
        ],
        [
            "This article is not currently tested due to licensing/installation issues with\nthe underlying gym and atari-py dependencies.\nHelp improve this article by developing an example with reduced dependency\nfootprint!",
            "markdown"
        ],
        [
            "This tutorial demonstrates how to implement a deep reinforcement learning (RL) agent from scratch using a policy gradient method that learns to play the  video game using screen pixels as inputs with NumPy. Your Pong agent will obtain experience on the go using an  as its .",
            "markdown"
        ],
        [
            "Pong is a 2D game from 1972 where two players use \u201crackets\u201d to play a form of table tennis. Each player moves the racket up and down the screen and tries to hit a ball in their opponent\u2019s direction by touching it. The goal is to hit the ball such that it goes past the opponent\u2019s racket (they miss their shot). According to the rules, if a player reaches 21 points, they win. In Pong, the RL agent that learns to play against an opponent is displayed on the right.",
            "markdown"
        ],
        [
            "<img alt=\"Diagram showing operations detailed in this tutorial\" src=\"../_images/tutorial-deep-reinforcement-learning-with-pong-from-pixels.png\"/>",
            "markdown"
        ],
        [
            "This example is based on the  developed by  for the  in 2017 at UC Berkeley. His  from 2016 also provides more background on the mechanics and theory used in Pong RL.",
            "markdown"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels->Prerequisites": [
        [
            "<strong>OpenAI Gym</strong>: To help with the game environment, you will use  \u2014 an open-source Python interface  that helps perform RL tasks while supporting many simulation environments.",
            "markdown"
        ],
        [
            "<strong>Python and NumPy</strong>: The reader should have some knowledge of Python, NumPy array manipulation, and linear algebra.",
            "markdown"
        ],
        [
            "<strong>Deep learning and deep RL</strong>: You should be familiar with main concepts of , which are explained in the  paper published in 2015 by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, who are regarded as some of the pioneers of the field. The tutorial will try to guide you through the main concepts of deep RL and you will find various literature with links to original sources for your convenience.",
            "markdown"
        ],
        [
            "<strong>Jupyter notebook environments</strong>: Because RL experiments can require high computing power, you can run the tutorial on the cloud for free using  or  (which offers free limited GPU and TPU acceleration).",
            "markdown"
        ],
        [
            "<strong>Matplotlib</strong>: For plotting images. Check out the  guide to set it up in your environment.",
            "markdown"
        ],
        [
            "This tutorial can also be run locally in an isolated environment, such as  and .",
            "markdown"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels->Table of contents": [
        [
            "A note on RL and deep RL",
            "markdown"
        ],
        [
            "Deep RL glossary",
            "markdown"
        ],
        [
            "Set up Pong",
            "markdown"
        ],
        [
            "Preprocess frames (the observation)",
            "markdown"
        ],
        [
            "Create the policy (the neural network) and the forward pass",
            "markdown"
        ],
        [
            "Set up the update step (backpropagation)",
            "markdown"
        ],
        [
            "Define the discounted rewards (expected return) function",
            "markdown"
        ],
        [
            "Train the agent for 3 episodes",
            "markdown"
        ],
        [
            "Next steps",
            "markdown"
        ],
        [
            "Appendix",
            "markdown"
        ],
        [
            "Notes on RL and deep RL",
            "markdown"
        ],
        [
            "How to set up video playback in your Jupyter notebook",
            "markdown"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels->Table of contents->A note on RL and deep RL": [
        [
            "In , your agent learns from trial and error by interacting with an environment using a so-called policy to gain experience. After taking one action, the agent receives information about its reward (which it may or may not get) and the next observation of the environment. It can then proceed to take another action. This happens over a number of episodes and/or until the task is deemed to be complete.",
            "markdown"
        ],
        [
            "The agent\u2019s policy works by \u201cmapping\u201d the agent\u2019s observations to its actions \u2014 that is, assigning a presentation of what the agent observes with required actions. The overall goal is usually to optimize the agent\u2019s policy such that it maximizes the expected rewards from each observation.",
            "markdown"
        ],
        [
            "For detailed information about RL, there is an  by Richard Sutton and Andrew Barton.",
            "markdown"
        ],
        [
            "Check out the Appendix at the end of the tutorial for more information.",
            "markdown"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels->Table of contents->Deep RL glossary": [
        [
            "Below is a concise glossary of deep RL terms you may find useful for the remaining part of the tutorial:",
            "markdown"
        ],
        [
            "In a finite-horizon world, such as a game of Pong, the learning agent can explore (and exploit) the <em>environment</em> over an <em>episode</em>. It usually takes many episodes for the agent to learn.",
            "markdown"
        ],
        [
            "The agent interacts with the <em>environment</em> using <em>actions</em>.",
            "markdown"
        ],
        [
            "After taking an action, the agent receives some feedback through a <em>reward</em> (if there is one), depending on which action it takes and the <em>state</em> it is in. The state contains information about the environment.",
            "markdown"
        ],
        [
            "The agent\u2019s <em>observation</em> is a partial observation of the state \u2014 this is the term this tutorial prefers (instead of <em>state</em>).",
            "markdown"
        ],
        [
            "The agent can choose an action based on cumulative <em>rewards</em> (also known as the <em>value function</em>) and the <em>policy</em>. The <em>cumulative reward function</em> estimates the quality of the observations the agent visits using its <em>policy</em>.",
            "markdown"
        ],
        [
            "The <em>policy</em> (defined by a neural network) outputs action choices (as (log) probabilities) that should maximize the cumulative rewards from the state the agent is in.",
            "markdown"
        ],
        [
            "The <em>expected return from an observation</em>, conditional to the action, is called the <em>action-value</em> function. To provide more weight to shorter-term rewards versus the longer-term ones, you usually use a <em>discount factor</em> (often a floating point number between 0.9 and 0.99).",
            "markdown"
        ],
        [
            "The sequence of actions and states (observations) during each policy \u201crun\u201d by the agent is sometimes referred to as a <em>trajectory</em> \u2014 such a sequence yields <em>rewards</em>.",
            "markdown"
        ],
        [
            "You will train your Pong agent through an \u201con-policy\u201d method using policy gradients \u2014 it\u2019s an algorithm belonging to a family of <em>policy-based</em> methods. Policy gradient methods typically update the parameters of the policy with respect to the long-term cumulative reward using  that is widely used in machine learning. And, since the goal is to maximize the function (the rewards), not minimize it, the process is also called <em>gradient ascent</em>. In other words, you use a policy for the agent to take actions and the objective is to maximize the rewards, which you do by computing the gradients and use them to update the parameters in the policy (neural) network.",
            "markdown"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels->Set up Pong": [
        [
            "<strong>1.</strong> First, you should install OpenAI Gym (using pip install gym[atari] - this package is currently not available on conda), and import NumPy, Gym and the necessary modules:",
            "markdown"
        ],
        [
            "import numpy as np\nimport gym",
            "code"
        ],
        [
            "Gym can monitor and save the output using the Monitor wrapper:",
            "markdown"
        ],
        [
            "from gym import wrappers\nfrom gym.wrappers import Monitor",
            "code"
        ],
        [
            "<strong>2.</strong> Instantiate a Gym environment for the game of Pong:",
            "markdown"
        ],
        [
            "env = gym.make(\"Pong-v0\")",
            "code"
        ],
        [
            "<strong>3.</strong> Let\u2019s review which actions are available in the Pong-v0 environment:",
            "markdown"
        ],
        [
            "print(env.action_space)",
            "code"
        ],
        [
            "print(env.get_action_meanings())",
            "code"
        ],
        [
            "There are 6 actions. However, LEFTFIRE is actually LEFT, RIGHTFIRE \u2014 RIGHT, and NOOP \u2014 FIRE.",
            "markdown"
        ],
        [
            "For simplicity, your policy network will have one output \u2014 a (log) probability for \u201cmoving up\u201d (indexed at 2 or RIGHT). The other available action will be indexed at 3 (\u201cmove down\u201d or LEFT).",
            "markdown"
        ],
        [
            "<strong>4.</strong> Gym can save videos of the agent\u2019s learning in an MP4 format \u2014 wrap Monitor() around the environment by running the following:",
            "markdown"
        ],
        [
            "env = Monitor(env, \"./video\", force=True)",
            "code"
        ],
        [
            "While you can perform all kinds of RL experiments in a Jupyter notebook, rendering images or videos of a Gym environment to visualize how your agent plays the game of Pong after training can be rather challenging. If you want to set up video playback in a notebook, you can find the details in the Appendix at the end of this tutorial.",
            "markdown"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)": [
        [
            "In this section you will set up a function to preprocess the input data (game observation) to make it digestible for the neural network, which can only work with inputs that are in a form of tensors (multidimensional arrays) of floating-point type.",
            "markdown"
        ],
        [
            "Your agent will use the frames from the Pong game \u2014 pixels from screen frames \u2014 as input-observations for the policy network. The game observation tells the agent about where the ball is before it is fed (with a forward pass) into the neural network (the policy). This is similar to DeepMind\u2019s  method (which is further discussed in the Appendix).",
            "markdown"
        ],
        [
            "Pong screen frames are 210x160 pixels over 3 color dimensions (red, green and blue). The arrays are encoded with uint8 (or 8-bit integers), and these observations are stored on a Gym Box instance.",
            "markdown"
        ],
        [
            "<strong>1.</strong> Check the Pong\u2019s observations:",
            "markdown"
        ],
        [
            "print(env.observation_space)",
            "code"
        ],
        [
            "In Gym, the agent\u2019s actions and observations can be part of the Box (n-dimensional) or Discrete (fixed-range integers) classes.",
            "markdown"
        ],
        [
            "<strong>2.</strong> You can view a random observation \u2014 one frame \u2014 by:",
            "markdown"
        ],
        [
            "1) Setting the random `seed` before initialization (optional).\n\n2) Calling  Gym's `reset()` to reset the environment, which returns an initial observation.\n\n3) Using Matplotlib to display the `render`ed observation.",
            "code"
        ],
        [
            "(You can refer to the OpenAI Gym core  for more information about Gym\u2019s core classes and methods.)",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n\nenv.seed(42)\nenv.reset()\nrandom_frame = env.render(mode=\"rgb_array\")\nprint(random_frame.shape)\nplt.imshow(random_frame)",
            "code"
        ],
        [
            "To feed the observations into the policy (neural) network, you need to convert them into 1D grayscale vectors with 6,400 (80x80x1) floating point arrays. (During training, you will use NumPy\u2019s  function to flatten these arrays.)",
            "markdown"
        ],
        [
            "<strong>3.</strong> Set up a helper function for frame (observation) preprocessing:",
            "markdown"
        ],
        [
            "def frame_preprocessing(observation_frame):\n    # Crop the frame.\n    observation_frame = observation_frame[35:195]\n    # Downsample the frame by a factor of 2.\n    observation_frame = observation_frame[::2, ::2, 0]\n    # Remove the background and apply other enhancements.\n    observation_frame[observation_frame == 144] = 0  # Erase the background (type 1).\n    observation_frame[observation_frame == 109] = 0  # Erase the background (type 2).\n    observation_frame[observation_frame != 0] = 1  # Set the items (rackets, ball) to 1.\n    # Return the preprocessed frame as a 1D floating-point array.\n    return observation_frame.astype(float)",
            "code"
        ],
        [
            "<strong>4.</strong> Preprocess the random frame from earlier to test the function \u2014 the input for the policy network is an 80x80 1D image:",
            "markdown"
        ],
        [
            "preprocessed_random_frame = frame_preprocessing(random_frame)\nplt.imshow(preprocessed_random_frame, cmap=\"gray\")\nprint(preprocessed_random_frame.shape)",
            "code"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels->Create the policy (the neural network) and the forward pass": [
        [
            "Next, you will define the policy as a simple feedforward network that uses a game observation as an input and outputs an action log probability:",
            "markdown"
        ],
        [
            "For the <em>input</em>, it will use the Pong video game frames \u2014 the preprocessed 1D vectors with 6,400 (80x80) floating point arrays.",
            "markdown"
        ],
        [
            "The <em>hidden layer</em> will compute the weighted sum of inputs using NumPy\u2019s dot product function  for the arrays and then apply a <em>non-linear activation function</em>, such as .",
            "markdown"
        ],
        [
            "Then, the <em>output layer</em> will perform the matrix-multiplication again of  weight parameters and the hidden layer\u2019s output (with ), and send that information through a  <em>activation function</em>.",
            "markdown"
        ],
        [
            "In the end, the policy network will output one action log probability (given that observation) for the agent \u2014 the probability for Pong action indexed in the environment at 2 (\u201cmoving the racket up\u201d).",
            "markdown"
        ],
        [
            "<strong>1.</strong> Let\u2019s instantiate certain parameters for the input, hidden, and output layers, and start setting up the network model.",
            "markdown"
        ],
        [
            "Start by creating a random number generator instance for the experiment\n(seeded for reproducibility):",
            "markdown"
        ],
        [
            "rng = np.random.default_rng(seed=12288743)",
            "code"
        ],
        [
            "Then:",
            "markdown"
        ],
        [
            "Set the input (observation) dimensionality - your preprocessed screen frames:",
            "markdown"
        ],
        [
            "D = 80 * 80",
            "code"
        ],
        [
            "Set the number of hidden layer neurons.",
            "markdown"
        ],
        [
            "H = 200",
            "code"
        ],
        [
            "Instantiate your policy (neural) network model as an empty dictionary.",
            "markdown"
        ],
        [
            "model = {}",
            "code"
        ],
        [
            "In a neural network, <em>weights</em> are important adjustable parameters that the network fine-tunes by forward and backward propagating the data.",
            "markdown"
        ],
        [
            "<strong>2.</strong> Using a technique called , set up the network model\u2019s initial weights with NumPy\u2019s  that returns random numbers over a standard Normal distribution, as well as :",
            "markdown"
        ],
        [
            "model[\"W1\"] = rng.standard_normal(size=(H, D)) / np.sqrt(D)\nmodel[\"W2\"] = rng.standard_normal(size=H) / np.sqrt(H)",
            "code"
        ],
        [
            "<strong>3.</strong> Your policy network starts by randomly initializing the weights and feeds the input data (frames) forward from the input layer through a hidden layer to the output layers. This process is called the <em>forward pass</em> or <em>forward propagation</em>, and is outlined in the function policy_forward():",
            "markdown"
        ],
        [
            "def policy_forward(x, model):\n    # Matrix-multiply the weights by the input in the one and only hidden layer.\n    h = np.dot(model[\"W1\"], x)\n    # Apply non-linearity with ReLU.\n    h[h &lt; 0] = 0\n    # Calculate the \"dot\" product in the outer layer.\n    # The input for the sigmoid function is called logit.\n    logit = np.dot(model[\"W2\"], h)\n    # Apply the sigmoid function (non-linear activation).\n    p = sigmoid(logit)\n    # Return a log probability for the action 2 (\"move up\")\n    # and the hidden \"state\" that you need for backpropagation.\n    return p, h",
            "code"
        ],
        [
            "Note that there are two <em>activation functions</em> for determining non-linear relationships between inputs and outputs. These  are applied to the output of the layers:",
            "markdown"
        ],
        [
            ": defined as h[h&lt;0] = 0 above. It returns 0 for negative inputs and the same value if it\u2019s positive.",
            "markdown"
        ],
        [
            ": defined below as sigmoid(). It \u201cwraps\u201d the last layer\u2019s output and returns an action log probability in the (0, 1) range.",
            "markdown"
        ],
        [
            "<strong>4.</strong> Define the sigmoid function separately with NumPy\u2019s  for computing exponentials:",
            "markdown"
        ],
        [
            "def sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))",
            "code"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels->Set up the update step (backpropagation)": [
        [
            "During learning in your deep RL algorithm, you use the action log probabilities (given an observation) and the discounted returns (for example, +1 or -1 in Pong) and perform the <em>backward pass</em> or <em>backpropagation</em> to update the parameters \u2014 the policy network\u2019s weights.",
            "markdown"
        ],
        [
            "<strong>1.</strong> Let\u2019s define the backward pass function (policy_backward()) with the help of NumPy\u2019s modules for array multiplication \u2014  (matrix multiplication),  (outer product computation), and  (to flatten arrays into 1D arrays):",
            "markdown"
        ],
        [
            "def policy_backward(eph, epdlogp, model):\n    dW2 = np.dot(eph.T, epdlogp).ravel()\n    dh = np.outer(epdlogp, model[\"W2\"])\n    dh[eph &lt;= 0] = 0\n    dW1 = np.dot(dh.T, epx)\n    # Return new \"optimized\" weights for the policy network.\n    return {\"W1\": dW1, \"W2\": dW2}",
            "code"
        ],
        [
            "Using the intermediate hidden \u201cstates\u201d of the network (eph) and the gradients of action log probabilities (epdlogp) for an episode, the policy_backward function propagates the gradients back through the policy network and update the weights.",
            "markdown"
        ],
        [
            "<strong>2.</strong> When applying backpropagation during agent training, you will need to save several variables for each episode. Let\u2019s instantiate empty lists to store them:",
            "markdown"
        ],
        [
            "# All preprocessed observations for the episode.\nxs = []\n# All hidden \"states\" (from the network) for the episode.\nhs = []\n# All gradients of probability actions\n# (with respect to observations) for the episode.\ndlogps = []\n# All rewards for the episode.\ndrs = []",
            "code"
        ],
        [
            "You will reset these variables manually at the end of each episode during training after they are \u201cfull\u201d and reshape with NumPy\u2019s . This is demonstrated in the training stage towards the end of the tutorial.",
            "markdown"
        ],
        [
            "<strong>3.</strong> Next, to perform a gradient ascent when optimizing the agent\u2019s policy, it is common to use deep learning <em>optimizers</em> (you\u2019re performing optimization with gradients). In this example, you\u2019ll use  \u2014 an adaptive optimization . Let\u2019s set a discounting factor \u2014 a decay rate \u2014 for the optimizer:",
            "markdown"
        ],
        [
            "decay_rate = 0.99",
            "code"
        ],
        [
            "<strong>4.</strong> You will also need to store the gradients (with the help of NumPy\u2019s ) for the optimization step during training:",
            "markdown"
        ],
        [
            "First, save the update buffers that add up gradients over a batch:",
            "markdown"
        ],
        [
            "grad_buffer = {k: np.zeros_like(v) for k, v in model.items()}",
            "code"
        ],
        [
            "Second, store the RMSProp memory for the optimizer for gradient ascent:",
            "markdown"
        ],
        [
            "rmsprop_cache = {k: np.zeros_like(v) for k, v in model.items()}",
            "code"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels->Define the discounted rewards (expected return) function": [
        [
            "In this section, you will set up a function for computing discounted rewards (discount_rewards()) \u2014 the expected return from an observation \u2014 that uses a 1D array of rewards as inputs (with the help of NumPy\u2019s ) function.",
            "markdown"
        ],
        [
            "To provide more weight to shorter-term rewards over longer-term ones, you will use a <em>discount factor</em> (gamma) that is often a floating-point number between 0.9 and 0.99.",
            "markdown"
        ],
        [
            "gamma = 0.99\n\n\ndef discount_rewards(r, gamma):\n    discounted_r = np.zeros_like(r)\n    running_add = 0\n    # From the last reward to the first...\n    for t in reversed(range(0, r.size)):\n        # ...reset the reward sum\n        if r[t] != 0:\n            running_add = 0\n        # ...compute the discounted reward\n        running_add = running_add * gamma + r[t]\n        discounted_r[t] = running_add\n    return discounted_r",
            "code"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels->Train the agent for a number of episodes": [
        [
            "This section covers how to set up the training process during which your agent will be learning to play Pong using its policy.",
            "markdown"
        ],
        [
            "The pseudocode for the policy gradient method for Pong:",
            "markdown"
        ],
        [
            "Instantiate the policy \u2014 your neural network \u2014 and randomly initialize the weights in the policy network.",
            "markdown"
        ],
        [
            "Initialize a random observation.",
            "markdown"
        ],
        [
            "Randomly initialize the weights in the policy network.",
            "markdown"
        ],
        [
            "Repeat over a number of episodes:",
            "markdown"
        ],
        [
            "Input an observation into the policy network and output action probabilities for the agent (forward propagation).",
            "markdown"
        ],
        [
            "The agent takes an action for each observation, observes the received rewards and collects trajectories (over a predefined number of episodes or batch size) of state-action experiences.",
            "markdown"
        ],
        [
            "Compute the  (with a positive sign, since you need to maximize the rewards and not minimize the loss).",
            "markdown"
        ],
        [
            "For every batch of episodes:",
            "markdown"
        ],
        [
            "Calculate the gradients of your action log probabilities using the cross-entropy.",
            "markdown"
        ],
        [
            "Compute the cumulative return and, to provide more weight to shorter-term rewards versus the longer-term ones, use a discount factor discount.",
            "markdown"
        ],
        [
            "Multiply the gradients of the action log probabilities by the discounted rewards (the \u201cadvantage\u201d).",
            "markdown"
        ],
        [
            "Perform gradient ascent (backpropagation) to optimize the policy network\u2019s parameters (its weights).",
            "markdown"
        ],
        [
            "Maximize the probability of actions that lead to high rewards.",
            "markdown"
        ],
        [
            "<img alt=\"Diagram showing operations detailed in this tutorial\" src=\"../_images/tutorial-deep-reinforcement-learning-with-pong-from-pixels.png\"/>",
            "markdown"
        ],
        [
            "You can stop the training at any time or/and check saved MP4 videos of saved plays on your disk in the /video directory. You can set the maximum number of episodes that is more appropriate for your setup.",
            "markdown"
        ],
        [
            "<strong>1.</strong> For demo purposes, let\u2019s limit the number of episodes for training to 3. If you are using hardware acceleration (CPUs and GPUs), you can increase the number to 1,000 or beyond. For comparison, Andrej Karpathy\u2019s original experiment took about 8,000 episodes.",
            "markdown"
        ],
        [
            "max_episodes = 3",
            "code"
        ],
        [
            "<strong>2.</strong> Set the batch size and the learning rate values:",
            "markdown"
        ],
        [
            "The <em>batch size</em> dictates how often (in episodes) the model performs a parameter update. It is the number of times your agent can collect the state-action trajectories. At the end of the collection, you can perform the maximization of action-probability multiples.",
            "markdown"
        ],
        [
            "The  helps limit the magnitude of weight updates to prevent them from overcorrecting.",
            "markdown"
        ],
        [
            "batch_size = 3\nlearning_rate = 1e-4",
            "code"
        ],
        [
            "<strong>3.</strong> Set the game rendering default variable for Gym\u2019s render method (it is used to display the observation and is optional but can be useful during debugging):",
            "markdown"
        ],
        [
            "render = False",
            "code"
        ],
        [
            "<strong>4.</strong> Set the agent\u2019s initial (random) observation by calling reset():",
            "markdown"
        ],
        [
            "observation = env.reset()",
            "code"
        ],
        [
            "<strong>5.</strong> Initialize the previous observation:",
            "markdown"
        ],
        [
            "prev_x = None",
            "code"
        ],
        [
            "<strong>6.</strong> Initialize the reward variables and the episode count:",
            "markdown"
        ],
        [
            "running_reward = None\nreward_sum = 0\nepisode_number = 0",
            "code"
        ],
        [
            "<strong>7.</strong> To simulate motion between the frames, set the single input frame (x) for the policy network as the difference between the current and previous preprocessed frames:",
            "markdown"
        ],
        [
            "def update_input(prev_x, cur_x, D):\n    if prev_x is not None:\n        x = cur_x - prev_x\n    else:\n        x = np.zeros(D)\n    return x",
            "code"
        ],
        [
            "<strong>8.</strong> Finally, start the training loop, using the functions you have predefined:",
            "markdown"
        ],
        [
            ":tags: [output_scroll]\n\nwhile episode_number &lt; max_episodes:\n    # (For rendering.)\n    if render:\n        env.render()\n\n    # 1. Preprocess the observation (a game frame) and flatten with NumPy's `ravel()`.\n    cur_x = frame_preprocessing(observation).ravel()\n\n    # 2. Instantiate the observation for the policy network\n    x = update_input(prev_x, cur_x, D)\n    prev_x = cur_x\n\n    # 3. Perform the forward pass through the policy network using the observations\n    # (preprocessed frames as inputs) and store the action log probabilities\n    # and hidden \"states\" (for backpropagation) during the course of each episode.\n    aprob, h = policy_forward(x, model)\n    # 4. Let the action indexed at `2` (\"move up\") be that probability\n    # if it's higher than a randomly sampled value\n    # or use action `3` (\"move down\") otherwise.\n    action = 2 if rng.uniform() &lt; aprob else 3\n\n    # 5. Cache the observations and hidden \"states\" (from the network)\n    # in separate variables for backpropagation.\n    xs.append(x)\n    hs.append(h)\n\n    # 6. Compute the gradients of action log probabilities:\n    # - If the action was to \"move up\" (index `2`):\n    y = 1 if action == 2 else 0\n    # - The cross-entropy:\n    # `y*log(aprob) + (1 - y)*log(1-aprob)`\n    # or `log(aprob)` if y = 1, else: `log(1 - aprob)`.\n    # (Recall: you used the sigmoid function (`1/(1+np.exp(-x)`) to output\n    # `aprob` action probabilities.)\n    # - Then the gradient: `y - aprob`.\n    # 7. Append the gradients of your action log probabilities.\n    dlogps.append(y - aprob)\n    # 8. Take an action and update the parameters with Gym's `step()`\n    # function; obtain a new observation.\n    observation, reward, done, info = env.step(action)\n    # 9. Update the total sum of rewards.\n    reward_sum += reward\n    # 10. Append the reward for the previous action.\n    drs.append(reward)\n\n    # After an episode is finished:\n    if done:\n        episode_number += 1\n        # 11. Collect and reshape stored values with `np.vstack()` of:\n        # - Observation frames (inputs),\n        epx = np.vstack(xs)\n        # - hidden \"states\" (from the network),\n        eph = np.vstack(hs)\n        # - gradients of action log probabilities,\n        epdlogp = np.vstack(dlogps)\n        # - and received rewards for the past episode.\n        epr = np.vstack(drs)\n\n        # 12. Reset the stored variables for the new episode:\n        xs = []\n        hs = []\n        dlogps = []\n        drs = []\n\n        # 13. Discount the rewards for the past episode using the helper\n        # function you defined earlier...\n        discounted_epr = discount_rewards(epr, gamma)\n        # ...and normalize them because they have high variance\n        # (this is explained below.)\n        discounted_epr -= np.mean(discounted_epr)\n        discounted_epr /= np.std(discounted_epr)\n\n        # 14. Multiply the discounted rewards by the gradients of the action\n        # log probabilities (the \"advantage\").\n        epdlogp *= discounted_epr\n        # 15. Use the gradients to perform backpropagation and gradient ascent.\n        grad = policy_backward(eph, epdlogp, model)\n        # 16. Save the policy gradients in a buffer.\n        for k in model:\n            grad_buffer[k] += grad[k]\n        # 17. Use the RMSProp optimizer to perform the policy network\n        # parameter (weight) update at every batch size\n        # (by default: every 10 episodes).\n        if episode_number % batch_size == 0:\n            for k, v in model.items():\n                # The gradient.\n                g = grad_buffer[k]\n                # Use the RMSProp discounting factor.\n                rmsprop_cache[k] = (\n                    decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g ** 2\n                )\n                # Update the policy network with a learning rate\n                # and the RMSProp optimizer using gradient ascent\n                # (hence, there's no negative sign)\n                model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n                # Reset the gradient buffer at the end.\n                grad_buffer[k] = np.zeros_like(v)\n\n        # 18. Measure the total discounted reward.\n        running_reward = (\n            reward_sum\n            if running_reward is None\n            else running_reward * 0.99 + reward_sum * 0.01\n        )\n        print(\n            \"Resetting the Pong environment. Episode total reward: {} Running mean: {}\".format(\n                reward_sum, running_reward\n            )\n        )\n\n        # 19. Set the agent's initial observation by calling Gym's `reset()` function\n        # for the next episode and setting the reward sum back to 0.\n        reward_sum = 0\n        observation = env.reset()\n        prev_x = None\n\n    # 20. Display the output during training.\n    if reward != 0:\n        print(\n            \"Episode {}: Game finished. Reward: {}...\".format(episode_number, reward)\n            + (\"\" if reward == -1 else \" POSITIVE REWARD!\")\n        )",
            "code"
        ],
        [
            "A few notes:",
            "markdown"
        ],
        [
            "If you have previously run an experiment and want to repeat it, your Monitor instance may still be running, which may throw an error the next time you try to traini the agent. Therefore, you should first shut down Monitor by calling env.close() by uncommenting and running the cell below:",
            "markdown"
        ],
        [
            "# env.close()",
            "code"
        ],
        [
            "In Pong, if a player doesn\u2019t hit the ball back, they receive a negative reward (-1) and the other player gets a +1 reward. The rewards that the agent receives by playing Pong have a significant variance. Therefore, it\u2019s best practice to normalize them with the same mean (using ) and standard deviation (using NumPy\u2019s ).",
            "markdown"
        ],
        [
            "When using only NumPy, the deep RL training process, including backpropagation, spans several lines of code that may appear quite long. One of the main reasons for this is you\u2019re not using a deep learning framework with an automatic differentiation library that usually simplifies such experiments. This tutorial shows how to perform everything from scratch but you can also use one of many Python-based frameworks with \u201cautodiff\u201d and \u201cautograd\u201d, which you will learn about at the end of the tutorial.",
            "markdown"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels->Next steps": [
        [
            "You may notice that training an RL agent takes a long time if you increase the number of episodes from 100 to 500 or 1,000+, depending on the hardware \u2014 CPUs and GPUs \u2014 you are using for this task.",
            "markdown"
        ],
        [
            "Policy gradient methods can learn a task if you give them a lot of time, and optimization in RL is a challenging problem. Training agents to learn to play Pong or any other task can be sample-inefficient and require a lot of episodes. You may also notice in your training output that even after hundreds of episodes, the rewards may have high variance.",
            "markdown"
        ],
        [
            "In addition, like in many deep learning-based algorithms, you should take into account a large amount of parameters that your policy has to learn. In Pong, this number adds up to 1 million or more with 200 nodes in the hidden layer of the network and the input dimension being of size 6,400 (80x80). Therefore, adding more CPUs and GPUs to assist with training can always be an option.",
            "markdown"
        ],
        [
            "You can use a much more advanced policy gradient-based algorithm that can help speed up training, improve the sensitivity to parameters, and resolve other issues. For example, there are \u201cself-play\u201d methods, such as  developed by  et al in 2017, which were  to train the  agent over 10 months to play Dota 2 at a competitive level. Of course, if you apply these methods to smaller Gym environments, it should take hours, not months to train.",
            "markdown"
        ],
        [
            "In general, there are many RL challenges and possible solutions and you can explore some of them in  by , Sam Ritter, , Zeb Kurth-Nelson, , and  (2019).",
            "markdown"
        ],
        [
            "If you want to learn more about deep RL, you should check out the following free educational material:",
            "markdown"
        ],
        [
            ": developed by OpenAI.",
            "markdown"
        ],
        [
            "Deep RL lectures taught by practitioners at  and .",
            "markdown"
        ],
        [
            "RL  taught by  (DeepMind, UCL).",
            "markdown"
        ],
        [
            "Building a neural network from scratch with NumPy is a great way to learn more about NumPy and about deep learning. However, for real-world applications you should use specialized frameworks \u2014 such as , ,  or  \u2014 that provide NumPy-like APIs, have built-in  and GPU support, and are designed for high-performance numerical computing and machine learning.",
            "markdown"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels->Appendix->Notes on RL and deep RL": [
        [
            "In  deep learning for tasks, such as image recognition, language translation, or text classification, you\u2019re more likely to use a lot of labeled data. However, in RL, agents typically don\u2019t receive direct explicit feedback indicating correct or wrong actions \u2014 they rely on other signals, such as rewards.",
            "markdown"
        ],
        [
            "<em>Deep RL</em> combines RL with . The field had its first major success in more complex environments, such as video games, in 2013 \u2014 a year after the  breakthrough in computer vision. Volodymyr Mnih and colleagues at DeepMind published a research paper called  (and  in 2015) that showed that they were able to train an agent that could play several classic games from the Arcade Learning Environment at a human-level. Their RL algorithm \u2014 called a deep Q-network (DQN) \u2014 used  in a neural network that approximated  and used .",
            "markdown"
        ],
        [
            "Unlike the simple policy gradient method that you used in this example, DQN uses a type of \u201coff-policy\u201d <em>value-based</em> method (that approximates Q learning), while the original  uses policy gradients and .",
            "markdown"
        ],
        [
            "Policy gradients <em>with function approximation</em>, such as neural networks, were  in 2000 by Richard Sutton et al. They were influenced by a number of previous works, including statistical gradient-following algorithms, such as  (Ronald Williams, 1992), as well as  (Geoffrey Hinton, 1986), which helps deep learning algorithms learn. RL with neural-network function approximation were introduced in the 1990s in research by Gerald Tesauro (, 1995), who worked with IBM on an agent that learned to  in 1992, and Long-Ji Lin (, 1993).",
            "markdown"
        ],
        [
            "Since 2013, researchers have come up with many notable approaches for learning to solve complex tasks using deep RL, such as  for the game of Go (David Silver et al, 2016),  that mastered Go, Chess, and Shogi with self-play (David Silver et al, 2017-2018),  for Dota 2 with  (OpenAI, 2019), and  for StarCraft 2 that used an  algorithm with , , and  (Oriol Vinyals et al, 2019). In addition, there have been other experiments, such as deep RL for  by engineers at Electronic Arts/DICE.",
            "markdown"
        ],
        [
            "One of the reasons why video games are popular in deep RL research is that, unlike real-world experiments, such as RL with  (  et al, 2006), virtual simulations can offer safer testing environments.",
            "markdown"
        ],
        [
            "If you\u2019re interested in learning about the implications of deep RL on other fields, such as neuroscience, you can refer to a  by  et al (2020).",
            "markdown"
        ]
    ],
    "numpy->Articles->Deep reinforcement learning with Pong from pixels->Appendix->How to set up video playback in your Jupyter notebook": [
        [
            "If you\u2019re using  \u2014 a free Jupyter notebook-based tool \u2014 you can set up the Docker image and add freeglut3-dev, xvfb, and x11-utils to the apt.txt configuration file to install the initial dependencies. Then, to binder/environment.yml under channels, add gym, pyvirtualdisplay and anything else you may need, such as python=3.7, pip, and jupyterlab. Check the following  for more information.",
            "markdown"
        ],
        [
            "If you\u2019re using  (another free Jupyter notebook-based tool), you can enable video playback of the game environments installing and setting up /, , , , , and other dependencies, as described further below.",
            "markdown"
        ],
        [
            "If you\u2019re using Google Colaboratory, run the following commands in the notebook cells to help with video playback:",
            "markdown"
        ],
        [
            "# Install Xvfb and X11 dependencies.\n!apt-get install -y xvfb x11-utils &gt; /dev/null 2&gt;&amp;1\n# To work with videos, install FFmpeg.\n!apt-get install -y ffmpeg &gt; /dev/null 2&gt;&amp;1\n# Install PyVirtualDisplay for visual feedback and other libraries/dependencies.\n!pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate &gt; /dev/null 2&gt;&amp;1",
            "code"
        ],
        [
            "Then, add this Python code:",
            "markdown"
        ],
        [
            "# Import the virtual display module.\nfrom pyvirtualdisplay import Display\n# Import ipythondisplay and HTML from IPython for image and video rendering.\nfrom IPython import display as ipythondisplay\nfrom IPython.display import HTML\n\n# Initialize the virtual buffer at 400x300 (adjustable size).\n# With Xvfb, you should set `visible=False`.\ndisplay = Display(visible=False, size=(400, 300))\ndisplay.start()\n\n# Check that no display is present.\n# If no displays are present, the expected output is `:0`.\n!echo $DISPLAY\n\n# Define a helper function to display videos in Jupyter notebooks:.\n# (Source: https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/)\n\nimport sys\nimport math\nimport glob\nimport io\nimport base64\n\ndef show_any_video(mp4video=0):\n    mp4list = glob.glob('video/*.mp4')\n    if len(mp4list) &gt; 0:\n        mp4 = mp4list[mp4video]\n        video = io.open(mp4, 'r+b').read()\n        encoded = base64.b64encode(video)\n        ipythondisplay.display(HTML(data='''&lt;video alt=\"test\" autoplay\n                                            loop controls style=\"height: 400px;\"&gt;\n                                            &lt;source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /&gt;\n                                            &lt;/video&gt;'''.format(encoded.decode('ascii'))))\n\n    else:\n        print('Could not find the video!')",
            "code"
        ],
        [
            "If you want to view the last (very quick) gameplay inside a Jupyter notebook and implemented the show_any_video() function earlier, run this inside a cell:",
            "markdown"
        ],
        [
            "show_any_video(-1)",
            "code"
        ],
        [
            "If you\u2019re following the instructions in this tutorial in a local environment on Linux or macOS, you can add most of the code into one <strong>Python (.py)</strong> file. Then, you can run your Gym experiment through python your-code.py in your terminal. To enable rendering, you can use the command-line interface by following the  (make sure you have Gym and Xvfb installed, as described in the guide).",
            "markdown"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade": [
        [
            "Caution",
            "markdown"
        ],
        [
            "This article is not currently tested. Help improve this tutorial by making it\nfully executable!",
            "markdown"
        ],
        [
            "This tutorial demonstrates how to build a simple  from scratch in NumPy to perform sentiment analysis on a socially relevant and ethically acquired dataset.",
            "markdown"
        ],
        [
            "Your deep learning model (the LSTM) is a form of a Recurrent Neural Network and will learn to classify a piece of text as positive or negative from the IMDB reviews dataset. The dataset contains 50,000 movie reviews and corresponding labels. Based on the numeric representations of these reviews and their corresponding labels  the neural network will be trained to learn the sentiment using forward propagation and backpropagation through time since we are dealing with sequential data here. The output will be a vector containing the probabilities that the text samples are positive.",
            "markdown"
        ],
        [
            "Today, Deep Learning is getting adopted in everyday life and now it is more important to ensure that decisions that have been taken using AI are not reflecting discriminatory behavior towards a set of populations. It is important to take fairness into consideration while consuming the output from AI. Throughout the tutorial we\u2019ll try to question all the steps in our pipeline from an ethics point of view.",
            "markdown"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->Prerequisites": [
        [
            "You are expected to be familiar with the Python programming language and array manipulation with NumPy. In addition, some understanding of Linear Algebra and Calculus is recommended. You should also be familiar with how Neural Networks work. For reference, you can visit the ,  and  tutorials.",
            "markdown"
        ],
        [
            "To get a refresher on Deep Learning basics, You should consider reading , which is an interactive deep learning book with multi-framework code, math, and discussions. You can also go through the  to understand how a basic neural network is implemented from scratch.",
            "markdown"
        ],
        [
            "In addition to NumPy, you will be utilizing the following Python standard modules for data loading and processing:",
            "markdown"
        ],
        [
            " for handling dataframes",
            "markdown"
        ],
        [
            " for data visualization",
            "markdown"
        ],
        [
            " to download and cache datasets",
            "markdown"
        ],
        [
            "This tutorial can be run locally in an isolated environment, such as  or . You can use  to run each notebook cell.",
            "markdown"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->Table of contents": [
        [
            "Data Collection",
            "markdown"
        ],
        [
            "Preprocess the datasets",
            "markdown"
        ],
        [
            "Build and train a LSTM network from scratch",
            "markdown"
        ],
        [
            "Perform sentiment analysis on collected speeches",
            "markdown"
        ],
        [
            "Next steps",
            "markdown"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->1. Data Collection": [
        [
            "Before you begin there are a few pointers you should always keep in mind before choosing the data you wish to train your model on:",
            "markdown"
        ],
        [
            "<strong>Identifying Data Bias</strong> - Bias is an inherent component of the human thought process. Therefore data sourced from human activities reflects that bias. Some ways in which this bias tends to occur in Machine Learning datasets are:",
            "markdown"
        ],
        [
            "<em>Bias in historical data</em>: Historical data are often skewed towards, or against, particular groups.\nData can also be severely imbalanced with limited information on protected groups.",
            "markdown"
        ],
        [
            "<em>Bias in data collection mechanisms</em>: Lack of representativeness introduces inherent biases in the data collection process.",
            "markdown"
        ],
        [
            "<em>Bias towards observable outcomes</em>: In some scenarios, we have the information about True Outcomes only for a certain section of the population. In the absence of information on all outcomes, one cannot even measure fairness",
            "markdown"
        ],
        [
            "<strong>Preserving human anonymity for sensitive data</strong>:  identified a list of sensitive topics that need to be handled with extra care. We present the same below along with a few additions:",
            "markdown"
        ],
        [
            "personal daily routines (including location data);",
            "markdown"
        ],
        [
            "individual details about impairment and/or medical records;",
            "markdown"
        ],
        [
            "emotional accounts of pain and chronic illness;",
            "markdown"
        ],
        [
            "financial information about income and/or welfare payments;",
            "markdown"
        ],
        [
            "discrimination and abuse episodes;",
            "markdown"
        ],
        [
            "criticism/praise of individual providers of healthcare and support services;",
            "markdown"
        ],
        [
            "suicidal thoughts;",
            "markdown"
        ],
        [
            "criticism/praise of a power structure especially if it compromises their safety;",
            "markdown"
        ],
        [
            "personally-identifying information (even if anonymized in some way) including things like fingerprints or voice.\n\n\n\n<blockquote>",
            "markdown"
        ],
        [
            "While it can be difficult taking consent from so many people especially on on-line platforms, the necessity of it depends upon the sensitivity of the topics your data includes and other indicators like whether the platform the data was obtained from allows users to operate under pseudonyms. If the website has a policy that forces the use of a real name, then the users need to be asked for consent.\n</blockquote>",
            "markdown"
        ],
        [
            "In this section, you will be collecting two different datasets: the IMDb movie reviews dataset, and a collection of 10 speeches curated for this tutorial including activists from different countries around the world, different times, and different topics. The former would be used to train the deep learning model while the latter will be used to perform sentiment analysis on.",
            "markdown"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->1. Data Collection->Collecting the IMDb reviews dataset": [
        [
            "IMDb Reviews Dataset is a large movie review dataset collected and prepared by Andrew L. Maas from the popular movie rating service, IMDb. The IMDb Reviews dataset is used for binary sentiment classification, whether a review is positive or negative. It contains 25,000 movie reviews for training and 25,000 for testing. All these 50,000 reviews are labeled data that may be used for supervised deep learning. For ease of reproducibility, we\u2019ll be sourcing  the data from .\n<blockquote>",
            "markdown"
        ],
        [
            "The IMDb platform allows the usage of their public datasets for personal and non-commercial use. We did our best to ensure that these reviews do not contain any of the aforementioned sensitive topics pertaining to the reviewer.\n</blockquote>",
            "markdown"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->1. Data Collection->Collecting and loading the speech transcripts": [
        [
            "We have chosen speeches by activists around the globe talking about issues like climate change, feminism, lgbtqa+ rights and racism. These were sourced from newspapers, the official website of the United Nations and the archives of established universities as cited in the table below. A CSV file was created containing the transcribed speeches, their speaker and the source the speeches were obtained from.\nWe made sure to include different demographics in our data and included a range of different topics, most of which focus on social and/or ethical issues.\n\n<!-- #region -->",
            "markdown"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets": [
        [
            "Preprocessing data is an extremely crucial step before building any Deep learning model, however in an attempt to keep the tutorial focused on building the model, we will not dive deep into the code for preprocessing. Given below is a brief overview of all the steps we undertake to clean our data and convert it to its numeric representation.\n</blockquote>",
            "markdown"
        ],
        [
            "<strong>Text Denoising</strong> : Before converting your text into vectors, it is important to clean it and remove all unhelpful parts a.k.a the noise from your data by converting all characters to lowercase, removing html tags, brackets and stop words (words that don\u2019t add much meaning to a sentence). Without this step the dataset is often a cluster of words that the computer doesn\u2019t understand.",
            "markdown"
        ],
        [
            "<strong>Converting words to vectors</strong> : A word embedding is a learned representation for text where words that have the same meaning have a similar representation. Individual words are represented as real-valued vectors in a predefined vector space. GloVe is an unsupervised algorithm developed by Stanford for generating word embeddings by generating global word-word co-occurence matrix from a corpus. You can download the zipped files containing the embeddings from https://nlp.stanford.edu/projects/glove/. Here you can choose any of the four options for different sizes or training datasets. We have chosen the least memory consuming embedding file.\n\n<blockquote>",
            "markdown"
        ],
        [
            "The GloVe word embeddings include sets that were trained on billions of tokens, some up to 840 billion tokens. These algorithms exhibit stereotypical biases, such as gender bias which can be traced back to the original training data. For example certain occupations seem to be more biased towards a particular gender, reinforcing problematic stereotypes. The nearest solution to this problem are some de-biasing algorithms as the one presented in https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6835575.pdf which one can use on embeddings of their choice to mitigate bias, if present.\n</blockquote>\n<!-- #endregion -->",
            "markdown"
        ],
        [
            "You\u2019ll start with importing the necessary packages to build our Deep Learning network.",
            "markdown"
        ],
        [
            "# Importing the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pooch\nimport string\nimport re\nimport zipfile\nimport os\n\n# Creating the random instance\nrng = np.random.default_rng()",
            "code"
        ],
        [
            "Next, you\u2019ll define set of text preprocessing helper functions.",
            "markdown"
        ],
        [
            "class TextPreprocess:\n    \"\"\"Text Preprocessing for a Natural Language Processing model.\"\"\"\n\n    def txt_to_df(self, file):\n        \"\"\"Function to convert a txt file to pandas dataframe.\n\n        Parameters\n        ----------\n        file : str\n            Path to the txt file.\n\n        Returns\n        -------\n        Pandas dataframe\n            txt file converted to a dataframe.\n\n        \"\"\"\n        with open(imdb_train, 'r') as in_file:\n            stripped = (line.strip() for line in in_file)\n            reviews = {}\n            for line in stripped:\n                lines = [splits for splits in line.split(\"\\t\") if splits != \"\"]\n                reviews[lines[1]] = float(lines[0])\n        df = pd.DataFrame(reviews.items(), columns=['review', 'sentiment'])\n        df = df.sample(frac=1).reset_index(drop=True)\n        return df\n\n    def unzipper(self, zipped, to_extract):\n        \"\"\"Function to extract a file from a zipped folder.\n\n        Parameters\n        ----------\n        zipped : str\n            Path to the zipped folder.\n\n        to_extract: str\n            Path to the file to be extracted from the zipped folder\n\n        Returns\n        -------\n        str\n            Path to the extracted file.\n\n        \"\"\"\n        fh = open(zipped, 'rb')\n        z = zipfile.ZipFile(fh)\n        outdir = os.path.split(zipped)[0]\n        z.extract(to_extract, outdir)\n        fh.close()\n        output_file = os.path.join(outdir, to_extract)\n        return output_file\n\n    def cleantext(self, df, text_column=None,\n                  remove_stopwords=True, remove_punc=True):\n        \"\"\"Function to clean text data.\n\n        Parameters\n        ----------\n        df : pandas dataframe\n            The dataframe housing the input data.\n        text_column : str\n            Column in dataframe whose text is to be cleaned.\n        remove_stopwords : bool\n            if True, remove stopwords from text\n        remove_punc : bool\n            if True, remove punctuation symbols from text\n\n        Returns\n        -------\n        Numpy array\n            Cleaned text.\n\n        \"\"\"\n        # converting all characters to lowercase\n        df[text_column] = df[text_column].str.lower()\n\n        # List of stopwords taken from https://gist.github.com/sebleier/554280\n        stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\",\n                     \"all\", \"am\", \"an\", \"and\", \"any\", \"are\",\n                     \"as\", \"at\", \"be\", \"because\",\n                     \"been\", \"before\", \"being\", \"below\",\n                     \"between\", \"both\", \"but\", \"by\", \"could\",\n                     \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n                     \"each\", \"few\", \"for\", \"from\", \"further\",\n                     \"had\", \"has\", \"have\", \"having\", \"he\",\n                     \"he'd\", \"he'll\", \"he's\", \"her\", \"here\",\n                     \"here's\", \"hers\", \"herself\", \"him\",\n                     \"himself\", \"his\", \"how\", \"how's\", \"i\",\n                     \"i'd\", \"i'll\", \"i'm\", \"i've\",\n                     \"if\", \"in\", \"into\",\n                     \"is\", \"it\", \"it's\", \"its\",\n                     \"itself\", \"let's\", \"me\", \"more\",\n                     \"most\", \"my\", \"myself\", \"nor\", \"of\",\n                     \"on\", \"once\", \"only\", \"or\",\n                     \"other\", \"ought\", \"our\", \"ours\",\n                     \"ourselves\", \"out\", \"over\", \"own\", \"same\",\n                     \"she\", \"she'd\", \"she'll\", \"she's\", \"should\",\n                     \"so\", \"some\", \"such\", \"than\", \"that\",\n                     \"that's\", \"the\", \"their\", \"theirs\", \"them\",\n                     \"themselves\", \"then\", \"there\", \"there's\",\n                     \"these\", \"they\", \"they'd\", \"they'll\",\n                     \"they're\", \"they've\", \"this\", \"those\",\n                     \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n                     \"very\", \"was\", \"we\", \"we'd\", \"we'll\",\n                     \"we're\", \"we've\", \"were\", \"what\",\n                     \"what's\", \"when\", \"when's\",\n                     \"where\", \"where's\",\n                     \"which\", \"while\", \"who\", \"who's\",\n                     \"whom\", \"why\", \"why's\", \"with\",\n                     \"would\", \"you\", \"you'd\", \"you'll\",\n                     \"you're\", \"you've\",\n                     \"your\", \"yours\", \"yourself\", \"yourselves\"]\n\n        def remove_stopwords(data, column):\n            data[f'{column} without stopwords'] = data[column].apply(\n                lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n            return data\n\n        def remove_tags(string):\n            result = re.sub('&lt;*&gt;', '', string)\n            return result\n\n        # remove html tags and brackets from text\n        if remove_stopwords:\n            data_without_stopwords = remove_stopwords(df, text_column)\n            data_without_stopwords[f'clean_{text_column}'] = data_without_stopwords[f'{text_column} without stopwords'].apply(\n                lambda cw: remove_tags(cw))\n        if remove_punc:\n            data_without_stopwords[f'clean_{text_column}'] = data_without_stopwords[f'clean_{text_column}'].str.replace(\n                '[{}]'.format(string.punctuation), ' ', regex=True)\n\n        X = data_without_stopwords[f'clean_{text_column}'].to_numpy()\n\n        return X\n\n\n    def sent_tokeniser(self, x):\n        \"\"\"Function to split text into sentences.\n\n        Parameters\n        ----------\n        x : str\n            piece of text\n\n        Returns\n        -------\n        list\n            sentences with punctuation removed.\n\n        \"\"\"\n        sentences = re.split(r'(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;=\\.|\\?)\\s', x)\n        sentences.pop()\n        sentences_cleaned = [re.sub(r'[^\\w\\s]', '', x) for x in sentences]\n        return sentences_cleaned\n\n    def word_tokeniser(self, text):\n        \"\"\"Function to split text into tokens.\n\n        Parameters\n        ----------\n        x : str\n            piece of text\n\n        Returns\n        -------\n        list\n            words with punctuation removed.\n\n        \"\"\"\n        tokens = re.split(r\"([-\\s.,;!?])+\", text)\n        words = [x for x in tokens if (\n            x not in '- \\t\\n.,;!?\\\\' and '\\\\' not in x)]\n        return words\n\n    def loadGloveModel(self, emb_path):\n        \"\"\"Function to read from the word embedding file.\n\n        Returns\n        -------\n        Dict\n            mapping from word to corresponding word embedding.\n\n        \"\"\"\n        print(\"Loading Glove Model\")\n        File = emb_path\n        f = open(File, 'r')\n        gloveModel = {}\n        for line in f:\n            splitLines = line.split()\n            word = splitLines[0]\n            wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n            gloveModel[word] = wordEmbedding\n        print(len(gloveModel), \" words loaded!\")\n        return gloveModel\n\n    def text_to_paras(self, text, para_len):\n        \"\"\"Function to split text into paragraphs.\n\n        Parameters\n        ----------\n        text : str\n            piece of text\n\n        para_len : int\n            length of each paragraph\n\n        Returns\n        -------\n        list\n            paragraphs of specified length.\n\n        \"\"\"\n        # split the speech into a list of words\n        words = text.split()\n        # obtain the total number of paragraphs\n        no_paras = int(np.ceil(len(words)/para_len))\n        # split the speech into a list of sentences\n        sentences = self.sent_tokeniser(text)\n        # aggregate the sentences into paragraphs\n        k, m = divmod(len(sentences), no_paras)\n        agg_sentences = [sentences[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(no_paras)]\n        paras = np.array([' '.join(sents) for sents in agg_sentences])\n\n        return paras",
            "code"
        ],
        [
            " is a Python package made by scientists that manages downloading data files over HTTP and storing them in a local directory. We use this to set up a download manager which contains all of the information needed to fetch the data files in our registry and store them in the specified cache folder.",
            "markdown"
        ],
        [
            "data = pooch.create(\n    # folder where the data will be stored in the\n    # default cache folder of your Operating System\n    path=pooch.os_cache(\"numpy-nlp-tutorial\"),\n    # Base URL of the remote data store\n    base_url=\"\",\n    # The cache file registry. A dictionary with all files managed by this pooch.\n    # The keys are the file names and values are their respective hash codes which\n    # ensure we download the same, uncorrupted file each time.\n    registry={\n        \"imdb_train.txt\": \"6a38ea6ab5e1902cc03f6b9294ceea5e8ab985af991f35bcabd301a08ea5b3f0\",\n         \"imdb_test.txt\": \"7363ef08ad996bf4233b115008d6d7f9814b7cc0f4d13ab570b938701eadefeb\",\n        \"glove.6B.50d.zip\": \"617afb2fe6cbd085c235baf7a465b96f4112bd7f7ccb2b2cbd649fed9cbcf2fb\",\n    },\n    # Now specify custom URLs for some of the files in the registry.\n    urls={\n        \"imdb_train.txt\": \"doi:10.5281/zenodo.4117827/imdb_train.txt\",\n        \"imdb_test.txt\": \"doi:10.5281/zenodo.4117827/imdb_test.txt\",\n        \"glove.6B.50d.zip\": 'https://nlp.stanford.edu/data/glove.6B.zip'\n    }\n)",
            "code"
        ],
        [
            "Download the IMDb training and testing data files:",
            "markdown"
        ],
        [
            "imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "code"
        ],
        [
            "Instantiate the TextPreprocess class to perform various operations on our datasets:",
            "markdown"
        ],
        [
            "textproc = TextPreprocess()",
            "code"
        ],
        [
            "Convert each IMDb file to a pandas dataframe for a more convenient preprocessing of the datasets:",
            "markdown"
        ],
        [
            "train_df = textproc.txt_to_df(imdb_train)\ntest_df = textproc.txt_to_df(imdb_test)",
            "code"
        ],
        [
            "Now, you will clean the dataframes obtained above by removing occurrences of stopwords and punctuation marks. You will also retrieve the sentiment values from each dataframe to obtain the target variables:",
            "markdown"
        ],
        [
            "X_train = textproc.cleantext(train_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:2000]\n\nX_test = textproc.cleantext(test_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:1000]\n\ny_train = train_df['sentiment'].to_numpy()[0:2000]\ny_test = test_df['sentiment'].to_numpy()[0:1000]",
            "code"
        ],
        [
            "The same process is applicable on the collected speeches:\n<blockquote>",
            "markdown"
        ],
        [
            "Since we will be performing paragraph wise sentiment analysis on each speech further ahead in the tutorial, we\u2019ll need the punctuation marks to split the text into paragraphs, hence we refrain from removing their punctuation marks at this stage\n</blockquote>",
            "markdown"
        ],
        [
            "speech_data_path = 'tutorial-nlp-from-scratch/speeches.csv'\nspeech_df = pd.read_csv(speech_data_path)\nX_pred = textproc.cleantext(speech_df,\n                            text_column='speech',\n                            remove_stopwords=True,\n                            remove_punc=False)\nspeakers = speech_df['speaker'].to_numpy()",
            "code"
        ],
        [
            "You will now download the GloVe embeddings, unzip them and build a dictionary mapping each word and word embedding. This will act as a cache for when you need to replace each word with its respective word embedding.",
            "markdown"
        ],
        [
            "glove = data.fetch('glove.6B.50d.zip')\nemb_path = textproc.unzipper(glove, 'glove.6B.300d.txt')\nemb_matrix = textproc.loadGloveModel(emb_path)",
            "code"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6": [
        [
            "It is time to start implementing our LSTM! You will have to first familiarize yourself with some high-level concepts of the basic building blocks of a deep learning model. You can refer to the  for the same.",
            "markdown"
        ],
        [
            "You will then learn how a Recurrent Neural Network differs from a plain Neural Network and what makes it so suitable for processing sequential data. Afterwards, you will construct the building blocks of a simple deep learning model in Python and NumPy and train it to learn to classify the sentiment of a piece of text as positive or negative with a certain level of accuracy",
            "markdown"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Introduction to a Long Short Term Memory Network": [
        [
            "In a  (MLP), the information only moves in one direction \u2014 from the input layer, through the hidden layers, to the output layer. The information moves straight through the network and never takes the previous nodes into account at a later stage. Because it only considers the current input, the features learned are not shared across different positions of the sequence. Moreover, it cannot process sequences with varying lengths.",
            "markdown"
        ],
        [
            "Unlike an MLP, the RNN was designed to work with sequence prediction problems.RNNs introduce state variables to store past information, together with the current inputs, to determine the current outputs. Since an RNN shares the learned features with all the data points in a sequence regardless of its length, it is capable of processing sequences with varying lengths.",
            "markdown"
        ],
        [
            "The problem with an RNN however, is that it cannot retain long-term memory because the influence of a given input on the hidden layer, and therefore on the network output, either decays or blows up exponentially as it cycles around the network\u2019s recurrent connections. This shortcoming is referred to as the vanishing gradient problem. Long Short-Term Memory (LSTM) is an RNN architecture specifically designed to address the .",
            "markdown"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Overview of the Model Architecture": [
        [
            "<img alt=\"Overview of the model architecture, showing a series of animated boxes. There are five identical boxes labeled A and receiving as input one of the words in the phrase &quot;life's a box of chocolates&quot;. Each box is highlighted in turn, representing the memory blocks of the LSTM network as information passes through them, ultimately reaching a &quot;Positive&quot; output value.\" src=\"../_images/lstm.gif\"/>",
            "markdown"
        ],
        [
            "In the above gif, the rectangles labeled \\(A\\) are called Cells and they are the <strong>Memory Blocks</strong> of our LSTM network. They are responsible for choosing what to remember in a sequence and pass on that information to the next cell via two states called the hidden state \\(H_{t}\\) and the cell state \\(C_{t}\\) where \\(t\\) indicates the time-step. Each Cell has dedicated gates which are responsible for storing, writing or reading the information passed to an LSTM. You will now look closely at the architecture of the network by implementing each mechanism happening inside of it.",
            "markdown"
        ],
        [
            "Lets start with writing a function to randomly initialize the parameters which will be learned while our model trains",
            "markdown"
        ],
        [
            "def initialise_params(hidden_dim, input_dim):\n    # forget gate\n    Wf = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bf = rng.standard_normal(size=(hidden_dim, 1))\n    # input gate\n    Wi = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bi = rng.standard_normal(size=(hidden_dim, 1))\n    # candidate memory gate\n    Wcm = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bcm = rng.standard_normal(size=(hidden_dim, 1))\n    # output gate\n    Wo = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bo = rng.standard_normal(size=(hidden_dim, 1))\n\n    # fully connected layer for classification\n    W2 = rng.standard_normal(size=(1, hidden_dim))\n    b2 = np.zeros((1, 1))\n\n    parameters = {\n        \"Wf\": Wf,\n        \"bf\": bf,\n        \"Wi\": Wi,\n        \"bi\": bi,\n        \"Wcm\": Wcm,\n        \"bcm\": bcm,\n        \"Wo\": Wo,\n        \"bo\": bo,\n        \"W2\": W2,\n        \"b2\": b2\n    }\n    return parameters",
            "code"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Forward Propagation": [
        [
            "Now that you have your initialized parameters, you can pass the input data in a forward direction through the network. Each layer accepts the input data, processes it and passes it to the successive layer. This process is called Forward Propagation. You will undertake the following mechanism to implement it:",
            "markdown"
        ],
        [
            "Loading the word embeddings of the input data",
            "markdown"
        ],
        [
            "Passing the embeddings to an LSTM",
            "markdown"
        ],
        [
            "Perform all the gate mechanisms in every memory block of the LSTM to obtain the final hidden state",
            "markdown"
        ],
        [
            "Passing the final hidden state through a fully connected layer to obtain the probability with which the sequence is positive",
            "markdown"
        ],
        [
            "Storing all the calculated values in a cache to utilize during backpropagation",
            "markdown"
        ],
        [
            " belongs to the family of non-linear activation functions. It helps the network to update or forget the data. If the sigmoid of a value results in 0, the information is considered forgotten. Similarly, the information stays if it is 1.",
            "markdown"
        ],
        [
            "def sigmoid(x):\n    n = np.exp(np.fmin(x, 0))\n    d = (1 + np.exp(-np.abs(x)))\n    return n / d",
            "code"
        ],
        [
            "The <strong>Forget Gate</strong> takes the current word embedding and the previous hidden state concatenated together as input. and decides what parts of the old memory cell content need attention and which can be ignored.",
            "markdown"
        ],
        [
            "def fp_forget_gate(concat, parameters):\n    ft = sigmoid(np.dot(parameters['Wf'], concat)\n                 + parameters['bf'])\n    return ft",
            "code"
        ],
        [
            "The <strong>Input Gate</strong> takes the current word embedding and the previous hidden state concatenated together as input. and governs how much of the new data we take into account via the <strong>Candidate Memory Gate</strong> which utilizes the  to regulate the values flowing through the network.",
            "markdown"
        ],
        [
            "def fp_input_gate(concat, parameters):\n    it = sigmoid(np.dot(parameters['Wi'], concat)\n                 + parameters['bi'])\n    cmt = np.tanh(np.dot(parameters['Wcm'], concat)\n                  + parameters['bcm'])\n    return it, cmt",
            "code"
        ],
        [
            "Finally we have the <strong>Output Gate</strong> which takes information from the current word embedding, previous hidden state and the cell state which has been updated with information from the forget and input gates to update the value of the hidden state.",
            "markdown"
        ],
        [
            "def fp_output_gate(concat, next_cs, parameters):\n    ot = sigmoid(np.dot(parameters['Wo'], concat)\n                 + parameters['bo'])\n    next_hs = ot * np.tanh(next_cs)\n    return ot, next_hs",
            "code"
        ],
        [
            "The following image summarizes each gate mechanism in the memory block of a LSTM network:\n<blockquote>",
            "markdown"
        ],
        [
            "Image has been modified from  source\n</blockquote>",
            "markdown"
        ],
        [
            "<img alt='Diagram showing three sections of a memory block, labeled \"Forget gate\", \"Input gate\" and \"Output gate\". Each gate contains several subparts, representing the operations performed at that stage of the process.' src=\"../_images/mem_block.png\"/>",
            "markdown"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->But how do you obtain sentiment from the LSTM\u2019s output?": [
        [
            "The hidden state you obtain from the output gate of the last memory block in a sequence is considered to be a representation of all the information contained in a sequence. To classify this information into various classes (2 in our case, positive and negative) we use a <strong>Fully Connected layer</strong> which firstly maps this information to a predefined output size (1 in our case). Then, an activation function such as the sigmoid converts this output to a value between 0 and 1. We\u2019ll consider values greater than 0.5 to be indicative of a positive sentiment.",
            "markdown"
        ],
        [
            "def fp_fc_layer(last_hs, parameters):\n    z2 = (np.dot(parameters['W2'], last_hs)\n          + parameters['b2'])\n    a2 = sigmoid(z2)\n    return a2",
            "code"
        ],
        [
            "Now you will put all these functions together to summarize the <strong>Forward Propagation</strong> step in our model architecture:",
            "markdown"
        ],
        [
            "def forward_prop(X_vec, parameters, input_dim):\n\n    hidden_dim = parameters['Wf'].shape[0]\n    time_steps = len(X_vec)\n\n    # Initialise hidden and cell state before passing to first time step\n    prev_hs = np.zeros((hidden_dim, 1))\n    prev_cs = np.zeros(prev_hs.shape)\n\n    # Store all the intermediate and final values here\n    caches = {'lstm_values': [], 'fc_values': []}\n\n    # Hidden state from the last cell in the LSTM layer is calculated.\n    for t in range(time_steps):\n        # Retrieve word corresponding to current time step\n        x = X_vec[t]\n        # Retrieve the embedding for the word and reshape it to make the LSTM happy\n        xt = emb_matrix.get(x, rng.random(size=(input_dim, 1)))\n        xt = xt.reshape((input_dim, 1))\n\n        # Input to the gates is concatenated previous hidden state and current word embedding\n        concat = np.vstack((prev_hs, xt))\n\n        # Calculate output of the forget gate\n        ft = fp_forget_gate(concat, parameters)\n\n        # Calculate output of the input gate\n        it, cmt = fp_input_gate(concat, parameters)\n        io = it * cmt\n\n        # Update the cell state\n        next_cs = (ft * prev_cs) + io\n\n        # Calculate output of the output gate\n        ot, next_hs = fp_output_gate(concat, next_cs, parameters)\n\n        # store all the values used and calculated by\n        # the LSTM in a cache for backward propagation.\n        lstm_cache = {\n        \"next_hs\": next_hs,\n        \"next_cs\": next_cs,\n        \"prev_hs\": prev_hs,\n        \"prev_cs\": prev_cs,\n        \"ft\": ft,\n        \"it\" : it,\n        \"cmt\": cmt,\n        \"ot\": ot,\n        \"xt\": xt,\n        }\n        caches['lstm_values'].append(lstm_cache)\n\n        # Pass the updated hidden state and cell state to the next time step\n        prev_hs = next_hs\n        prev_cs = next_cs\n\n    # Pass the LSTM output through a fully connected layer to\n    # obtain probability of the sequence being positive\n    a2 = fp_fc_layer(next_hs, parameters)\n\n    # store all the values used and calculated by the\n    # fully connected layer in a cache for backward propagation.\n    fc_cache = {\n    \"a2\" : a2,\n    \"W2\" : parameters['W2']\n    }\n    caches['fc_values'].append(fc_cache)\n    return caches",
            "code"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Backpropagation": [
        [
            "After each forward pass through the network, you will implement the backpropagation through time algorithm to accumulate gradients of each parameter over the time steps. Backpropagation through a LSTM is not as straightforward as through other common Deep Learning architectures, due to the special way its underlying layers interact. Nonetheless, the approach is largely the same; identifying dependencies and applying the chain rule.",
            "markdown"
        ],
        [
            "Lets start with defining a function to initialize gradients of each parameter as arrays made up of zeros with same dimensions as the corresponding parameter",
            "markdown"
        ],
        [
            "# Initialise the gradients\ndef initialize_grads(parameters):\n    grads = {}\n    for param in parameters.keys():\n        grads[f'd{param}'] = np.zeros((parameters[param].shape))\n    return grads",
            "code"
        ],
        [
            "Now, for each gate and the fully connected layer, we define a function to calculate the gradient of the loss with respect to the input passed and the parameters used. To understand the mathematics behind how the derivatives were calculated we suggest you to follow this helpful  by Christina Kouridi.",
            "markdown"
        ],
        [
            "Define a function to calculate the gradients in the <strong>Forget Gate</strong>:",
            "markdown"
        ],
        [
            "def bp_forget_gate(hidden_dim, concat, dh_prev, dc_prev, cache, gradients, parameters):\n    # dft = dL/da2 * da2/dZ2 * dZ2/dh_prev * dh_prev/dc_prev * dc_prev/dft\n    dft = ((dc_prev * cache[\"prev_cs\"] + cache[\"ot\"]\n           * (1 - np.square(np.tanh(cache[\"next_cs\"])))\n           * cache[\"prev_cs\"] * dh_prev) * cache[\"ft\"] * (1 - cache[\"ft\"]))\n    # dWf = dft * dft/dWf\n    gradients['dWf'] += np.dot(dft, concat.T)\n    # dbf = dft * dft/dbf\n    gradients['dbf'] += np.sum(dft, axis=1, keepdims=True)\n    # dh_f = dft * dft/dh_prev\n    dh_f = np.dot(parameters[\"Wf\"][:, :hidden_dim].T, dft)\n    return dh_f, gradients",
            "code"
        ],
        [
            "Define a function to calculate the gradients in the <strong>Input Gate</strong> and <strong>Candidate Memory Gate</strong>:",
            "markdown"
        ],
        [
            "def bp_input_gate(hidden_dim, concat, dh_prev, dc_prev, cache, gradients, parameters):\n    # dit = dL/da2 * da2/dZ2 * dZ2/dh_prev * dh_prev/dc_prev * dc_prev/dit\n    dit = ((dc_prev * cache[\"cmt\"] + cache[\"ot\"]\n           * (1 - np.square(np.tanh(cache[\"next_cs\"])))\n           * cache[\"cmt\"] * dh_prev) * cache[\"it\"] * (1 - cache[\"it\"]))\n    # dcmt = dL/da2 * da2/dZ2 * dZ2/dh_prev * dh_prev/dc_prev * dc_prev/dcmt\n    dcmt = ((dc_prev * cache[\"it\"] + cache[\"ot\"]\n            * (1 - np.square(np.tanh(cache[\"next_cs\"])))\n            * cache[\"it\"] * dh_prev) * (1 - np.square(cache[\"cmt\"])))\n    # dWi = dit * dit/dWi\n    gradients['dWi'] += np.dot(dit, concat.T)\n    # dWcm = dcmt * dcmt/dWcm\n    gradients['dWcm'] += np.dot(dcmt, concat.T)\n    # dbi = dit * dit/dbi\n    gradients['dbi'] += np.sum(dit, axis=1, keepdims=True)\n    # dWcm = dcmt * dcmt/dbcm\n    gradients['dbcm'] += np.sum(dcmt, axis=1, keepdims=True)\n    # dhi = dit * dit/dh_prev\n    dh_i = np.dot(parameters[\"Wi\"][:, :hidden_dim].T, dit)\n    # dhcm = dcmt * dcmt/dh_prev\n    dh_cm = np.dot(parameters[\"Wcm\"][:, :hidden_dim].T, dcmt)\n    return dh_i, dh_cm, gradients",
            "code"
        ],
        [
            "Define a function to calculate the gradients for the <strong>Output Gate</strong>:",
            "markdown"
        ],
        [
            "def bp_output_gate(hidden_dim, concat, dh_prev, dc_prev, cache, gradients, parameters):\n    # dot = dL/da2 * da2/dZ2 * dZ2/dh_prev * dh_prev/dot\n    dot = (dh_prev * np.tanh(cache[\"next_cs\"])\n           * cache[\"ot\"] * (1 - cache[\"ot\"]))\n    # dWo = dot * dot/dWo\n    gradients['dWo'] += np.dot(dot, concat.T)\n    # dbo = dot * dot/dbo\n    gradients['dbo'] += np.sum(dot, axis=1, keepdims=True)\n    # dho = dot * dot/dho\n    dh_o = np.dot(parameters[\"Wo\"][:, :hidden_dim].T, dot)\n    return dh_o, gradients",
            "code"
        ],
        [
            "Define a function to calculate the gradients for the <strong>Fully Connected Layer</strong>:",
            "markdown"
        ],
        [
            "def bp_fc_layer (target, caches, gradients):\n    # dZ2 = dL/da2 * da2/dZ2\n    predicted = np.array(caches['fc_values'][0]['a2'])\n    target = np.array(target)\n    dZ2 = predicted - target\n    # dW2 = dL/da2 * da2/dZ2 * dZ2/dW2\n    last_hs = caches['lstm_values'][-1][\"next_hs\"]\n    gradients['dW2'] = np.dot(dZ2, last_hs.T)\n    # db2 = dL/da2 * da2/dZ2 * dZ2/db2\n    gradients['db2'] = np.sum(dZ2)\n    # dh_last = dZ2 * W2\n    W2 = caches['fc_values'][0][\"W2\"]\n    dh_last = np.dot(W2.T, dZ2)\n    return dh_last, gradients",
            "code"
        ],
        [
            "Put all these functions together to summarize the <strong>Backpropagation</strong> step for our model:",
            "markdown"
        ],
        [
            "def backprop(y, caches, hidden_dim, input_dim, time_steps, parameters):\n\n    # Initialize gradients\n    gradients = initialize_grads(parameters)\n\n    # Calculate gradients for the fully connected layer\n    dh_last, gradients = bp_fc_layer(target, caches, gradients)\n\n    # Initialize gradients w.r.t previous hidden state and previous cell state\n    dh_prev = dh_last\n    dc_prev = np.zeros((dh_prev.shape))\n\n    # loop back over the whole sequence\n    for t in reversed(range(time_steps)):\n        cache = caches['lstm_values'][t]\n\n        # Input to the gates is concatenated previous hidden state and current word embedding\n        concat = np.concatenate((cache[\"prev_hs\"], cache[\"xt\"]), axis=0)\n\n        # Compute gates related derivatives\n        # Calculate derivative w.r.t the input and parameters of forget gate\n        dh_f, gradients = bp_forget_gate(hidden_dim, concat, dh_prev, dc_prev, cache, gradients, parameters)\n\n        # Calculate derivative w.r.t the input and parameters of input gate\n        dh_i, dh_cm, gradients = bp_input_gate(hidden_dim, concat, dh_prev, dc_prev, cache, gradients, parameters)\n\n        # Calculate derivative w.r.t the input and parameters of output gate\n        dh_o, gradients = bp_output_gate(hidden_dim, concat, dh_prev, dc_prev, cache, gradients, parameters)\n\n        # Compute derivatives w.r.t prev. hidden state and the prev. cell state\n        dh_prev = dh_f + dh_i + dh_cm + dh_o\n        dc_prev = (dc_prev * cache[\"ft\"] + cache[\"ot\"]\n                   * (1 - np.square(np.tanh(cache[\"next_cs\"])))\n                   * cache[\"ft\"] * dh_prev)\n\n    return gradients",
            "code"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Updating the Parameters": [
        [
            "We update the parameters through an optimization algorithm called  which is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing. Specifically, the algorithm calculates an exponential moving average of the gradient and the squared gradient, and the parameters beta1 and beta2 control the decay rates of these moving averages. Adam has shown increased convergence and robustness over other gradient descent algorithms and is often recommended as the default optimizer for training.",
            "markdown"
        ],
        [
            "Define a function to initialise the moving averages for each parameter",
            "markdown"
        ],
        [
            "# initialise the moving averages\ndef initialise_mav(hidden_dim, input_dim, params):\n    v = {}\n    s = {}\n    # Initialize dictionaries v, s\n    for key in params:\n        v['d' + key] = np.zeros(params[key].shape)\n        s['d' + key] = np.zeros(params[key].shape)\n    # Return initialised moving averages\n    return v, s",
            "code"
        ],
        [
            "Define a function to update the parameters",
            "markdown"
        ],
        [
            "# Update the parameters using Adam optimization\ndef update_parameters(parameters, gradients, v, s,\n                      learning_rate=0.01, beta1=0.9, beta2=0.999):\n    for key in parameters:\n        # Moving average of the gradients\n        v['d' + key] = (beta1 * v['d' + key]\n                        + (1 - beta1) * gradients['d' + key])\n\n        # Moving average of the squared gradients\n        s['d' + key] = (beta2 * s['d' + key]\n                        + (1 - beta2) * (gradients['d' + key] ** 2))\n\n        # Update parameters\n        parameters[key] = (parameters[key] - learning_rate\n                           * v['d' + key] / np.sqrt(s['d' + key] + 1e-8))\n    # Return updated parameters and moving averages\n    return parameters, v, s",
            "code"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Training the Network": [
        [
            "You will start by initializing all the parameters and hyperparameters being used in your network",
            "markdown"
        ],
        [
            "hidden_dim = 64\ninput_dim = emb_matrix['memory'].shape[0]\nlearning_rate = 0.001\nepochs = 10\nparameters = initialise_params(hidden_dim,\n                               input_dim)\nv, s = initialise_mav(hidden_dim,\n                      input_dim,\n                      parameters)",
            "code"
        ],
        [
            "To optimize your deep learning network, you need to calculate a loss based on how well the model is doing on the training data. Loss value implies how poorly or well a model behaves after each iteration of optimization. <br/>\nDefine a function to calculate the loss using ",
            "markdown"
        ],
        [
            "def loss_f(A, Y):\n    # define value of epsilon to prevent zero division error inside a log\n    epsilon = 1e-5\n    # Implement formula for negative log likelihood\n    loss = (- Y * np.log(A + epsilon)\n            - (1 - Y) * np.log(1 - A + epsilon))\n    # Return loss\n    return np.squeeze(loss)",
            "code"
        ],
        [
            "Set up the neural network\u2019s learning experiment with a training loop and start the training process. You will also evaluate the model\u2019s performance on the training dataset to see how well the model is <em>learning</em> and the testing dataset to see how well it is <em>generalizing</em>.\n<blockquote>",
            "markdown"
        ],
        [
            "Skip running this cell if you already have the trained parameters stored in a npy file\n</blockquote>",
            "markdown"
        ],
        [
            "# To store training losses\ntraining_losses = []\n# To store testing losses\ntesting_losses = []\n\n# This is a training loop.\n# Run the learning experiment for a defined number of epochs (iterations).\nfor epoch in range(epochs):\n    #################\n    # Training step #\n    #################\n    train_j = []\n    for sample, target in zip(X_train, y_train):\n        # split text sample into words/tokens\n        b = textproc.word_tokeniser(sample)\n\n        # Forward propagation/forward pass:\n        caches = forward_prop(b,\n                              parameters,\n                              input_dim)\n\n        # Backward propagation/backward pass:\n        gradients = backprop(target,\n                             caches,\n                             hidden_dim,\n                             input_dim,\n                             len(b),\n                             parameters)\n\n        # Update the weights and biases for the LSTM and fully connected layer\n        parameters, v, s = update_parameters(parameters,\n                                             gradients,\n                                             v,\n                                             s,\n                                             learning_rate=learning_rate,\n                                             beta1=0.999,\n                                             beta2=0.9)\n\n        # Measure the training error (loss function) between the actual\n        # sentiment (the truth) and the prediction by the model.\n        y_pred = caches['fc_values'][0]['a2'][0][0]\n        loss = loss_f(y_pred, target)\n        # Store training set losses\n        train_j.append(loss)\n\n    ###################\n    # Evaluation step #\n    ###################\n    test_j = []\n    for sample, target in zip(X_test, y_test):\n        # split text sample into words/tokens\n        b = textproc.word_tokeniser(sample)\n\n        # Forward propagation/forward pass:\n        caches = forward_prop(b,\n                              parameters,\n                              input_dim)\n\n        # Measure the testing error (loss function) between the actual\n        # sentiment (the truth) and the prediction by the model.\n        y_pred = caches['fc_values'][0]['a2'][0][0]\n        loss = loss_f(y_pred, target)\n\n        # Store testing set losses\n        test_j.append(loss)\n\n    # Calculate average of training and testing losses for one epoch\n    mean_train_cost = np.mean(train_j)\n    mean_test_cost = np.mean(test_j)\n    training_losses.append(mean_train_cost)\n    testing_losses.append(mean_test_cost)\n    print('Epoch {} finished. \\t  Training Loss : {} \\t  Testing Loss : {}'.\n          format(epoch + 1, mean_train_cost, mean_test_cost))\n\n# save the trained parameters to a npy file\nnp.save('tutorial-nlp-from-scratch/parameters.npy', parameters)",
            "code"
        ],
        [
            "It is a good practice to plot the training and testing losses as the learning curves are often helpful in diagnosing the behavior of a Machine Learning model.",
            "markdown"
        ],
        [
            "fig = plt.figure()\nax = fig.add_subplot(111)\n\n# plot the training loss\nax.plot(range(0, len(training_losses)), training_losses, label='training loss')\n# plot the testing loss\nax.plot(range(0, len(testing_losses)), testing_losses, label='testing loss')\n\n# set the x and y labels\nax.set_xlabel(\"epochs\")\nax.set_ylabel(\"loss\")\n\nplt.legend(title='labels', bbox_to_anchor=(1.0, 1), loc='upper left')\nplt.show()",
            "code"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data": [
        [
            "Once your model is trained, you can use the updated parameters to start making our predictions. You can break each speech into paragraphs of uniform size before passing them to the Deep Learning model and predicting the sentiment of each paragraph",
            "markdown"
        ],
        [
            "# To store predicted sentiments\npredictions = {}\n\n# define the length of a paragraph\npara_len = 100\n\n# Retrieve trained values of the parameters\nif os.path.isfile('tutorial-nlp-from-scratch/parameters.npy'):\n    parameters = np.load('tutorial-nlp-from-scratch/parameters.npy', allow_pickle=True).item()\n\n# This is the prediction loop.\nfor index, text in enumerate(X_pred):\n    # split each speech into paragraphs\n    paras = textproc.text_to_paras(text, para_len)\n    # To store the network outputs\n    preds = []\n\n    for para in paras:\n        # split text sample into words/tokens\n        para_tokens = textproc.word_tokeniser(para)\n        # Forward Propagation\n        caches = forward_prop(para_tokens,\n                              parameters,\n                              input_dim)\n\n        # Retrieve the output of the fully connected layer\n        sent_prob = caches['fc_values'][0]['a2'][0][0]\n        preds.append(sent_prob)\n\n    threshold = 0.5\n    preds = np.array(preds)\n    # Mark all predictions &gt; threshold as positive and &lt; threshold as negative\n    pos_indices = np.where(preds &gt; threshold)  # indices where output &gt; 0.5\n    neg_indices = np.where(preds &lt; threshold)  # indices where output &lt; 0.5\n    # Store predictions and corresponding piece of text\n    predictions[speakers[index]] = {'pos_paras': paras[pos_indices[0]],\n                                    'neg_paras': paras[neg_indices[0]]}",
            "code"
        ],
        [
            "Visualizing the sentiment predictions:",
            "markdown"
        ],
        [
            "x_axis = []\ndata = {'positive sentiment': [], 'negative sentiment': []}\nfor speaker in predictions:\n    # The speakers will be used to label the x-axis in our plot\n    x_axis.append(speaker)\n    # number of paras with positive sentiment\n    no_pos_paras = len(predictions[speaker]['pos_paras'])\n    # number of paras with negative sentiment\n    no_neg_paras = len(predictions[speaker]['neg_paras'])\n    # Obtain percentage of paragraphs with positive predicted sentiment\n    pos_perc = no_pos_paras / (no_pos_paras + no_neg_paras)\n    # Store positive and negative percentages\n    data['positive sentiment'].append(pos_perc*100)\n    data['negative sentiment'].append(100*(1-pos_perc))\n\nindex = pd.Index(x_axis, name='speaker')\ndf = pd.DataFrame(data, index=index)\nax = df.plot(kind='bar', stacked=True)\nax.set_ylabel('percentage')\nax.legend(title='labels', bbox_to_anchor=(1, 1), loc='upper left')\nplt.show()",
            "code"
        ],
        [
            "In the plot above, you\u2019re shown what percentages of each speech are expected to carry a positive and negative  sentiment. Since this implementation prioritized simplicity and clarity over performance, we cannot expect these results to be very accurate. Moreover, while making the sentiment predictions for one paragraph we did not use the neighboring paragraphs for context which would have led to more accurate predictions. We encourage the reader to play around with the model and make some tweaks suggested in Next Steps and observe how the model performance changes.",
            "markdown"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->Looking at our Neural Network from an ethical perspective": [
        [
            "It\u2019s crucial to understand that accurately identifying a text\u2019s sentiment is not easy primarily because of the complex ways in which humans express sentiment, using irony, sarcasm, humor, or, in social media, abbreviation. Moreover neatly placing text into two categories: \u2018positive\u2019 and \u2018negative\u2019 can be problematic because it is being done without any context. Words or abbreviations can convey very different sentiments depending on age and location, none of which we took into account while building our model.",
            "markdown"
        ],
        [
            "Along with data, there are also growing concerns that data processing algorithms are influencing policy and daily lives in ways that are not transparent and introduce biases. Certain biases such as the  are essential to help a Machine Learning model generalize better, for example the LSTM we built earlier is biased towards preserving contextual information over long sequences which makes it so suitable for processing sequential data. The problem arises when  creep into algorithmic predictions. Optimizing Machine algorithms via methods like  can then further amplify these biases by learning every bit of information in the data.",
            "markdown"
        ],
        [
            "There are also cases where bias is only in the output and not the inputs (data, algorithm). For example, in sentiment analysis . End users of sentiment analysis should be aware that its small gender biases can affect the conclusions drawn from it and apply correction factors when necessary. Hence, it is important that demands for algorithmic accountability should include the ability to test the outputs of a system, including the ability to drill down into different user groups by gender, ethnicity and other characteristics, to identify, and hopefully suggest corrections for, system output biases.\n<!-- #endregion -->",
            "markdown"
        ]
    ],
    "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->Next Steps": [
        [
            "You have learned how to build and train a simple Long Short Term Memory network from scratch using just NumPy to perform sentiment analysis.",
            "markdown"
        ],
        [
            "To further enhance and optimize your neural network model, you can consider one of a mixture of the following:",
            "markdown"
        ],
        [
            "Alter the architecture by introducing multiple LSTM layers to make the network deeper.",
            "markdown"
        ],
        [
            "Use a higher epoch size to train longer and add more regularization techniques, such as early stopping, to prevent overfitting.",
            "markdown"
        ],
        [
            "Introduce a validation set for an unbiased evaluation of the model fit.",
            "markdown"
        ],
        [
            "Apply batch normalization for faster and more stable training.",
            "markdown"
        ],
        [
            "Tune other parameters, such as the learning rate and hidden layer size.",
            "markdown"
        ],
        [
            "Initialize weights using  to prevent vanishing/exploding gradients instead of initializing them randomly.",
            "markdown"
        ],
        [
            "Replace LSTM with a  to use both left and right context for predicting sentiment.",
            "markdown"
        ],
        [
            "Nowadays, LSTMs have been replaced by the ( which uses  to tackle all the problems that plague an LSTM such as as lack of , lack of  and a long gradient chain for lengthy sequences",
            "markdown"
        ],
        [
            "Building a neural network from scratch with NumPy is a great way to learn more about NumPy and about deep learning. However, for real-world applications you should use specialized frameworks \u2014 such as PyTorch, JAX, TensorFlow or MXNet \u2014 that provide NumPy-like APIs, have built-in automatic differentiation and GPU support, and are designed for high-performance numerical computing and machine learning.",
            "markdown"
        ],
        [
            "Finally, to know more about how ethics come into play when developing a machine learning model, you can refer to the following resources :",
            "markdown"
        ],
        [
            "Data ethics resources by the Turing Institute. https://www.turing.ac.uk/research/data-ethics",
            "markdown"
        ],
        [
            "Considering how artificial intelligence shifts power, an  and  by Pratyusha Kalluri",
            "markdown"
        ],
        [
            "More ethics resources on  by Rachel Thomas and the",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Modern Pandas": [
        [
            "This is part 1 in my series on writing modern idiomatic pandas.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Modern Pandas->Effective Pandas->Introduction": [
        [
            "This series is about how to make effective use of , a data analysis library for the Python programming language.\nIt\u2019s targeted at an intermediate level: people who have some experience with pandas, but are looking to improve.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Modern Pandas->Effective Pandas->Prior Art": [
        [
            "There are many great resources for learning pandas; this is not one of them.\nFor beginners, I typically recommend  , especially if they\u2019re familiar with SQL. Of course, there\u2019s the pandas  itself. I gave  at PyData Seattle targeted as an introduction if you prefer video form. Wes McKinney\u2019s  is still the goto book (and is also a really good introduction to NumPy as well). Jake VanderPlas\u2019s , in early release, is great too.\nKevin Markham has a  for beginners learning pandas.",
            "markdown"
        ],
        [
            "With all those resources (and many more that I\u2019ve slighted through omission), why write another? Surely the law of diminishing returns is kicking in by now.\nStill, I thought there was room for a guide that is up to date (as of March 2016) and emphasizes idiomatic pandas code (code that is <em>pandorable</em>).\nThis series probably won\u2019t be appropriate for people completely new to python\nor NumPy and pandas.\nBy luck, this first post happened to cover topics that are relatively introductory,\nso read some of the linked material and come back, or  if you\nhave questions.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Modern Pandas->Effective Pandas->Get the Data": [
        [
            "We\u2019ll be working with  from the BTS (R users can install Hadley\u2019s  dataset for similar data.",
            "markdown"
        ],
        [
            "import os\nimport zipfile\n\nimport requests\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep",
            "code"
        ],
        [
            "import requests\n\nheaders = {\n    'Referer': 'https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&amp;DB_Short_Name=On-Time',\n    'Origin': 'https://www.transtats.bts.gov',\n    'Content-Type': 'application/x-www-form-urlencoded',\n}\n\nparams = (\n    ('Table_ID', '236'),\n    ('Has_Group', '3'),\n    ('Is_Zipped', '0'),\n)\n\nwith open('modern-1-url.txt', encoding='utf-8') as f:\n    data = f.read().strip()\n\nos.makedirs('data', exist_ok=True)\ndest = \"data/flights.csv.zip\"\n\nif not os.path.exists(dest):\n    r = requests.post('https://www.transtats.bts.gov/DownLoad_Table.asp',\n                      headers=headers, params=params, data=data, stream=True)\n\n    with open(\"data/flights.csv.zip\", 'wb') as f:\n        for chunk in r.iter_content(chunk_size=102400): \n            if chunk:\n                f.write(chunk)",
            "code"
        ],
        [
            "That download returned a ZIP file.\nThere\u2019s an open  for automatically decompressing ZIP archives with a single CSV,\nbut for now we have to extract it ourselves and then read it in.",
            "markdown"
        ],
        [
            "zf = zipfile.ZipFile(\"data/flights.csv.zip\")\nfp = zf.extract(zf.filelist[0].filename, path='data/')\ndf = pd.read_csv(fp, parse_dates=[\"FL_DATE\"]).rename(columns=str.lower)\n\ndf.info()",
            "code"
        ],
        [
            "&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 450017 entries, 0 to 450016\nData columns (total 33 columns):\nfl_date                  450017 non-null datetime64[ns]\nunique_carrier           450017 non-null object\nairline_id               450017 non-null int64\ntail_num                 449378 non-null object\nfl_num                   450017 non-null int64\norigin_airport_id        450017 non-null int64\norigin_airport_seq_id    450017 non-null int64\norigin_city_market_id    450017 non-null int64\norigin                   450017 non-null object\norigin_city_name         450017 non-null object\ndest_airport_id          450017 non-null int64\ndest_airport_seq_id      450017 non-null int64\ndest_city_market_id      450017 non-null int64\ndest                     450017 non-null object\ndest_city_name           450017 non-null object\ncrs_dep_time             450017 non-null int64\ndep_time                 441476 non-null float64\ndep_delay                441476 non-null float64\ntaxi_out                 441244 non-null float64\nwheels_off               441244 non-null float64\nwheels_on                440746 non-null float64\ntaxi_in                  440746 non-null float64\ncrs_arr_time             450017 non-null int64\narr_time                 440746 non-null float64\narr_delay                439645 non-null float64\ncancelled                450017 non-null float64\ncancellation_code        8886 non-null object\ncarrier_delay            97699 non-null float64\nweather_delay            97699 non-null float64\nnas_delay                97699 non-null float64\nsecurity_delay           97699 non-null float64\nlate_aircraft_delay      97699 non-null float64\nunnamed: 32              0 non-null float64\ndtypes: datetime64[ns](1), float64(15), int64(10), object(7)\nmemory usage: 113.3+ MB",
            "code"
        ]
    ],
    "pandas_toms_blog->Modern Pandas->Effective Pandas->Indexing": [
        [
            "Or, <em>explicit is better than implicit</em>.\nBy my count, 7 of the top-15 voted pandas questions on  are about indexing. This seems as good a place as any to start.",
            "markdown"
        ],
        [
            "By indexing, we mean the selection of subsets of a DataFrame or Series.\nDataFrames (and to a lesser extent, Series) provide a difficult set of challenges:Like lists, you can index by location.Like dictionaries, you can index by label.Like NumPy arrays, you can index by boolean masks.Any of these indexers could be scalar indexes, or they could be arrays, or they could be slices.Any of these should work on the index (row labels) or columns of a DataFrame.And any of these should work on hierarchical indexes.",
            "markdown"
        ],
        [
            "The complexity of pandas\u2019 indexing is a microcosm for the complexity of the pandas API in general.\nThere\u2019s a reason for the complexity (well, most of it), but that\u2019s not <em>much</em> consolation while you\u2019re learning.\nStill, all of these ways of indexing really are useful enough to justify their inclusion in the library.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing": [
        [
            "Or, <em>explicit is better than implicit</em>.",
            "markdown"
        ],
        [
            "By my count, 7 of the top-15 voted pandas questions on  are about slicing. This seems as good a place as any to start.",
            "markdown"
        ],
        [
            "Brief history digression: For years the preferred method for row and/or column selection was .ix.",
            "markdown"
        ],
        [
            "df.ix[10:15, ['fl_date', 'tail_num']]",
            "code"
        ],
        [
            "/Users/taugspurger/Envs/blog/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n  \"\"\"Entry point for launching an IPython kernel.",
            "code"
        ],
        [
            "As you can see, this method is now deprecated. Why\u2019s that? This simple little operation hides some complexity. What if, rather than our default range(n) index, we had an integer index like",
            "markdown"
        ],
        [
            "# filter the warning for now on\nimport warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)",
            "code"
        ],
        [
            "first = df.groupby('airline_id')[['fl_date', 'unique_carrier']].first()\nfirst.head()",
            "code"
        ],
        [
            "Can you predict ahead of time what our slice from above will give when passed to .ix?",
            "markdown"
        ],
        [
            "first.ix[10:15, ['fl_date', 'tail_num']]",
            "code"
        ],
        [
            "Surprise, an empty DataFrame! Which in data analysis is rarely a good thing. What happened?",
            "markdown"
        ],
        [
            "We had an integer index, so the call to .ix used its label-based mode. It was looking for integer <em>labels</em> between 10:15 (inclusive). It didn\u2019t find any. Since we sliced a range it returned an empty DataFrame, rather than raising a KeyError.",
            "markdown"
        ],
        [
            "By way of contrast, suppose we had a string index, rather than integers.",
            "markdown"
        ],
        [
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "code"
        ],
        [
            "And it works again! Now that we had a string index, .ix used its positional-mode. It looked for <em>rows</em> 10-15 (exclusive on the right).",
            "markdown"
        ],
        [
            "But you can\u2019t reliably predict what the outcome of the slice will be ahead of time. It\u2019s on the <em>reader</em> of the code (probably your future self) to know the dtypes so you can reckon whether .ix will use label indexing (returning the empty DataFrame) or positional indexing (like the last example).\nIn general, methods whose behavior depends on the data, like .ix dispatching to label-based indexing on integer Indexes but location-based indexing on non-integer, are hard to use correctly. We\u2019ve been trying to stamp them out in pandas.",
            "markdown"
        ],
        [
            "Since pandas 0.12, these tasks have been cleanly separated into two methods:.loc for label-based indexing.iloc for positional indexing",
            "markdown"
        ],
        [
            "first.loc[['AA', 'AS', 'DL'], ['fl_date', 'tail_num']]",
            "code"
        ],
        [
            "first.iloc[[0, 1, 3], [0, 1]]",
            "code"
        ],
        [
            ".ix is deprecated, but will hang around for a little while.\nBut if you\u2019ve been using .ix out of habit, or if you didn\u2019t know any better, maybe give .loc and .iloc a shot. I\u2019d recommend carefully updating your code to decide if you\u2019ve been using positional or label indexing, and choose the appropriate indexer. For the intrepid reader, Joris Van den Bossche (a core pandas dev)  of the pandas __getitem__ API.\nA later post in this series will go into more detail on using Indexes effectively;\nthey are useful objects in their own right, but for now we\u2019ll move on to a closely related topic.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Modern Pandas->Effective Pandas->SettingWithCopy": [
        [
            "Pandas used to get <em>a lot</em> of questions about assignments seemingly not working. We\u2019ll take  question as a representative question.",
            "markdown"
        ],
        [
            "f = pd.DataFrame({'a':[1,2,3,4,5], 'b':[10,20,30,40,50]})\nf",
            "code"
        ],
        [
            "The user wanted to take the rows of b where a was 3 or less, and set them equal to b / 10\nWe\u2019ll use boolean indexing to select those rows f['a'] &lt;= 3,",
            "markdown"
        ],
        [
            "# ignore the context manager for now\nwith pd.option_context('mode.chained_assignment', None):\n    f[f['a'] &lt;= 3]['b'] = f[f['a'] &lt;= 3 ]['b'] / 10\nf",
            "code"
        ],
        [
            "And nothing happened. Well, something did happen, but nobody witnessed it. If an object without any references is modified, does it make a sound?",
            "markdown"
        ],
        [
            "The warning I silenced above with the context manager links to  that\u2019s quite helpful. I\u2019ll summarize the high points here.",
            "markdown"
        ],
        [
            "The \u201cfailure\u201d to update f comes down to what\u2019s called <em>chained indexing</em>, a practice to be avoided.\nThe \u201cchained\u201d comes from indexing multiple times, one after another, rather than one single indexing operation.\nAbove we had two operations on the left-hand side, one __getitem__ and one __setitem__ (in python, the square brackets are syntactic sugar for __getitem__ or __setitem__ if it\u2019s for assignment). So f[f['a'] &lt;= 3]['b'] becomesgetitem: f[f['a'] &lt;= 3]setitem: _['b'] = ... # using _ to represent the result of 1.",
            "markdown"
        ],
        [
            "In general, pandas can\u2019t guarantee whether that first __getitem__ returns a view or a copy of the underlying data.\nThe changes <em>will</em> be made to the thing I called _ above, the result of the __getitem__ in 1.\nBut we don\u2019t know that _ shares the same memory as our original f.\nAnd so we can\u2019t be sure that whatever changes are being made to _ will be reflected in f.",
            "markdown"
        ],
        [
            "Done properly, you would write",
            "markdown"
        ],
        [
            "f.loc[f['a'] &lt;= 3, 'b'] = f.loc[f['a'] &lt;= 3, 'b'] / 10\nf",
            "code"
        ],
        [
            "Now this is all in a single call to __setitem__ and pandas can ensure that the assignment happens properly.",
            "markdown"
        ],
        [
            "The rough rule is any time you see back-to-back square brackets, ][, you\u2019re in asking for trouble. Replace that with a .loc[..., ...] and you\u2019ll be set.",
            "markdown"
        ],
        [
            "The other bit of advice is that a SettingWithCopy warning is raised when the <em>assignment</em> is made.\nThe potential copy could be made earlier in your code.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing": [
        [
            "MultiIndexes might just be my favorite feature of pandas.\nThey let you represent higher-dimensional datasets in a familiar two-dimensional table, which my brain can sometimes handle.\nEach additional level of the MultiIndex represents another dimension.\nThe cost of this is somewhat harder label indexing.",
            "markdown"
        ],
        [
            "My very first bug report to pandas, back in ,\nwas about indexing into a MultiIndex.\nI bring it up now because I genuinely couldn\u2019t tell whether the result I got was a bug or not.\nAlso, from that bug report<blockquote>",
            "markdown"
        ],
        [
            "Sorry if this isn\u2019t actually a bug. Still very new to python. Thanks!</blockquote>",
            "markdown"
        ],
        [
            "Adorable.",
            "markdown"
        ],
        [
            "That operation was made much easier by  addition in 2014, which lets you slice arbitrary levels of a MultiIndex..\nLet\u2019s make a MultiIndexed DataFrame to work with.",
            "markdown"
        ],
        [
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "code"
        ],
        [
            "And just to clear up some terminology, the <em>levels</em> of a MultiIndex are the\nformer column names (unique_carrier, origin&amp;mldr;).\nThe labels are the actual values in a level, ('AA', 'ABQ', &amp;mldr;).\nLevels can be referred to by name or position, with 0 being the outermost level.",
            "markdown"
        ],
        [
            "Slicing the outermost index level is pretty easy, we just use our regular .loc[row_indexer, column_indexer]. We\u2019ll select the columns dep_time and dep_delay where the carrier was American Airlines, Delta, or US Airways.",
            "markdown"
        ],
        [
            "hdf.loc[['AA', 'DL', 'US'], ['dep_time', 'dep_delay']]",
            "code"
        ],
        [
            "142945 rows \u00d7 2 columns",
            "markdown"
        ],
        [
            "So far, so good. What if you wanted to select the rows whose origin was Chicago O\u2019Hare (ORD) or Des Moines International Airport (DSM).\nWell, .loc wants [row_indexer, column_indexer] so let\u2019s wrap the two elements of our row indexer (the list of carriers and the list of origins) in a tuple to make it a single unit:",
            "markdown"
        ],
        [
            "hdf.loc[(['AA', 'DL', 'US'], ['ORD', 'DSM']), ['dep_time', 'dep_delay']]",
            "code"
        ],
        [
            "5582 rows \u00d7 2 columns",
            "markdown"
        ],
        [
            "Now try to do any flight from ORD or DSM, not just from those carriers.\nThis used to be a pain.\nYou might have to turn to the .xs method, or pass in df.index.get_level_values(0) and zip that up with the indexers your wanted, or maybe reset the index and do a boolean mask, and set the index again&amp;mldr; ugh.",
            "markdown"
        ],
        [
            "But now, you can use an IndexSlice.",
            "markdown"
        ],
        [
            "hdf.loc[pd.IndexSlice[:, ['ORD', 'DSM']], ['dep_time', 'dep_delay']]",
            "code"
        ],
        [
            "19466 rows \u00d7 2 columns",
            "markdown"
        ],
        [
            "The : says include every label in this level.\nThe IndexSlice object is just sugar for the actual python slice object needed to remove slice each level.",
            "markdown"
        ],
        [
            "pd.IndexSlice[:, ['ORD', 'DSM']]",
            "code"
        ],
        [
            "(slice(None, None, None), ['ORD', 'DSM'])",
            "code"
        ],
        [
            "We\u2019ll talk more about working with Indexes (including MultiIndexes) in a later post. I have an unproven thesis that they\u2019re underused because IndexSlice is underused, causing people to think they\u2019re more unwieldy than they actually are. But let\u2019s close out part one.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Modern Pandas->Effective Pandas->WrapUp": [
        [
            "This first post covered Indexing, a topic that\u2019s central to pandas.\nThe power provided by the DataFrame comes with some unavoidable complexities.\nBest practices (using .loc and .iloc) will spare you many a headache.\nWe then toured a couple of commonly misunderstood sub-topics, setting with copy and Hierarchical Indexing.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Method Chaining": [
        [
            "This is part 2 in my series on writing modern idiomatic pandas.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Method Chaining->Method Chaining": [
        [
            "Method chaining, where you call methods on an object one after another, is in vogue at the moment.\nIt\u2019s always been a style of programming that\u2019s been possible with pandas,\nand over the past several releases, we\u2019ve added methods that enable even more chaining. (0.16.0): For adding new columns to a DataFrame in a chain (inspired by dplyr\u2019s mutate) (0.16.2): For including user-defined methods in method chains. (0.18.0): For altering axis names (in additional to changing the actual labels as before). (0.18): Took the top-level pd.rolling_* and pd.expanding_* functions and made them NDFrame methods with a groupby-like API. (0.18.0) Added a new groupby-like API (0.18.1): In the next release you\u2019ll be able to pass a callable to the indexing methods, to be evaluated within the DataFrame\u2019s context (like .query, but with code instead of strings).",
            "markdown"
        ],
        [
            "My scripts will typically start off with large-ish chain at the start getting things into a manageable state.\nIt\u2019s good to have the bulk of your munging done with right away so you can start to do Science\u2122:",
            "markdown"
        ],
        [
            "Here\u2019s a quick example:",
            "markdown"
        ],
        [
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style='ticks', context='talk')\n\nimport prep",
            "code"
        ],
        [
            "def read(fp):\n    df = (pd.read_csv(fp)\n            .rename(columns=str.lower)\n            .drop('unnamed: 36', axis=1)\n            .pipe(extract_city_name)\n            .pipe(time_to_datetime, ['dep_time', 'arr_time', 'crs_arr_time', 'crs_dep_time'])\n            .assign(fl_date=lambda x: pd.to_datetime(x['fl_date']),\n                    dest=lambda x: pd.Categorical(x['dest']),\n                    origin=lambda x: pd.Categorical(x['origin']),\n                    tail_num=lambda x: pd.Categorical(x['tail_num']),\n                    unique_carrier=lambda x: pd.Categorical(x['unique_carrier']),\n                    cancellation_code=lambda x: pd.Categorical(x['cancellation_code'])))\n    return df\n\ndef extract_city_name(df):\n    '''\n    Chicago, IL -&gt; Chicago for origin_city_name and dest_city_name\n    '''\n    cols = ['origin_city_name', 'dest_city_name']\n    city = df[cols].apply(lambda x: x.str.extract(\"(.*), \\w{2}\", expand=False))\n    df = df.copy()\n    df[['origin_city_name', 'dest_city_name']] = city\n    return df\n\ndef time_to_datetime(df, columns):\n    '''\n    Combine all time items into datetimes.\n\n    2014-01-01,0914 -&gt; 2014-01-01 09:14:00\n    '''\n    df = df.copy()\n    def converter(col):\n        timepart = (col.astype(str)\n                       .str.replace('\\.0$', '')  # NaNs force float dtype\n                       .str.pad(4, fillchar='0'))\n        return pd.to_datetime(df['fl_date'] + ' ' +\n                               timepart.str.slice(0, 2) + ':' +\n                               timepart.str.slice(2, 4),\n                               errors='coerce')\n    df[columns] = df[columns].apply(converter)\n    return df\n\noutput = 'data/flights.h5'\n\nif not os.path.exists(output):\n    df = read(\"data/627361791_T_ONTIME.csv\")\n    df.to_hdf(output, 'flights', format='table')\nelse:\n    df = pd.read_hdf(output, 'flights', format='table')\ndf.info()",
            "code"
        ],
        [
            "&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 450017 entries, 0 to 450016\nData columns (total 33 columns):\nfl_date                  450017 non-null datetime64[ns]\nunique_carrier           450017 non-null category\nairline_id               450017 non-null int64\ntail_num                 449378 non-null category\nfl_num                   450017 non-null int64\norigin_airport_id        450017 non-null int64\norigin_airport_seq_id    450017 non-null int64\norigin_city_market_id    450017 non-null int64\norigin                   450017 non-null category\norigin_city_name         450017 non-null object\ndest_airport_id          450017 non-null int64\ndest_airport_seq_id      450017 non-null int64\ndest_city_market_id      450017 non-null int64\ndest                     450017 non-null category\ndest_city_name           450017 non-null object\ncrs_dep_time             450017 non-null datetime64[ns]\ndep_time                 441445 non-null datetime64[ns]\ndep_delay                441476 non-null float64\ntaxi_out                 441244 non-null float64\nwheels_off               441244 non-null float64\nwheels_on                440746 non-null float64\ntaxi_in                  440746 non-null float64\ncrs_arr_time             450017 non-null datetime64[ns]\narr_time                 440555 non-null datetime64[ns]\narr_delay                439645 non-null float64\ncancelled                450017 non-null float64\ncancellation_code        8886 non-null category\ncarrier_delay            97699 non-null float64\nweather_delay            97699 non-null float64\nnas_delay                97699 non-null float64\nsecurity_delay           97699 non-null float64\nlate_aircraft_delay      97699 non-null float64\nunnamed: 32              0 non-null float64\ndtypes: category(5), datetime64[ns](5), float64(13), int64(8), object(2)\nmemory usage: 103.2+ MB",
            "code"
        ],
        [
            "I find method chains readable, though some people don\u2019t.\nBoth the code and the flow of execution are from top to bottom, and the function parameters are always near the function itself, unlike with heavily nested function calls.",
            "markdown"
        ],
        [
            "My favorite example demonstrating this comes from  (pdf). Compare these two ways of telling the same story:",
            "markdown"
        ],
        [
            "tumble_after(\n    broke(\n        fell_down(\n            fetch(went_up(jack_jill, \"hill\"), \"water\"),\n            jack),\n        \"crown\"),\n    \"jill\"\n)",
            "code"
        ],
        [
            "and",
            "markdown"
        ],
        [
            "jack_jill %&gt;%\n    went_up(\"hill\") %&gt;%\n    fetch(\"water\") %&gt;%\n    fell_down(\"jack\") %&gt;%\n    broke(\"crown\") %&gt;%\n    tumble_after(\"jill\")",
            "code"
        ],
        [
            "Even if you weren\u2019t aware that in R %&gt;% (pronounced <em>pipe</em>) calls the function on the right with the thing on the left as an argument, you can still make out what\u2019s going on. Compare that with the first style, where you need to unravel the code to figure out the order of execution and which arguments are being passed where.",
            "markdown"
        ],
        [
            "Admittedly, you probably wouldn\u2019t write the first one.\nIt\u2019d be something like",
            "markdown"
        ],
        [
            "on_hill = went_up(jack_jill, 'hill')\nwith_water = fetch(on_hill, 'water')\nfallen = fell_down(with_water, 'jack')\nbroken = broke(fallen, 'jack')\nafter = tmple_after(broken, 'jill')",
            "code"
        ],
        [
            "I don\u2019t like this version because I have to spend time coming up with appropriate names for variables.\nThat\u2019s bothersome when we don\u2019t <em>really</em> care about the on_hill variable. We\u2019re just passing it into the next step.",
            "markdown"
        ],
        [
            "A fourth way of writing the same story may be available. Suppose you owned a JackAndJill object, and could define the methods on it. Then you\u2019d have something like R\u2019s %&gt;% example.",
            "markdown"
        ],
        [
            "jack_jill = JackAndJill()\n(jack_jill.went_up('hill')\n    .fetch('water')\n    .fell_down('jack')\n    .broke('crown')\n    .tumble_after('jill')\n)",
            "code"
        ],
        [
            "But the problem is you don\u2019t own the ndarray or DataFrame or , and the exact method you want may not exist.\nMonekypatching on your own methods is fragile.\nIt\u2019s not easy to correctly subclass pandas\u2019 DataFrame to extend it with your own methods.\nComposition, where you create a class that holds onto a DataFrame internally, may be fine for your own code, but it won\u2019t interact well with the rest of the ecosystem so your code will be littered with lines extracting and repacking the underlying DataFrame.",
            "markdown"
        ],
        [
            "Perhaps you could submit a pull request to pandas implementing your method.\nBut then you\u2019d need to convince the maintainers that it\u2019s broadly useful enough to merit its inclusion (and worth their time to maintain it). And DataFrame has something like 250+ methods, so we\u2019re reluctant to add more.",
            "markdown"
        ],
        [
            "Enter . All the benefits of having your specific function as a method on the DataFrame, without us having to maintain it, and without it overloading the already large pandas API. A win for everyone.",
            "markdown"
        ],
        [
            "jack_jill = pd.DataFrame()\n(jack_jill.pipe(went_up, 'hill')\n    .pipe(fetch, 'water')\n    .pipe(fell_down, 'jack')\n    .pipe(broke, 'crown')\n    .pipe(tumble_after, 'jill')\n)",
            "code"
        ],
        [
            "This really is just right-to-left function execution. The first argument to pipe, a callable, is called with the DataFrame on the left as its first argument, and any additional arguments you specify.",
            "markdown"
        ],
        [
            "I hope the analogy to data analysis code is clear.\nCode is read more often than it is written.\nWhen you or your coworkers or research partners have to go back in two months to update your script, having the story of raw data to results be told as clearly as possible will save you time.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Method Chaining->Method Chaining->Costs": [
        [
            "One drawback to excessively long chains is that debugging can be harder.\nIf something looks wrong at the end, you don\u2019t have intermediate values to inspect. There\u2019s a close parallel here to python\u2019s generators. Generators are great for keeping memory consumption down, but they can be hard to debug since values are consumed.",
            "markdown"
        ],
        [
            "For my typical exploratory workflow, this isn\u2019t really a big problem. I\u2019m working with a single dataset that isn\u2019t being updated, and the path from raw data to usuable data isn\u2019t so large that I can\u2019t drop an import pdb; pdb.set_trace() in the middle of my code to poke around.",
            "markdown"
        ],
        [
            "For large workflows, you\u2019ll probably want to move away from pandas to something more structured, like  or .",
            "markdown"
        ],
        [
            "When writing medium sized  jobs in python that will be run repeatedly, I\u2019ll use decorators to inspect and log properties about the DataFrames at each step of the process.",
            "markdown"
        ],
        [
            "from functools import wraps\nimport logging\n\ndef log_shape(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        logging.info(\"%s,%s\" % (func.__name__, result.shape))\n        return result\n    return wrapper\n\ndef log_dtypes(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        logging.info(\"%s,%s\" % (func.__name__, result.dtypes))\n        return result\n    return wrapper\n\n\n@log_shape\n@log_dtypes\ndef load(fp):\n    df = pd.read_csv(fp, index_col=0, parse_dates=True)\n\n@log_shape\n@log_dtypes\ndef update_events(df, new_events):\n    df.loc[new_events.index, 'foo'] = new_events\n    return df",
            "code"
        ],
        [
            "This plays nicely with , a little library I wrote to validate data as it flows through the pipeline (it essentialy turns those logging statements into excpetions if something looks wrong).",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Method Chaining->Method Chaining->Inplace?": [
        [
            "Most pandas methods have an inplace keyword that\u2019s False by default.\nIn general, you shouldn\u2019t do inplace operations.",
            "markdown"
        ],
        [
            "First, if you like method chains then you simply can\u2019t use inplace since the return value is None, terminating the chain.",
            "markdown"
        ],
        [
            "Second, I suspect people have a mental model of inplace operations happening, you know, inplace. That is, extra memory doesn\u2019t need to be allocated for the result. .\nQuoting Jeff Reback from that answer<blockquote>",
            "markdown"
        ],
        [
            "Their is <strong>no guarantee</strong> that an inplace operation is actually faster. Often they are actually the same operation that works on a copy, but the top-level reference is reassigned.</blockquote>",
            "markdown"
        ],
        [
            "That is, the pandas code might look something like this",
            "markdown"
        ],
        [
            "def dataframe_method(self, inplace=False):\n    data = self.copy()  # regardless of inplace\n    result = ...\n    if inplace:\n        self._update_inplace(data)\n    else:\n        return result",
            "code"
        ],
        [
            "There\u2019s a lot of defensive copying in pandas.\nPart of this comes down to pandas being built on top of NumPy, and not having full control over how memory is handled and shared.\nWe saw it above when we defined our own functions extract_city_name and time_to_datetime.\nWithout the copy, adding the columns would modify the input DataFrame, which just isn\u2019t polite.",
            "markdown"
        ],
        [
            "Finally, inplace operations don\u2019t make sense in projects like  or , where you\u2019re manipulating expressions or building up a DAG of tasks to be executed, rather than manipulating the data directly.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Method Chaining->Method Chaining->Application": [
        [
            "I feel like we haven\u2019t done much coding, mostly just me shouting from the top of a soapbox (sorry about that).\nLet\u2019s do some exploratory analysis.",
            "markdown"
        ],
        [
            "What does the daily flight pattern look like?",
            "markdown"
        ],
        [
            "(df.dropna(subset=['dep_time', 'unique_carrier'])\n   .loc[df['unique_carrier']\n       .isin(df['unique_carrier'].value_counts().index[:5])]\n   .set_index('dep_time')\n   # TimeGrouper to resample &amp; groupby at once\n   .groupby(['unique_carrier', pd.TimeGrouper(\"H\")])\n   .fl_num.count()\n   .unstack(0)\n   .fillna(0)\n   .rolling(24)\n   .sum()\n   .rename_axis(\"Flights per Day\", axis=1)\n   .plot()\n)\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_2_method_chaining_8_0.png\"/>",
            "markdown"
        ],
        [
            "import statsmodels.api as sm",
            "code"
        ],
        [
            "/Users/taugspurger/miniconda3/envs/modern-pandas/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n  from pandas.core import datetools",
            "code"
        ],
        [
            "Does a plane with multiple flights on the same day get backed up, causing later flights to be delayed more?",
            "markdown"
        ],
        [
            "%config InlineBackend.figure_format = 'png'\nflights = (df[['fl_date', 'tail_num', 'dep_time', 'dep_delay']]\n           .dropna()\n           .sort_values('dep_time')\n           .loc[lambda x: x.dep_delay &lt; 500]\n           .assign(turn = lambda x:\n                x.groupby(['fl_date', 'tail_num'])\n                 .dep_time\n                 .transform('rank').astype(int)))\n\nfig, ax = plt.subplots(figsize=(15, 5))\nsns.boxplot(x='turn', y='dep_delay', data=flights, ax=ax)\nax.set_ylim(-50, 50)\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_2_method_chaining_11_0.png\"/>",
            "markdown"
        ],
        [
            "Doesn\u2019t really look like it. Maybe other planes are swapped in when one gets delayed,\nbut we don\u2019t have data on <em>scheduled</em> flights per plane.",
            "markdown"
        ],
        [
            "Do flights later in the day have longer delays?",
            "markdown"
        ],
        [
            "plt.figure(figsize=(15, 5))\n(df[['fl_date', 'tail_num', 'dep_time', 'dep_delay']]\n    .dropna()\n    .assign(hour=lambda x: x.dep_time.dt.hour)\n    .query('5 &lt; dep_delay &lt; 600')\n    .pipe((sns.boxplot, 'data'), 'hour', 'dep_delay'))\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_2_method_chaining_13_0.png\"/>",
            "markdown"
        ],
        [
            "There could be something here. I didn\u2019t show it here since I filtered them out,\nbut the vast majority of flights do leave on time.",
            "markdown"
        ],
        [
            "Thanks for reading!\nThis section was a bit more abstract, since we were talking about styles\nof coding rather than how to actually accomplish tasks.\nI\u2019m sometimes guilty of putting too much work into making my data wrangling code look nice and feel correct, at the expense of actually analyzing the data.\nThis isn\u2019t a competition to have the best or cleanest pandas code; pandas is always just a means to the end that is your research or business problem.\nThanks for indulging me.\nNext time we\u2019ll talk about a much more practical topic: performance.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Indexes": [
        [
            "This is part 3 in my series on writing modern idiomatic pandas.",
            "markdown"
        ],
        [
            "Indexes can be a difficult concept to grasp at first.\nI suspect this is partly becuase they\u2019re somewhat peculiar to pandas.\nThese aren\u2019t like the indexes put on relational database tables for performance optimizations.\nRather, they\u2019re more like the row_labels of an R DataFrame, but much more capable.",
            "markdown"
        ],
        [
            "Indexes offermetadata containereasy label-based row selectioneasy label-based alignment in operationslabel-based concatenation",
            "markdown"
        ],
        [
            "To demonstrate these, we\u2019ll first fetch some more data.\nThis will be weather data from sensors at a bunch of airports across the US.\nSee  for the example scraper I based this off of.",
            "markdown"
        ],
        [
            "%matplotlib inline\n\nimport json\nimport glob\nimport datetime\nfrom io import StringIO\n\nimport requests\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style('ticks')\n\n# States are broken into networks. The networks have a list of ids, each representing a station.\n# We will take that list of ids and pass them as query parameters to the URL we built up ealier.\nstates = \"\"\"AK AL AR AZ CA CO CT DE FL GA HI IA ID IL IN KS KY LA MA MD ME\n MI MN MO MS MT NC ND NE NH NJ NM NV NY OH OK OR PA RI SC SD TN TX UT VA VT\n WA WI WV WY\"\"\".split()\n\n# IEM has Iowa AWOS sites in its own labeled network\nnetworks = ['AWOS'] + ['{}_ASOS'.format(state) for state in states]",
            "code"
        ],
        [
            "def get_weather(stations, start=pd.Timestamp('2014-01-01'),\n                end=pd.Timestamp('2014-01-31')):\n    '''\n    Fetch weather data from MESONet between ``start`` and ``stop``.\n    '''\n    url = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n           \"&amp;data=tmpf&amp;data=relh&amp;data=sped&amp;data=mslp&amp;data=p01i&amp;data=vsby&amp;data=gust_mph&amp;data=skyc1&amp;data=skyc2&amp;data=skyc3\"\n           \"&amp;tz=Etc/UTC&amp;format=comma&amp;latlon=no\"\n           \"&amp;{start:year1=%Y&amp;month1=%m&amp;day1=%d}\"\n           \"&amp;{end:year2=%Y&amp;month2=%m&amp;day2=%d}&amp;{stations}\")\n    stations = \"&amp;\".join(\"station=%s\" % s for s in stations)\n    weather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n                           comment=\"#\")\n                 .rename(columns={\"valid\": \"date\"})\n                 .rename(columns=str.strip)\n                 .assign(date=lambda df: pd.to_datetime(df['date']))\n                 .set_index([\"station\", \"date\"])\n                 .sort_index())\n    float_cols = ['tmpf', 'relh', 'sped', 'mslp', 'p01i', 'vsby', \"gust_mph\"]\n    weather[float_cols] = weather[float_cols].apply(pd.to_numeric, errors=\"corce\")\n    return weather",
            "code"
        ],
        [
            "def get_ids(network):\n    url = \"http://mesonet.agron.iastate.edu/geojson/network.php?network={}\"\n    r = requests.get(url.format(network))\n    md = pd.io.json.json_normalize(r.json()['features'])\n    md['network'] = network\n    return md",
            "code"
        ],
        [
            "Talk briefly about the gem of a method that is json_normalize.",
            "markdown"
        ],
        [
            "url = \"http://mesonet.agron.iastate.edu/geojson/network.php?network={}\"\nr = requests.get(url.format(\"AWOS\"))\njs = r.json()",
            "code"
        ],
        [
            "js['features'][:2]",
            "code"
        ],
        [
            "[{'geometry': {'coordinates': [-94.2723694444, 43.0796472222],\n   'type': 'Point'},\n  'id': 'AXA',\n  'properties': {'sid': 'AXA', 'sname': 'ALGONA'},\n  'type': 'Feature'},\n {'geometry': {'coordinates': [-93.569475, 41.6878083333], 'type': 'Point'},\n  'id': 'IKV',\n  'properties': {'sid': 'IKV', 'sname': 'ANKENY'},\n  'type': 'Feature'}]",
            "code"
        ],
        [
            "pd.DataFrame(js['features']).head().to_html()",
            "code"
        ],
        [
            "js['features'][0]\n{\n    'geometry': {\n        'coordinates': [-94.2723694444, 43.0796472222],\n        'type': 'Point'\n    },\n    'id': 'AXA',\n    'properties': {\n        'sid': 'AXA',\n        'sname': 'ALGONA'\n    },\n    'type': 'Feature'\n}",
            "code"
        ],
        [
            "js['features']\n\n[{'geometry': {'coordinates': [-94.2723694444, 43.0796472222],\n  'type': 'Point'},\n  'id': 'AXA',\n  'properties': {'sid': 'AXA', 'sname': 'ALGONA'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.569475, 41.6878083333], 'type': 'Point'},\n  'id': 'IKV',\n  'properties': {'sid': 'IKV', 'sname': 'ANKENY'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.0465277778, 41.4058805556],\n  'type': 'Point'},\n  'id': 'AIO',\n  'properties': {'sid': 'AIO', 'sname': 'ATLANTIC'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-94.9204416667, 41.6993527778],\n  'type': 'Point'},\n  'id': 'ADU',\n  'properties': {'sid': 'ADU', 'sname': 'AUDUBON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.848575, 42.0485694444], 'type': 'Point'},\n  'id': 'BNW',\n  'properties': {'sid': 'BNW', 'sname': 'BOONE MUNI'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-94.7888805556, 42.0443611111],\n  'type': 'Point'},\n  'id': 'CIN',\n  'properties': {'sid': 'CIN', 'sname': 'CARROLL'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-92.8983388889, 40.6831805556],\n  'type': 'Point'},\n  'id': 'TVK',\n  'properties': {'sid': 'TVK', 'sname': 'Centerville'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.3607694444, 41.0184305556],\n  'type': 'Point'},\n  'id': 'CNC',\n  'properties': {'sid': 'CNC', 'sname': 'CHARITON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-92.6132222222, 43.0730055556],\n  'type': 'Point'},\n  'id': 'CCY',\n  'properties': {'sid': 'CCY', 'sname': 'CHARLES CITY'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.553775, 42.7304194444], 'type': 'Point'},\n  'id': 'CKP',\n  'properties': {'sid': 'CKP', 'sname': 'Cherokee'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.0222722222, 40.7241527778],\n  'type': 'Point'},\n  'id': 'ICL',\n  'properties': {'sid': 'ICL', 'sname': 'CLARINDA'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.7592583333, 42.7430416667],\n  'type': 'Point'},\n  'id': 'CAV',\n  'properties': {'sid': 'CAV', 'sname': 'CLARION'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-90.332796, 41.829504], 'type': 'Point'},\n  'id': 'CWI',\n  'properties': {'sid': 'CWI', 'sname': 'CLINTON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.7604083333, 41.2611111111],\n  'type': 'Point'},\n  'id': 'CBF',\n  'properties': {'sid': 'CBF', 'sname': 'COUNCIL BLUFFS'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-94.3607972222, 41.0187888889],\n  'type': 'Point'},\n  'id': 'CSQ',\n  'properties': {'sid': 'CSQ', 'sname': 'CRESTON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.7433138889, 43.2755194444],\n  'type': 'Point'},\n  'id': 'DEH',\n  'properties': {'sid': 'DEH', 'sname': 'DECORAH'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.3799888889, 41.9841944444],\n  'type': 'Point'},\n  'id': 'DNS',\n  'properties': {'sid': 'DNS', 'sname': 'DENISON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.9834111111, 41.0520888889],\n  'type': 'Point'},\n  'id': 'FFL',\n  'properties': {'sid': 'FFL', 'sname': 'FAIRFIELD'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.6236694444, 43.2323166667],\n  'type': 'Point'},\n  'id': 'FXY',\n  'properties': {'sid': 'FXY', 'sname': 'Forest City'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-94.203203, 42.549741], 'type': 'Point'},\n  'id': 'FOD',\n  'properties': {'sid': 'FOD', 'sname': 'FORT DODGE'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.3267166667, 40.6614833333],\n  'type': 'Point'},\n  'id': 'FSW',\n  'properties': {'sid': 'FSW', 'sname': 'FORT MADISON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-92.7331972222, 41.7097305556],\n  'type': 'Point'},\n  'id': 'GGI',\n  'properties': {'sid': 'GGI', 'sname': 'Grinnell'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.3354555556, 41.5834194444],\n  'type': 'Point'},\n  'id': 'HNR',\n  'properties': {'sid': 'HNR', 'sname': 'HARLAN'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.9504, 42.4544277778], 'type': 'Point'},\n  'id': 'IIB',\n  'properties': {'sid': 'IIB', 'sname': 'INDEPENDENCE'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.2650805556, 42.4690972222],\n  'type': 'Point'},\n  'id': 'IFA',\n  'properties': {'sid': 'IFA', 'sname': 'Iowa Falls'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.4273916667, 40.4614611111],\n  'type': 'Point'},\n  'id': 'EOK',\n  'properties': {'sid': 'EOK', 'sname': 'KEOKUK MUNI'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.1113916667, 41.2984472222],\n  'type': 'Point'},\n  'id': 'OXV',\n  'properties': {'sid': 'OXV', 'sname': 'Knoxville'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-96.19225, 42.775375], 'type': 'Point'},\n  'id': 'LRJ',\n  'properties': {'sid': 'LRJ', 'sname': 'LE MARS'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.1604555556, 42.2203611111],\n  'type': 'Point'},\n  'id': 'MXO',\n  'properties': {'sid': 'MXO', 'sname': 'MONTICELLO MUNI'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.5122277778, 40.9452527778],\n  'type': 'Point'},\n  'id': 'MPZ',\n  'properties': {'sid': 'MPZ', 'sname': 'MOUNT PLEASANT'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.140575, 41.3669944444], 'type': 'Point'},\n  'id': 'MUT',\n  'properties': {'sid': 'MUT', 'sname': 'MUSCATINE'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.0190416667, 41.6701111111],\n  'type': 'Point'},\n  'id': 'TNU',\n  'properties': {'sid': 'TNU', 'sname': 'NEWTON MUNI'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.9759888889, 42.6831388889],\n  'type': 'Point'},\n  'id': 'OLZ',\n  'properties': {'sid': 'OLZ', 'sname': 'OELWEIN'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-96.0605861111, 42.9894916667],\n  'type': 'Point'},\n  'id': 'ORC',\n  'properties': {'sid': 'ORC', 'sname': 'Orange City'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.6876138889, 41.0471722222],\n  'type': 'Point'},\n  'id': 'I75',\n  'properties': {'sid': 'I75', 'sname': 'Osceola'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-92.4918666667, 41.227275], 'type': 'Point'},\n  'id': 'OOA',\n  'properties': {'sid': 'OOA', 'sname': 'Oskaloosa'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-92.9431083333, 41.3989138889],\n  'type': 'Point'},\n  'id': 'PEA',\n  'properties': {'sid': 'PEA', 'sname': 'PELLA'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-94.1637083333, 41.8277916667],\n  'type': 'Point'},\n  'id': 'PRO',\n  'properties': {'sid': 'PRO', 'sname': 'Perry'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.2624111111, 41.01065], 'type': 'Point'},\n  'id': 'RDK',\n  'properties': {'sid': 'RDK', 'sname': 'RED OAK'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.8353138889, 43.2081611111],\n  'type': 'Point'},\n  'id': 'SHL',\n  'properties': {'sid': 'SHL', 'sname': 'SHELDON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.4112333333, 40.753275], 'type': 'Point'},\n  'id': 'SDA',\n  'properties': {'sid': 'SDA', 'sname': 'SHENANDOAH MUNI'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.2399194444, 42.5972277778],\n  'type': 'Point'},\n  'id': 'SLB',\n  'properties': {'sid': 'SLB', 'sname': 'Storm Lake'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-92.0248416667, 42.2175777778],\n  'type': 'Point'},\n  'id': 'VTI',\n  'properties': {'sid': 'VTI', 'sname': 'VINTON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.6748111111, 41.2751444444],\n  'type': 'Point'},\n  'id': 'AWG',\n  'properties': {'sid': 'AWG', 'sname': 'WASHINGTON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.8690777778, 42.4392305556],\n  'type': 'Point'},\n  'id': 'EBS',\n  'properties': {'sid': 'EBS', 'sname': 'Webster City'},\n  'type': 'Feature'}]",
            "code"
        ],
        [
            "stations = pd.io.json.json_normalize(js['features']).id\nurl = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n       \"&amp;data=tmpf&amp;data=relh&amp;data=sped&amp;data=mslp&amp;data=p01i&amp;data=vsby&amp;data=gust_mph&amp;data=skyc1&amp;data=skyc2&amp;data=skyc3\"\n       \"&amp;tz=Etc/UTC&amp;format=comma&amp;latlon=no\"\n       \"&amp;{start:year1=%Y&amp;month1=%m&amp;day1=%d}\"\n       \"&amp;{end:year2=%Y&amp;month2=%m&amp;day2=%d}&amp;{stations}\")\nstations = \"&amp;\".join(\"station=%s\" % s for s in stations)\nstart = pd.Timestamp('2014-01-01')\nend=pd.Timestamp('2014-01-31')\n\nweather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n                       comment=\"#\"))",
            "code"
        ],
        [
            "import os\nids = pd.concat([get_ids(network) for network in networks], ignore_index=True)\ngr = ids.groupby('network')\n\nos.makedirs(\"weather\", exist_ok=True)\n\nfor i, (k, v) in enumerate(gr):\n    print(\"{}/{}\".format(i, len(network)), end='\\r')\n    weather = get_weather(v['id'])\n    weather.to_csv(\"weather/{}.csv\".format(k))\n\nweather = pd.concat([\n    pd.read_csv(f, parse_dates='date', index_col=['station', 'date'])\n    for f in glob.glob('weather/*.csv')])\n\nweather.to_hdf(\"weather.h5\", \"weather\")",
            "code"
        ],
        [
            "weather = pd.read_hdf(\"weather.h5\", \"weather\").sort_index()\n\nweather.head()",
            "code"
        ],
        [
            "OK, that was a bit of work. Here\u2019s a plot to reward ourselves.",
            "markdown"
        ],
        [
            "airports = ['DSM', 'ORD', 'JFK', 'PDX']\n\ng = sns.FacetGrid(weather.sort_index().loc[airports].reset_index(),\n                  col='station', hue='station', col_wrap=2, size=4)\ng.map(sns.regplot, 'sped', 'gust_mph')\nplt.savefig('../content/images/indexes_wind_gust_facet.png');",
            "code"
        ],
        [
            "airports = ['DSM', 'ORD', 'JFK', 'PDX']\n\ng = sns.FacetGrid(weather.sort_index().loc[airports].reset_index(),\n                  col='station', hue='station', col_wrap=2, size=4)\ng.map(sns.regplot, 'sped', 'gust_mph')\nplt.savefig('../content/images/indexes_wind_gust_facet.svg', transparent=True);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"Indexes_files/Indexes_18_0.png\"/>",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Indexes->Set Operations": [
        [
            "Indexes are set-like (technically <em>multi</em>sets, since you can have duplicates), so they support most python set operations. Indexes are immutable so you won\u2019t find any of the inplace set operations.\nOne other difference is that since Indexes are also array like, you can\u2019t use some infix operators like - for difference. If you have a numeric index it is unclear whether you intend to perform math operations or set operations.\nYou can use &amp; for intersetion, | for union, and ^ for symmetric difference though, since there\u2019s no ambiguity.",
            "markdown"
        ],
        [
            "For example, lets find the set of airports that we have weather and flight information on. Since weather had a MultiIndex of airport,datetime, we\u2019ll use the levels attribute to get at the airport data, separate from the date data.",
            "markdown"
        ],
        [
            "# Bring in the flights data\n\nflights = pd.read_hdf('flights.h5', 'flights')\n\nweather_locs = weather.index.levels[0]\n# The `categories` attribute of a Categorical is an Index\norigin_locs = flights.origin.cat.categories\ndest_locs = flights.dest.cat.categories\n\nairports = weather_locs &amp; origin_locs &amp; dest_locs\nairports",
            "code"
        ],
        [
            "Index(['ABE', 'ABI', 'ABQ', 'ABR', 'ABY', 'ACT', 'ACV', 'AEX', 'AGS', 'ALB',\n       ...\n       'TUL', 'TUS', 'TVC', 'TWF', 'TXK', 'TYR', 'TYS', 'VLD', 'VPS', 'XNA'],\n      dtype='object', length=267)",
            "code"
        ],
        [
            "print(\"Weather, no flights:\\n\\t\", weather_locs.difference(origin_locs | dest_locs), end='\\n\\n')\n\nprint(\"Flights, no weather:\\n\\t\", (origin_locs | dest_locs).difference(weather_locs), end='\\n\\n')\n\nprint(\"Dropped Stations:\\n\\t\", (origin_locs | dest_locs) ^ weather_locs)",
            "code"
        ],
        [
            "Weather, no flights:\n\t Index(['01M', '04V', '04W', '05U', '06D', '08D', '0A9', '0CO', '0E0', '0F2',\n       ...\n       'Y50', 'Y51', 'Y63', 'Y70', 'YIP', 'YKM', 'YKN', 'YNG', 'ZPH', 'ZZV'],\n      dtype='object', length=1909)\n\nFlights, no weather:\n\t Index(['ADK', 'ADQ', 'ANC', 'BET', 'BKG', 'BQN', 'BRW', 'CDV', 'CLD', 'FAI',\n       'FCA', 'GUM', 'HNL', 'ITO', 'JNU', 'KOA', 'KTN', 'LIH', 'MQT', 'OGG',\n       'OME', 'OTZ', 'PPG', 'PSE', 'PSG', 'SCC', 'SCE', 'SIT', 'SJU', 'STT',\n       'STX', 'WRG', 'YAK', 'YUM'],\n      dtype='object')\n\nDropped Stations:\n\t Index(['01M', '04V', '04W', '05U', '06D', '08D', '0A9', '0CO', '0E0', '0F2',\n       ...\n       'Y63', 'Y70', 'YAK', 'YIP', 'YKM', 'YKN', 'YNG', 'YUM', 'ZPH', 'ZZV'],\n      dtype='object', length=1943)",
            "code"
        ]
    ],
    "pandas_toms_blog->Indexes->Flavors": [
        [
            "Pandas has many subclasses of the regular Index, each tailored to a specific kind of data.\nMost of the time these will be created for you automatically, so you don\u2019t have to worry about which one to choose.Int64IndexRangeIndex (Memory-saving special case of Int64Index)FloatIndexDatetimeIndex: Datetime64[ns] precision dataPeriodIndex: Regularly-spaced, arbitrary precision datetime data.TimedeltaIndex: Timedelta dataCategoricalIndex:",
            "markdown"
        ],
        [
            "Some of these are purely optimizations, others use information about the data to provide additional methods.\nAnd while sometimes you might work with indexes directly (like the set operations above), most of they time you\u2019ll be operating on a Series or DataFrame, which in turn makes use of its Index.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Indexes->Flavors->Row Slicing": [
        [
            "We saw in part one that they\u2019re great for making <em>row</em> subsetting as easy as column subsetting.",
            "markdown"
        ],
        [
            "weather.loc['DSM'].head()",
            "code"
        ],
        [
            "Without indexes we\u2019d probably resort to boolean masks.",
            "markdown"
        ],
        [
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "code"
        ],
        [
            "Slightly less convenient, but still doable.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Indexes->Flavors->Indexes for Easier Arithmetic, Analysis": [
        [
            "It\u2019s nice to have your metadata (labels on each observation) next to you actual values. But if you store them in an array, they\u2019ll get in the way. Say we wanted to translate the farenheit temperature to celcius.",
            "markdown"
        ],
        [
            "# With indecies\ntemp = weather['tmpf']\n\nc = (temp - 32) * 5 / 9\nc.to_frame()",
            "code"
        ],
        [
            "3303647 rows \u00d7 1 columns",
            "markdown"
        ],
        [
            "# without\ntemp2 = weather.reset_index()[['station', 'date', 'tmpf']]\n\ntemp2['tmpf'] = (temp2['tmpf'] - 32) * 5 / 9\ntemp2.head()",
            "code"
        ],
        [
            "Again, not terrible, but not as good.\nAnd, what if you had wanted to keep farenheit around as well, instead of overwriting it like we did?\nThen you\u2019d need to make a copy of everything, including the station and date columns.\nWe don\u2019t have that problem, since indexes are mutable and safely shared between DataFrames / Series.",
            "markdown"
        ],
        [
            "temp.index is c.index",
            "code"
        ],
        [
            "True",
            "code"
        ]
    ],
    "pandas_toms_blog->Indexes->Flavors->Indexes for Alignment": [
        [
            "I\u2019ve saved the best for last.\nAutomatic alignment, or reindexing, is fundamental to pandas.",
            "markdown"
        ],
        [
            "All binary operations (add, multiply, etc&amp;mldr;) between Series/DataFrames first <em>align</em> and then proceed.",
            "markdown"
        ],
        [
            "Let\u2019s suppose we have hourly observations on temperature and windspeed.\nAnd suppose some of the observations were invalid, and not reported (simulated below by sampling from the full dataset). We\u2019ll assume the missing windspeed observations were potentially different from the missing temperature observations.",
            "markdown"
        ],
        [
            "dsm = weather.loc['DSM']\n\nhourly = dsm.resample('H').mean()\n\ntemp = hourly['tmpf'].sample(frac=.5, random_state=1).sort_index()\nsped = hourly['sped'].sample(frac=.5, random_state=2).sort_index()",
            "code"
        ],
        [
            "temp.head().to_frame()",
            "code"
        ],
        [
            "sped.head()",
            "code"
        ],
        [
            "date\n2014-01-01 01:00:00    11.4\n2014-01-01 02:00:00     8.0\n2014-01-01 03:00:00     9.1\n2014-01-01 04:00:00     9.1\n2014-01-01 05:00:00    10.3\nName: sped, dtype: float64",
            "code"
        ],
        [
            "Notice that the two indexes aren\u2019t identical.",
            "markdown"
        ],
        [
            "Suppose that the windspeed : temperature ratio is meaningful.\nWhen we go to compute that, pandas will automatically align the two by index label.",
            "markdown"
        ],
        [
            "sped / temp",
            "code"
        ],
        [
            "date\n2014-01-01 00:00:00         NaN\n2014-01-01 01:00:00         NaN\n2014-01-01 02:00:00    0.731261\n2014-01-01 03:00:00    0.831810\n2014-01-01 04:00:00    0.906375\n                         ...   \n2014-01-30 13:00:00         NaN\n2014-01-30 14:00:00    0.584712\n2014-01-30 17:00:00         NaN\n2014-01-30 21:00:00         NaN\n2014-01-30 23:00:00         NaN\ndtype: float64",
            "code"
        ],
        [
            "This lets you focus on doing the operation, rather than manually aligning things, ensuring that the arrays are the same length and in the same order.\nBy deault, missing values are inserted where the two don\u2019t align.\nYou can use the method version of any binary operation to specify a fill_value",
            "markdown"
        ],
        [
            "sped.div(temp, fill_value=1)",
            "code"
        ],
        [
            "date\n2014-01-01 00:00:00     0.091408\n2014-01-01 01:00:00    11.400000\n2014-01-01 02:00:00     0.731261\n2014-01-01 03:00:00     0.831810\n2014-01-01 04:00:00     0.906375\n                         ...    \n2014-01-30 13:00:00     0.027809\n2014-01-30 14:00:00     0.584712\n2014-01-30 17:00:00     0.023267\n2014-01-30 21:00:00     0.035663\n2014-01-30 23:00:00    13.700000\ndtype: float64",
            "code"
        ],
        [
            "And since I couldn\u2019t find anywhere else to put it, you can control the axis the operation is aligned along as well.",
            "markdown"
        ],
        [
            "hourly.div(sped, axis='index')",
            "code"
        ],
        [
            "720 rows \u00d7 7 columns",
            "markdown"
        ],
        [
            "The non row-labeled version of this is messy.",
            "markdown"
        ],
        [
            "temp2 = temp.reset_index()\nsped2 = sped.reset_index()\n\n# Find rows where the operation is defined\ncommon_dates = pd.Index(temp2.date) &amp; sped2.date\npd.concat([\n    # concat to not lose date information\n    sped2.loc[sped2['date'].isin(common_dates), 'date'],\n    (sped2.loc[sped2.date.isin(common_dates), 'sped'] /\n     temp2.loc[temp2.date.isin(common_dates), 'tmpf'])],\n    axis=1).dropna(how='all')",
            "code"
        ],
        [
            "170 rows \u00d7 2 columns",
            "markdown"
        ],
        [
            "Yeah, I prefer the temp / sped version.",
            "markdown"
        ],
        [
            "Alignment isn\u2019t limited to arithmetic operations, although those are the most obvious and easiest to demonstrate.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Indexes->Merging": [
        [
            "There are two ways of merging DataFrames / Series in pandasRelational Database style with pd.mergeArray style with pd.concat",
            "markdown"
        ],
        [
            "Personally, I think in terms of the concat style.\nI learned pandas before I ever really used SQL, so it comes more naturally to me I suppose.\npd.merge has more flexibilty, though I think <em>most</em> of the time you don\u2019t need this flexibilty.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Indexes->Merging->Concat Version": [
        [
            "pd.concat([temp, sped], axis=1).head()",
            "code"
        ],
        [
            "The axis parameter controls how the data should be stacked, 0 for vertically, 1 for horizontally.\nThe join parameter controls the merge behavior on the shared axis, (the Index for axis=1). By default it\u2019s like a union of the two indexes, or an outer join.",
            "markdown"
        ],
        [
            "pd.concat([temp, sped], axis=1, join='inner')",
            "code"
        ],
        [
            "170 rows \u00d7 2 columns",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Indexes->Merging->Merge Version": [
        [
            "Since we\u2019re joining by index here the merge version is quite similar.\nWe\u2019ll see an example later of a one-to-many join where the two differ.",
            "markdown"
        ],
        [
            "pd.merge(temp.to_frame(), sped.to_frame(), left_index=True, right_index=True).head()",
            "code"
        ],
        [
            "pd.merge(temp.to_frame(), sped.to_frame(), left_index=True, right_index=True,\n         how='outer').head()",
            "code"
        ],
        [
            "Like I said, I typically prefer concat to merge.\nThe exception here is one-to-many type joins. Let\u2019s walk through one of those,\nwhere we join the flight data to the weather data.\nTo focus just on the merge, we\u2019ll aggregate hour weather data to be daily, rather than trying to find the closest recorded weather observation to each departure (you could do that, but it\u2019s not the focus right now). We\u2019ll then join the one (airport, date) record to the many (airport, date, flight) records.",
            "markdown"
        ],
        [
            "Quick tangent, to get the weather data to daily frequency, we\u2019ll need to resample (more on that in the timeseries section). The resample essentially involves breaking the recorded values into daily buckets and computing the aggregation function on each bucket. The only wrinkle is that we have to resample <em>by station</em>, so we\u2019ll use the pd.TimeGrouper helper.",
            "markdown"
        ],
        [
            "idx_cols = ['unique_carrier', 'origin', 'dest', 'tail_num', 'fl_num', 'fl_date']\ndata_cols = ['crs_dep_time', 'dep_delay', 'crs_arr_time', 'arr_delay',\n             'taxi_out', 'taxi_in', 'wheels_off', 'wheels_on', 'distance']\n\ndf = flights.set_index(idx_cols)[data_cols].sort_index()",
            "code"
        ],
        [
            "def mode(x):\n    '''\n    Arbitrarily break ties.\n    '''\n    return x.value_counts().index[0]\n\naggfuncs = {'tmpf': 'mean', 'relh': 'mean',\n            'sped': 'mean', 'mslp': 'mean',\n            'p01i': 'mean', 'vsby': 'mean',\n            'gust_mph': 'mean', 'skyc1': mode,\n            'skyc2': mode, 'skyc3': mode}\n# TimeGrouper works on a DatetimeIndex, so we move `station` to the\n# columns and then groupby it as well.\ndaily = (weather.reset_index(level=\"station\")\n                .groupby([pd.TimeGrouper('1d'), \"station\"])\n                .agg(aggfuncs))\n\ndaily.head()",
            "code"
        ]
    ],
    "pandas_toms_blog->Indexes->Merging->The merge version": [
        [
            "m = pd.merge(flights, daily.reset_index().rename(columns={'date': 'fl_date', 'station': 'origin'}),\n             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n\nm.head()",
            "code"
        ],
        [
            "5 rows \u00d7 40 columns",
            "markdown"
        ],
        [
            "m.sample(n=10000).pipe((sns.jointplot, 'data'), 'sped', 'dep_delay')\nplt.savefig('../content/images/indexes_sped_delay_join.svg', transparent=True)",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"Indexes_files/Indexes_63_0.png\"/>",
            "markdown"
        ],
        [
            "m.groupby('skyc1').dep_delay.agg(['mean', 'count']).sort_values(by='mean')",
            "code"
        ],
        [
            "import statsmodels.api as sm",
            "code"
        ],
        [
            "mod = sm.OLS.from_formula('dep_delay ~ C(skyc1) + distance + tmpf + relh + sped + mslp', data=m)\nres = mod.fit()\nres.summary()",
            "code"
        ],
        [
            "fig, ax = plt.subplots()\nax.scatter(res.fittedvalues, res.resid, color='k', marker='.', alpha=.25)\nax.set(xlabel='Predicted', ylabel='Residual')\nsns.despine()\nplt.savefig('../content/images/indexes_resid_fit.png', transparent=True)",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"Indexes_files/Indexes_67_0.png\"/>",
            "markdown"
        ],
        [
            "weather.head()",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\n\n\ndef read(fp):\n    df = (pd.read_csv(fp)\n            .rename(columns=str.lower)\n            .drop('unnamed: 36', axis=1)\n            .pipe(extract_city_name)\n            .pipe(time_to_datetime, ['dep_time', 'arr_time', 'crs_arr_time', 'crs_dep_time'])\n            .assign(fl_date=lambda x: pd.to_datetime(x['fl_date']),\n                    dest=lambda x: pd.Categorical(x['dest']),\n                    origin=lambda x: pd.Categorical(x['origin']),\n                    tail_num=lambda x: pd.Categorical(x['tail_num']),\n                    unique_carrier=lambda x: pd.Categorical(x['unique_carrier']),\n                    cancellation_code=lambda x: pd.Categorical(x['cancellation_code'])))\n    return df\n\ndef extract_city_name(df):\n    '''\n    Chicago, IL -&gt; Chicago for origin_city_name and dest_city_name\n    '''\n    cols = ['origin_city_name', 'dest_city_name']\n    city = df[cols].apply(lambda x: x.str.extract(\"(.*), \\w{2}\", expand=False))\n    df = df.copy()\n    df[['origin_city_name', 'dest_city_name']] = city\n    return df\n\ndef time_to_datetime(df, columns):\n    '''\n    Combine all time items into datetimes.\n    \n    2014-01-01,0914 -&gt; 2014-01-01 09:14:00\n    '''\n    df = df.copy()\n    def converter(col):\n        timepart = (col.astype(str)\n                       .str.replace('\\.0$', '')  # NaNs force float dtype\n                       .str.pad(4, fillchar='0'))\n        return  pd.to_datetime(df['fl_date'] + ' ' +\n                               timepart.str.slice(0, 2) + ':' +\n                               timepart.str.slice(2, 4),\n                               errors='coerce')\n        return datetime_part\n    df[columns] = df[columns].apply(converter)\n    return df\n\n\nflights = read(\"878167309_T_ONTIME.csv\")",
            "code"
        ],
        [
            "locs = weather.index.levels[0] &amp; flights.origin.unique()",
            "code"
        ],
        [
            "(weather.reset_index(level='station')\n .query('station in @locs')\n .groupby(['station', pd.TimeGrouper('H')])).mean()",
            "code"
        ],
        [
            "191445 rows \u00d7 7 columns",
            "markdown"
        ],
        [
            "df = (flights.copy()[['unique_carrier', 'tail_num', 'origin', 'dep_time']]\n      .query('origin in @locs'))",
            "code"
        ],
        [
            "weather.loc['DSM']",
            "code"
        ],
        [
            "896 rows \u00d7 10 columns",
            "markdown"
        ],
        [
            "df = df",
            "code"
        ],
        [
            "471949 rows \u00d7 36 columns",
            "markdown"
        ],
        [
            "dep.head()",
            "code"
        ],
        [
            "0        2014-01-01 09:14:00\n1        2014-01-01 11:32:00\n2        2014-01-01 11:57:00\n3        2014-01-01 13:07:00\n4        2014-01-01 17:53:00\n                 ...        \n163906   2014-01-11 16:57:00\n163910   2014-01-11 11:04:00\n181062   2014-01-12 17:02:00\n199092   2014-01-13 23:36:00\n239150   2014-01-16 16:46:00\nName: dep_time, dtype: datetime64[ns]",
            "code"
        ],
        [
            "flights.dep_time",
            "code"
        ],
        [
            "0        2014-01-01 09:14:00\n1        2014-01-01 11:32:00\n2        2014-01-01 11:57:00\n3        2014-01-01 13:07:00\n4        2014-01-01 17:53:00\n                 ...        \n471944   2014-01-31 09:05:00\n471945   2014-01-31 09:24:00\n471946   2014-01-31 10:39:00\n471947   2014-01-31 09:28:00\n471948   2014-01-31 11:22:00\nName: dep_time, dtype: datetime64[ns]",
            "code"
        ],
        [
            "flights.dep_time.unique()",
            "code"
        ],
        [
            "array(['2014-01-01T03:14:00.000000000-0600',\n       '2014-01-01T05:32:00.000000000-0600',\n       '2014-01-01T05:57:00.000000000-0600', ...,\n       '2014-01-30T18:44:00.000000000-0600',\n       '2014-01-31T17:16:00.000000000-0600',\n       '2014-01-30T18:47:00.000000000-0600'], dtype='datetime64[ns]')",
            "code"
        ],
        [
            "stations",
            "code"
        ],
        [
            "flights.dep_time.head()",
            "code"
        ],
        [
            "0   2014-01-01 09:14:00\n1   2014-01-01 11:32:00\n2   2014-01-01 11:57:00\n3   2014-01-01 13:07:00\n4   2014-01-01 17:53:00\nName: dep_time, dtype: datetime64[ns]",
            "code"
        ]
    ],
    "pandas_toms_blog->Fast Pandas": [
        [
            "This is part 4 in my series on writing modern idiomatic pandas.",
            "markdown"
        ],
        [
            ", the creator of pandas, is kind of obsessed with performance. From micro-optimizations for element access, to  a fast hash table inside pandas, we all benefit from his and others\u2019 hard work.\nThis post will focus mainly on making efficient use of pandas and NumPy.",
            "markdown"
        ],
        [
            "One thing I\u2019ll explicitly not touch on is storage formats.\nPerformance is just one of many factors that go into choosing a storage format.\nJust know that pandas can talk to , and the format that strikes the right balance between performance, portability, data-types, metadata handling, etc., is an  topic of discussion.",
            "markdown"
        ],
        [
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\nsns.set_style('ticks')\nsns.set_context('talk')\npd.options.display.max_rows = 10",
            "code"
        ]
    ],
    "pandas_toms_blog->Fast Pandas->Constructors": [
        [
            "It\u2019s pretty common to have many similar sources (say a bunch of CSVs) that need to be combined into a single DataFrame. There are two routes to the same end:Initialize one DataFrame and append to thatMake many smaller DataFrames and concatenate at the end",
            "markdown"
        ],
        [
            "For pandas, the second option is faster.\nDataFrame appends are expensive relative to a list append.\nDepending on the values, pandas might have to recast the data to a different type.\nAnd indexes are immutable, so each time you append pandas has to create an entirely new one.",
            "markdown"
        ],
        [
            "In the last section we downloaded a bunch of weather files, one per state, writing each to a separate CSV.\nOne could imagine coming back later to read them in, using the following code.",
            "markdown"
        ],
        [
            "The idiomatic python way",
            "markdown"
        ],
        [
            "files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)",
            "code"
        ],
        [
            "This is pretty standard code, quite similar to building up a list of tuples, say.\nThe only nitpick is that you\u2019d probably use a list-comprehension if you were just making a list.\nBut we don\u2019t have special syntax for DataFrame-comprehensions (if only), so you\u2019d fall back to the \u201cinitialize empty container, append to said container\u201d pattern.",
            "markdown"
        ],
        [
            "But there\u2019s a better, pandorable, way",
            "markdown"
        ],
        [
            "files = glob.glob('weather/*.csv')\nweather_dfs = [pd.read_csv(fp, names=columns) for fp in files]\nweather = pd.concat(weather_dfs)",
            "code"
        ],
        [
            "Subjectively this is cleaner and more beautiful.\nThere\u2019s fewer lines of code.\nYou don\u2019t have this extraneous detail of building an empty DataFrame.\nAnd objectively the pandorable way is faster, as we\u2019ll test next.",
            "markdown"
        ],
        [
            "We\u2019ll define two functions for building an identical DataFrame. The first append_df, creates an empty DataFrame and appends to it. The second, concat_df, creates many DataFrames, and concatenates them at the end. We also write a short decorator that runs the functions a handful of times and records the results.",
            "markdown"
        ],
        [
            "import time\n\nsize_per = 5000\nN = 100\ncols = list('abcd')\n\ndef timed(n=30):\n    '''\n    Running a microbenchmark. Never use this.\n    '''\n    def deco(func):\n        def wrapper(*args, **kwargs):\n            timings = []\n            for i in range(n):\n                t0 = time.time()\n                func(*args, **kwargs)\n                t1 = time.time()\n                timings.append(t1 - t0)\n            return timings\n        return wrapper\n    return deco\n    \n@timed(60)\ndef append_df():\n    '''\n    The pythonic (bad) way\n    '''\n    df = pd.DataFrame(columns=cols)\n    for _ in range(N):\n        df.append(pd.DataFrame(np.random.randn(size_per, 4), columns=cols))\n    return df\n\n@timed(60)\ndef concat_df():\n    '''\n    The pandorabe (good) way\n    '''\n    dfs = [pd.DataFrame(np.random.randn(size_per, 4), columns=cols)\n           for _ in range(N)]\n    return pd.concat(dfs, ignore_index=True)",
            "code"
        ],
        [
            "t_append = append_df()\nt_concat = concat_df()\n\ntimings = (pd.DataFrame({\"Append\": t_append, \"Concat\": t_concat})\n             .stack()\n             .reset_index()\n             .rename(columns={0: 'Time (s)',\n                              'level_1': 'Method'}))\ntimings.head()",
            "code"
        ],
        [
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_4_performance_6_0.png\"/>",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Fast Pandas->Datatypes": [
        [
            "The pandas type system essentially  with a few extensions (categorical, datetime64 with timezone, timedelta64).\nAn advantage of the DataFrame over a 2-dimensional NumPy array is that the DataFrame can have columns of various types within a single table.\nThat said, each column should have a specific dtype; you don\u2019t want to be mixing bools with ints with strings within a single column.\nFor one thing, this is slow.\nIt forces the column to be have an object dtype (the fallback python-object container type), which means you don\u2019t get any of the type-specific optimizations in pandas or NumPy.\nFor another, it means you\u2019re probably violating the maxims of tidy data, which we\u2019ll discuss next time.",
            "markdown"
        ],
        [
            "When should you have object columns?\nThere are a few places where the NumPy / pandas type system isn\u2019t as rich as you might like.\nThere\u2019s no integer NA (at the moment anyway), so if you have any missing values, represented by NaN, your otherwise integer column will be floats.\nThere\u2019s also no date dtype (distinct from datetime).\nConsider the needs of your application: can you treat an integer 1 as 1.0?\nCan you treat date(2016, 1, 1) as datetime(2016, 1, 1, 0, 0)?\nIn my experience, this is rarely a problem other than when writing to something with a stricter schema like a database.\nBut at that point it\u2019s fine to cast to one of the less performant types, since you\u2019re just not doing numeric operations anymore.",
            "markdown"
        ],
        [
            "The last case of object dtype data is text data.\nPandas doesn\u2019t have any fixed-width string dtypes, so you\u2019re stuck with python objects.\nThere is an important exception here, and that\u2019s low-cardinality text data, for which you\u2019ll want to use the category dtype (see below).",
            "markdown"
        ],
        [
            "If you have object data (either strings or python objects) that needs to be converted, checkout the ,  and  methods.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization": [
        [
            "We know that  (scare quotes since that statement is too broad to be meaningful).\nThere are various steps that can be taken to improve your code\u2019s performance from relatively simple changes, to rewriting your code in a lower-level language, to trying to parallelize it.\nAnd while you might have many options, there\u2019s typically an order you would proceed in.",
            "markdown"
        ],
        [
            "First (and I know it\u2019s clich\u00e9 to say so, but still) benchmark your code.\nMake sure you actually need to spend time optimizing it.\nThere are     and visualizing where things are slow.",
            "markdown"
        ],
        [
            "Second, consider your algorithm.\nMake sure you aren\u2019t doing more work than you need to.\nA common one I see is doing a full sort on an array, just to select the N largest or smallest items.\nPandas has methods for that.",
            "markdown"
        ],
        [
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "code"
        ],
        [
            "# Select the 5 largest delays\ndelays.nlargest(5).sort_values()",
            "code"
        ],
        [
            "112623    1480.0\n158136    1545.0\n152911    1934.0\n60246     1970.0\n59719     2755.0\nName: DEP_DELAY, dtype: float64",
            "code"
        ],
        [
            "delays.nsmallest(5).sort_values()",
            "code"
        ],
        [
            "300895   -59.0\n235921   -58.0\n197897   -56.0\n332533   -56.0\n344542   -55.0\nName: DEP_DELAY, dtype: float64",
            "code"
        ],
        [
            "We follow up the nlargest or nsmallest with a sort (the result of nlargest/smallest is unordered), but it\u2019s much easier to sort 5 items that 500,000. The timings bear this out:",
            "markdown"
        ],
        [
            "%timeit delays.sort_values().tail(5)",
            "code"
        ],
        [
            "31 ms \u00b1 1.05 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)",
            "code"
        ],
        [
            "%timeit delays.nlargest(5).sort_values()",
            "code"
        ],
        [
            "7.87 ms \u00b1 113 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)",
            "code"
        ],
        [
            "\u201cUse the right algorithm\u201d is easy to say, but harder to apply in practice since you have to actually figure out the best algorithm to use.\nThat one comes down to experience.",
            "markdown"
        ],
        [
            "Assuming you\u2019re at a spot that needs optimizing, and you\u2019ve got the correct algorithm, <em>and</em> there isn\u2019t a readily available optimized version of what you need in pandas/numpy/scipy/scikit-learn/statsmodels/&amp;mldr;, then what?",
            "markdown"
        ],
        [
            "The first place to turn is probably a vectorized NumPy implementation.\nVectorization here means operating directly on arrays, rather than looping over lists scalars.\nThis is generally much less work than rewriting it in something like Cython, and you can get pretty good results just by making <em>effective</em> use of NumPy and pandas.\nWhile not every operation can be vectorized, many can.",
            "markdown"
        ],
        [
            "Let\u2019s work through an example calculating the  between airports.\nGrab the table of airport latitudes and longitudes from the  and extract it to a CSV.",
            "markdown"
        ],
        [
            "from utils import download_airports\nimport zipfile",
            "code"
        ],
        [
            "if not os.path.exists(\"data/airports.csv.zip\"):\n    download_airports()",
            "code"
        ],
        [
            "coord = (pd.read_csv(\"data/airports.csv.zip\", index_col=['AIRPORT'],\n                     usecols=['AIRPORT', 'LATITUDE', 'LONGITUDE'])\n           .groupby(level=0).first()\n           .dropna()\n           .sample(n=500, random_state=42)\n           .sort_index())\n\ncoord.head()",
            "code"
        ],
        [
            "For whatever reason, suppose we\u2019re interested in all the pairwise distances (I\u2019ve limited it to just a sample of 500 airports to make this manageable.\nIn the real world you <em>probably</em> don\u2019t need <em>all</em> the pairwise distances and would be better off with a . Remember: think about what you actually need, and find the right algorithm for that).",
            "markdown"
        ],
        [
            "MultiIndexes have an alternative from_product constructor for getting the  of the arrays you pass in.\nWe\u2019ll give it coords.index twice (to get its Cartesian product with itself).\nThat gives a MultiIndex of all the combination.\nWith some minor reshaping of coords we\u2019ll have a DataFrame with all the latitude/longitude pairs.",
            "markdown"
        ],
        [
            "idx = pd.MultiIndex.from_product([coord.index, coord.index],\n                                 names=['origin', 'dest'])\n\npairs = pd.concat([coord.add_suffix('_1').reindex(idx, level='origin'),\n                   coord.add_suffix('_2').reindex(idx, level='dest')],\n                  axis=1)\npairs.head()",
            "code"
        ],
        [
            "idx = idx[idx.get_level_values(0) &lt;= idx.get_level_values(1)]",
            "code"
        ],
        [
            "len(idx)",
            "code"
        ],
        [
            "125250",
            "code"
        ],
        [
            "We\u2019ll break that down a bit, but don\u2019t lose sight of the real target: our great-circle distance calculation.",
            "markdown"
        ],
        [
            "The add_suffix (and add_prefix) method is handy for quickly renaming the columns.",
            "markdown"
        ],
        [
            "coord.add_suffix('_1').head()",
            "code"
        ],
        [
            "Alternatively you could use the more general .rename like coord.rename(columns=lambda x: x + '_1').",
            "markdown"
        ],
        [
            "Next, we have the reindex.\nLike I mentioned in the prior chapter, indexes are crucial to pandas.\n.reindex is all about aligning a Series or DataFrame to a given index.\nIn this case we use .reindex to align our original DataFrame to the new\nMultiIndex of combinations.\nBy default, the output will have the original value if that index label was already present, and NaN otherwise.\nIf we just called coord.reindex(idx), with no additional arguments, we\u2019d get a DataFrame of all NaNs.",
            "markdown"
        ],
        [
            "coord.reindex(idx).head()",
            "code"
        ],
        [
            "That\u2019s because there weren\u2019t any values of idx that were in coord.index,\nwhich makes sense since coord.index is just a regular one-level Index, while idx is a MultiIndex.\nWe use the level keyword to handle the transition from the original single-level Index, to the two-leveled idx.<blockquote>",
            "markdown"
        ],
        [
            "level : int or name</blockquote>",
            "markdown"
        ],
        [
            "Broadcast across a level, matching Index values on the\npassed MultiIndex level",
            "markdown"
        ],
        [
            "coord.reindex(idx, level='dest').head()",
            "code"
        ],
        [
            "If you ever need to do an operation that mixes regular single-level indexes with Multilevel Indexes, look for a level keyword argument.\nFor example, all the arithmatic methods (.mul, .add, etc.) have them.",
            "markdown"
        ],
        [
            "This is a bit wasteful since the distance from airport A to B is the same as B to A.\nWe could easily fix this with a idx = idx[idx.get_level_values(0) &lt;= idx.get_level_values(1)], but we\u2019ll ignore that for now.",
            "markdown"
        ],
        [
            "Quick tangent, I got some&amp;mldr; let\u2019s say skepticism, on my last piece about the value of indexes.\nHere\u2019s an alternative version for the skeptics",
            "markdown"
        ],
        [
            "from itertools import product, chain\ncoord2 = coord.reset_index()",
            "code"
        ],
        [
            "x = product(coord2.add_suffix('_1').itertuples(index=False),\n            coord2.add_suffix('_2').itertuples(index=False))\ny = [list(chain.from_iterable(z)) for z in x]\n\ndf2 = (pd.DataFrame(y, columns=['origin', 'LATITUDE_1', 'LONGITUDE_1',\n                                'dest', 'LATITUDE_1', 'LONGITUDE_2'])\n       .set_index(['origin', 'dest']))\ndf2.head()",
            "code"
        ],
        [
            "It\u2019s also readable (it\u2019s Python after all), though a bit slower.\nTo me the .reindex method seems more natural.\nMy thought process was, \u201cI need all the combinations of origin &amp; destination (MultiIndex.from_product).\nNow I need to align this original DataFrame to this new MultiIndex (coords.reindex).\u201d",
            "markdown"
        ],
        [
            "With that diversion out of the way, let\u2019s turn back to our great-circle distance calculation.\nOur first implementation is pure python.\nThe algorithm itself isn\u2019t too important, all that matters is that we\u2019re doing math operations on scalars.",
            "markdown"
        ],
        [
            "import math\n\ndef gcd_py(lat1, lng1, lat2, lng2):\n    '''\n    Calculate great circle distance between two points.\n    http://www.johndcook.com/blog/python_longitude_latitude/\n    \n    Parameters\n    ----------\n    lat1, lng1, lat2, lng2: float\n    \n    Returns\n    -------\n    distance:\n      distance from ``(lat1, lng1)`` to ``(lat2, lng2)`` in kilometers.\n    '''\n    # python2 users will have to use ascii identifiers (or upgrade)\n    degrees_to_radians = math.pi / 180.0\n    \u03d51 = (90 - lat1) * degrees_to_radians\n    \u03d52 = (90 - lat2) * degrees_to_radians\n    \n    \u03b81 = lng1 * degrees_to_radians\n    \u03b82 = lng2 * degrees_to_radians\n    \n    cos = (math.sin(\u03d51) * math.sin(\u03d52) * math.cos(\u03b81 - \u03b82) +\n           math.cos(\u03d51) * math.cos(\u03d52))\n    # round to avoid precision issues on identical points causing ValueErrors\n    cos = round(cos, 8)\n    arc = math.acos(cos)\n    return arc * 6373  # radius of earth, in kilometers",
            "code"
        ],
        [
            "The second implementation uses NumPy.\nAside from numpy having a builtin deg2rad convenience function (which is probably a bit slower than multiplying by a constant $\\frac{\\pi}{180}$), basically all we\u2019ve done is swap the math prefix for np.\nThanks to NumPy\u2019s broadcasting, we can write code that works on scalars or arrays of conformable shape.",
            "markdown"
        ],
        [
            "def gcd_vec(lat1, lng1, lat2, lng2):\n    '''\n    Calculate great circle distance.\n    http://www.johndcook.com/blog/python_longitude_latitude/\n    \n    Parameters\n    ----------\n    lat1, lng1, lat2, lng2: float or array of float\n    \n    Returns\n    -------\n    distance:\n      distance from ``(lat1, lng1)`` to ``(lat2, lng2)`` in kilometers.\n    '''\n    # python2 users will have to use ascii identifiers\n    \u03d51 = np.deg2rad(90 - lat1)\n    \u03d52 = np.deg2rad(90 - lat2)\n    \n    \u03b81 = np.deg2rad(lng1)\n    \u03b82 = np.deg2rad(lng2)\n    \n    cos = (np.sin(\u03d51) * np.sin(\u03d52) * np.cos(\u03b81 - \u03b82) +\n           np.cos(\u03d51) * np.cos(\u03d52))\n    arc = np.arccos(cos)\n    return arc * 6373",
            "code"
        ],
        [
            "To use the python version on our DataFrame, we can either iterate&amp;mldr;",
            "markdown"
        ],
        [
            "%%time\npd.Series([gcd_py(*x) for x in pairs.itertuples(index=False)],\n          index=pairs.index)",
            "code"
        ],
        [
            "CPU times: user 833 ms, sys: 12.7 ms, total: 846 ms\nWall time: 847 ms\n\n\n\n\n\norigin  dest\n8F3     8F3         0.000000\n        A03      4744.967448\n        A09      4407.533212\n        A18      4744.593127\n        A24      3820.092688\n                    ...     \nZZU     YUY     12643.665960\n        YYL     13687.592278\n        ZBR      4999.647307\n        ZXO     14925.531303\n        ZZU         0.000000\nLength: 250000, dtype: float64",
            "code"
        ],
        [
            "Or use DataFrame.apply.",
            "markdown"
        ],
        [
            "%%time\nr = pairs.apply(lambda x: gcd_py(x['LATITUDE_1'], x['LONGITUDE_1'],\n                                 x['LATITUDE_2'], x['LONGITUDE_2']), axis=1);",
            "code"
        ],
        [
            "CPU times: user 14.4 s, sys: 61.2 ms, total: 14.4 s\nWall time: 14.4 s",
            "code"
        ],
        [
            "But as you can see, you don\u2019t want to use apply, especially with axis=1 (calling the function on each row). It\u2019s doing a lot more work handling dtypes in the background, and trying to infer the correct output shape that are pure overhead in this case. On top of that, it has to essentially use a for loop internally.",
            "markdown"
        ],
        [
            "You <em>rarely</em> want to use DataFrame.apply and almost never should use it with axis=1. Better to write functions that take arrays, and pass those in directly. Like we did with the vectorized version",
            "markdown"
        ],
        [
            "%%time\nr = gcd_vec(pairs['LATITUDE_1'], pairs['LONGITUDE_1'],\n            pairs['LATITUDE_2'], pairs['LONGITUDE_2'])",
            "code"
        ],
        [
            "CPU times: user 31.1 ms, sys: 26.4 ms, total: 57.5 ms\nWall time: 37.2 ms\n\n\n/Users/taugspurger/miniconda3/envs/modern-pandas/lib/python3.6/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in arccos",
            "code"
        ],
        [
            "r.head()",
            "code"
        ],
        [
            "origin  dest\n8F3     8F3        0.000000\n        A03     4744.967484\n        A09     4407.533240\n        A18     4744.593111\n        A24     3820.092639\ndtype: float64",
            "code"
        ],
        [
            "I try not to use the word \u201ceasy\u201d when teaching, but that optimization was easy right?\nWhy then, do I come across uses of apply, in my code and others\u2019, even when the vectorized version is available?\nThe difficulty lies in knowing about broadcasting, and seeing where to apply it.",
            "markdown"
        ],
        [
            "For example, the README for  (by Cam Davidson Pilon, also author of , , and ) used to have an example of passing  into a DataFrame.apply.",
            "markdown"
        ],
        [
            "data.apply(lambda r: bgf.conditional_expected_number_of_purchases_up_to_time(\n    t, r['frequency'], r['recency'], r['T']), axis=1\n)",
            "code"
        ],
        [
            "If you look at the function , it\u2019s doing a fairly complicated computation involving a negative log likelihood and the Gamma function from scipy.special.\nBut crucially, it was already vectorized.\nWe were able to change the example to just pass the arrays (Series in this case) into the function, rather than applying the function to each row.",
            "markdown"
        ],
        [
            "bgf.conditional_expected_number_of_purchases_up_to_time(\n    t, data['frequency'], data['recency'], data['T']\n)",
            "code"
        ],
        [
            "This got us another 30x speedup on the example dataset.\nI bring this up because it\u2019s very natural to have to translate an equation to code and think, \u201cOk now I need to apply this function to each row\u201d, so you reach for DataFrame.apply.\nSee if you can just pass in the NumPy array or Series itself instead.",
            "markdown"
        ],
        [
            "Not all operations this easy to vectorize.\nSome operations are iterative by nature, and rely on the results of surrounding computations to proceed. In cases like this you can hope that one of the scientific python libraries has implemented it efficiently for you, or write your own solution using Numba / C / Cython / Fortran.",
            "markdown"
        ],
        [
            "Other examples take a bit more thought or knowledge to vectorize.\nLet\u2019s look at \nexample, taken from Jeff Reback\u2019s PyData London talk, that groupwise normalizes a dataset by subtracting the mean and dividing by the standard deviation for each group.",
            "markdown"
        ],
        [
            "import random\n\ndef create_frame(n, n_groups):\n    # just setup code, not benchmarking this\n    stamps = pd.date_range('20010101', periods=n, freq='ms')\n    random.shuffle(stamps.values)    \n    return pd.DataFrame({'name': np.random.randint(0,n_groups,size=n),\n                         'stamp': stamps,\n                         'value': np.random.randint(0,n,size=n),\n                         'value2': np.random.randn(n)})\n\n\ndf = create_frame(1000000,10000)\n\ndef f_apply(df):\n    # Typical transform\n    return df.groupby('name').value2.apply(lambda x: (x-x.mean())/x.std())\n\ndef f_unwrap(df):\n    # \"unwrapped\"\n    g = df.groupby('name').value2\n    v = df.value2\n    return (v-g.transform(np.mean))/g.transform(np.std)",
            "code"
        ],
        [
            "Timing it we see that the \u201cunwrapped\u201d version, get\u2019s quite a bit better performance.",
            "markdown"
        ],
        [
            "%timeit f_apply(df)",
            "code"
        ],
        [
            "4.28 s \u00b1 161 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)",
            "code"
        ],
        [
            "%timeit f_unwrap(df)",
            "code"
        ],
        [
            "53.3 ms \u00b1 1.97 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)",
            "code"
        ],
        [
            "Pandas GroupBy objects intercept calls for common functions like mean, sum, etc. and substitutes them with optimized Cython versions.\nSo the unwrapped .transform(np.mean) and .transform(np.std) are fast, while the x.mean and x.std in the .apply(lambda x: x - x.mean()/x.std()) aren\u2019t.",
            "markdown"
        ],
        [
            "Groupby.apply is always going to be around, beacuse it offers maximum flexibility. If you need to , it can handle that. It just might not be the fastest (which may be OK sometimes).",
            "markdown"
        ],
        [
            "This last example is admittedly niche.\nI\u2019d like to think that there aren\u2019t too many places in pandas where the natural thing to do .transform((x - x.mean()) / x.std()) is slower than the less obvious alternative.\nIdeally the user wouldn\u2019t have to know about GroupBy having special fast implementations of common methods.\nBut that\u2019s where we are now.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Fast Pandas->Categoricals": [
        [
            "Thanks to some great work by , , and others, pandas 0.15 gained a new  data type. Categoricals are nice for many reasons beyond just efficiency, but we\u2019ll focus on that here.",
            "markdown"
        ],
        [
            "Categoricals are an efficient way of representing data (typically strings) that have a low <em>cardinality</em>, i.e. relatively few distinct values relative to the size of the array. Internally, a Categorical stores the categories once, and an array of codes, which are just integers that indicate which category belongs there. Since it\u2019s cheaper to store a code than a category, we save on memory (shown next).",
            "markdown"
        ],
        [
            "import string\n\ns = pd.Series(np.random.choice(list(string.ascii_letters), 100000))\nprint('{:0.2f} KB'.format(s.memory_usage(index=False) / 1000))",
            "code"
        ],
        [
            "800.00 KB",
            "code"
        ],
        [
            "c = s.astype('category')\nprint('{:0.2f} KB'.format(c.memory_usage(index=False) / 1000))",
            "code"
        ],
        [
            "102.98 KB",
            "code"
        ],
        [
            "Beyond saving memory, having codes and a fixed set of categories offers up a bunch of algorithmic optimizations that pandas and others can take advantage of.",
            "markdown"
        ],
        [
            " has a very nice  on using categoricals, and optimizing code in general.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Fast Pandas->Going Further": [
        [
            "The pandas documentation has a section on , focusing on using Cython or numba to speed up a computation. I\u2019ve focused more on the lower-hanging fruit of picking the right algorithm, vectorizing your code, and using pandas or numpy more effetively. There are further optimizations availble if these aren\u2019t enough.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Fast Pandas->Summary": [
        [
            "This post was more about how to make effective use of numpy and pandas, than writing your own highly-optimized code.\nIn my day-to-day work of data analysis it\u2019s not worth the time to write and compile a cython extension.\nI\u2019d rather rely on pandas to be fast at what matters (label lookup on large arrays, factorizations for groupbys and merges, numerics).\nIf you want to learn more about what pandas does to make things fast, checkout Jeff Tratner\u2019 talk from PyData Seattle  on pandas\u2019 internals.",
            "markdown"
        ],
        [
            "Next time we\u2019ll look at a differnt kind of optimization: using the Tidy Data principles to facilitate efficient data analysis.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Tidy Data": [
        [
            "This is part 5 in my series on writing modern idiomatic pandas.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data": [
        [
            "Structuring datasets to facilitate analysis </blockquote>",
            "markdown"
        ],
        [
            "So, you\u2019ve sat down to analyze a new dataset.\nWhat do you do first?",
            "markdown"
        ],
        [
            "In episode 11 of , Hilary and Roger discussed their typical approaches.\nI\u2019m with Hilary on this one, you should make sure your data is tidy.\nBefore you do any plots, filtering, transformations, summary statistics, regressions&amp;mldr;\nWithout a tidy dataset, you\u2019ll be fighting your tools to get the result you need.\nWith a tidy dataset, it\u2019s relatively easy to do all of those.",
            "markdown"
        ],
        [
            "Hadley Wickham kindly summarized tidiness as a dataset whereEach variable forms a columnEach observation forms a rowEach type of observational unit forms a table",
            "markdown"
        ],
        [
            "And today we\u2019ll only concern ourselves with the first two.\nAs quoted at the top, this really is about facilitating analysis: going as quickly as possible from question to answer.",
            "markdown"
        ],
        [
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\npd.options.display.max_rows = 10\nsns.set(style='ticks', context='talk')",
            "code"
        ]
    ],
    "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data": [
        [
            " StackOverflow question asked about calculating the number of days of rest NBA teams have between games.\nThe answer would have been difficult to compute with the raw data.\nAfter transforming the dataset to be tidy, we\u2019re able to quickly get the answer.",
            "markdown"
        ],
        [
            "We\u2019ll grab some NBA game data from basketball-reference.com using pandas\u2019 read_html function, which returns a list of DataFrames.",
            "markdown"
        ],
        [
            "fp = 'data/nba.csv'\n\nif not os.path.exists(fp):\n    tables = pd.read_html(\"http://www.basketball-reference.com/leagues/NBA_2016_games.html\")\n    games = tables[0]\n    games.to_csv(fp)\nelse:\n    games = pd.read_csv(fp)\ngames.head()",
            "code"
        ],
        [
            "Side note: pandas\u2019 read_html is pretty good. On simple websites it almost always works.\nIt provides a couple parameters for controlling what gets selected from the webpage if the defaults fail.\nI\u2019ll always use it first, before moving on to  or  if the page is more complicated.",
            "markdown"
        ],
        [
            "As you can see, we have a bit of general munging to do before tidying.\nEach month slips in an extra row of mostly NaNs, the column names aren\u2019t too useful, and we have some dtypes to fix up.",
            "markdown"
        ],
        [
            "column_names = {'Date': 'date', 'Start (ET)': 'start',\n                'Unamed: 2': 'box', 'Visitor/Neutral': 'away_team', \n                'PTS': 'away_points', 'Home/Neutral': 'home_team',\n                'PTS.1': 'home_points', 'Unamed: 7': 'n_ot'}\n\ngames = (games.rename(columns=column_names)\n    .dropna(thresh=4)\n    [['date', 'away_team', 'away_points', 'home_team', 'home_points']]\n    .assign(date=lambda x: pd.to_datetime(x['date'], format='%a, %b %d, %Y'))\n    .set_index('date', append=True)\n    .rename_axis([\"game_id\", \"date\"])\n    .sort_index())\ngames.head()",
            "code"
        ],
        [
            "A quick aside on that last block.dropna has a thresh argument. If at least thresh items are missing, the row is dropped. We used it to remove the \u201cMonth headers\u201d that slipped into the table.assign can take a callable. This lets us refer to the DataFrame in the previous step of the chain. Otherwise we would have to assign temp_df = games.dropna()... And then do the pd.to_datetime on that.set_index has an append keyword. We keep the original index around since it will be our unique identifier per game.We use .rename_axis to set the index names (this behavior is new in pandas 0.18; before .rename_axis only took a mapping for changing labels).",
            "markdown"
        ],
        [
            "The Question:<blockquote>",
            "markdown"
        ],
        [
            "<strong>How many days of rest did each team get between each game?</strong></blockquote>",
            "markdown"
        ],
        [
            "Whether or not your dataset is tidy depends on your question. Given our question, what is an observation?",
            "markdown"
        ],
        [
            "In this case, an observation is a (team, game) pair, which we don\u2019t have yet. Rather, we have two observations per row, one for home and one for away. We\u2019ll fix that with pd.melt.",
            "markdown"
        ],
        [
            "pd.melt works by taking observations that are spread across columns (away_team, home_team), and melting them down into one column with multiple rows. However, we don\u2019t want to lose the metadata (like game_id and date) that is shared between the observations. By including those columns as id_vars, the values will be repeated as many times as needed to stay with their observations.",
            "markdown"
        ],
        [
            "tidy = pd.melt(games.reset_index(),\n               id_vars=['game_id', 'date'], value_vars=['away_team', 'home_team'],\n               value_name='team')\ntidy.head()",
            "code"
        ],
        [
            "The DataFrame tidy meets our rules for tidiness: each variable is in a column, and each observation (team, date pair) is on its own row.\nNow the translation from question (\u201cHow many days of rest between games\u201d) to operation (\u201cdate of today\u2019s game - date of previous game - 1\u201d) is direct:",
            "markdown"
        ],
        [
            "# For each team... get number of days between games\ntidy.groupby('team')['date'].diff().dt.days - 1",
            "code"
        ],
        [
            "0       NaN\n1       NaN\n2       NaN\n3       NaN\n4       NaN\n       ... \n2455    7.0\n2456    1.0\n2457    1.0\n2458    3.0\n2459    2.0\nName: date, Length: 2460, dtype: float64",
            "code"
        ],
        [
            "That\u2019s the essence of tidy data, the reason why it\u2019s worth considering what shape your data should be in.\nIt\u2019s about setting yourself up for success so that the answers naturally flow from the data (just kidding, it\u2019s usually still difficult. But hopefully less so).",
            "markdown"
        ],
        [
            "Let\u2019s assign that back into our DataFrame",
            "markdown"
        ],
        [
            "tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()",
            "code"
        ],
        [
            "To show the inverse of melt, let\u2019s take rest values we just calculated and place them back in the original DataFrame with a pivot_table.",
            "markdown"
        ],
        [
            "by_game = (pd.pivot_table(tidy, values='rest',\n                          index=['game_id', 'date'],\n                          columns='variable')\n             .rename(columns={'away_team': 'away_rest',\n                              'home_team': 'home_rest'}))\ndf = pd.concat([games, by_game], axis=1)\ndf.dropna().head()",
            "code"
        ],
        [
            "One somewhat subtle point: an \u201cobservation\u201d depends on the question being asked.\nSo really, we have two tidy datasets, tidy for answering team-level questions, and df for answering game-level questions.",
            "markdown"
        ],
        [
            "One potentially interesting question is \u201cwhat was each team\u2019s average days of rest, at home and on the road?\u201d With a tidy dataset (the DataFrame tidy, since it\u2019s team-level), seaborn makes this easy (more on seaborn in a future post):",
            "markdown"
        ],
        [
            "sns.set(style='ticks', context='paper')",
            "code"
        ],
        [
            "g = sns.FacetGrid(tidy, col='team', col_wrap=6, hue='team', size=2)\ng.map(sns.barplot, 'variable', 'rest');",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_17_0.png\"/>",
            "markdown"
        ],
        [
            "An example of a game-level statistic is the distribution of rest differences in games:",
            "markdown"
        ],
        [
            "df['home_win'] = df['home_points'] &gt; df['away_points']\ndf['rest_spread'] = df['home_rest'] - df['away_rest']\ndf.dropna().head()",
            "code"
        ],
        [
            "delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_20_0.png\"/>",
            "markdown"
        ],
        [
            "Or the win percent by rest difference",
            "markdown"
        ],
        [
            "fig, ax = plt.subplots(figsize=(12, 6))\nsns.barplot(x='rest_spread', y='home_win', data=df.query('-3 &lt;= rest_spread &lt;= 3'),\n            color='#4c72b0', ax=ax)\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_22_0.png\"/>",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Stack / Unstack": [
        [
            "Pandas has two useful methods for quickly converting from wide to long format (stack) and long to wide (unstack).",
            "markdown"
        ],
        [
            "rest = (tidy.groupby(['date', 'variable'])\n            .rest.mean()\n            .dropna())\nrest.head()",
            "code"
        ],
        [
            "date        variable \n2015-10-28  away_team    0.000000\n            home_team    0.000000\n2015-10-29  away_team    0.333333\n            home_team    0.000000\n2015-10-30  away_team    1.083333\nName: rest, dtype: float64",
            "code"
        ],
        [
            "rest is in a \u201clong\u201d form since we have a single column of data, with multiple \u201ccolumns\u201d of metadata (in the MultiIndex). We use .unstack to move from long to wide.",
            "markdown"
        ],
        [
            "rest.unstack().head()",
            "code"
        ],
        [
            "unstack moves a level of a MultiIndex (innermost by default) up to the columns.\nstack is the inverse.",
            "markdown"
        ],
        [
            "rest.unstack().stack()",
            "code"
        ],
        [
            "date        variable \n2015-10-28  away_team    0.000000\n            home_team    0.000000\n2015-10-29  away_team    0.333333\n            home_team    0.000000\n2015-10-30  away_team    1.083333\n                           ...   \n2016-04-11  home_team    0.666667\n2016-04-12  away_team    1.000000\n            home_team    1.400000\n2016-04-13  away_team    0.500000\n            home_team    1.214286\nLength: 320, dtype: float64",
            "code"
        ],
        [
            "With .unstack you can move between those APIs that expect there data in long-format and those APIs that work with wide-format data. For example, DataFrame.plot(), works with wide-form data, one line per column.",
            "markdown"
        ],
        [
            "with sns.color_palette() as pal:\n    b, g = pal.as_hex()[:2]\n\nax=(rest.unstack()\n        .query('away_team &lt; 7')\n        .rolling(7)\n        .mean()\n        .plot(figsize=(12, 6), linewidth=3, legend=False))\nax.set(ylabel='Rest (7 day MA)')\nax.annotate(\"Home\", (rest.index[-1][0], 1.02), color=g, size=14)\nax.annotate(\"Away\", (rest.index[-1][0], 0.82), color=b, size=14)\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_30_0.png\"/>",
            "markdown"
        ],
        [
            "The most conenient form will depend on exactly what you\u2019re doing.\nWhen interacting with databases you\u2019ll often deal with long form data.\nPandas\u2019 DataFrame.plot often expects wide-form data, while seaborn often expect long-form data. Regressions will expect wide-form data. Either way, it\u2019s good to be comfortable with stack and unstack (and MultiIndexes) to quickly move between the two.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?": [
        [
            "We\u2019ve gone to all that work tidying our dataset, let\u2019s put it to use.\nWhat\u2019s the effect (in terms of probability to win) of being\nthe home team?",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 1: Create an outcome variable": [
        [
            "We need to create an indicator for whether the home team won.\nAdd it as a column called home_win in games.",
            "markdown"
        ],
        [
            "df['home_win'] = df.home_points &gt; df.away_points",
            "code"
        ]
    ],
    "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team": [
        [
            "In the 10-minute literature review I did on the topic, it seems like people include a team-strength variable in their regressions.\nI suppose that makes sense; if stronger teams happened to play against weaker teams at home more often than away, it\u2019d look like the home-effect is stronger than it actually is.\nWe\u2019ll do a terrible job of controlling for team strength by calculating each team\u2019s win percent and using that as a predictor.\nIt\u2019d be better to use some kind of independent measure of team strength, but this will do for now.",
            "markdown"
        ],
        [
            "We\u2019ll use a similar melt operation as earlier, only now with the home_win variable we just created.",
            "markdown"
        ],
        [
            "wins = (\n    pd.melt(df.reset_index(),\n            id_vars=['game_id', 'date', 'home_win'],\n            value_name='team', var_name='is_home',\n            value_vars=['home_team', 'away_team'])\n   .assign(win=lambda x: x.home_win == (x.is_home == 'home_team'))\n   .groupby(['team', 'is_home'])\n   .win\n   .agg(['sum', 'count', 'mean'])\n   .rename(columns=dict(sum='n_wins',\n                        count='n_games',\n                        mean='win_pct'))\n)\nwins.head()",
            "code"
        ],
        [
            "Pause for visualiztion, because why not",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(wins.reset_index(), hue='team', size=7, aspect=.5, palette=['k'])\ng.map(sns.pointplot, 'is_home', 'win_pct').set(ylim=(0, 1));",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_38_0.png\"/>",
            "markdown"
        ],
        [
            "(It\u2019d be great if there was a library built on top of matplotlib that auto-labeled each point decently well. Apparently this is a difficult problem to do in general).",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(wins.reset_index(), col='team', hue='team', col_wrap=5, size=2)\ng.map(sns.pointplot, 'is_home', 'win_pct')",
            "code"
        ],
        [
            "&lt;seaborn.axisgrid.FacetGrid at 0x11a0fe588&gt;",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_40_1.png\"/>",
            "markdown"
        ],
        [
            "Those two graphs show that most teams have a higher win-percent at home than away. So we can continue to investigate.\nLet\u2019s aggregate over home / away to get an overall win percent per team.",
            "markdown"
        ],
        [
            "win_percent = (\n    # Use sum(games) / sum(games) instead of mean\n    # since I don't know if teams play the same\n    # number of games at home as away\n    wins.groupby(level='team', as_index=True)\n        .apply(lambda x: x.n_wins.sum() / x.n_games.sum())\n)\nwin_percent.head()",
            "code"
        ],
        [
            "team\nAtlanta Hawks        0.585366\nBoston Celtics       0.585366\nBrooklyn Nets        0.256098\nCharlotte Hornets    0.585366\nChicago Bulls        0.512195\ndtype: float64",
            "code"
        ],
        [
            "win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_43_1.png\"/>",
            "markdown"
        ],
        [
            "Is there a relationship between overall team strength and their home-court advantage?",
            "markdown"
        ],
        [
            "plt.figure(figsize=(8, 5))\n(wins.win_pct\n    .unstack()\n    .assign(**{'Home Win % - Away %': lambda x: x.home_team - x.away_team,\n               'Overall %': lambda x: (x.home_team + x.away_team) / 2})\n     .pipe((sns.regplot, 'data'), x='Overall %', y='Home Win % - Away %')\n)\nsns.despine()\nplt.tight_layout()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_45_0.png\"/>",
            "markdown"
        ],
        [
            "Let\u2019s get the team strength back into df.\nYou could you pd.merge, but I prefer .map when joining a Series.",
            "markdown"
        ],
        [
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "code"
        ],
        [
            "import statsmodels.formula.api as sm\n\ndf['home_win'] = df.home_win.astype(int)  # for statsmodels",
            "code"
        ],
        [
            "mod = sm.logit('home_win ~ home_strength + away_strength + home_rest + away_rest', df)\nres = mod.fit()\nres.summary()",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.552792\n         Iterations 6",
            "code"
        ],
        [
            "The strength variables both have large coefficeints (really we should be using some independent measure of team strength here, win_percent is showing up on the left and right side of the equation). The rest variables don\u2019t seem to matter as much.",
            "markdown"
        ],
        [
            "With .assign we can quickly explore variations in formula.",
            "markdown"
        ],
        [
            "(sm.Logit.from_formula('home_win ~ strength_diff + rest_spread',\n                       df.assign(strength_diff=df.home_strength - df.away_strength))\n    .fit().summary())",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.553499\n         Iterations 6",
            "code"
        ],
        [
            "mod = sm.Logit.from_formula('home_win ~ home_rest + away_rest', df)\nres = mod.fit()\nres.summary()",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.676549\n         Iterations 4",
            "code"
        ],
        [
            "Overall not seeing to much support for rest mattering, but we got to see some more tidy data.",
            "markdown"
        ],
        [
            "That\u2019s it for today.\nNext time we\u2019ll look at data visualization.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Visualization": [
        [
            "This is part 6 in my series on writing modern idiomatic pandas.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis": [
        [
            "A few weeks ago, the R community went through some hand-wringing about plotting packages.\nFor outsiders (like me) the details aren\u2019t that important, but some brief background might be useful so we can transfer the takeaways to Python.\nThe competing systems are \u201cbase R\u201d, which is the plotting system built into the language, and ggplot2, Hadley Wickham\u2019s implementation of the grammar of graphics.\nFor those interested in more details, start with",
            "markdown"
        ],
        [
            "The most important takeaways are thatEither system is capable of producing anything the other canggplot2 is usually better for exploratory analysis",
            "markdown"
        ],
        [
            "Item 2 is not universally agreed upon, and it certainly isn\u2019t true for every type of chart, but we\u2019ll take it as fact for now.\nI\u2019m not foolish enough to attempt a formal analogy here, like \u201cmatplotlib is python\u2019s base R\u201d.\nBut there\u2019s at least a rough comparison:\nlike dplyr/tidyr and ggplot2, the combination of pandas and seaborn allows for fast iteration and exploration.\nWhen you need to, you can \u201cdrop down\u201d into matplotlib for further refinement.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Overview": [
        [
            "Here\u2019s a brief sketch of the plotting landscape as of April 2016.\nFor some reason, plotting tools feel a bit more personal than other parts of this series so far, so I feel the need to blanket this who discussion in a caveat: this is my personal take, shaped by my personal background and tastes.\nAlso, I\u2019m not at all an expert on visualization, just a consumer.\nFor real advice, you should  to the  in this .\nTake this all with an extra grain or two of salt.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->": [
        [
            "Matplotlib is an amazing project, and is the foundation of pandas\u2019 built-in plotting and Seaborn.\nIt handles everything from the integration with various drawing backends, to several APIs handling drawing charts or adding and transforming individual glyphs (artists).\nI\u2019ve found knowing the  useful.\nYou\u2019re less likely to need things like  or , but when you do the documentation is there.",
            "markdown"
        ],
        [
            "Matplotlib has built up something of a bad reputation for being verbose.\nI think that complaint is valid, but misplaced.\nMatplotlib lets you control essentially anything on the figure.\nAn overly-verbose API just means there\u2019s an opportunity for a higher-level, domain specific, package to exist (like seaborn for statistical graphics).",
            "markdown"
        ],
        [
            "DataFrame and Series have a .plot namespace, with various chart types available (line, hist, scatter, etc.).\nPandas objects provide additional metadata that can be used to enhance plots (the Index for a better automatic x-axis then range(n) or Index names as axis labels for example).",
            "markdown"
        ],
        [
            "And since pandas had fewer backwards-compatibility constraints, it had a bit better default aesthetics.\nThe  will level this, and pandas has , in favor of matplotlib\u2019s (technically  it when fixing matplotlib 1.5 compatibility, so we deprecated it after the fact).",
            "markdown"
        ],
        [
            "At this point, I see pandas DataFrame.plot as a useful exploratory tool for quick throwaway plots.",
            "markdown"
        ],
        [
            ", created by Michael Waskom, \u201cprovides a high-level interface for drawing attractive statistical graphics.\u201d Seaborn gives a great API for quickly exploring different visual representations of your data. We\u2019ll be focusing on that today",
            "markdown"
        ],
        [
            " is a (still under heavy development) visualiztion library that targets the browser.",
            "markdown"
        ],
        [
            "Like matplotlib, Bokeh has a few APIs at various levels of abstraction.\nThey have a glyph API, which I suppose is most similar to matplotlib\u2019s Artists API, for drawing single or arrays of glpyhs (circles, rectangles, polygons, etc.).\nMore recently they introduced a Charts API, for producing canned charts from data structures like dicts or DataFrames.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Other Libraries": [
        [
            "This is a (probably incomplete) list of other visualization libraries that I don\u2019t know enough about to comment on",
            "markdown"
        ],
        [
            "It\u2019s also possible to use Javascript tools like D3 directly in the Jupyter notebook, but we won\u2019t go into those today.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples": [
        [
            "I do want to pause and explain the type of work I\u2019m doing with these packages.\nThe vast majority of plots I create are for exploratory analysis, helping me understand the dataset I\u2019m working with.\nThey aren\u2019t intended for the client (whoever that is) to see.\nOccasionally that exploratory plot will evolve towards a final product that will be used to explain things to the client.\nIn this case I\u2019ll either polish the exploratory plot, or rewrite it in another system more suitable for the final product (in D3 or Bokeh, say, if it needs to be an interactive document in the browser).",
            "markdown"
        ],
        [
            "Now that we have a feel for the overall landscape (from my point of view), let\u2019s delve into a few examples.\nWe\u2019ll use the diamonds dataset from ggplot2.\nYou could use Vincent Arelbundock\u2019s  to find it (pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/diamonds.csv')), but I wanted to checkout .",
            "markdown"
        ],
        [
            "import os\nimport feather\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa",
            "code"
        ],
        [
            "%load_ext rpy2.ipython",
            "code"
        ],
        [
            "%%R\nsuppressPackageStartupMessages(library(ggplot2))\nlibrary(feather)\nwrite_feather(diamonds, 'diamonds.fthr')",
            "code"
        ],
        [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "code"
        ],
        [
            "df.info()",
            "code"
        ],
        [
            "&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 53940 entries, 0 to 53939\nData columns (total 10 columns):\ncarat      53940 non-null float64\ncut        53940 non-null category\ncolor      53940 non-null category\nclarity    53940 non-null category\ndepth      53940 non-null float64\ntable      53940 non-null float64\nprice      53940 non-null int32\nx          53940 non-null float64\ny          53940 non-null float64\nz          53940 non-null float64\ndtypes: category(3), float64(6), int32(1)\nmemory usage: 2.8 MB",
            "code"
        ],
        [
            "It\u2019s not clear to me where the scientific community will come down on Bokeh for exploratory analysis.\nThe ability to share interactive graphics is compelling.\nThe trend towards more and more analysis and communication happening in the browser will only enhance this feature of Bokeh.",
            "markdown"
        ],
        [
            "Personally though, I have a lot of inertia behind matplotlib so I haven\u2019t switched to Bokeh for day-to-day exploratory analysis.",
            "markdown"
        ],
        [
            "I have greatly enjoyed Bokeh for building dashboards and  with Bokeh server.\nIt\u2019s still young, and I\u2019ve hit , but I\u2019m happy to put up with some awkwardness to avoid writing more javascript.",
            "markdown"
        ],
        [
            "sns.set(context='talk', style='ticks')\n\n%matplotlib inline",
            "code"
        ]
    ],
    "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Matplotlib": [
        [
            "Since it\u2019s relatively new, I should point out that matplotlib 1.5 added support for plotting labeled data.",
            "markdown"
        ],
        [
            "fig, ax = plt.subplots()\n\nax.scatter(x='carat', y='depth', data=df, c='k', alpha=.15);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_13_0.png\"/>",
            "markdown"
        ],
        [
            "This isn\u2019t limited to just DataFrames.\nIt supports anything that uses __getitem__ (square-brackets) with string keys.\nOther than that, I don\u2019t have much to add to the .",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Pandas Built-in Plotting": [
        [
            "The metadata in DataFrames gives a bit better defaults on plots.",
            "markdown"
        ],
        [
            "df.plot.scatter(x='carat', y='depth', c='k', alpha=.15)\nplt.tight_layout()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_17_0.png\"/>",
            "markdown"
        ],
        [
            "We get axis labels from the column names.\nNothing major, just nice.",
            "markdown"
        ],
        [
            "Pandas can be more convenient for plotting a bunch of columns with a shared x-axis (the index), say several timeseries.",
            "markdown"
        ],
        [
            "from pandas_datareader import fred\n\ngdp = fred.FredReader(['GCEC96', 'GPDIC96'], start='2000-01-01').read()\n\ngdp.rename(columns={\"GCEC96\": \"Government Expenditure\",\n                    \"GPDIC96\": \"Private Investment\"}).plot(figsize=(12, 6))\nplt.tight_layout()",
            "code"
        ],
        [
            "/Users/taugspurger/miniconda3/envs/modern-pandas/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: pandas.core.common.is_list_like is deprecated. import from the public API: pandas.api.types.is_list_like instead\n  This is separate from the ipykernel package so we can avoid doing imports until",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_19_1.png\"/>",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn": [
        [
            "The rest of this post will focus on seaborn, and why I think it\u2019s especially great for exploratory analysis.",
            "markdown"
        ],
        [
            "I would encourage you to read Seaborn\u2019s , which describe its design philosophy and attempted goals. Some highlights:<blockquote>",
            "markdown"
        ],
        [
            "Seaborn aims to make visualization a central part of exploring and understanding data.</blockquote>",
            "markdown"
        ],
        [
            "It does this through a consistent, understandable (to me anyway) API.<blockquote>",
            "markdown"
        ],
        [
            "The plotting functions try to do something useful when called with a minimal set of arguments, and they expose a number of customizable options through additional parameters.</blockquote>",
            "markdown"
        ],
        [
            "Which works great for exploratory analysis, with the option to turn that into something more polished if it looks promising.<blockquote>",
            "markdown"
        ],
        [
            "Some of the functions plot directly into a matplotlib axes object, while others operate on an entire figure and produce plots with several panels.</blockquote>",
            "markdown"
        ],
        [
            "The fact that seaborn is built on matplotlib means that if you are familiar with the pyplot API, your knowledge will still be useful.",
            "markdown"
        ],
        [
            "Most seaborn plotting functions (one per chart-type) take an x, y, hue, and data arguments (only some are required, depending on the plot type). If you\u2019re working with DataFrames, you\u2019ll pass in strings referring to column names, and the DataFrame for data.",
            "markdown"
        ],
        [
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_22_0.png\"/>",
            "markdown"
        ],
        [
            "sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_23_0.png\"/>",
            "markdown"
        ],
        [
            "Bivariate relationships can easily be explored, either one at a time:",
            "markdown"
        ],
        [
            "sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_25_0.png\"/>",
            "markdown"
        ],
        [
            "Or many at once",
            "markdown"
        ],
        [
            "g = sns.pairplot(df, hue='cut')",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_27_0.png\"/>",
            "markdown"
        ],
        [
            "pairplot is a convenience wrapper around PairGrid, and offers our first look at an important seaborn abstraction, the Grid. <em>Seaborn Grids provide a link between a matplotlib Figure with multiple axes and features in your dataset.</em>",
            "markdown"
        ],
        [
            "There are two main ways of interacting with grids. First, seaborn provides convenience-wrapper functions like pairplot, that have good defaults for common tasks. If you need more flexibility, you can work with the Grid directly by mapping plotting functions over each axes.",
            "markdown"
        ],
        [
            "def core(df, \u03b1=.05):\n    mask = (df &gt; df.quantile(\u03b1)).all(1) &amp; (df &lt; df.quantile(1 - \u03b1)).all(1)\n    return df[mask]",
            "code"
        ],
        [
            "cmap = sns.cubehelix_palette(as_cmap=True, dark=0, light=1, reverse=True)\n\n(df.select_dtypes(include=[np.number])\n   .pipe(core)\n   .pipe(sns.PairGrid)\n   .map_upper(plt.scatter, marker='.', alpha=.25)\n   .map_diag(sns.kdeplot)\n   .map_lower(plt.hexbin, cmap=cmap, gridsize=20)\n);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_30_1.png\"/>",
            "markdown"
        ],
        [
            "This last example shows the tight integration with matplotlib. g.axes is an array of matplotlib.Axes and g.fig is a matplotlib.Figure.\nThis is a pretty common pattern when using seaborn: use a seaborn plotting method (or grid) to get a good start, and then adjust with matplotlib as needed.",
            "markdown"
        ],
        [
            "I <em>think</em> (not an expert on this at all) that one thing people like about the grammar of graphics is its flexibility.\nYou aren\u2019t limited to a fixed set of chart types defined by the library author.\nInstead, you construct your chart by layering scales, aesthetics and geometries.\nAnd using ggplot2 in R is a delight.",
            "markdown"
        ],
        [
            "That said, I wouldn\u2019t really call what seaborn / matplotlib offer that limited.\nYou can create pretty complex charts suited to your needs.",
            "markdown"
        ],
        [
            "agged = df.groupby(['cut', 'color']).mean().sort_index().reset_index()\n\ng = sns.PairGrid(agged, x_vars=agged.columns[2:], y_vars=['cut', 'color'],\n                 size=5, aspect=.65)\ng.map(sns.stripplot, orient=\"h\", size=10, palette='Blues_d');",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_32_1.png\"/>",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(df, col='color', hue='color', col_wrap=4)\ng.map(sns.regplot, 'carat', 'price');",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_33_1.png\"/>",
            "markdown"
        ],
        [
            "Initially I had many more examples showing off seaborn, but I\u2019ll spare you.\nSeaborn\u2019s  is thorough (and just beautiful to look at).",
            "markdown"
        ],
        [
            "We\u2019ll end with a nice scikit-learn integration for exploring the parameter-space on a GridSearch object.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV",
            "code"
        ],
        [
            "For those unfamiliar with machine learning or scikit-learn, the basic idea is your algorithm (RandomForestClassifer) is trying to maximize some objective function (percent of correctly classified items in this case).\nThere are various <em>hyperparameters</em> that affect the fit.\nWe can search this space by trying out a bunch of possible values for each parameter with the GridSearchCV estimator.",
            "markdown"
        ],
        [
            "df = sns.load_dataset('titanic')\n\nclf = RandomForestClassifier()\nparam_grid = dict(max_depth=[1, 2, 5, 10, 20, 30, 40],\n                  min_samples_split=[2, 5, 10],\n                  min_samples_leaf=[2, 3, 5])\nest = GridSearchCV(clf, param_grid=param_grid, n_jobs=4)\n\ny = df['survived']\nX = df.drop(['survived', 'who', 'alive'], axis=1)\n\nX = pd.get_dummies(X, drop_first=True)\nX = X.fillna(value=X.median())\nest.fit(X, y);",
            "code"
        ],
        [
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "code"
        ],
        [
            "sns.factorplot(x='param_max_depth', y='mean_test_score',\n               col='param_min_samples_split',\n               hue='param_min_samples_leaf',\n               data=scores);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_39_0.png\"/>",
            "markdown"
        ],
        [
            "Thanks for reading!\nI want to reiterate at the end that this is just <em>my</em> way of doing data visualization.\nYour needs might differ, meaning you might need different tools.\nYou can still use pandas to get it to the point where it\u2019s ready to be visualized!",
            "markdown"
        ],
        [
            "As always, .",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series": [
        [
            "This is part 7 in my series on writing modern idiomatic pandas.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries": [
        [
            "Pandas started out in the financial world, so naturally it has strong timeseries support.",
            "markdown"
        ],
        [
            "The first half of this post will look at pandas\u2019 capabilities for manipulating time series data.\nThe second half will discuss modelling time series data with statsmodels.",
            "markdown"
        ],
        [
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader.data as web\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style='ticks', context='talk')\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa",
            "code"
        ],
        [
            "Let\u2019s grab some stock data for Goldman Sachs using the  package, which spun off of pandas:",
            "markdown"
        ],
        [
            "gs = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01',\n                    end='2010-01-01')\ngs.head().round(2)",
            "code"
        ],
        [
            "There isn\u2019t a special data-container just for time series in pandas, they\u2019re just Series or DataFrames with a DatetimeIndex.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->Special Slicing": [
        [
            "Looking at the elements of gs.index, we see that DatetimeIndexes are made up of pandas.Timestamps:",
            "markdown"
        ],
        [
            "Looking at the elements of gs.index, we see that DatetimeIndexes are made up of pandas.Timestamps:",
            "markdown"
        ],
        [
            "gs.index[0]",
            "code"
        ],
        [
            "Timestamp('2006-01-03 00:00:00')",
            "code"
        ],
        [
            "A Timestamp is mostly compatible with the datetime.datetime class, but much amenable to storage in arrays.",
            "markdown"
        ],
        [
            "Working with Timestamps can be awkward, so Series and DataFrames with DatetimeIndexes have some special slicing rules.\nThe first special case is <em>partial-string indexing</em>. Say we wanted to select all the days in 2006. Even with Timestamp\u2019s convenient constructors, it\u2019s a pai",
            "markdown"
        ],
        [
            "gs.loc[pd.Timestamp('2006-01-01'):pd.Timestamp('2006-12-31')].head()",
            "code"
        ],
        [
            "Thanks to partial-string indexing, it\u2019s as simple as",
            "markdown"
        ],
        [
            "gs.loc['2006'].head()",
            "code"
        ],
        [
            "Since label slicing is inclusive, this slice selects any observation where the year is 2006.",
            "markdown"
        ],
        [
            "The second \u201cconvenience\u201d is __getitem__ (square-bracket) fall-back indexing. I\u2019m only going to mention it here, with the caveat that you should never use it.\nDataFrame __getitem__ typically looks in the column: gs['2006'] would search gs.columns for '2006', not find it, and raise a KeyError. But DataFrames with a DatetimeIndex catch that KeyError and try to slice the index.\nIf it succeeds in slicing the index, the result like gs.loc['2006'] is returned.\nIf it fails, the KeyError is re-raised.\nThis is confusing because in pretty much every other case DataFrame.__getitem__ works on columns, and it\u2019s fragile because if you happened to have a column '2006' you <em>would</em> get just that column, and no fall-back indexing would occur. Just use gs.loc['2006'] when slicing DataFrame indexes.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->Special Methods->Resampling": [
        [
            "Resampling is similar to a groupby: you split the time series into groups (5-day buckets below), apply a function to each group (mean), and combine the result (one row per group).",
            "markdown"
        ],
        [
            "gs.resample(\"5d\").mean().head()",
            "code"
        ],
        [
            "gs.resample(\"W\").agg(['mean', 'sum']).head()",
            "code"
        ],
        [
            "You can up-sample to convert to a higher frequency.\nThe new points are filled with NaNs.",
            "markdown"
        ],
        [
            "gs.resample(\"6H\").mean().head()",
            "code"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->Special Methods->Rolling / Expanding / EW": [
        [
            "These methods aren\u2019t unique to DatetimeIndexes, but they often make sense with time series, so I\u2019ll show them here.",
            "markdown"
        ],
        [
            "gs.Close.plot(label='Raw')\ngs.Close.rolling(28).mean().plot(label='28D MA')\ngs.Close.expanding().mean().plot(label='Expanding Average')\ngs.Close.ewm(alpha=0.03).mean().plot(label='EWMA($\\\\alpha=.03$)')\n\nplt.legend(bbox_to_anchor=(1.25, .5))\nplt.tight_layout()\nplt.ylabel(\"Close ($)\")\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_23_0.png\"/>",
            "markdown"
        ],
        [
            "Each of .rolling, .expanding, and .ewm return a deferred object, similar to a GroupBy.",
            "markdown"
        ],
        [
            "roll = gs.Close.rolling(30, center=True)\nroll",
            "code"
        ],
        [
            "Rolling [window=30,center=True,axis=0]",
            "code"
        ],
        [
            "m = roll.agg(['mean', 'std'])\nax = m['mean'].plot()\nax.fill_between(m.index, m['mean'] - m['std'], m['mean'] + m['std'],\n                alpha=.25)\nplt.tight_layout()\nplt.ylabel(\"Close ($)\")\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_26_0.png\"/>",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->Grab Bag->Offsets": [
        [
            "These are similar to dateutil.relativedelta, but works with arrays.",
            "markdown"
        ],
        [
            "gs.index + pd.DateOffset(months=3, days=-2)",
            "code"
        ],
        [
            "DatetimeIndex(['2006-04-01', '2006-04-02', '2006-04-03', '2006-04-04',\n               '2006-04-07', '2006-04-08', '2006-04-09', '2006-04-10',\n               '2006-04-11', '2006-04-15',\n               ...\n               '2010-03-15', '2010-03-16', '2010-03-19', '2010-03-20',\n               '2010-03-21', '2010-03-22', '2010-03-26', '2010-03-27',\n               '2010-03-28', '2010-03-29'],\n              dtype='datetime64[ns]', name='Date', length=1007, freq=None)",
            "code"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->Grab Bag->Holiday Calendars": [
        [
            "There are a whole bunch of special calendars, useful for traders probabaly.",
            "markdown"
        ],
        [
            "from pandas.tseries.holiday import USColumbusDay",
            "code"
        ],
        [
            "USColumbusDay.dates('2015-01-01', '2020-01-01')",
            "code"
        ],
        [
            "DatetimeIndex(['2015-10-12', '2016-10-10', '2017-10-09', '2018-10-08',\n               '2019-10-14'],\n              dtype='datetime64[ns]', freq='WOM-2MON')",
            "code"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->Grab Bag->Timezones": [
        [
            "Pandas works with pytz for nice timezone-aware datetimes.\nThe typical workflow islocalize timezone-naive timestamps to some timezoneconvert to desired timezone",
            "markdown"
        ],
        [
            "If you already have timezone-aware Timestamps, there\u2019s no need for step one.",
            "markdown"
        ],
        [
            "# tz naiive -&gt; tz aware..... to desired UTC\ngs.tz_localize('US/Eastern').tz_convert('UTC').head()",
            "code"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series": [
        [
            "The rest of this post will focus on time series in the econometric sense.\nMy indented reader for this section isn\u2019t all that clear, so I apologize upfront for any sudden shifts in complexity.\nI\u2019m roughly targeting material that could be presented in a first or second semester applied statisctics course.\nWhat follows certainly isn\u2019t a replacement for that.\nAny formality will be restricted to footnotes for the curious.\nI\u2019ve put a whole bunch of resources at the end for people earger to learn more.",
            "markdown"
        ],
        [
            "We\u2019ll focus on modelling Average Monthly Flights. Let\u2019s download the data.\nIf you\u2019ve been following along in the series, you\u2019ve seen most of this code before, so feel free to skip.",
            "markdown"
        ],
        [
            "import os\nimport io\nimport glob\nimport zipfile\nfrom utils import download_timeseries\n\nimport statsmodels.api as sm\n\n\ndef download_many(start, end):\n    months = pd.period_range(start, end=end, freq='M')\n    # We could easily parallelize this loop.\n    for i, month in enumerate(months):\n        download_timeseries(month)\n\n\ndef time_to_datetime(df, columns):\n    '''\n    Combine all time items into datetimes.\n\n    2014-01-01,1149.0 -&gt; 2014-01-01T11:49:00\n    '''\n    def converter(col):\n        timepart = (col.astype(str)\n                       .str.replace('\\.0$', '')  # NaNs force float dtype\n                       .str.pad(4, fillchar='0'))\n        return  pd.to_datetime(df['fl_date'] + ' ' +\n                               timepart.str.slice(0, 2) + ':' +\n                               timepart.str.slice(2, 4),\n                               errors='coerce')\n        return datetime_part\n    df[columns] = df[columns].apply(converter)\n    return df\n\n\ndef read_one(fp):\n    df = (pd.read_csv(fp, encoding='latin1')\n            .rename(columns=str.lower)\n            .drop('unnamed: 6', axis=1)\n            .pipe(time_to_datetime, ['dep_time', 'arr_time', 'crs_arr_time',\n                                     'crs_dep_time'])\n            .assign(fl_date=lambda x: pd.to_datetime(x['fl_date'])))\n    return df",
            "code"
        ],
        [
            "/Users/taugspurger/miniconda3/envs/modern-pandas/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n  from pandas.core import datetools",
            "code"
        ],
        [
            "store = 'data/ts.hdf5'\n\nif not os.path.exists(store):\n    download_many('2000-01-01', '2016-01-01')\n\n    zips = glob.glob(os.path.join('data', 'timeseries', '*.zip'))\n    csvs = [unzip_one(fp) for fp in zips]\n    dfs = [read_one(fp) for fp in csvs]\n    df = pd.concat(dfs, ignore_index=True)\n\n    df['origin'] = df['origin'].astype('category')\n    df.to_hdf(store, 'ts', format='table')\nelse:\n    df = pd.read_hdf(store, 'ts')",
            "code"
        ],
        [
            "with pd.option_context('display.max_rows', 100):\n    print(df.dtypes)",
            "code"
        ],
        [
            "fl_date         datetime64[ns]\norigin                category\ncrs_dep_time    datetime64[ns]\ndep_time        datetime64[ns]\ncrs_arr_time    datetime64[ns]\narr_time        datetime64[ns]\ndtype: object",
            "code"
        ],
        [
            "We can calculate the historical values with a resample.",
            "markdown"
        ],
        [
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "code"
        ],
        [
            "2000-01-01    15176.677419\n2000-02-01    15327.551724\n2000-03-01    15578.838710\n2000-04-01    15442.100000\n2000-05-01    15448.677419\nFreq: MS, Name: fl_date, dtype: float64",
            "code"
        ],
        [
            "Note that I use the \"MS\" frequency code there.\nPandas defaults to end of month (or end of year).\nAppend an 'S' to get the start.",
            "markdown"
        ],
        [
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_41_0.png\"/>",
            "markdown"
        ],
        [
            "import statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm",
            "code"
        ],
        [
            "One note of warning: I\u2019m using the development version of statsmodels (commit de15ec8 to be precise).\nNot all of the items I\u2019ve shown here are available in the currently-released version.",
            "markdown"
        ],
        [
            "Think back to a typical regression problem, ignoring anything to do with time series for now.\nThe usual task is to predict some value $y$ using some a linear combination of features in $X$.",
            "markdown"
        ],
        [
            "$$y = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p + \\epsilon$$",
            "markdown"
        ],
        [
            "When working with time series, some of the most important (and sometimes <em>only</em>) features are the previous, or <em>lagged</em>, values of $y$.",
            "markdown"
        ],
        [
            "Let\u2019s start by trying just that \u201cmanually\u201d: running a regression of y on lagged values of itself.\nWe\u2019ll see that this regression suffers from a few problems: multicollinearity, autocorrelation, non-stationarity, and seasonality.\nI\u2019ll explain what each of those are in turn and why they\u2019re problems.\nAfterwards, we\u2019ll use a second model, seasonal ARIMA, which handles those problems for us.",
            "markdown"
        ],
        [
            "First, let\u2019s create a dataframe with our lagged values of y using the .shift method, which shifts the index i periods, so it lines up with that observation.",
            "markdown"
        ],
        [
            "X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()",
            "code"
        ],
        [
            "We can fit the lagged model using statsmodels (which uses  to translate the formula string to a design matrix).",
            "markdown"
        ],
        [
            "mod_lagged = smf.ols('y ~ trend + L1 + L2 + L3 + L4 + L5',\n                     data=X.assign(trend=np.arange(len(X))))\nres_lagged = mod_lagged.fit()\nres_lagged.summary()",
            "code"
        ],
        [
            "There are a few problems with this approach though.\nSince our lagged values are highly correlated with each other, our regression suffers from .\nThat ruins our estimates of the slopes.",
            "markdown"
        ],
        [
            "sns.heatmap(X.corr());",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_48_0.png\"/>",
            "markdown"
        ],
        [
            "Second, we\u2019d intuitively expect the $\\beta_i$s to gradually decline to zero.\nThe immediately preceding period <em>should</em> be most important ($\\beta_1$ is the largest coefficient in absolute value), followed by $\\beta_2$, and $\\beta_3$&amp;mldr;\nLooking at the regression summary and the bar graph below, this isn\u2019t the case (the cause is related to multicollinearity).",
            "markdown"
        ],
        [
            "ax = res_lagged.params.drop(['Intercept', 'trend']).plot.bar(rot=0)\nplt.ylabel('Coefficeint')\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_50_0.png\"/>",
            "markdown"
        ],
        [
            "Finally, our degrees of freedom drop since we lose two for each variable (one for estimating the coefficient, one for the lost observation as a result of the shift).\nAt least in (macro)econometrics, each observation is precious and we\u2019re loath to throw them away, though sometimes that\u2019s unavoidable.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->Autocorrelation": [
        [
            "Another problem our lagged model suffered from is  (also know as serial correlation).\nRoughly speaking, autocorrelation is when there\u2019s a clear pattern in the residuals of your regression (the observed minus the predicted).\nLet\u2019s fit a simple model of $y = \\beta_0 + \\beta_1 T + \\epsilon$, where T is the time trend (np.arange(len(y))).",
            "markdown"
        ],
        [
            "# `Results.resid` is a Series of residuals: y - \u0177\nmod_trend = sm.OLS.from_formula(\n    'y ~ trend', data=y.to_frame(name='y')\n                       .assign(trend=np.arange(len(y))))\nres_trend = mod_trend.fit()",
            "code"
        ],
        [
            "Residuals (the observed minus the expected, or $\\hat{e_t} = y_t - \\hat{y_t}$) are supposed to be .\nThat\u2019s  many of the properties of linear regression are founded upon.\nIn this case there\u2019s a correlation between one residual and the next: if the residual at time $t$ was above expectation, then the residual at time $t + 1$ is <em>much</em> more likely to be above average as well ($e_t &gt; 0 \\implies E_t[e_{t+1}] &gt; 0$).",
            "markdown"
        ],
        [
            "We\u2019ll define a helper function to plot the residuals time series, and some diagnostics about them.",
            "markdown"
        ],
        [
            "def tsplot(y, lags=None, figsize=(10, 8)):\n    fig = plt.figure(figsize=figsize)\n    layout = (2, 2)\n    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n    acf_ax = plt.subplot2grid(layout, (1, 0))\n    pacf_ax = plt.subplot2grid(layout, (1, 1))\n    \n    y.plot(ax=ts_ax)\n    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n    [ax.set_xlim(1.5) for ax in [acf_ax, pacf_ax]]\n    sns.despine()\n    plt.tight_layout()\n    return ts_ax, acf_ax, pacf_ax",
            "code"
        ],
        [
            "Calling it on the residuals from the linear trend:",
            "markdown"
        ],
        [
            "tsplot(res_trend.resid, lags=36);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_58_0.png\"/>",
            "markdown"
        ],
        [
            "The top subplot shows the time series of our residuals $e_t$, which should be white noise (but it isn\u2019t).\nThe bottom shows the  of the residuals as a correlogram.\nIt measures the correlation between a value and it\u2019s lagged self, e.g. $corr(e_t, e_{t-1}), corr(e_t, e_{t-2}), \\ldots$.\nThe partial autocorrelation plot in the bottom-right shows a similar concept.\nIt\u2019s partial in the sense that the value for $corr(e_t, e_{t-k})$ is the correlation between those two periods, after controlling for the values at all shorter lags.",
            "markdown"
        ],
        [
            "Autocorrelation is a problem in regular regressions like above, but we\u2019ll use it to our advantage when we setup an ARIMA model below.\nThe basic idea is pretty sensible: if your regression residuals have a clear pattern, then there\u2019s clearly some structure in the data that you aren\u2019t taking advantage of.\nIf a positive residual today means you\u2019ll likely have a positive residual tomorrow, why not incorporate that information into your forecast, and lower your forecasted value for tomorrow?\nThat\u2019s pretty much what ARIMA does.",
            "markdown"
        ],
        [
            "It\u2019s important that your dataset be stationary, otherwise you run the risk of finding .\nA common example is the relationship between number of TVs per person and life expectancy.\nIt\u2019s not likely that there\u2019s an actual causal relationship there.\nRather, there could be a third variable that\u2019s driving both (wealth, say).\n had some stern words for the econometrics literature on this.<blockquote>",
            "markdown"
        ],
        [
            "We find it very curious that whereas virtually every textbook on econometric methodology contains explicit warnings of the dangers of autocorrelated errors, this phenomenon crops up so frequently in well-respected applied work.</blockquote>",
            "markdown"
        ],
        [
            "(:fire:), but in that academic passive-aggressive way.",
            "markdown"
        ],
        [
            "The typical way to handle non-stationarity is to difference the non-stationary variable until is is stationary.",
            "markdown"
        ],
        [
            "y.to_frame(name='y').assign(\u0394y=lambda x: x.y.diff()).plot(subplots=True)\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_61_0.png\"/>",
            "markdown"
        ],
        [
            "Our original series actually doesn\u2019t look <em>that</em> bad.\nIt doesn\u2019t look like nominal GDP say, where there\u2019s a clearly rising trend.\nBut we have more rigorous methods for detecting whether a series is non-stationary than simply plotting and squinting at it.\nOne popular method is the Augmented Dickey-Fuller test.\nIt\u2019s a statistical hypothesis test that roughly says:",
            "markdown"
        ],
        [
            "$H_0$ (null hypothesis): $y$ is non-stationary, needs to be differenced",
            "markdown"
        ],
        [
            "$H_A$ (alternative hypothesis): $y$ is stationary, doesn\u2019t need to be differenced",
            "markdown"
        ],
        [
            "I don\u2019t want to get into the weeds on exactly what the test statistic is, and what the distribution looks like.\nThis is implemented in statsmodels as .\nThe return type is a bit busy for me, so we\u2019ll wrap it in a namedtuple.",
            "markdown"
        ],
        [
            "from collections import namedtuple\n\nADF = namedtuple(\"ADF\", \"adf pvalue usedlag nobs critical icbest\")",
            "code"
        ],
        [
            "ADF(*smt.adfuller(y))._asdict()",
            "code"
        ],
        [
            "OrderedDict([('adf', -1.3206520699512339),\n             ('pvalue', 0.61967180643147923),\n             ('usedlag', 15),\n             ('nobs', 177),\n             ('critical',\n              {'1%': -3.4678453197999071,\n               '10%': -2.575551186759871,\n               '5%': -2.8780117454974392}),\n             ('icbest', 2710.6120408261486)])",
            "code"
        ],
        [
            "So we failed to reject the null hypothesis that the original series was non-stationary.\nLet\u2019s difference it.",
            "markdown"
        ],
        [
            "ADF(*smt.adfuller(y.diff().dropna()))._asdict()",
            "code"
        ],
        [
            "OrderedDict([('adf', -3.6412428797327996),\n             ('pvalue', 0.0050197770854934548),\n             ('usedlag', 14),\n             ('nobs', 177),\n             ('critical',\n              {'1%': -3.4678453197999071,\n               '10%': -2.575551186759871,\n               '5%': -2.8780117454974392}),\n             ('icbest', 2696.3891181091631)])",
            "code"
        ],
        [
            "This looks better.\nIt\u2019s not statistically significant at the 5% level, but who cares what statisticins say anyway.",
            "markdown"
        ],
        [
            "We\u2019ll fit another OLS model of $\\Delta y = \\beta_0 + \\beta_1 L \\Delta y_{t-1} + e_t$",
            "markdown"
        ],
        [
            "data = (y.to_frame(name='y')\n         .assign(\u0394y=lambda df: df.y.diff())\n         .assign(L\u0394y=lambda df: df.\u0394y.shift()))\nmod_stationary = smf.ols('\u0394y ~ L\u0394y', data=data.dropna())\nres_stationary = mod_stationary.fit()",
            "code"
        ],
        [
            "tsplot(res_stationary.resid, lags=24);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_69_0.png\"/>",
            "markdown"
        ],
        [
            "So we\u2019ve taken care of multicolinearity, autocorelation, and stationarity, but we still aren\u2019t done.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->Seasonality": [
        [
            "We have strong monthly seasonality:",
            "markdown"
        ],
        [
            "smt.seasonal_decompose(y).plot();",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_73_0.png\"/>",
            "markdown"
        ],
        [
            "There are a few ways to handle seasonality.\nWe\u2019ll just rely on the SARIMAX method to do it for us.\nFor now, recognize that it\u2019s a problem to be solved.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->ARIMA": [
        [
            "So, we\u2019ve sketched the problems with regular old regression: multicollinearity, autocorrelation, non-stationarity, and seasonality.\nOur tool of choice, smt.SARIMAX, which stands for Seasonal ARIMA with eXogenous regressors, can handle all these.\nWe\u2019ll walk through the components in pieces.",
            "markdown"
        ],
        [
            "ARIMA stands for AutoRegressive Integrated Moving Average.\nIt\u2019s a relatively simple yet flexible way of modeling univariate time series.\nIt\u2019s made up of three components, and is typically written as $\\mathrm{ARIMA}(p, d, q)$.",
            "markdown"
        ],
        [
            "ARIMA stands for AutoRegressive Integrated Moving Average, and it\u2019s a relatively simple way of modeling univariate time series.\nIt\u2019s made up of three components, and is typically written as $\\mathrm{ARIMA}(p, d, q)$.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->ARIMA->": [
        [
            "The idea is to predict a variable by a linear combination of its lagged values (<em>auto</em>-regressive as in regressing a value on its past <em>self</em>).\nAn AR(p), where $p$ represents the number of lagged values used, is written as",
            "markdown"
        ],
        [
            "$$y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + e_t$$",
            "markdown"
        ],
        [
            "$c$ is a constant and $e_t$ is white noise.\nThis looks a lot like a linear regression model with multiple predictors, but the predictors happen to be lagged values of $y$ (though they are estimated differently).",
            "markdown"
        ],
        [
            "MA models look somewhat similar to the AR component, but it\u2019s dealing with different values.",
            "markdown"
        ],
        [
            "$$y_t = c + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + \\ldots + \\theta_q e_{t-q}$$",
            "markdown"
        ],
        [
            "$c$ again is a constant and $e_t$ again is white noise.\nBut now the coefficients are the <em>residuals</em> from previous predictions.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->ARIMA->Integrated": [
        [
            "Integrated is like the opposite of differencing, and is the part that deals with stationarity.\nIf you have to difference your dataset 1 time to get it stationary, then $d=1$.\nWe\u2019ll introduce one bit of notation for differencing: $\\Delta y_t = y_t - y_{t-1}$ for $d=1$.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->ARIMA->Combining": [
        [
            "Putting that together, an ARIMA(1, 1, 1) process is written as",
            "markdown"
        ],
        [
            "$$\\Delta y_t = c + \\phi_1 \\Delta y_{t-1} + \\theta_t e_{t-1} + e_t$$",
            "markdown"
        ],
        [
            "Using <em>lag notation</em>, where $L y_t = y_{t-1}$, i.e. y.shift() in pandas, we can rewrite that as",
            "markdown"
        ],
        [
            "$$(1 - \\phi_1 L) (1 - L)y_t = c + (1 + \\theta L)e_t$$",
            "markdown"
        ],
        [
            "That was for our specific $\\mathrm{ARIMA}(1, 1, 1)$ model. For the general $\\mathrm{ARIMA}(p, d, q)$, that becomes",
            "markdown"
        ],
        [
            "$$(1 - \\phi_1 L - \\ldots - \\phi_p L^p) (1 - L)^d y_t = c + (1 + \\theta L + \\ldots + \\theta_q L^q)e_t$$",
            "markdown"
        ],
        [
            "We went through that <em>extremely</em> quickly, so don\u2019t feel bad if things aren\u2019t clear.\nFortunately, the model is pretty easy to use with statsmodels (using it <em>correctly</em>, in a statistical sense, is another matter).",
            "markdown"
        ],
        [
            "mod = smt.SARIMAX(y, trend='c', order=(1, 1, 1))\nres = mod.fit()\ntsplot(res.resid[2:], lags=24);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_81_0.png\"/>",
            "markdown"
        ],
        [
            "res.summary()",
            "code"
        ],
        [
            "There\u2019s a bunch of output there with various tests, estimated parameters, and information criteria.\nLet\u2019s just say that things are looking better, but we still haven\u2019t accounted for seasonality.",
            "markdown"
        ],
        [
            "A seasonal ARIMA model is written as $\\mathrm{ARIMA}(p,d,q)\u00d7(P,D,Q)_s$.\nLowercase letters are for the non-seasonal component, just like before. Upper-case letters are a similar specification for the seasonal component, where $s$ is the periodicity (4 for quarterly, 12 for monthly).",
            "markdown"
        ],
        [
            "It\u2019s like we have two processes, one for non-seasonal component and one for seasonal components, and we multiply them together with regular algebra rules.",
            "markdown"
        ],
        [
            "The general form of that looks like (quoting the  here)",
            "markdown"
        ],
        [
            "$$\\phi_p(L)\\tilde{\\phi}_P(L^S)\\Delta^d\\Delta_s^D y_t = A(t) + \\theta_q(L)\\tilde{\\theta}_Q(L^s)e_t$$",
            "markdown"
        ],
        [
            "where$\\phi_p(L)$ is the non-seasonal autoregressive lag polynomial$\\tilde{\\phi}_P(L^S)$ is the seasonal autoregressive lag polynomial$\\Delta^d\\Delta_s^D$ is the time series, differenced $d$ times, and seasonally differenced $D$ times.$A(t)$ is the trend polynomial (including the intercept)$\\theta_q(L)$ is the non-seasonal moving average lag polynomial$\\tilde{\\theta}_Q(L^s)$ is the seasonal moving average lag polynomial",
            "markdown"
        ],
        [
            "I don\u2019t find that to be very clear, but maybe an example will help.\nWe\u2019ll fit a seasonal ARIMA$(1,1,2)\u00d7(0, 1, 2)_{12}$.",
            "markdown"
        ],
        [
            "So the nonseasonal component is$p=1$: period autoregressive: use $y_{t-1}$$d=1$: one first-differencing of the data (one month)$q=2$: use the previous two non-seasonal residual, $e_{t-1}$ and $e_{t-2}$, to forecast",
            "markdown"
        ],
        [
            "And the seasonal component is$P=0$: Don\u2019t use any previous seasonal values$D=1$: Difference the series 12 periods back: y.diff(12)$Q=2$: Use the two previous seasonal residuals",
            "markdown"
        ],
        [
            "mod_seasonal = smt.SARIMAX(y, trend='c',\n                           order=(1, 1, 2), seasonal_order=(0, 1, 2, 12),\n                           simple_differencing=False)\nres_seasonal = mod_seasonal.fit()",
            "code"
        ],
        [
            "res_seasonal.summary()",
            "code"
        ],
        [
            "tsplot(res_seasonal.resid[12:], lags=24);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_86_0.png\"/>",
            "markdown"
        ],
        [
            "Things look much better now.",
            "markdown"
        ],
        [
            "One thing I didn\u2019t really talk about is order selection. How to choose $p, d, q, P, D$ and $Q$.\nR\u2019s forecast package does have a handy auto.arima function that does this for you.\nPython / statsmodels don\u2019t have that at the minute.\nThe alternative seems to be experience (boo), intuition (boo), and good-old grid-search.\nYou can fit a bunch of models for a bunch of combinations of the parameters and use the  or  to choose the best.\n is a useful reference, and  StackOverflow answer recommends a few options.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->Forecasting": [
        [
            "Now that we fit that model, let\u2019s put it to use.\nFirst, we\u2019ll make a bunch of one-step ahead forecasts.\nAt each point (month), we take the history up to that point and make a forecast for the next month.\nSo the forecast for January 2014 has available all the data up through December 2013.",
            "markdown"
        ],
        [
            "pred = res_seasonal.get_prediction(start='2001-03-01')\npred_ci = pred.conf_int()",
            "code"
        ],
        [
            "ax = y.plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7)\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_ylabel(\"Monthly Flights\")\nplt.legend()\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_91_0.png\"/>",
            "markdown"
        ],
        [
            "There are a few places where the observed series slips outside the 95% confidence interval.\nThe series seems especially unstable before 2005.",
            "markdown"
        ],
        [
            "Alternatively, we can make <em>dynamic</em> forecasts as of some month (January 2013 in the example below).\nThat means the forecast from that point forward only use information available as of January 2013.\nThe predictions are generated in a similar way: a bunch of one-step forecasts.\nOnly instead of plugging in the <em>actual</em> values beyond January 2013, we plug in the <em>forecast</em> values.",
            "markdown"
        ],
        [
            "pred_dy = res_seasonal.get_prediction(start='2002-03-01', dynamic='2013-01-01')\npred_dy_ci = pred_dy.conf_int()",
            "code"
        ],
        [
            "ax = y.plot(label='observed')\npred_dy.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_dy_ci.index,\n                pred_dy_ci.iloc[:, 0],\n                pred_dy_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_ylabel(\"Monthly Flights\")\n\n# Highlight the forecast area\nax.fill_betweenx(ax.get_ylim(), pd.Timestamp('2013-01-01'), y.index[-1],\n                 alpha=.1, zorder=-1)\nax.annotate('Dynamic $\\\\longrightarrow$', (pd.Timestamp('2013-02-01'), 550))\n\nplt.legend()\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_94_0.png\"/>",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->Resources": [
        [
            "This is a collection of links for those interested.",
            "markdown"
        ],
        [
            "Time series modeling in Python",
            "markdown"
        ],
        [
            "General Textbooks: A great introduction: Readable undergraduate resource, has a few chapters on time series: My favorite PhD level textbook: A classic: Extremely dry, but useful if you\u2019re implementing this stuff",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Time Series->Timeseries->Conclusion": [
        [
            "Congratulations if you made it this far, this piece just kept growing (and I still had to cut stuff).\nThe main thing cut was talking about how SARIMAX is implemented on top of using statsmodels\u2019 statespace framework.\nThe statespace framework, developed mostly by Chad Fulton over the past couple years, is really nice.\nYou can pretty easily  with custom models, but still get all the benefits of the framework\u2019s estimation and results facilities.\nI\u2019d recommend reading the .\nWe also didn\u2019t get to talk at all about Skipper Seabold\u2019s work on VARs, but maybe some other time.",
            "markdown"
        ],
        [
            "As always, .",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Scaling": [
        [
            "This is part 1 in my series on writing modern idiomatic pandas.",
            "markdown"
        ],
        [
            "As I sit down to write this, the third-most popular pandas question on StackOverflow covers . This is in tension with the fact that a pandas DataFrame is an in memory container. <em>You can\u2019t have a DataFrame larger than your machine\u2019s RAM</em>. In practice, your available RAM should be several times the size of your dataset, as you or pandas will have to make intermediate copies as part of the analysis.",
            "markdown"
        ],
        [
            "Historically, pandas users have scaled to larger datasets by switching away from pandas or using iteration. Both of these are perfectly valid approaches, but changing your workflow in response to scaling data is unfortunate. I use pandas because it\u2019s a pleasant experience, and I would like that experience to scale to larger datasets. That\u2019s what , a parallel computing library, enables. We\u2019ll discuss Dask in detail later. But first, let\u2019s work through scaling a simple analysis to a larger than memory dataset.",
            "markdown"
        ],
        [
            "Our task is to find the 100 most-common occupations reported in the FEC\u2019s . The files are split by election cycle (2007-2008, 2009-2010, &amp;mldr;). You can find some scripts for downloading the data in . My laptop can read in each cycle\u2019s file individually, but the full dataset is too large to read in at once. Let\u2019s read in just 2010\u2019s file, and do the \u201csmall data\u201d version.",
            "markdown"
        ],
        [
            "from pathlib import Path\n\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.read_parquet(\"data/indiv-10.parq\", columns=['occupation'], engine='pyarrow')\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "code"
        ],
        [
            "    RETIRED                    279775\n    ATTORNEY                   166768\n    PRESIDENT                   81336\n    PHYSICIAN                   73015\n    HOMEMAKER                   66057\n                                ...  \n    C.E.O.                       1945\n    EMERGENCY PHYSICIAN          1944\n    BUSINESS EXECUTIVE           1924\n    BUSINESS REPRESENTATIVE      1879\n    GOVERNMENT AFFAIRS           1867\n    Name: occupation, Length: 100, dtype: int64",
            "code"
        ],
        [
            "After reading in the file, our actual analysis is a simple 1-liner using two operations built into pandas. Truly, the best of all possible worlds.",
            "markdown"
        ],
        [
            "Next, we\u2019ll do the analysis for the entire dataset, which is larger than memory, in two ways. First we\u2019ll use just pandas and iteration. Then we\u2019ll use Dask.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Scaling->Using Iteration": [
        [
            "To do this with just pandas we have to rewrite our code, taking care to never have too much data in RAM at once. We willCreate a global total_counts Series that contains the counts from all of the files processed so farRead in a fileCompute a temporary variable counts with the counts for just this fileAdd that temporary counts into the global total_countsSelect the 100 largest with .nlargest",
            "markdown"
        ],
        [
            "This works since the total_counts Series is relatively small, and each year\u2019s data fits in RAM individually. Our peak memory usage should be the size of the largest individual cycle (2015-2016) plus the size of total_counts (which we can essentially ignore).",
            "markdown"
        ],
        [
            "files = sorted(Path(\"data/\").glob(\"indiv-*.parq\"))\n\ntotal_counts = pd.Series()\n\nfor year in files:\n    df = pd.read_parquet(year, columns=['occupation'],\n                         engine=\"pyarrow\")\n    counts = df.occupation.value_counts()\n    total_counts = total_counts.add(counts, fill_value=0)\n\ntotal_counts = total_counts.nlargest(100).sort_values(ascending=False)",
            "code"
        ],
        [
            "RETIRED                    4769520\nNOT EMPLOYED               2656988\nATTORNEY                   1340434\nPHYSICIAN                   659082\nHOMEMAKER                   494187\n                            ...   \nCHIEF EXECUTIVE OFFICER      26551\nSURGEON                      25521\nEDITOR                       25457\nOPERATOR                     25151\nORTHOPAEDIC SURGEON          24384\nName: occupation, Length: 100, dtype: int64",
            "code"
        ],
        [
            "While this works, our small one-liner has ballooned in size (and complexity; should you <em>really</em> have to know about Series.add\u2019s fill_value parameter for this simple analysis?). If only there was a better way&amp;mldr;",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Scaling->Using Dask": [
        [
            "With Dask, we essentially recover our original code. We\u2019ll change our import to use dask.dataframe.read_parquet, which returns a Dask DataFrame.",
            "markdown"
        ],
        [
            "import dask.dataframe as dd",
            "code"
        ],
        [
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "code"
        ],
        [
            "RETIRED                    4769520\nNOT EMPLOYED               2656988\nATTORNEY                   1340434\nPHYSICIAN                   659082\nHOMEMAKER                   494187\n                            ...   \nCHIEF EXECUTIVE OFFICER      26551\nSURGEON                      25521\nEDITOR                       25457\nOPERATOR                     25151\nORTHOPAEDIC SURGEON          24384\nName: occupation, Length: 100, dtype: int64",
            "code"
        ],
        [
            "There are a couple differences from the original pandas version, which we\u2019ll discuss next, but overall I hope you agree that the Dask version is nicer than the version using iteration.",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Scaling->Dask": [
        [
            "Now that we\u2019ve seen dask.dataframe in action, let\u2019s step back and discuss Dask a bit. Dask is an open-source project that natively parallizes Python. I\u2019m a happy user of and contributor to Dask.",
            "markdown"
        ],
        [
            "At a high-level, Dask provides familiar APIs for , , and  ways to parallelize .",
            "markdown"
        ],
        [
            "At a low-level, each of these is built on high-performance  that executes operations in parallel. The  aren\u2019t too important; all we care about is thatDask works with <em>task graphs</em> (<em>tasks</em>: functions to call on data, and <em>graphs</em>: the relationships between tasks).This is a flexible and performant way to parallelize many different kinds of problems.",
            "markdown"
        ],
        [
            "To understand point 1, let\u2019s examine the difference between a Dask DataFrame and a pandas DataFrame. When we read in df with dd.read_parquet, we received a Dask DataFrame.",
            "markdown"
        ],
        [
            "df\n<strong>Dask DataFrame Structure:</strong>Dask Name: read-parquet, 35 tasks",
            "code"
        ],
        [
            "A Dask DataFrame consists of many pandas DataFrames arranged by the index. Dask is really just coordinating these pandas DataFrames.<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\" width=\"50%\"/>",
            "markdown"
        ],
        [
            "All the actual computation (reading from disk, computing the value counts, etc.) eventually use pandas internally. If I do df.occupation.str.len, Dask will coordinate calling pandas.Series.str.len on each of the pandas DataFrames.",
            "markdown"
        ],
        [
            "Those reading carefully will notice a problem with the statement \u201cA Dask DataFrame consists of many pandas DataFrames\u201d. Our initial problem was that we didn\u2019t have enough memory for those DataFrames! How can Dask be coordinating DataFrames if there isn\u2019t enough memory? This brings us to the second major difference: Dask DataFrames (and arrays) are lazy. Operations on them don\u2019t execute and produce the final result immediately. Rather, calling methods on them builds up a task graph.",
            "markdown"
        ],
        [
            "We can visualize task graphs using graphviz. For the blog, I\u2019ve trimmed down the example to be a subset of the entire graph.",
            "markdown"
        ],
        [
            "df.visualize(rankdir='LR')",
            "code"
        ],
        [
            "<img alt=\"\" loading=\"lazy\" src=\"/images/scalable-read-simple.svg\"/>",
            "markdown"
        ],
        [
            "df (the dask DataFrame consisting of many pandas DataFrames) has a task graph with 5 calls to a parquet reader (one for each file), each of which produces a DataFrame when called.",
            "markdown"
        ],
        [
            "Calling additional methods on df adds additional tasks to this graph. For example, our most_common Series has three additional callsSelect the occupation column (__getitem__)Perform the value countsSelect the 100 largest values",
            "markdown"
        ],
        [
            "most_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "code"
        ],
        [
            "    Dask Series Structure:\n    npartitions=1\n        int64\n          ...\n    Name: occupation, dtype: int64\n    Dask Name: series-nlargest-agg, 113 tasks",
            "code"
        ],
        [
            "Which we can visualize.",
            "markdown"
        ],
        [
            "most_common.visualize(rankdir='LR')",
            "code"
        ],
        [
            "<img alt=\"\" loading=\"lazy\" src=\"/images/scalable-most-common.svg\"/>",
            "markdown"
        ],
        [
            "So most_common doesn\u2019t hold the actual answer yet. Instead, it holds a recipe for the answer; a list of all the steps to take to get the concrete result. One way to ask for the result is with the compute method.",
            "markdown"
        ],
        [
            "most_common.compute()",
            "code"
        ],
        [
            "    RETIRED                    4769520\n    NOT EMPLOYED               2656988\n    ATTORNEY                   1340434\n    PHYSICIAN                   659082\n    HOMEMAKER                   494187\n                                ...   \n    CHIEF EXECUTIVE OFFICER      26551\n    SURGEON                      25521\n    EDITOR                       25457\n    OPERATOR                     25151\n    ORTHOPAEDIC SURGEON          24384\n    Name: occupation, Length: 100, dtype: int64",
            "code"
        ],
        [
            "At this point, the task graph is handed to a , which is responsible for executing a task graph. Schedulers can analyze a task graph and find sections that can run <em>in parallel</em>. (Dask includes several schedulers. See  for how to choose, though Dask has good defaults.)",
            "markdown"
        ],
        [
            "So that\u2019s a high-level tour of how Dask works:",
            "markdown"
        ],
        [
            "<img alt=\"collections, schedulers\" loading=\"lazy\" src=\"http://dask.pydata.org/en/latest/_images/collections-schedulers.png\"/>Various collections collections like dask.dataframe and dask.array\nprovide users familiar APIs for working with large datasets.Computations are represented as a task graph. These graphs could be built by\nhand, or more commonly built by one of the collections.Dask schedulers run task graphs in parallel (potentially distributed across\na cluster), reusing libraries like NumPy and pandas to do the computations.",
            "markdown"
        ],
        [
            "Let\u2019s finish off this post by continuing to explore the FEC dataset with Dask. At this point, we\u2019ll use the distributed scheduler for it\u2019s nice diagnostics.",
            "markdown"
        ],
        [
            "import dask.dataframe as dd\nfrom dask import compute\nfrom dask.distributed import Client\nimport seaborn as sns\n\nclient = Client(processes=False)",
            "code"
        ],
        [
            "Calling Client without providing a scheduler address will make a local \u201ccluster\u201d of threads or processes on your machine. There are  to deploy a Dask cluster onto an actual cluster of machines, though we\u2019re particularly fond of . This highlights one of my favorite features of Dask: it scales down to use a handful of threads on a laptop <em>or</em> up to a cluster with thousands of nodes. Dask can comfortably handle medium-sized datasets (dozens of GBs, so larger than RAM) on a laptop. Or it can scale up to very large datasets with a cluster.",
            "markdown"
        ],
        [
            "individual_cols = ['cmte_id', 'entity_tp', 'employer', 'occupation',\n                   'transaction_dt', 'transaction_amt']\n\nindiv = dd.read_parquet('data/indiv-*.parq',\n                        columns=individual_cols,\n                        engine=\"pyarrow\")\nindiv\n<strong>Dask DataFrame Structure:</strong>Dask Name: read-parquet, 5 tasks",
            "code"
        ],
        [
            "We can compute summary statistics like the average mean and standard deviation of the transaction amount:",
            "markdown"
        ],
        [
            "avg_transaction = indiv.transaction_amt.mean()",
            "code"
        ],
        [
            "We can answer questions like \u201cWhich employer\u2019s employees donated the most?\u201d",
            "markdown"
        ],
        [
            "total_by_employee = (\n    indiv.groupby('employer')\n        .transaction_amt.sum()\n        .nlargest(10)\n)",
            "code"
        ],
        [
            "Or \u201cwhat is the average amount donated per occupation?\u201d",
            "markdown"
        ],
        [
            "avg_by_occupation = (\n    indiv.groupby(\"occupation\")\n        .transaction_amt.mean()\n        .nlargest(10)\n)",
            "code"
        ],
        [
            "Since Dask is lazy, we haven\u2019t actually computed anything.",
            "markdown"
        ],
        [
            "total_by_employee",
            "code"
        ],
        [
            "    Dask Series Structure:\n    npartitions=1\n        int64\n          ...\n    Name: transaction_amt, dtype: int64\n    Dask Name: series-nlargest-agg, 13 tasks",
            "code"
        ],
        [
            "avg_transaction, avg_by_occupation and total_by_employee are three separate computations (they have different task graphs), but we know they share some structure: they\u2019re all reading in the same data, they might select the same subset of columns, and so on. Dask is able to avoid redundant computation when you use the top-level dask.compute function.",
            "markdown"
        ],
        [
            "%%time\navg_transaction, by_employee, by_occupation = compute(\n    avg_transaction, total_by_employee, avg_by_occupation\n)",
            "code"
        ],
        [
            "    CPU times: user 57.5 s, sys: 14.4 s, total: 1min 11s\n    Wall time: 54.9 s",
            "code"
        ],
        [
            "avg_transaction",
            "code"
        ],
        [
            "    566.0899206077507",
            "code"
        ],
        [
            "by_employee",
            "code"
        ],
        [
            "    employer\n    RETIRED                1019973117\n    SELF-EMPLOYED           834547641\n    SELF                    537402882\n    SELF EMPLOYED           447363032\n    NONE                    418011322\n    HOMEMAKER               355195126\n    NOT EMPLOYED            345770418\n    FAHR, LLC               166679844\n    CANDIDATE                75186830\n    ADELSON DRUG CLINIC      53358500\n    Name: transaction_amt, dtype: int64",
            "code"
        ],
        [
            "by_occupation",
            "code"
        ],
        [
            "    occupation\n    CHAIRMAN CEO &amp; FOUNDER                   1,023,333.33\n    PAULSON AND CO., INC.                    1,000,000.00\n    CO-FOUNDING DIRECTOR                       875,000.00\n    CHAIRMAN/CHIEF TECHNOLOGY OFFICER          750,350.00\n    CO-FOUNDER, DIRECTOR, CHIEF INFORMATIO     675,000.00\n    CO-FOUNDER, DIRECTOR                       550,933.33\n    MOORE CAPITAL GROUP, LP                    500,000.00\n    PERRY HOMES                                500,000.00\n    OWNER, FOUNDER AND CEO                     500,000.00\n    CHIEF EXECUTIVE OFFICER/PRODUCER           500,000.00\n    Name: transaction_amt, dtype: float64",
            "code"
        ],
        [
            "Things like filtering work well. Let\u2019s find the 10 most common occupations and filter the dataset down to just those.",
            "markdown"
        ],
        [
            "top_occupations = (\n    indiv.occupation.value_counts()\n        .nlargest(10).index\n).compute()\ntop_occupations",
            "code"
        ],
        [
            "    Index(['RETIRED', 'NOT EMPLOYED', 'ATTORNEY', 'PHYSICIAN', 'HOMEMAKER',\n           'PRESIDENT', 'PROFESSOR', 'CONSULTANT', 'EXECUTIVE', 'ENGINEER'],\n          dtype='object')",
            "code"
        ],
        [
            "We\u2019ll filter the raw records down to just the ones from those occupations. Then we\u2019ll compute a few summary statistics on the transaction amounts for each group.",
            "markdown"
        ],
        [
            "donations = (\n    indiv[indiv.occupation.isin(top_occupations)]\n        .groupby(\"occupation\")\n        .transaction_amt\n        .agg(['count', 'mean', 'sum', 'max'])\n)",
            "code"
        ],
        [
            "total_avg, occupation_avg = compute(indiv.transaction_amt.mean(),\n                                    donations['mean'])",
            "code"
        ],
        [
            "These are small, concrete results so we can turn to familiar tools like matplotlib to visualize the result.",
            "markdown"
        ],
        [
            "ax = occupation_avg.sort_values(ascending=False).plot.barh(color='k', width=0.9);\nlim = ax.get_ylim()\nax.vlines(total_avg, *lim, color='C1', linewidth=3)\nax.legend(['Average donation'])\nax.set(xlabel=\"Donation Amount\", title=\"Average Dontation by Occupation\")\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern-pandas-08_49_0.png\"/>",
            "markdown"
        ],
        [
            "Dask inherits all of pandas\u2019 great time-series support. We can get the total amount donated per day using a .",
            "markdown"
        ],
        [
            "daily = (\n    indiv[['transaction_dt', 'transaction_amt']].dropna()\n        .set_index('transaction_dt')['transaction_amt']\n        .resample(\"D\")\n        .sum()\n).compute()\ndaily",
            "code"
        ],
        [
            "    1916-01-23    1000\n    1916-01-24       0\n    1916-01-25       0\n    1916-01-26       0\n    1916-01-27       0\n                  ... \n    2201-05-29       0\n    2201-05-30       0\n    2201-05-31       0\n    2201-06-01       0\n    2201-06-02    2000\n    Name: transaction_amt, Length: 104226, dtype: int64",
            "code"
        ],
        [
            "It seems like we have some bad data. This should just be 2007-2016. We\u2019ll filter it down to the real subset before plotting.\nNotice that the seamless transition from dask.dataframe operations above, to pandas operations below.",
            "markdown"
        ],
        [
            "subset = daily.loc['2011':'2016']\nax = subset.div(1000).plot(figsize=(12, 6))\nax.set(ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\",)\nsns.despine();",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern-pandas-08_54_0.png\"/>",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Scaling->Joining": [
        [
            "Like pandas, Dask supports joining together multiple datasets.",
            "markdown"
        ],
        [
            "Individual donations are made to <em>committees</em>. Committees are what make the actual expenditures (buying a TV ad).\nSome committees are directly tied to a candidate (this are campaign committees). Other committees are tied to a group (like the Republican National Committee). Either may be tied to a party.",
            "markdown"
        ],
        [
            "Let\u2019s read in the committees. The total number of committees is small, so we\u2019ll .compute immediately to get a pandas DataFrame (the reads still happen in parallel!).",
            "markdown"
        ],
        [
            "committee_cols = ['cmte_id', 'cmte_nm', 'cmte_tp', 'cmte_pty_affiliation']\ncm = dd.read_parquet(\"data/cm-*.parq\",\n                     columns=committee_cols).compute()\n\n# Some committees change thier name, but the ID stays the same\ncm = cm.groupby('cmte_id').last()\ncm",
            "code"
        ],
        [
            "28612 rows \u00d7 3 columns",
            "markdown"
        ],
        [
            "We\u2019ll use dd.merge, which is analogous to pd.merge for joining a Dask DataFrame with a pandas or Dask DataFrame.",
            "markdown"
        ],
        [
            "indiv = indiv[(indiv.transaction_dt &gt;= pd.Timestamp(\"2007-01-01\")) &amp;\n              (indiv.transaction_dt &lt;= pd.Timestamp(\"2018-01-01\"))]\n\ndf2 = dd.merge(indiv, cm.reset_index(), on='cmte_id')\ndf2\n<strong>Dask DataFrame Structure:</strong>Dask Name: merge, 141 tasks",
            "code"
        ],
        [
            "Now we can find which party raised more over the course of each election. We\u2019ll group by the day and party and sum the transaction amounts.",
            "markdown"
        ],
        [
            "indiv = indiv.repartition(npartitions=10)\ndf2 = dd.merge(indiv, cm.reset_index(), on='cmte_id')\ndf2\n<strong>Dask DataFrame Structure:</strong>Dask Name: merge, 141 tasks",
            "code"
        ],
        [
            "party_donations = (\n    df2.groupby([df2.transaction_dt, 'cmte_pty_affiliation'])\n       .transaction_amt.sum()\n).compute().sort_index()",
            "code"
        ],
        [
            "We\u2019ll filter that down to just Republican and Democrats and plot.",
            "markdown"
        ],
        [
            "ax = (\n    party_donations.loc[:, ['REP', 'DEM']]\n        .unstack(\"cmte_pty_affiliation\").iloc[1:-2]\n        .rolling('30D').mean().plot(color=['C0', 'C3'], figsize=(12, 6),\n                                    linewidth=3)\n)\nsns.despine()\nax.set(title=\"Daily Donations (30-D Moving Average)\", xlabel=\"Date\");",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern-pandas-08_64_0.png\"/>",
            "markdown"
        ]
    ],
    "pandas_toms_blog->Scaling->Try It Out!": [
        [
            "So that\u2019s a taste of Dask. Next time you hit a scaling problem with pandas (or NumPy, scikit-learn, or your custom code), feel free to",
            "markdown"
        ],
        [
            "pip install dask[complete]",
            "code"
        ],
        [
            "or",
            "markdown"
        ],
        [
            "conda install dask",
            "code"
        ],
        [
            "The  has links to all the relevant documentation, and  where you can try out Dask before installing.",
            "markdown"
        ],
        [
            "As always, reach out to me on  or in the comments if you have anything to share.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->An introduction to seaborn": [
        [
            "Seaborn is a library for making statistical graphics in Python. It builds on top of  and integrates closely with  data structures.",
            "markdown"
        ],
        [
            "Seaborn helps you explore and understand your data. Its plotting functions operate on dataframes and arrays containing whole datasets and internally perform the necessary semantic mapping and statistical aggregation to produce informative plots. Its dataset-oriented, declarative API lets you focus on what the different elements of your plots mean, rather than on the details of how to draw them.",
            "markdown"
        ],
        [
            "Here\u2019s an example of what seaborn can do:",
            "markdown"
        ],
        [
            "# Import seaborn\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n",
            "code"
        ],
        [
            "A few things have happened here. Let\u2019s go through them one by one:",
            "markdown"
        ],
        [
            "# Import seaborn\nimport seaborn as sns\n",
            "code"
        ],
        [
            "Seaborn is the only library we need to import for this simple example. By convention, it is imported with the shorthand sns.",
            "markdown"
        ],
        [
            "Behind the scenes, seaborn uses matplotlib to draw its plots. For interactive work, it\u2019s recommended to use a Jupyter/IPython interface in , or else you\u2019ll have to call  when you want to see the plot.",
            "markdown"
        ],
        [
            "# Apply the default theme\nsns.set_theme()\n",
            "code"
        ],
        [
            "This uses the matplotlib rcParam system and will affect how all matplotlib plots look, even if you don\u2019t make them with seaborn. Beyond the default theme, there are , and you can independently control the style and scaling of the plot to quickly translate your work between presentation contexts (e.g., making a version of your figure that will have readable fonts when projected during a talk). If you like the matplotlib defaults or prefer a different theme, you can skip this step and still use the seaborn plotting functions.",
            "markdown"
        ],
        [
            "# Load an example dataset\ntips = sns.load_dataset(\"tips\")\n",
            "code"
        ],
        [
            "Most code in the docs will use the  function to get quick access to an example dataset. There\u2019s nothing special about these datasets: they are just pandas dataframes, and we could have loaded them with  or built them by hand. Most of the examples in the documentation will specify data using pandas dataframes, but seaborn is very flexible about the  that it accepts.",
            "markdown"
        ],
        [
            "# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n",
            "code"
        ],
        [
            "This plot shows the relationship between five variables in the tips dataset using a single call to the seaborn function . Notice how we provided only the names of the variables and their roles in the plot. Unlike when using matplotlib directly, it wasn\u2019t necessary to specify attributes of the plot elements in terms of the color values or marker codes. Behind the scenes, seaborn handled the translation from values in the dataframe to arguments that matplotlib understands. This declarative approach lets you stay focused on the questions that you want to answer, rather than on the details of how to control matplotlib.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics": [
        [
            "There is no universally best way to visualize data. Different questions are best answered by different plots. Seaborn makes it easy to switch between different visual representations by using a consistent dataset-oriented API.",
            "markdown"
        ],
        [
            "The function  is named that way because it is designed to visualize many different statistical <em>relationships</em>. While scatter plots are often effective, relationships where one variable represents a measure of time are better represented by a line. The  function has a convenient kind parameter that lets you easily switch to this alternate representation:",
            "markdown"
        ],
        [
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "code"
        ],
        [
            "Notice how the size and style parameters are used in both the scatter and line plots, but they affect the two visualizations differently: changing the marker area and symbol in the scatter plot vs the line width and dashing in the line plot. We did not need to keep those details in mind, letting us focus on the overall structure of the plot and the information we want it to convey.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Statistical estimation": [
        [
            "Often, we are interested in the <em>average</em> value of one variable as a function of other variables. Many seaborn functions will automatically perform the statistical estimation that is necessary to answer these questions:",
            "markdown"
        ],
        [
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", col=\"region\",\n    hue=\"event\", style=\"event\",\n)\n",
            "code"
        ],
        [
            "When statistical values are estimated, seaborn will use bootstrapping to compute confidence intervals and draw error bars representing the uncertainty of the estimate.",
            "markdown"
        ],
        [
            "Statistical estimation in seaborn goes beyond descriptive statistics. For example, it is possible to enhance a scatterplot by including a linear regression model (and its uncertainty) using :",
            "markdown"
        ],
        [
            "sns.lmplot(data=tips, x=\"total_bill\", y=\"tip\", col=\"time\", hue=\"smoker\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations": [
        [
            "Statistical analyses require knowledge about the distribution of variables in your dataset. The seaborn function  supports several approaches to visualizing distributions. These include classic techniques like histograms and computationally-intensive approaches like kernel density estimation:",
            "markdown"
        ],
        [
            "sns.displot(data=tips, x=\"total_bill\", col=\"time\", kde=True)\n",
            "code"
        ],
        [
            "Seaborn also tries to promote techniques that are powerful but less familiar, such as calculating and plotting the empirical cumulative distribution function of the data:",
            "markdown"
        ],
        [
            "sns.displot(data=tips, kind=\"ecdf\", x=\"total_bill\", col=\"time\", hue=\"smoker\", rug=True)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Plots for categorical data": [
        [
            "Several specialized plot types in seaborn are oriented towards visualizing categorical data. They can be accessed through . These plots offer different levels of granularity. At the finest level, you may wish to see every observation by drawing a \u201cswarm\u201d plot: a scatter plot that adjusts the positions of the points along the categorical axis so that they don\u2019t overlap:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, kind=\"swarm\", x=\"day\", y=\"total_bill\", hue=\"smoker\")\n",
            "code"
        ],
        [
            "Alternately, you could use kernel density estimation to represent the underlying distribution that the points are sampled from:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, kind=\"violin\", x=\"day\", y=\"total_bill\", hue=\"smoker\", split=True)\n",
            "code"
        ],
        [
            "Or you could show only the mean value and its confidence interval within each nested category:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, kind=\"bar\", x=\"day\", y=\"total_bill\", hue=\"smoker\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->An introduction to seaborn->Multivariate views on complex datasets": [
        [
            "Some seaborn functions combine multiple kinds of plots to quickly give informative summaries of a dataset. One, , focuses on a single relationship. It plots the joint distribution between two variables along with each variable\u2019s marginal distribution:",
            "markdown"
        ],
        [
            "penguins = sns.load_dataset(\"penguins\")\nsns.jointplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\")\n",
            "code"
        ],
        [
            "The other, , takes a broader view: it shows joint and marginal distributions for all pairwise relationships and for each variable, respectively:",
            "markdown"
        ],
        [
            "sns.pairplot(data=penguins, hue=\"species\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->An introduction to seaborn->Multivariate views on complex datasets->Lower-level tools for building figures": [
        [
            "These tools work by combining  plotting functions with objects that manage the layout of the figure, linking the structure of a dataset to a . Both elements are part of the public API, and you can use them directly to create complex figures with only a few more lines of code:",
            "markdown"
        ],
        [
            "g = sns.PairGrid(penguins, hue=\"species\", corner=True)\ng.map_lower(sns.kdeplot, hue=None, levels=5, color=\".2\")\ng.map_lower(sns.scatterplot, marker=\"+\")\ng.map_diag(sns.histplot, element=\"step\", linewidth=0, kde=True)\ng.add_legend(frameon=True)\ng.legend.set_bbox_to_anchor((.61, .6))\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization": [
        [
            "Seaborn creates complete graphics with a single function call: when possible, its functions will automatically add informative axis labels and legends that explain the semantic mappings in the plot.",
            "markdown"
        ],
        [
            "In many cases, seaborn will also choose default values for its parameters based on characteristics of the data. For example, the  that we have seen so far used distinct hues (blue, orange, and sometimes green) to represent different levels of the categorical variables assigned to hue. When mapping a numeric variable, some functions will switch to a continuous gradient:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\"\n)\n",
            "code"
        ],
        [
            "When you\u2019re ready to share or publish your work, you\u2019ll probably want to polish the figure beyond what the defaults achieve. Seaborn allows for several levels of customization. It defines multiple built-in  that apply to all figures, its functions have standardized parameters that can modify the semantic mappings for each plot, and additional keyword arguments are passed down to the underlying matplotlib artists, allowing even more control. Once you\u2019ve created a plot, its properties can be modified through both the seaborn API and by dropping down to the matplotlib layer for fine-grained tweaking:",
            "markdown"
        ],
        [
            "sns.set_theme(style=\"ticks\", font_scale=1.25)\ng = sns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n    palette=\"crest\", marker=\"x\", s=100,\n)\ng.set_axis_labels(\"Bill length (mm)\", \"Bill depth (mm)\", labelpad=10)\ng.legend.set_title(\"Body mass (g)\")\ng.figure.set_size_inches(6.5, 4.5)\ng.ax.margins(.15)\ng.despine(trim=True)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization->Relationship to matplotlib": [
        [
            "Seaborn\u2019s integration with matplotlib allows you to use it across the many environments that matplotlib supports, including exploratory analysis in notebooks, real-time interaction in GUI applications, and archival output in a number of raster and vector formats.",
            "markdown"
        ],
        [
            "While you can be productive using only seaborn functions, full customization of your graphics will require some knowledge of matplotlib\u2019s concepts and API. One aspect of the learning curve for new users of seaborn will be knowing when dropping down to the matplotlib layer is necessary to achieve a particular customization. On the other hand, users coming from matplotlib will find that much of their knowledge transfers.",
            "markdown"
        ],
        [
            "Matplotlib has a comprehensive and powerful API; just about any attribute of the figure can be changed to your liking. A combination of seaborn\u2019s high-level interface and matplotlib\u2019s deep customizability will allow you both to quickly explore your data and to create graphics that can be tailored into a  final product.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization->Next steps": [
        [
            "You have a few options for where to go next. You might first want to learn how to . Once that\u2019s done, you can browse the  to get a broader sense for what kind of graphics seaborn can produce. Or you can read through the rest of the  for a deeper discussion of the different tools and what they are designed to accomplish. If you have a specific plot in mind and want to know how to make it, you could check out the , which documents each function\u2019s parameters and shows many examples to illustrate usage.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Overview of seaborn plotting functions": [
        [
            "Most of your interactions with seaborn will happen through a set of plotting functions. Later chapters in the tutorial will explore the specific features offered by each function. This chapter will introduce, at a high-level, the different kinds of functions that you will encounter.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Similar functions for similar tasks": [
        [
            "The seaborn namespace is flat; all of the functionality is accessible at the top level. But the code itself is hierarchically structured, with modules of functions that achieve similar visualization goals through different means. Most of the docs are structured around these modules: you\u2019ll encounter names like \u201crelational\u201d, \u201cdistributional\u201d, and \u201ccategorical\u201d.",
            "markdown"
        ],
        [
            "For example, the  defines functions that specialize in representing the distribution of datapoints. This includes familiar methods like the histogram:",
            "markdown"
        ],
        [
            "penguins = sns.load_dataset(\"penguins\")\nsns.histplot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\")\n",
            "code"
        ],
        [
            "Along with similar, but perhaps less familiar, options such as kernel density estimation:",
            "markdown"
        ],
        [
            "sns.kdeplot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\")\n",
            "code"
        ],
        [
            "Functions within a module share a lot of underlying code and offer similar features that may not be present in other components of the library (such as multiple=\"stack\" in the examples above). They are designed to facilitate switching between different visual representations as you explore a dataset, because different representations often have complementary strengths and weaknesses.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Figure-level vs. axes-level functions": [
        [
            "In addition to the different modules, there is a cross-cutting classification of seaborn functions as \u201caxes-level\u201d or \u201cfigure-level\u201d. The examples above are axes-level functions. They plot data onto a single matplotlib.pyplot.Axes object, which is the return value of the function.",
            "markdown"
        ],
        [
            "In contrast, figure-level functions interface with matplotlib through a seaborn object, usually a , that manages the figure. Each module has a single figure-level function, which offers a unitary interface to its various axes-level functions. The organization looks a bit like this:",
            "markdown"
        ],
        [
            "For example,  is the figure-level function for the distributions module. Its default behavior is to draw a histogram, using the same code as  behind the scenes:",
            "markdown"
        ],
        [
            "sns.displot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\")\n",
            "code"
        ],
        [
            "To draw a kernel density plot instead, using the same code as , select it using the kind parameter:",
            "markdown"
        ],
        [
            "sns.displot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\", kind=\"kde\")\n",
            "code"
        ],
        [
            "You\u2019ll notice that the figure-level plots look mostly like their axes-level counterparts, but there are a few differences. Notably, the legend is placed outside the plot. They also have a slightly different shape (more on that shortly).",
            "markdown"
        ],
        [
            "The most useful feature offered by the figure-level functions is that they can easily create figures with multiple subplots. For example, instead of stacking the three distributions for each species of penguins in the same axes, we can \u201cfacet\u201d them by plotting each distribution across the columns of the figure:",
            "markdown"
        ],
        [
            "sns.displot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", col=\"species\")\n",
            "code"
        ],
        [
            "The figure-level functions wrap their axes-level counterparts and pass the kind-specific keyword arguments (such as the bin size for a histogram) down to the underlying function. That means they are no less flexible, but there is a downside: the kind-specific parameters don\u2019t appear in the function signature or docstrings. Some of their features might be less discoverable, and you may need to look at two different pages of the documentation before understanding how to achieve a specific goal.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Axes-level functions make self-contained plots": [
        [
            "The axes-level functions are written to act like drop-in replacements for matplotlib functions. While they add axis labels and legends automatically, they don\u2019t modify anything beyond the axes that they are drawn into. That means they can be composed into arbitrarily-complex matplotlib figures with predictable results.",
            "markdown"
        ],
        [
            "The axes-level functions call  internally, which hooks into the matplotlib state-machine interface so that they draw their plots on the \u201ccurrently-active\u201d axes. But they additionally accept an ax= argument, which integrates with the object-oriented interface and lets you specify exactly where each plot should go:",
            "markdown"
        ],
        [
            "f, axs = plt.subplots(1, 2, figsize=(8, 4), gridspec_kw=dict(width_ratios=[4, 3]))\nsns.scatterplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\", ax=axs[0])\nsns.histplot(data=penguins, x=\"species\", hue=\"species\", shrink=.8, alpha=.8, legend=False, ax=axs[1])\nf.tight_layout()\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Figure-level functions own their figure": [
        [
            "In contrast, figure-level functions cannot (easily) be composed with other plots. By design, they \u201cown\u201d their own figure, including its initialization, so there\u2019s no notion of using a figure-level function to draw a plot onto an existing axes. This constraint allows the figure-level functions to implement features such as putting the legend outside of the plot.",
            "markdown"
        ],
        [
            "Nevertheless, it is possible to go beyond what the figure-level functions offer by accessing the matplotlib axes on the object that they return and adding other elements to the plot that way:",
            "markdown"
        ],
        [
            "tips = sns.load_dataset(\"tips\")\ng = sns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\ng.ax.axline(xy1=(10, 2), slope=.2, color=\"b\", dashes=(5, 2))\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Customizing plots from a figure-level function": [
        [
            "The figure-level functions return a  instance, which has a few methods for customizing attributes of the plot in a way that is \u201csmart\u201d about the subplot organization. For example, you can change the labels on the external axes using a single line of code:",
            "markdown"
        ],
        [
            "g = sns.relplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", col=\"sex\")\ng.set_axis_labels(\"Flipper length (mm)\", \"Bill length (mm)\")\n",
            "code"
        ],
        [
            "While convenient, this does add a bit of extra complexity, as you need to remember that this method is not part of the matplotlib API and exists only when using a figure-level function.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Specifying figure sizes": [
        [
            "To increase or decrease the size of a matplotlib plot, you set the width and height of the entire figure, either in the , while setting up the plot (e.g. with the figsize parameter of ), or by calling a method on the figure object (e.g. matplotlib.Figure.set_size_inches()). When using an axes-level function in seaborn, the same rules apply: the size of the plot is determined by the size of the figure it is part of and the axes layout in that figure.",
            "markdown"
        ],
        [
            "When using a figure-level function, there are several key differences. First, the functions themselves have parameters to control the figure size (although these are actually parameters of the underlying  that manages the figure). Second, these parameters, height and aspect, parameterize the size slightly differently than the width, height parameterization in matplotlib (using the seaborn parameters, width = height * aspect). Most importantly, the parameters correspond to the size of each <em>subplot</em>, rather than the size of the overall figure.",
            "markdown"
        ],
        [
            "To illustrate the difference between these approaches, here is the default output of  with one subplot:",
            "markdown"
        ],
        [
            "f, ax = plt.subplots()\n",
            "code"
        ],
        [
            "A figure with multiple columns will have the same overall size, but the axes will be squeezed horizontally to fit in the space:",
            "markdown"
        ],
        [
            "f, ax = plt.subplots(1, 2, sharey=True)\n",
            "code"
        ],
        [
            "In contrast, a plot created by a figure-level function will be square. To demonstrate that, let\u2019s set up an empty plot by using  directly. This happens behind the scenes in functions like , , or :",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(penguins)\n",
            "code"
        ],
        [
            "When additional columns are added, the figure itself will become wider, so that its subplots have the same size and shape:",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(penguins, col=\"sex\")\n",
            "code"
        ],
        [
            "And you can adjust the size and shape of each subplot without accounting for the total number of rows and columns in the figure:",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(penguins, col=\"sex\", height=3.5, aspect=.75)\n",
            "code"
        ],
        [
            "The upshot is that you can assign faceting variables without stopping to think about how you\u2019ll need to adjust the total figure size. A downside is that, when you do want to change the figure size, you\u2019ll need to remember that things work a bit differently than they do in matplotlib.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Relative merits of figure-level functions": [
        [
            "Here is a summary of the pros and cons that we have discussed above:",
            "markdown"
        ],
        [
            "On balance, the figure-level functions add some additional complexity that can make things more confusing for beginners, but their distinct features give them additional power. The tutorial documentation mostly uses the figure-level functions, because they produce slightly cleaner plots, and we generally recommend their use for most applications. The one situation where they are not a good choice is when you need to make a complex, standalone figure that composes multiple different plot kinds. At this point, it\u2019s recommended to set up the figure using matplotlib directly and to fill in the individual components using axes-level functions.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Combining multiple views on the data": [
        [
            "Two important plotting functions in seaborn don\u2019t fit cleanly into the classification scheme discussed above. These functions,  and , employ multiple kinds of plots from different modules to represent multiple aspects of a dataset in a single figure. Both plots are figure-level functions and create figures with multiple subplots by default. But they use different objects to manage the figure:  and , respectively.",
            "markdown"
        ],
        [
            " plots the relationship or joint distribution of two variables while adding marginal axes that show the univariate distribution of each one separately:",
            "markdown"
        ],
        [
            "sns.jointplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\")\n",
            "code"
        ],
        [
            " is similar \u2014 it combines joint and marginal views \u2014 but rather than focusing on a single relationship, it visualizes every pairwise combination of variables simultaneously:",
            "markdown"
        ],
        [
            "sns.pairplot(data=penguins, hue=\"species\")\n",
            "code"
        ],
        [
            "Behind the scenes, these functions are using axes-level functions that you have already met ( and ), and they also have a kind parameter that lets you quickly swap in a different representation:",
            "markdown"
        ],
        [
            "sns.jointplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\", kind=\"hist\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Data structures accepted by seaborn": [
        [
            "As a data visualization library, seaborn requires that you provide it with data. This chapter explains the various ways to accomplish that task. Seaborn supports several different dataset formats, and most functions accept data represented with objects from the  or  libraries as well as built-in Python types like lists and dictionaries. Understanding the usage patterns associated with these different options will help you quickly create useful visualizations for nearly any dataset.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "As of current writing (v0.11.0), the full breadth of options covered here are supported by only a subset of the modules in seaborn (namely, the  and  modules). The other modules offer much of the same flexibility, but have some exceptions (e.g.,  and  are limited to long-form data with named variables). The data-ingest code will be standardized over the next few release cycles, but until that point, be mindful of the specific documentation for each function if it is not doing what you expect with your dataset.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data": [
        [
            "Most plotting functions in seaborn are oriented towards <em>vectors</em> of data. When plotting x against y, each variable should be a vector. Seaborn accepts data <em>sets</em> that have more than one vector organized in some tabular fashion. There is a fundamental distinction between \u201clong-form\u201d and \u201cwide-form\u201d data tables, and seaborn will treat each differently.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data": [
        [
            "A long-form data table has the following characteristics:",
            "markdown"
        ],
        [
            "Each variable is a column",
            "markdown"
        ],
        [
            "Each observation is a row",
            "markdown"
        ],
        [
            "As a simple example, consider the \u201cflights\u201d dataset, which records the number of airline passengers who flew in each month from 1949 to 1960. This dataset has three variables (<em>year</em>, <em>month</em>, and number of <em>passengers</em>):",
            "markdown"
        ],
        [
            "flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "code"
        ],
        [
            "With long-form data, columns in the table are given roles in the plot by explicitly assigning them to one of the variables. For example, making a monthly plot of the number of passengers per year looks like this:",
            "markdown"
        ],
        [
            "sns.relplot(data=flights, x=\"year\", y=\"passengers\", hue=\"month\", kind=\"line\")\n",
            "code"
        ],
        [
            "The advantage of long-form data is that it lends itself well to this explicit specification of the plot. It can accommodate datasets of arbitrary complexity, so long as the variables and observations can be clearly defined. But this format takes some getting used to, because it is often not the model of the data that one has in their head.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data": [
        [
            "For simple datasets, it is often more intuitive to think about data the way it might be viewed in a spreadsheet, where the columns and rows contain <em>levels</em> of different variables. For example, we can convert the flights dataset into a wide-form organization by  \u201cpivoting\u201d it so that each column has each month\u2019s time series over years:",
            "markdown"
        ],
        [
            "flights_wide = flights.pivot(index=\"year\", columns=\"month\", values=\"passengers\")\nflights_wide.head()\n",
            "code"
        ],
        [
            "Here we have the same three variables, but they are organized differently. The variables in this dataset are linked to the <em>dimensions</em> of the table, rather than to named fields. Each observation is defined by both the value at a cell in the table and the coordinates of that cell with respect to the row and column indices.",
            "markdown"
        ],
        [
            "With long-form data, we can access variables in the dataset by their name. That is not the case with wide-form data. Nevertheless, because there is a clear association between the dimensions of the table and the variable in the dataset, seaborn is able to assign those variables roles in the plot.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Seaborn treats the argument to data as wide form when neither x nor y are assigned.",
            "markdown"
        ],
        [
            "sns.relplot(data=flights_wide, kind=\"line\")\n",
            "code"
        ],
        [
            "This plot looks very  similar to the one before. Seaborn has assigned the index of the dataframe to x, the values of the dataframe to y, and it has drawn a separate line for each month. There is a notable difference between the two plots, however. When the dataset went through the \u201cpivot\u201d operation that converted it from long-form to wide-form, the information about what the values mean was lost. As a result, there is no y axis label. (The lines also have dashes here, because  has mapped the column variable to both the hue and style semantic so that the plot is more accessible. We didn\u2019t do that in the long-form case, but we could have by setting style=\"month\").",
            "markdown"
        ],
        [
            "Thus far, we did much less typing while using wide-form data and made nearly the same plot. This seems easier! But a big advantage of long-form data is that, once you have the data in the correct format, you no longer need to think about its <em>structure</em>. You can design your plots by thinking only about the variables contained within it. For example, to draw lines that represent the monthly time series for each year, simply reassign the variables:",
            "markdown"
        ],
        [
            "sns.relplot(data=flights, x=\"month\", y=\"passengers\", hue=\"year\", kind=\"line\")\n",
            "code"
        ],
        [
            "To achieve the same remapping with the wide-form dataset, we would need to transpose the table:",
            "markdown"
        ],
        [
            "sns.relplot(data=flights_wide.transpose(), kind=\"line\")\n",
            "code"
        ],
        [
            "(This example also illustrates another wrinkle, which is that seaborn currently considers the column variable in a wide-form dataset to be categorical regardless of its datatype, whereas, because the long-form variable is numeric, it is assigned a quantitative color palette and legend. This may change in the future).",
            "markdown"
        ],
        [
            "The absence of explicit variable assignments also means that each plot type needs to define a fixed mapping between the dimensions of the wide-form data and the roles in the plot. Because this natural mapping may vary across plot types, the results are less predictable when using wide-form data. For example, the  plots assign the <em>column</em> dimension of the table to x and then aggregate across the rows (ignoring the index):",
            "markdown"
        ],
        [
            "sns.catplot(data=flights_wide, kind=\"box\")\n",
            "code"
        ],
        [
            "When using pandas to represent wide-form data, you are limited to just a few variables (no more than three). This is because seaborn does not make use of multi-index information, which is how pandas represents additional variables in a tabular format. The  project offers labeled N-dimensional array objects, which can be considered a generalization of wide-form data to higher dimensions. At present, seaborn does not directly support objects from xarray, but they can be transformed into a long-form  using the to_pandas method and then plotted in seaborn like any other long-form data set.",
            "markdown"
        ],
        [
            "In summary, we can think of long-form and wide-form datasets as looking something like this:",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data": [
        [
            "Many datasets cannot be clearly interpreted using either long-form or wide-form rules. If datasets that are clearly long-form or wide-form are , we might say that these more ambiguous datasets are \u201cmessy\u201d. In a messy dataset, the variables are neither uniquely defined by the keys nor by the dimensions of the table. This often occurs with <em>repeated-measures</em> data, where it is natural to organize a table such that each row corresponds to the <em>unit</em> of data collection. Consider this simple dataset from a psychology experiment in which twenty subjects performed a memory task where they studied anagrams while their attention was either divided or focused:",
            "markdown"
        ],
        [
            "anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "code"
        ],
        [
            "The attention variable is <em>between-subjects</em>, but there is also a <em>within-subjects</em> variable: the number of possible solutions to the anagrams, which varied from 1 to 3. The dependent measure is a score of memory performance. These two variables (number and score) are jointly encoded across several columns. As a result, the whole dataset is neither clearly long-form nor clearly wide-form.",
            "markdown"
        ],
        [
            "How might we tell seaborn to plot the average score as a function of attention and number of solutions? We\u2019d first need to coerce the data into one of our two structures. Let\u2019s transform it to a tidy long-form table, such that each variable is a column and each row is an observation. We can use the method  to accomplish this task:",
            "markdown"
        ],
        [
            "anagrams_long = anagrams.melt(id_vars=[\"subidr\", \"attnr\"], var_name=\"solutions\", value_name=\"score\")\nanagrams_long.head()\n",
            "code"
        ],
        [
            "Now we can make the plot that we want:",
            "markdown"
        ],
        [
            "sns.catplot(data=anagrams_long, x=\"solutions\", y=\"score\", hue=\"attnr\", kind=\"point\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Further reading and take-home points": [
        [
            "For a longer discussion about tabular data structures, you could read the  paper by Hadley Whickham. Note that seaborn uses a slightly different set of concepts than are defined in the paper. While the paper associates tidyness with long-form structure, we have drawn a distinction between \u201ctidy wide-form\u201d data, where there is a clear mapping between variables in the dataset and the dimensions of the table, and \u201cmessy data\u201d, where no such mapping exists.",
            "markdown"
        ],
        [
            "The long-form structure has clear advantages. It allows you to create figures by explicitly assigning variables in the dataset to roles in plot, and you can do so with more than three variables. When possible, try to represent your data with a long-form structure when embarking on serious analysis. Most of the examples in the seaborn documentation will use long-form data. But in cases where it is more natural to keep the dataset wide, remember that seaborn can remain useful.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing long-form data": [
        [
            "While long-form data has a precise definition, seaborn is fairly flexible in terms of how it is actually organized across the data structures in memory. The examples in the rest of the documentation will typically use  objects and reference variables in them by assigning names of their columns to the variables in the plot. But it is also possible to store vectors in a Python dictionary or a class that implements that interface:",
            "markdown"
        ],
        [
            "flights_dict = flights.to_dict()\nsns.relplot(data=flights_dict, x=\"year\", y=\"passengers\", hue=\"month\", kind=\"line\")\n",
            "code"
        ],
        [
            "Many pandas operations, such as the split-apply-combine operations of a group-by, will produce a dataframe where information has moved from the columns of the input dataframe to the index of the output. So long as the name is retained, you can still reference the data as normal:",
            "markdown"
        ],
        [
            "flights_avg = flights.groupby(\"year\").mean()\nsns.relplot(data=flights_avg, x=\"year\", y=\"passengers\", kind=\"line\")\n",
            "code"
        ],
        [
            "/var/folders/qk/cdrdfhfn5g554pnb30pp4ylr0000gn/T/ipykernel_77263/885836857.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  flights_avg = flights.groupby(\"year\").mean()\n",
            "code"
        ],
        [
            "Additionally, it\u2019s possible to pass vectors of data directly as arguments to x, y, and other plotting variables. If these vectors are pandas objects, the name attribute will be used to label the plot:",
            "markdown"
        ],
        [
            "year = flights_avg.index\npassengers = flights_avg[\"passengers\"]\nsns.relplot(x=year, y=passengers, kind=\"line\")\n",
            "code"
        ],
        [
            "Numpy arrays and other objects that implement the Python sequence interface work too, but if they don\u2019t have names, the plot will not be as informative without further tweaking:",
            "markdown"
        ],
        [
            "sns.relplot(x=year.to_numpy(), y=passengers.to_list(), kind=\"line\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data": [
        [
            "The options for passing wide-form data are even more flexible. As with long-form data, pandas objects are preferable because the name (and, in some cases, index) information can be used. But in essence, any format that can be viewed as a single vector or a collection of vectors can be passed to data, and a valid plot can usually be constructed.",
            "markdown"
        ],
        [
            "The example we saw above used a rectangular , which can be thought of as a collection of its columns. A dict or list of pandas objects will also work, but we\u2019ll lose the axis labels:",
            "markdown"
        ],
        [
            "flights_wide_list = [col for _, col in flights_wide.items()]\nsns.relplot(data=flights_wide_list, kind=\"line\")\n",
            "code"
        ],
        [
            "The vectors in a collection do not need to have the same length. If they have an index, it will be used to align them:",
            "markdown"
        ],
        [
            "two_series = [flights_wide.loc[:1955, \"Jan\"], flights_wide.loc[1952:, \"Aug\"]]\nsns.relplot(data=two_series, kind=\"line\")\n",
            "code"
        ],
        [
            "Whereas an ordinal index will be used for numpy arrays or simple Python sequences:",
            "markdown"
        ],
        [
            "two_arrays = [s.to_numpy() for s in two_series]\nsns.relplot(data=two_arrays, kind=\"line\")\n",
            "code"
        ],
        [
            "But a dictionary of such vectors will at least use the keys:",
            "markdown"
        ],
        [
            "two_arrays_dict = {s.name: s.to_numpy() for s in two_series}\nsns.relplot(data=two_arrays_dict, kind=\"line\")\n",
            "code"
        ],
        [
            "Rectangular numpy arrays are treated just like a dataframe without index information, so they are viewed as a collection of column vectors. Note that this is different from how numpy indexing operations work, where a single indexer will access a row. But it is consistent with how pandas would turn the array into a dataframe or how matplotlib would plot it:",
            "markdown"
        ],
        [
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface": [
        [
            "The seaborn.objects namespace was introduced in version 0.12 as a completely new interface for making seaborn plots. It offers a more consistent and flexible API, comprising a collection of composable classes for transforming and plotting data. In contrast to the existing seaborn functions, the new interface aims to support end-to-end plot specification and customization without dropping down to matplotlib (although it will remain possible to do so if necessary).",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The objects interface is currently experimental and incomplete. It is stable enough for serious use, but there certainly are some rough edges and missing features.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Specifying a plot and mapping data": [
        [
            "The objects interface should be imported with the following convention:",
            "markdown"
        ],
        [
            "import seaborn.objects as so\n",
            "code"
        ],
        [
            "The seaborn.objects namespace will provide access to all of the relevant classes. The most important is . You specify plots by instantiating a  object and calling its methods. Let\u2019s see a simple example:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n    .add(so.Dot())\n)\n",
            "code"
        ],
        [
            "This code, which produces a scatter plot, should look reasonably familiar. Just as when using , we passed a tidy dataframe (penguins) and assigned two of its columns to the x and y coordinates of the plot. But instead of starting with the type of chart and then adding some data assignments, here we started with the data assignments and then added a graphical element.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Specifying a plot and mapping data->Setting properties": [
        [
            "The  class is an example of a : an object that graphically represents data values. Each mark will have a number of properties that can be set to change its appearance:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n    .add(so.Dot(color=\"g\", pointsize=4))\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Specifying a plot and mapping data->Mapping properties": [
        [
            "As with seaborn\u2019s functions, it is also possible to <em>map</em> data values to various graphical properties:",
            "markdown"
        ],
        [
            "(\n    so.Plot(\n        penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n        color=\"species\", pointsize=\"body_mass_g\",\n    )\n    .add(so.Dot())\n)\n",
            "code"
        ],
        [
            "While this basic functionality is not novel, an important difference from the function API is that properties are mapped using the same parameter names that would set them directly (instead of having hue vs. color, etc.). What matters is <em>where</em> the property is defined: passing a value when you initialize  will set it directly, whereas assigning a variable when you set up the  will <em>map</em> the corresponding data.",
            "markdown"
        ],
        [
            "Beyond this difference, the objects interface also allows a much wider range of mark properties to be mapped:",
            "markdown"
        ],
        [
            "(\n    so.Plot(\n        penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n        edgecolor=\"sex\", edgewidth=\"body_mass_g\",\n    )\n    .add(so.Dot(color=\".8\"))\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Specifying a plot and mapping data->Defining groups": [
        [
            "The  mark represents each data point independently, so the assignment of a variable to a property only has the effect of changing each dot\u2019s appearance. For marks that group or connect observations, such as , it also determines the number of distinct graphical elements:",
            "markdown"
        ],
        [
            "(\n    so.Plot(healthexp, x=\"Year\", y=\"Life_Expectancy\", color=\"Country\")\n    .add(so.Line())\n)\n",
            "code"
        ],
        [
            "It is also possible to define a grouping without changing any visual properties, by using group:",
            "markdown"
        ],
        [
            "(\n    so.Plot(healthexp, x=\"Year\", y=\"Life_Expectancy\", group=\"Country\")\n    .add(so.Line())\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Transforming data before plotting->Statistical transformation": [
        [
            "As with many seaborn functions, the objects interface supports statistical transformations. These are performed by  objects, such as :",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\")\n    .add(so.Bar(), so.Agg())\n)\n",
            "code"
        ],
        [
            "In the function interface, statistical transformations are possible with some visual representations (e.g. ) but not others (e.g. ). The objects interface more cleanly separates representation and transformation, allowing you to compose  and  objects:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\")\n    .add(so.Dot(pointsize=10), so.Agg())\n)\n",
            "code"
        ],
        [
            "When forming groups by mapping properties, the  transformation is applied to each group separately:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\", color=\"sex\")\n    .add(so.Dot(pointsize=10), so.Agg())\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Transforming data before plotting->Resolving overplotting": [
        [
            "Some seaborn functions also have mechanisms that automatically resolve overplotting, as when  \u201cdodges\u201d bars once hue is assigned. The objects interface has less complex default behavior. Bars representing multiple groups will overlap by default:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\", color=\"sex\")\n    .add(so.Bar(), so.Agg())\n)\n",
            "code"
        ],
        [
            "Nevertheless, it is possible to compose the  mark with the  stat and a second transformation, implemented by :",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\", color=\"sex\")\n    .add(so.Bar(), so.Agg(), so.Dodge())\n)\n",
            "code"
        ],
        [
            "The  class is an example of a  transformation, which is like a  but only adjusts x and y coordinates. The  classes can be applied with any mark, and it\u2019s not necessary to use a  first:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\", color=\"sex\")\n    .add(so.Dot(), so.Dodge())\n)\n",
            "code"
        ],
        [
            "It\u2019s also possible to apply multiple  operations in sequence:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\", color=\"sex\")\n    .add(so.Dot(), so.Dodge(), so.Jitter(.3))\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Transforming data before plotting->Creating variables through transformation": [
        [
            "The  stat requires both x and y to already be defined, but variables can also be <em>created</em> through statistical transformation. For example, the  stat requires only one of x <em>or</em> y to be defined, and it will create the other by counting observations:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\")\n    .add(so.Bar(), so.Hist())\n)\n",
            "code"
        ],
        [
            "The  stat will also create new x values (by binning) when given numeric data:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"flipper_length_mm\")\n    .add(so.Bars(), so.Hist())\n)\n",
            "code"
        ],
        [
            "Notice how we used , rather than  for the plot with the continuous x axis. These two marks are related, but  has different defaults and works better for continuous histograms. It also produces a different, more efficient matplotlib artist. You will find the pattern of singular/plural marks elsewhere. The plural version is typically optimized for cases with larger numbers of marks.",
            "markdown"
        ],
        [
            "Some transforms accept both x and y, but add <em>interval</em> data for each coordinate. This is particularly relevant for plotting error bars after aggregating:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"body_mass_g\", y=\"species\", color=\"sex\")\n    .add(so.Range(), so.Est(errorbar=\"sd\"), so.Dodge())\n    .add(so.Dot(), so.Agg(), so.Dodge())\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Transforming data before plotting->Orienting marks and transforms": [
        [
            "When aggregating, dodging, and drawing a bar, the x and y variables are treated differently. Each operation has the concept of an <em>orientation</em>. The  tries to determine the orientation automatically based on the data types of the variables. For instance, if we flip the assignment of species and body_mass_g, we\u2019ll get the same plot, but oriented horizontally:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"body_mass_g\", y=\"species\", color=\"sex\")\n    .add(so.Bar(), so.Agg(), so.Dodge())\n)\n",
            "code"
        ],
        [
            "Sometimes, the correct orientation is ambiguous, as when both the x and y variables are numeric. In these cases, you can be explicit by passing the orient parameter to :",
            "markdown"
        ],
        [
            "(\n    so.Plot(tips, x=\"total_bill\", y=\"size\", color=\"time\")\n    .add(so.Bar(), so.Agg(), so.Dodge(), orient=\"y\")\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Building and displaying the plot": [
        [
            "Each example thus far has produced a single subplot with a single kind of mark on it. But  does not limit you to this.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Building and displaying the plot->Adding multiple layers": [
        [
            "More complex single-subplot graphics can be created by calling  repeatedly. Each time it is called, it defines a <em>layer</em> in the plot. For example, we may want to add a scatterplot (now using ) and then a regression fit:",
            "markdown"
        ],
        [
            "(\n    so.Plot(tips, x=\"total_bill\", y=\"tip\")\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit())\n)\n",
            "code"
        ],
        [
            "Variable mappings that are defined in the  constructor will be used for all layers:",
            "markdown"
        ],
        [
            "(\n    so.Plot(tips, x=\"total_bill\", y=\"tip\", color=\"time\")\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit())\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Building and displaying the plot->Layer-specific mappings": [
        [
            "You can also define a mapping such that it is used only in a specific layer. This is accomplished by defining the mapping within the call to  for the relevant layer:",
            "markdown"
        ],
        [
            "(\n    so.Plot(tips, x=\"total_bill\", y=\"tip\")\n    .add(so.Dots(), color=\"time\")\n    .add(so.Line(color=\".2\"), so.PolyFit())\n)\n",
            "code"
        ],
        [
            "Alternatively, define the layer for the entire plot, but <em>remove</em> it from a specific layer by setting the variable to None:",
            "markdown"
        ],
        [
            "(\n    so.Plot(tips, x=\"total_bill\", y=\"tip\", color=\"time\")\n    .add(so.Dots())\n    .add(so.Line(color=\".2\"), so.PolyFit(), color=None)\n)\n",
            "code"
        ],
        [
            "To recap, there are three ways to specify the value of a mark property: (1) by mapping a variable in all layers, (2) by mapping a variable in a specific layer, and (3) by setting the property directy:",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Building and displaying the plot->Faceting and pairing subplots": [
        [
            "As with seaborn\u2019s figure-level functions (, , etc.), the  interface can also produce figures with multiple \u201cfacets\u201d, or subplots containing subsets of data. This is accomplished with the  method:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"flipper_length_mm\")\n    .facet(\"species\")\n    .add(so.Bars(), so.Hist())\n)\n",
            "code"
        ],
        [
            "Call  with the variables that should be used to define the columns and/or rows of the plot:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"flipper_length_mm\")\n    .facet(col=\"species\", row=\"sex\")\n    .add(so.Bars(), so.Hist())\n)\n",
            "code"
        ],
        [
            "You can facet using a variable with a larger number of levels by \u201cwrapping\u201d across the other dimension:",
            "markdown"
        ],
        [
            "(\n    so.Plot(healthexp, x=\"Year\", y=\"Life_Expectancy\")\n    .facet(col=\"Country\", wrap=3)\n    .add(so.Line())\n)\n",
            "code"
        ],
        [
            "All layers will be faceted unless you explicitly exclude them, which can\nbe useful for providing additional context on each subplot:",
            "markdown"
        ],
        [
            "(\n    so.Plot(healthexp, x=\"Year\", y=\"Life_Expectancy\")\n    .facet(\"Country\", wrap=3)\n    .add(so.Line(alpha=.3), group=\"Country\", col=None)\n    .add(so.Line(linewidth=3))\n)\n",
            "code"
        ],
        [
            "An alternate way to produce subplots is . Like , this draws all of the data on each subplot, using different variables for the x and/or y coordinates:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, y=\"body_mass_g\", color=\"species\")\n    .pair(x=[\"bill_length_mm\", \"bill_depth_mm\"])\n    .add(so.Dots())\n)\n",
            "code"
        ],
        [
            "You can combine faceting and pairing so long as the operations add subplots on opposite dimensions:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, y=\"body_mass_g\", color=\"species\")\n    .pair(x=[\"bill_length_mm\", \"bill_depth_mm\"])\n    .facet(row=\"sex\")\n    .add(so.Dots())\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Building and displaying the plot->Integrating with matplotlib": [
        [
            "There may be cases where you want multiple subplots to appear in a figure with a more complex structure than what  or  can provide. The current solution is to delegate figure setup to matplotlib and to supply the matplotlib object that  should use with the  method. This object can be either a , , or ; the latter is most useful for constructing bespoke subplot layouts:",
            "markdown"
        ],
        [
            "f = mpl.figure.Figure(figsize=(8, 4))\nsf1, sf2 = f.subfigures(1, 2)\n(\n    so.Plot(penguins, x=\"body_mass_g\", y=\"flipper_length_mm\")\n    .add(so.Dots())\n    .on(sf1)\n    .plot()\n)\n(\n    so.Plot(penguins, x=\"body_mass_g\")\n    .facet(row=\"sex\")\n    .add(so.Bars(), so.Hist())\n    .on(sf2)\n    .plot()\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Building and displaying the plot->Building and displaying the plot": [
        [
            "An important thing to know is that  methods clone the object they are called on and return that clone instead of updating the object in place. This means that you can define a common plot spec and then produce several variations on it.",
            "markdown"
        ],
        [
            "So, take this basic specification:",
            "markdown"
        ],
        [
            "p = so.Plot(healthexp, \"Year\", \"Spending_USD\", color=\"Country\")\n",
            "code"
        ],
        [
            "We could use it to draw a line plot:",
            "markdown"
        ],
        [
            "p.add(so.Line())\n",
            "code"
        ],
        [
            "Or perhaps a stacked area plot:",
            "markdown"
        ],
        [
            "p.add(so.Area(), so.Stack())\n",
            "code"
        ],
        [
            "The  methods are fully declarative. Calling them updates the plot spec, but it doesn\u2019t actually do any plotting. One consequence of this is that methods can be called in any order, and many of them can be called multiple times.",
            "markdown"
        ],
        [
            "When does the plot actually get rendered?  is optimized for use in notebook environments. The rendering is automatically triggered when the  gets displayed in the Jupyter REPL. That\u2019s why we didn\u2019t see anything in the example above, where we defined a  but assigned it to p rather than letting it return out to the REPL.",
            "markdown"
        ],
        [
            "To see a plot in a notebook, either return it from the final line of a cell or call Jupyter\u2019s built-in display function on the object. The notebook integration bypasses  entirely, but you can use its figure-display machinery in other contexts by calling .",
            "markdown"
        ],
        [
            "You can also save the plot to a file (or buffer) by calling .",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Customizing the appearance": [
        [
            "The new interface aims to support a deep amount of customization through , reducing the need to switch gears and use matplotlib functionality directly. (But please be patient; not all of the features needed to achieve this goal have been implemented!)",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Customizing the appearance->Parameterizing scales": [
        [
            "All of the data-dependent properties are controlled by the concept of a  and the  method. This method accepts several different types of arguments. One possibility, which is closest to the use of scales in matplotlib, is to pass the name of a function that transforms the coordinates:",
            "markdown"
        ],
        [
            "(\n    so.Plot(diamonds, x=\"carat\", y=\"price\")\n    .add(so.Dots())\n    .scale(y=\"log\")\n)\n",
            "code"
        ],
        [
            " can also control the mappings for semantic properties like color. You can directly pass it any argument that you would pass to the palette parameter in seaborn\u2019s function interface:",
            "markdown"
        ],
        [
            "(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"clarity\")\n    .add(so.Dots())\n    .scale(color=\"flare\")\n)\n",
            "code"
        ],
        [
            "Another option is to provide a tuple of (min, max) values, controlling the range that the scale should map into. This works both for numeric properties and for colors:",
            "markdown"
        ],
        [
            "(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"clarity\", pointsize=\"carat\")\n    .add(so.Dots())\n    .scale(color=(\"#88c\", \"#555\"), pointsize=(2, 10))\n)\n",
            "code"
        ],
        [
            "For additional control, you can pass a  object. There are several different types of , each with appropriate parameters. For example,  lets you define the input domain (norm), the output range (values), and the function that maps between them (trans), while  allows you to specify an ordering:",
            "markdown"
        ],
        [
            "(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"carat\", marker=\"cut\")\n    .add(so.Dots())\n    .scale(\n        color=so.Continuous(\"crest\", norm=(0, 3), trans=\"sqrt\"),\n        marker=so.Nominal([\"o\", \"+\", \"x\"], order=[\"Ideal\", \"Premium\", \"Good\"]),\n    )\n)\n",
            "code"
        ],
        [
            "/Users/mwaskom/code/seaborn/seaborn/_core/properties.py:370: RuntimeWarning: invalid value encountered in cast\n  ixs = np.asarray(x, np.intp)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Customizing the appearance->Customizing legends and ticks": [
        [
            "The  objects are also how you specify which values should appear as tick labels / in the legend, along with how they appear. For example, the  method lets you control the density or locations of the ticks, and the  method lets you modify the format:",
            "markdown"
        ],
        [
            "(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"carat\")\n    .add(so.Dots())\n    .scale(\n        x=so.Continuous().tick(every=0.5),\n        y=so.Continuous().label(like=\"${x:.0f}\"),\n        color=so.Continuous().tick(at=[1, 2, 3, 4]),\n    )\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Customizing the appearance->Customizing limits, labels, and titles": [
        [
            " has a number of methods for simple customization, including , , and :",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"body_mass_g\", y=\"species\", color=\"island\")\n    .facet(col=\"sex\")\n    .add(so.Dot(), so.Jitter(.5))\n    .share(x=False)\n    .limit(y=(2.5, -.5))\n    .label(\n        x=\"Body mass (g)\", y=\"\",\n        color=str.capitalize,\n        title=\"{} penguins\".format,\n    )\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->The seaborn.objects interface->Customizing the appearance->Theme customization": [
        [
            "Finally,  supports data-independent theming through the  method. Currently, this method accepts a dictionary of matplotlib rc parameters. You can set them directly and/or pass a package of parameters from seaborn\u2019s theming functions:",
            "markdown"
        ],
        [
            "from seaborn import axes_style\nso.Plot().theme({**axes_style(\"whitegrid\"), \"grid.linestyle\": \":\"})\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Coordinate properties->x, y, xmin, xmax, ymin, ymax": [
        [
            "Coordinate properties determine where a mark is drawn on a plot. Canonically, the x coordinate is the horizontal positon and the y coordinate is the vertical position. Some marks accept a span (i.e., min, max) parameterization for one or both variables. Others may accept x and y but also use a baseline parameter to show a span. The layer\u2019s orient parameter determines how this works.",
            "markdown"
        ],
        [
            "If a variable does not contain numeric data, its scale will apply a conversion so that data can be drawn on a screen. For instance, Nominal scales assign an integer index to each distinct category, and Temporal scales represent dates as the number of days from a reference \u201cepoch\u201d:",
            "markdown"
        ],
        [
            "A Continuous scale can also apply a nonlinear transform between data values and spatial positions:",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Color properties->color, fillcolor, edgecolor": [
        [
            "All marks can be given a color, and many distinguish between the color of the mark\u2019s \u201cedge\u201d and \u201cfill\u201d. Often, simply using color will set both, while the more-specific properties allow further control:",
            "markdown"
        ],
        [
            "When the color property is mapped, the default palette depends on the type of scale. Nominal scales use discrete, unordered hues, while continuous scales (including temporal ones) use a sequential gradient:",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The default continuous scale is subject to change in future releases to improve discriminability.",
            "markdown"
        ],
        [
            "Color scales are parameterized by the name of a palette, such as 'viridis', 'rocket', or 'deep'. Some palette names can include parameters, including simple gradients (e.g. 'dark:blue') or the cubehelix system (e.g. 'ch:start=.2,rot=-.4`). See the  for guidance on making an appropriate selection.",
            "markdown"
        ],
        [
            "Continuous scales can also be parameterized by a tuple of colors that the scale should interpolate between. When using a nominal scale, it is possible to provide either the name of the palette (which will be discretely-sampled, if necessary), a list of individual color values, or a dictionary directly mapping data values to colors.",
            "markdown"
        ],
        [
            "Individual colors may be specified . These include indexed references to the current color cycle ('C0'), single-letter shorthands ('b'), grayscale values ('.4'), RGB hex codes ('#4c72b0'), X11 color names ('seagreen'), and XKCD color survey names ('purpleish'):",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Color properties->alpha, fillalpha, edgealpha": [
        [
            "The alpha property determines the mark\u2019s opacity. Lowering the alpha can be helpful for representing density in the case of overplotting:",
            "markdown"
        ],
        [
            "Mapping the alpha property can also be useful even when marks do not overlap because it conveys a sense of importance and can be combined with a color scale to represent two variables. Moreover, colors with lower alpha appear less saturated, which can improve the appearance of larger filled marks (such as bars).",
            "markdown"
        ],
        [
            "As with color, some marks define separate edgealpha and fillalpha properties for additional control.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Style properties->fill": [
        [
            "The fill property is relevant to marks with a distinction between the edge and interior and determines whether the interior is visible. It is a boolean state: fill can be set only to True or False:",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Style properties->marker": [
        [
            "The marker property is relevant for dot marks and some line marks. The API for specifying markers is very flexible, as detailed in the matplotlib API docs: .",
            "markdown"
        ],
        [
            "Markers can be specified using a number of simple string codes:",
            "markdown"
        ],
        [
            "They can also be programatically generated using a (num_sides, fill_style, angle) tuple:",
            "markdown"
        ],
        [
            "See the matplotlib docs for additional formats, including mathtex character codes ('$...$') and arrays of vertices.",
            "markdown"
        ],
        [
            "A marker property is always mapped with a nominal scale; there is no inherent ordering to the different shapes. If no scale is provided, the plot will programmatically generate a suitably large set of unique markers:",
            "markdown"
        ],
        [
            "While this ensures that the shapes are technically distinct, bear in mind that \u2014 in most cases \u2014\u00a0it will be difficult to tell the markers apart if more than a handful are used in a single plot.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The default marker scale is subject to change in future releases to improve discriminability.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Style properties->linestyle, edgestyle": [
        [
            "The linestyle property is relevant to line marks, and the edgestyle propety is relevant to a number of marks with \u201cedges. Both properties determine the \u201cdashing\u201d of a line in terms of on-off segments.",
            "markdown"
        ],
        [
            "Dashes can be specified with a small number of shorthand codes ('-', '--', '-.', and ':') or programatically using (on, off, ...) tuples. In the tuple specification, the unit is equal to the linewidth:",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Size properties->pointsize": [
        [
            "The pointsize property is relevant to dot marks and to line marks that can show markers at individual data points. The units correspond to the diameter of the mark in points.",
            "markdown"
        ],
        [
            "The pointsize scales with the square root of the data by default so that magnitude is represented by diameter rather than area:",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Size properties->linewidth": [
        [
            "The linewidth property is relevant to line marks and determines their thickness. The value should be non-negative and has point units:",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Size properties->edgewidth": [
        [
            "The edgewidth property is akin to linewidth but applies to marks with an edge/fill rather than to lines. It also has a different default range when used in a scale. The units are the same:",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Size properties->stroke": [
        [
            "The stroke property is akin to edgewidth but applies when a dot mark is defined by its stroke rather than its fill. It also has a slightly different default scale range, but otherwise behaves similarly:",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Text properties->halign, valign": [
        [
            "The halign and valign properties control the <em>horizontal</em> and <em>vertical</em> alignment of text marks. The options for horizontal alignment are 'left', 'right', and 'center', while the options for vertical alignment are 'top', 'bottom', 'center', 'baseline', and 'center_baseline'.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Text properties->fontsize": [
        [
            "The fontsize property controls the size of textual marks. The value has point units:",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Text properties->offset": [
        [
            "The offset property controls the spacing between a text mark and its anchor position. It applies when <em>not</em> using center alignment (i.e., when using left/right or top/bottom). The value has point units.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Other properties->text": [
        [
            "The text property is used to set the content of a textual mark. It is always used literally (not mapped), and cast to string when necessary.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Properties of Mark objects->Other properties->group": [
        [
            "The group property is special in that it does not change anything about the mark\u2019s appearance but defines additional data subsets that transforms should operate on independently.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing statistical relationships": [
        [
            "Statistical analysis is a process of understanding how variables in a dataset relate to each other and how those relationships depend on other variables. Visualization can be a core component of this process because, when data are visualized properly, the human visual system can see trends and patterns that indicate a relationship.",
            "markdown"
        ],
        [
            "We will discuss three seaborn functions in this tutorial. The one we will use most is . This is a  for visualizing statistical relationships using two common approaches: scatter plots and line plots.  combines a  with one of two axes-level functions:",
            "markdown"
        ],
        [
            " (with kind=\"scatter\"; the default)",
            "markdown"
        ],
        [
            " (with kind=\"line\")",
            "markdown"
        ],
        [
            "As we will see, these functions can be quite illuminating because they use simple and easily-understood representations of data that can nevertheless represent complex dataset structures. They can do so because they plot two-dimensional graphics that can be enhanced by mapping up to three additional variables using the semantics of hue, size, and style.",
            "markdown"
        ],
        [
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots": [
        [
            "The scatter plot is a mainstay of statistical visualization. It depicts the joint distribution of two variables using a cloud of points, where each point represents an observation in the dataset. This depiction allows the eye to infer a substantial amount of information about whether there is any meaningful relationship between them.",
            "markdown"
        ],
        [
            "There are several ways to draw a scatter plot in seaborn. The most basic, which should be used when both variables are numeric, is the  function. In the , we will see specialized tools for using scatterplots to visualize categorical data. The  is the default kind in  (it can also be forced by setting kind=\"scatter\"):",
            "markdown"
        ],
        [
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "code"
        ],
        [
            "While the points are plotted in two dimensions, another dimension can be added to the plot by coloring the points according to a third variable. In seaborn, this is referred to as using a \u201chue semantic\u201d, because the color of the point gains meaning:",
            "markdown"
        ],
        [
            "sns.relplot(data=tips, x=\"total_bill\", y=\"tip\", hue=\"smoker\")\n",
            "code"
        ],
        [
            "To emphasize the difference between the classes, and to improve accessibility, you can use a different marker style for each class:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", hue=\"smoker\", style=\"smoker\"\n)\n",
            "code"
        ],
        [
            "It\u2019s also possible to represent four variables by changing the hue and style of each point independently. But this should be done carefully, because the eye is much less sensitive to shape than to color:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", hue=\"smoker\", style=\"time\",\n)\n",
            "code"
        ],
        [
            "In the examples above, the hue semantic was categorical, so the default  was applied. If the hue semantic is numeric (specifically, if it can be cast to float), the default coloring switches to a sequential palette:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=tips, x=\"total_bill\", y=\"tip\", hue=\"size\",\n)\n",
            "code"
        ],
        [
            "In both cases, you can customize the color palette. There are many options for doing so. Here, we customize a sequential palette using the string interface to :",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\",\n    hue=\"size\", palette=\"ch:r=-.5,l=.75\"\n)\n",
            "code"
        ],
        [
            "The third kind of semantic variable changes the size of each point:",
            "markdown"
        ],
        [
            "sns.relplot(data=tips, x=\"total_bill\", y=\"tip\", size=\"size\")\n",
            "code"
        ],
        [
            "Unlike with , the literal value of the variable is not used to pick the area of the point. Instead, the range of values in data units is normalized into a range in area units. This range can be customized:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=tips, x=\"total_bill\", y=\"tip\",\n    size=\"size\", sizes=(15, 200)\n)\n",
            "code"
        ],
        [
            "More examples for customizing how the different semantics are used to show statistical relationships are shown in the  API examples.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots": [
        [
            "Scatter plots are highly effective, but there is no universally optimal type of visualisation. Instead, the visual representation should be adapted for the specifics of the dataset and to the question you are trying to answer with the plot.",
            "markdown"
        ],
        [
            "With some datasets, you may want to understand changes in one variable as a function of time, or a similarly continuous variable. In this situation, a good choice is to draw a line plot. In seaborn, this can be accomplished by the  function, either directly or with  by setting kind=\"line\":",
            "markdown"
        ],
        [
            "dowjones = sns.load_dataset(\"dowjones\")\nsns.relplot(data=dowjones, x=\"Date\", y=\"Price\", kind=\"line\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty": [
        [
            "More complex datasets will have multiple measurements for the same value of the x variable. The default behavior in seaborn is to aggregate the multiple measurements at each x value by plotting the mean and the 95% confidence interval around the mean:",
            "markdown"
        ],
        [
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "code"
        ],
        [
            "The confidence intervals are computed using bootstrapping, which can be time-intensive for larger datasets. It\u2019s therefore possible to disable them:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", errorbar=None,\n)\n",
            "code"
        ],
        [
            "Another good option, especially with larger data, is to represent the spread of the distribution at each timepoint by plotting the standard deviation instead of a confidence interval:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", errorbar=\"sd\",\n)\n",
            "code"
        ],
        [
            "To turn off aggregation altogether, set the estimator parameter to None This might produce a strange effect when the data have multiple observations at each point.",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\",\n    estimator=None,\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Plotting subsets of data with semantic mappings": [
        [
            "The  function has the same flexibility as : it can show up to three additional variables by modifying the hue, size, and style of the plot elements. It does so using the same API as , meaning that we don\u2019t need to stop and think about the parameters that control the look of lines vs. points in matplotlib.",
            "markdown"
        ],
        [
            "Using semantics in  will also determine how the data get aggregated. For example, adding a hue semantic with two levels splits the plot into two lines and error bands, coloring each to indicate which subset of the data they correspond to.",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"event\",\n)\n",
            "code"
        ],
        [
            "Adding a style semantic to a line plot changes the pattern of dashes in the line by default:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\",\n    hue=\"region\", style=\"event\",\n)\n",
            "code"
        ],
        [
            "But you can identify subsets by the markers used at each observation, either together with the dashes or instead of them:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"region\", style=\"event\",\n    dashes=False, markers=True,\n)\n",
            "code"
        ],
        [
            "As with scatter plots, be cautious about making line plots using multiple semantics. While sometimes informative, they can also be difficult to parse and interpret. But even when you are only examining changes across one additional variable, it can be useful to alter both the color and style of the lines. This can make the plot more accessible when printed to black-and-white or viewed by someone with color blindness:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"event\", style=\"event\",\n)\n",
            "code"
        ],
        [
            "When you are working with repeated measures data (that is, you have units that were sampled multiple times), you can also plot each sampling unit separately without distinguishing them through semantics. This avoids cluttering the legend:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri.query(\"event == 'stim'\"), kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"region\",\n    units=\"subject\", estimator=None,\n)\n",
            "code"
        ],
        [
            "The default colormap and handling of the legend in  also depends on whether the hue semantic is categorical or numeric:",
            "markdown"
        ],
        [
            "dots = sns.load_dataset(\"dots\").query(\"align == 'dots'\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    hue=\"coherence\", style=\"choice\",\n)\n",
            "code"
        ],
        [
            "It may happen that, even though the hue variable is numeric, it is poorly represented by a linear color scale. That\u2019s the case here, where the levels of the hue variable are logarithmically scaled. You can provide specific color values for each line by passing a list or dictionary:",
            "markdown"
        ],
        [
            "palette = sns.cubehelix_palette(light=.8, n_colors=6)\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    hue=\"coherence\", style=\"choice\", palette=palette,\n)\n",
            "code"
        ],
        [
            "Or you can alter how the colormap is normalized:",
            "markdown"
        ],
        [
            "from matplotlib.colors import LogNorm\npalette = sns.cubehelix_palette(light=.7, n_colors=6)\nsns.relplot(\n    data=dots.query(\"coherence &gt; 0\"), kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    hue=\"coherence\", style=\"choice\",\n    hue_norm=LogNorm(),\n)\n",
            "code"
        ],
        [
            "The third semantic, size, changes the width of the lines:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    size=\"coherence\", style=\"choice\",\n)\n",
            "code"
        ],
        [
            "While the size variable will typically be numeric, it\u2019s also possible to map a categorical variable with the width of the lines. Be cautious when doing so, because it will be difficult to distinguish much more than \u201cthick\u201d vs \u201cthin\u201d lines. However, dashes can be hard to perceive when lines have high-frequency variability, so using different widths may be more effective in that case:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    hue=\"coherence\", size=\"choice\", palette=palette,\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Controlling sorting and orientation": [
        [
            "Because  assumes that you are most often trying to draw y as a function of x, the default behavior is to sort the data by the x values before plotting. However, this can be disabled:",
            "markdown"
        ],
        [
            "healthexp = sns.load_dataset(\"healthexp\").sort_values(\"Year\")\nsns.relplot(\n    data=healthexp, kind=\"line\",\n    x=\"Spending_USD\", y=\"Life_Expectancy\", hue=\"Country\",\n    sort=False\n)\n",
            "code"
        ],
        [
            "It\u2019s also possible to sort (and aggregate) along the y axis:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n     x=\"signal\", y=\"timepoint\", hue=\"event\",\n    orient=\"y\",\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing statistical relationships->Showing multiple relationships with facets": [
        [
            "We\u2019ve emphasized in this tutorial that, while these functions <em>can</em> show several semantic variables at once, it\u2019s not always effective to do so. But what about when you do want to understand how a relationship between two variables depends on more than one other variable?",
            "markdown"
        ],
        [
            "The best approach may be to make more than one plot. Because  is based on the , this is easy to do. To show the influence of an additional variable, instead of assigning it to one of the semantic roles in the plot, use it to \u201cfacet\u201d the visualization. This means that you make multiple axes and plot subsets of the data on each of them:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", hue=\"smoker\", col=\"time\",\n)\n",
            "code"
        ],
        [
            "You can also show the influence of two variables this way: one by faceting on the columns and one by faceting on the rows. As you start adding more variables to the grid, you may want to decrease the figure size. Remember that the size  is parameterized by the height and aspect ratio of <em>each facet</em>:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"subject\",\n    col=\"region\", row=\"event\", height=3,\n    estimator=None\n)\n",
            "code"
        ],
        [
            "When you want to examine effects across many levels of a variable, it can be a good idea to facet that variable on the columns and then \u201cwrap\u201d the facets into the rows:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri.query(\"region == 'frontal'\"), kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"event\", style=\"event\",\n    col=\"subject\", col_wrap=5,\n    height=3, aspect=.75, linewidth=2.5,\n)\n",
            "code"
        ],
        [
            "These visualizations, which are sometimes called \u201clattice\u201d plots or \u201csmall-multiples\u201d, are very effective because they present the data in a format that makes it easy for the eye to detect both overall patterns and deviations from those patterns. While you should make use of the flexibility afforded by  and , always try to keep in mind that several simple plots are usually more effective than one complex plot.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data": [
        [
            "An early step in any effort to analyze or model data should be to understand how the variables are distributed. Techniques for distribution visualization can provide quick answers to many important questions. What range do the observations cover? What is their central tendency? Are they heavily skewed in one direction? Is there evidence for bimodality? Are there significant outliers? Do the answers to these questions vary across subsets defined by other variables?",
            "markdown"
        ],
        [
            "The  contains several functions designed to answer questions such as these. The axes-level functions are , , , and . They are grouped together within the figure-level , , and  functions.",
            "markdown"
        ],
        [
            "There are several different approaches to visualizing a distribution, and each has its relative advantages and drawbacks. It is important to understand these factors so that you can choose the best approach for your particular aim.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms": [
        [
            "Perhaps the most common approach to visualizing a distribution is the <em>histogram</em>. This is the default approach in , which uses the same underlying code as . A histogram is a bar plot where the axis representing the data variable is divided into a set of discrete bins and the count of observations falling within each bin is shown using the height of the corresponding bar:",
            "markdown"
        ],
        [
            "penguins = sns.load_dataset(\"penguins\")\nsns.displot(penguins, x=\"flipper_length_mm\")\n",
            "code"
        ],
        [
            "This plot immediately affords a few insights about the flipper_length_mm variable. For instance, we can see that the most common flipper length is about 195 mm, but the distribution appears bimodal, so this one number does not represent the data well.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size": [
        [
            "The size of the bins is an important parameter, and using the wrong bin size can mislead by obscuring important features of the data or by creating apparent features out of random variability. By default, / choose a default bin size based on the variance of the data and the number of observations. But you should not be over-reliant on such automatic approaches, because they depend on particular assumptions about the structure of your data. It is always advisable to check that your impressions of the distribution are consistent across different bin sizes. To choose the size directly, set the binwidth parameter:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", binwidth=3)\n",
            "code"
        ],
        [
            "In other circumstances, it may make more sense to specify the <em>number</em> of bins, rather than their size:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", bins=20)\n",
            "code"
        ],
        [
            "One example of a situation where defaults fail is when the variable takes a relatively small number of integer values. In that case, the default bin width may be too small, creating awkward gaps in the distribution:",
            "markdown"
        ],
        [
            "tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n",
            "code"
        ],
        [
            "One approach would be to specify the precise bin breaks by passing an array to bins:",
            "markdown"
        ],
        [
            "sns.displot(tips, x=\"size\", bins=[1, 2, 3, 4, 5, 6, 7])\n",
            "code"
        ],
        [
            "This can also be accomplished by setting discrete=True, which chooses bin breaks that represent the unique values in a dataset with bars that are centered on their corresponding value.",
            "markdown"
        ],
        [
            "sns.displot(tips, x=\"size\", discrete=True)\n",
            "code"
        ],
        [
            "It\u2019s also possible to visualize the distribution of a categorical variable using the logic of a histogram. Discrete bins are automatically set for categorical variables, but it may also be helpful to \u201cshrink\u201d the bars slightly to emphasize the categorical nature of the axis:",
            "markdown"
        ],
        [
            "sns.displot(tips, x=\"day\", shrink=.8)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables": [
        [
            "Once you understand the distribution of a variable, the next step is often to ask whether features of that distribution differ across other variables in the dataset. For example, what accounts for the bimodal distribution of flipper lengths that we saw above?  and  provide support for conditional subsetting via the hue semantic. Assigning a variable to hue will draw a separate histogram for each of its unique values and distinguish them by color:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\")\n",
            "code"
        ],
        [
            "By default, the different histograms are \u201clayered\u201d on top of each other and, in some cases, they may be difficult to distinguish. One option is to change the visual representation of the histogram from a bar plot to a \u201cstep\u201d plot:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", element=\"step\")\n",
            "code"
        ],
        [
            "Alternatively, instead of layering each bar, they can be \u201cstacked\u201d, or moved vertically. In this plot, the outline of the full histogram will match the plot with only a single variable:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\")\n",
            "code"
        ],
        [
            "The stacked histogram emphasizes the part-whole relationship between the variables, but it can obscure other features (for example, it is difficult to determine the mode of the Adelie distribution. Another option is \u201cdodge\u201d the bars, which moves them horizontally and reduces their width. This ensures that there are no overlaps and that the bars remain comparable in terms of height. But it only works well when the categorical variable has a small number of levels:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"sex\", multiple=\"dodge\")\n",
            "code"
        ],
        [
            "Because  is a figure-level function and is drawn onto a , it is also possible to draw each individual distribution in a separate subplot by assigning the second variable to col or row rather than (or in addition to) hue. This represents the distribution of each subset well, but it makes it more difficult to draw direct comparisons:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", col=\"sex\")\n",
            "code"
        ],
        [
            "None of these approaches are perfect, and  we will soon see some alternatives to a histogram that are better-suited to the task of comparison.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics": [
        [
            "Before we do, another point to note is that, when the subsets have unequal numbers of observations, comparing their distributions in terms of counts may not be ideal. One solution is to <em>normalize</em> the counts using the stat parameter:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\")\n",
            "code"
        ],
        [
            "By default, however, the normalization is applied to the entire distribution, so this simply rescales the height of the bars. By setting common_norm=False, each subset will be normalized independently:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\", common_norm=False)\n",
            "code"
        ],
        [
            "Density normalization scales the bars so that their <em>areas</em> sum to 1. As a result, the density axis is not directly interpretable. Another option is to normalize the bars to that their <em>heights</em> sum to 1. This makes most sense when the variable is discrete, but  it is an option for all histograms:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"probability\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation": [
        [
            "A histogram aims to approximate the underlying probability density function that generated the data by binning and counting observations. Kernel density estimation (KDE) presents a different solution to the same problem. Rather than using discrete bins, a KDE plot smooths the observations with a Gaussian kernel, producing a continuous density estimate:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", kind=\"kde\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Choosing the smoothing bandwidth": [
        [
            "Much like with the bin size in the histogram, the ability of the KDE to accurately represent the data depends on the choice of smoothing bandwidth. An over-smoothed estimate might erase meaningful features, but an under-smoothed estimate can obscure the true shape within random noise. The easiest way to check the robustness of the estimate is to adjust the default bandwidth:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", kind=\"kde\", bw_adjust=.25)\n",
            "code"
        ],
        [
            "Note how the narrow bandwidth makes the bimodality much more apparent, but the curve is much less smooth. In contrast, a larger bandwidth obscures the bimodality almost completely:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", kind=\"kde\", bw_adjust=2)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Conditioning on other variables": [
        [
            "As with histograms, if you assign a hue variable, a separate density estimate will be computed for each level of that variable:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", kind=\"kde\")\n",
            "code"
        ],
        [
            "In many cases, the layered KDE is easier to interpret than the layered histogram, so it is often a good choice for the task of comparison. Many of the same options for resolving multiple distributions apply to the KDE as well, however:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", kind=\"kde\", multiple=\"stack\")\n",
            "code"
        ],
        [
            "Note how the stacked plot filled in the area between each curve by default. It is also possible to fill in the curves for single or layered densities, although the default alpha value (opacity) will be different, so that the individual densities are easier to resolve.",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", kind=\"kde\", fill=True)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls": [
        [
            "KDE plots have many advantages. Important features of the data are easy to discern (central tendency, bimodality, skew), and they afford easy comparisons between subsets. But there are also situations where KDE poorly represents the underlying data. This is because the logic of KDE assumes that the underlying distribution is smooth and unbounded. One way this assumption can fail is when a variable reflects a quantity that is naturally bounded. If there are observations lying close to the bound (for example, small values of a variable that cannot be negative), the KDE curve may extend to unrealistic values:",
            "markdown"
        ],
        [
            "sns.displot(tips, x=\"total_bill\", kind=\"kde\")\n",
            "code"
        ],
        [
            "This can be partially avoided with the cut parameter, which specifies how far the curve should extend beyond the extreme datapoints. But this influences only where the curve is drawn; the density estimate will still smooth over the range where no data can exist, causing it to be artificially low at the extremes of the distribution:",
            "markdown"
        ],
        [
            "sns.displot(tips, x=\"total_bill\", kind=\"kde\", cut=0)\n",
            "code"
        ],
        [
            "The KDE approach also fails for discrete data or when data are naturally continuous but specific values are over-represented. The important thing to keep in mind is that the KDE will <em>always show you a smooth curve</em>, even when the data themselves are not smooth. For example, consider this distribution of diamond weights:",
            "markdown"
        ],
        [
            "diamonds = sns.load_dataset(\"diamonds\")\nsns.displot(diamonds, x=\"carat\", kind=\"kde\")\n",
            "code"
        ],
        [
            "While the KDE suggests that there are peaks around specific values, the histogram reveals a much more jagged distribution:",
            "markdown"
        ],
        [
            "sns.displot(diamonds, x=\"carat\")\n",
            "code"
        ],
        [
            "As a compromise, it is possible to combine these two approaches. While in histogram mode,  (as with ) has the option of including the smoothed KDE curve (note kde=True, not kind=\"kde\"):",
            "markdown"
        ],
        [
            "sns.displot(diamonds, x=\"carat\", kde=True)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data->Empirical cumulative distributions": [
        [
            "A third option for visualizing distributions computes the \u201cempirical cumulative distribution function\u201d (ECDF). This plot draws a monotonically-increasing curve through each datapoint such that the height of the curve reflects the proportion of observations with a smaller value:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", kind=\"ecdf\")\n",
            "code"
        ],
        [
            "The ECDF plot has two key advantages. Unlike the histogram or KDE, it directly represents each datapoint. That means there is no bin size or smoothing parameter to consider. Additionally, because the curve is monotonically increasing, it is well-suited for comparing multiple distributions:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", kind=\"ecdf\")\n",
            "code"
        ],
        [
            "The major downside to the ECDF plot is that it represents the shape of the distribution less intuitively than a histogram or density curve. Consider how the bimodality of flipper lengths is immediately apparent in the histogram, but to see it in the ECDF plot, you must look for varying slopes. Nevertheless, with practice, you can learn to answer all of the important questions about a distribution by examining the ECDF, and doing so can be a powerful approach.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions": [
        [
            "All of the examples so far have considered <em>univariate</em> distributions: distributions of a single variable, perhaps conditional on a second variable assigned to hue. Assigning a second variable to y, however, will plot a <em>bivariate</em> distribution:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "code"
        ],
        [
            "A bivariate histogram bins the data within rectangles that tile the plot and then shows the count of observations within each rectangle with the fill color (analogous to a ). Similarly, a bivariate KDE plot smoothes the (x, y) observations with a 2D Gaussian. The default representation then shows the <em>contours</em> of the 2D density:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\")\n",
            "code"
        ],
        [
            "Assigning a hue variable will plot multiple heatmaps or contour sets using different colors. For bivariate histograms, this will only work well if there is minimal overlap between the conditional distributions:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\")\n",
            "code"
        ],
        [
            "The contour approach of the bivariate KDE plot lends itself better to evaluating overlap, although a plot with too many contours can get busy:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", kind=\"kde\")\n",
            "code"
        ],
        [
            "Just as with univariate plots, the choice of bin size or smoothing bandwidth will determine how well the plot represents the underlying bivariate distribution. The same parameters apply, but they can be tuned for each variable by passing a pair of values:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5))\n",
            "code"
        ],
        [
            "To aid interpretation of the heatmap, add a colorbar to show the mapping between counts and color intensity:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5), cbar=True)\n",
            "code"
        ],
        [
            "The meaning of the bivariate density contours is less straightforward. Because the density is not directly interpretable, the contours are drawn at <em>iso-proportions</em> of the density, meaning that each curve shows a level set such that some proportion <em>p</em> of the density lies below it. The <em>p</em> values are evenly spaced, with the lowest level contolled by the thresh parameter and the number controlled by levels:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", thresh=.2, levels=4)\n",
            "code"
        ],
        [
            "The levels parameter also accepts a list of values, for more control:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", levels=[.01, .05, .1, .8])\n",
            "code"
        ],
        [
            "The bivariate histogram allows one or both variables to be discrete. Plotting one discrete and one continuous variable offers another way to compare conditional univariate distributions:",
            "markdown"
        ],
        [
            "sns.displot(diamonds, x=\"price\", y=\"clarity\", log_scale=(True, False))\n",
            "code"
        ],
        [
            "In contrast, plotting two discrete variables is an easy to way show the cross-tabulation of the observations:",
            "markdown"
        ],
        [
            "sns.displot(diamonds, x=\"color\", y=\"clarity\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings": [
        [
            "Several other figure-level plotting functions in seaborn make use of the  and  functions.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions": [
        [
            "The first is , which augments a bivariate relatonal or distribution plot with the marginal distributions of the two variables. By default,  represents the bivariate distribution using  and the marginal distributions using :",
            "markdown"
        ],
        [
            "sns.jointplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "code"
        ],
        [
            "Similar to , setting a different kind=\"kde\" in  will change both the joint and marginal plots the use :",
            "markdown"
        ],
        [
            "sns.jointplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\",\n    kind=\"kde\"\n)\n",
            "code"
        ],
        [
            " is a convenient interface to the  class, which offeres more flexibility when used directly:",
            "markdown"
        ],
        [
            "g = sns.JointGrid(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\ng.plot_joint(sns.histplot)\ng.plot_marginals(sns.boxplot)\n",
            "code"
        ],
        [
            "A less-obtrusive way to show marginal distributions uses a \u201crug\u201d plot, which adds a small tick on the edge of the plot to represent each individual observation. This is built into :",
            "markdown"
        ],
        [
            "sns.displot(\n    penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n    kind=\"kde\", rug=True\n)\n",
            "code"
        ],
        [
            "And the axes-level  function can be used to add rugs on the side of any other kind of plot:",
            "markdown"
        ],
        [
            "sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions": [
        [
            "The  function offers a similar blend of joint and marginal distributions. Rather than focusing on a single relationship, however,  uses a \u201csmall-multiple\u201d approach to visualize the univariate distribution of all variables in a dataset along with all of their pairwise relationships:",
            "markdown"
        ],
        [
            "sns.pairplot(penguins)\n",
            "code"
        ],
        [
            "As with /, using the underlying  directly will afford more flexibility with only a bit more typing:",
            "markdown"
        ],
        [
            "g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing categorical data": [
        [
            "In the  we saw how to use different visual representations to show the relationship between multiple variables in a dataset. In the examples, we focused on cases where the main relationship was between two numerical variables. If one of the main variables is \u201ccategorical\u201d (divided into discrete groups) it may be helpful to use a more specialized approach to visualization.",
            "markdown"
        ],
        [
            "In seaborn, there are several different ways to visualize a relationship involving categorical data. Similar to the relationship between  and either  or , there are two ways to make these plots. There are a number of axes-level functions for plotting categorical data in different ways and a figure-level interface, , that gives unified higher-level access to them.",
            "markdown"
        ],
        [
            "It\u2019s helpful to think of the different categorical plot kinds as belonging to three different families, which we\u2019ll discuss in detail below. They are:",
            "markdown"
        ],
        [
            "Categorical scatterplots:",
            "markdown"
        ],
        [
            " (with kind=\"strip\"; the default)",
            "markdown"
        ],
        [
            " (with kind=\"swarm\")",
            "markdown"
        ],
        [
            "Categorical distribution plots:",
            "markdown"
        ],
        [
            " (with kind=\"box\")",
            "markdown"
        ],
        [
            " (with kind=\"violin\")",
            "markdown"
        ],
        [
            " (with kind=\"boxen\")",
            "markdown"
        ],
        [
            "Categorical estimate plots:",
            "markdown"
        ],
        [
            " (with kind=\"point\")",
            "markdown"
        ],
        [
            " (with kind=\"bar\")",
            "markdown"
        ],
        [
            " (with kind=\"count\")",
            "markdown"
        ],
        [
            "These families represent the data using different levels of granularity. When deciding which to use, you\u2019ll have to think about the question that you want to answer. The unified API makes it easy to switch between different kinds and see your data from several perspectives.",
            "markdown"
        ],
        [
            "In this tutorial, we\u2019ll mostly focus on the figure-level interface, . Remember that this function is a higher-level interface each of the functions above, so we\u2019ll reference them when we show each kind of plot, keeping the more verbose kind-specific API documentation at hand.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing categorical data->Categorical scatterplots": [
        [
            "The default representation of the data in  uses a scatterplot. There are actually two different categorical scatter plots in seaborn. They take different approaches to resolving the main challenge in representing categorical data with a scatter plot, which is that all of the points belonging to one category would fall on the same position along the axis corresponding to the categorical variable. The approach used by , which is the default \u201ckind\u201d in  is to adjust the positions of points on the categorical axis with a small amount of random \u201cjitter\u201d:",
            "markdown"
        ],
        [
            "tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "code"
        ],
        [
            "The jitter parameter controls the magnitude of jitter or disables it altogether:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"day\", y=\"total_bill\", jitter=False)\n",
            "code"
        ],
        [
            "The second approach adjusts the points along the categorical axis using an algorithm that prevents them from overlapping. It can give a better representation of the distribution of observations, although it only works well for relatively small datasets. This kind of plot is sometimes called a \u201cbeeswarm\u201d and is drawn in seaborn by , which is activated by setting kind=\"swarm\" in :",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"swarm\")\n",
            "code"
        ],
        [
            "Similar to the relational plots, it\u2019s possible to add another dimension to a categorical plot by using a hue semantic. (The categorical plots do not currently support size or style semantics). Each different categorical plotting function handles the hue semantic differently. For the scatter plots, it is only necessary to change the color of the points:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\", kind=\"swarm\")\n",
            "code"
        ],
        [
            "Unlike with numerical data, it is not always obvious how to order the levels of the categorical variable along its axis. In general, the seaborn categorical plotting functions try to infer the order of categories from the data. If your data have a pandas Categorical datatype, then the default order of the categories can be set there. If the variable passed to the categorical axis looks numerical, the levels will be sorted. But the data are still treated as categorical and drawn at ordinal positions on the categorical axes (specifically, at 0, 1, \u2026) even when numbers are used to label them:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips.query(\"size != 3\"), x=\"size\", y=\"total_bill\")\n",
            "code"
        ],
        [
            "The other option for choosing a default ordering is to take the levels of the category as they appear in the dataset. The ordering can also be controlled on a plot-specific basis using the order parameter. This can be important when drawing multiple categorical plots in the same figure, which we\u2019ll see more of below:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"smoker\", y=\"tip\", order=[\"No\", \"Yes\"])\n",
            "code"
        ],
        [
            "We\u2019ve referred to the idea of \u201ccategorical axis\u201d. In these examples, that\u2019s always corresponded to the horizontal axis. But it\u2019s often helpful to put the categorical variable on the vertical axis (particularly when the category names are relatively long or there are many categories). To do this, swap the assignment of variables to axes:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"total_bill\", y=\"day\", hue=\"time\", kind=\"swarm\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions": [
        [
            "As the size of the dataset grows, categorical scatter plots become limited in the information they can provide about the distribution of values within each category. When this happens, there are several approaches for summarizing the distributional information in ways that facilitate easy comparisons across the category levels.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Boxplots": [
        [
            "The first is the familiar . This kind of plot shows the three quartile values of the distribution along with extreme values. The \u201cwhiskers\u201d extend to points that lie within 1.5 IQRs of the lower and upper quartile, and then observations that fall outside this range are displayed independently. This means that each value in the boxplot corresponds to an actual observation in the data.",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"box\")\n",
            "code"
        ],
        [
            "When adding a hue semantic, the box for each level of the semantic variable is moved along the categorical axis so they don\u2019t overlap:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"smoker\", kind=\"box\")\n",
            "code"
        ],
        [
            "This behavior is called \u201cdodging\u201d and is turned on by default because it is assumed that the semantic variable is nested within the main categorical variable. If that\u2019s not the case, you can disable the dodging:",
            "markdown"
        ],
        [
            "tips[\"weekend\"] = tips[\"day\"].isin([\"Sat\", \"Sun\"])\nsns.catplot(\n    data=tips, x=\"day\", y=\"total_bill\", hue=\"weekend\",\n    kind=\"box\", dodge=False,\n)\n",
            "code"
        ],
        [
            "A related function, , draws a plot that is similar to a box plot but optimized for showing more information about the shape of the distribution. It is best suited for larger datasets:",
            "markdown"
        ],
        [
            "diamonds = sns.load_dataset(\"diamonds\")\nsns.catplot(\n    data=diamonds.sort_values(\"color\"),\n    x=\"color\", y=\"price\", kind=\"boxen\",\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Violinplots": [
        [
            "A different approach is a , which combines a boxplot with the kernel density estimation procedure described in the  tutorial:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=tips, x=\"total_bill\", y=\"day\", hue=\"sex\", kind=\"violin\",\n)\n",
            "code"
        ],
        [
            "This approach uses the kernel density estimate to provide a richer description of the distribution of values. Additionally, the quartile and whisker values from the boxplot are shown inside the violin. The downside is that, because the violinplot uses a KDE, there are some other parameters that may need tweaking, adding some complexity relative to the straightforward boxplot:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=tips, x=\"total_bill\", y=\"day\", hue=\"sex\",\n    kind=\"violin\", bw=.15, cut=0,\n)\n",
            "code"
        ],
        [
            "It\u2019s also possible to \u201csplit\u201d the violins when the hue parameter has only two levels, which can allow for a more efficient use of space:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\",\n    kind=\"violin\", split=True,\n)\n",
            "code"
        ],
        [
            "Finally, there are several options for the plot that is drawn on the interior of the violins, including ways to show each individual observation instead of the summary boxplot values:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\",\n    kind=\"violin\", inner=\"stick\", split=True, palette=\"pastel\",\n)\n",
            "code"
        ],
        [
            "It can also be useful to combine  or  with a box plot or violin plot to show each observation along with a summary of the distribution:",
            "markdown"
        ],
        [
            "g = sns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"violin\", inner=None)\nsns.swarmplot(data=tips, x=\"day\", y=\"total_bill\", color=\"k\", size=3, ax=g.ax)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing categorical data->Estimating central tendency": [
        [
            "For other applications, rather than showing the distribution within each category, you might want to show an estimate of the central tendency of the values. Seaborn has two main ways to show this information. Importantly, the basic API for these functions is identical to that for the ones discussed above.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing categorical data->Estimating central tendency->Bar plots": [
        [
            "A familiar style of plot that accomplishes this goal is a bar plot. In seaborn, the  function operates on a full dataset and applies a function to obtain the estimate (taking the mean by default). When there are multiple observations in each category, it also uses bootstrapping to compute a confidence interval around the estimate, which is plotted using error bars:",
            "markdown"
        ],
        [
            "titanic = sns.load_dataset(\"titanic\")\nsns.catplot(data=titanic, x=\"sex\", y=\"survived\", hue=\"class\", kind=\"bar\")\n",
            "code"
        ],
        [
            "The default error bars show 95% confidence intervals, but (starting in\nv0.12), it is possible to select from a number of other representations:",
            "markdown"
        ],
        [
            "sns.catplot(data=titanic, x=\"age\", y=\"deck\", errorbar=(\"pi\", 95), kind=\"bar\")\n",
            "code"
        ],
        [
            "A special case for the bar plot is when you want to show the number of observations in each category rather than computing a statistic for a second variable. This is similar to a histogram over a categorical, rather than quantitative, variable. In seaborn, it\u2019s easy to do so with the  function:",
            "markdown"
        ],
        [
            "sns.catplot(data=titanic, x=\"deck\", kind=\"count\", palette=\"ch:.25\")\n",
            "code"
        ],
        [
            "Both  and  can be invoked with all of the options discussed above, along with others that are demonstrated in the detailed documentation for each function:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=titanic, y=\"deck\", hue=\"class\", kind=\"count\",\n    palette=\"pastel\", edgecolor=\".6\",\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing categorical data->Estimating central tendency->Point plots": [
        [
            "An alternative style for visualizing the same information is offered by the  function. This function also encodes the value of the estimate with height on the other axis, but rather than showing a full bar, it plots the point estimate and confidence interval. Additionally,  connects points from the same hue category. This makes it easy to see how the main relationship is changing as a function of the hue semantic, because your eyes are quite good at picking up on differences of slopes:",
            "markdown"
        ],
        [
            "sns.catplot(data=titanic, x=\"sex\", y=\"survived\", hue=\"class\", kind=\"point\")\n",
            "code"
        ],
        [
            "While the categorical functions lack the style semantic of the relational functions, it can still be a good idea to vary the marker and/or linestyle along with the hue to make figures that are maximally accessible and reproduce well in black and white:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=titanic, x=\"class\", y=\"survived\", hue=\"sex\",\n    palette={\"male\": \"g\", \"female\": \"m\"},\n    markers=[\"^\", \"o\"], linestyles=[\"-\", \"--\"],\n    kind=\"point\"\n)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Visualizing categorical data->Showing additional dimensions": [
        [
            "Just like , the fact that  is built on a  means that it is easy to add faceting variables to visualize higher-dimensional relationships:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=tips, x=\"day\", y=\"total_bill\", hue=\"smoker\",\n    kind=\"swarm\", col=\"time\", aspect=.7,\n)\n",
            "code"
        ],
        [
            "For further customization of the plot, you can use the methods on the  object that it returns:",
            "markdown"
        ],
        [
            "g = sns.catplot(\n    data=titanic,\n    x=\"fare\", y=\"embark_town\", row=\"class\",\n    kind=\"box\", orient=\"h\",\n    sharex=False, margin_titles=True,\n    height=1.5, aspect=4,\n)\ng.set(xlabel=\"Fare\", ylabel=\"\")\ng.set_titles(row_template=\"{row_name} class\")\nfor ax in g.axes.flat:\n    ax.xaxis.set_major_formatter('${x:.0f}')\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Statistical estimation and error bars": [
        [
            "Data visualization sometimes involves a step of aggregation or estimation, where multiple data points are reduced to a summary statistic such as the mean or median. When showing a summary statistic, it is usually appropriate to add <em>error bars</em>, which provide a visual cue about how well the summary represents the underlying data points.",
            "markdown"
        ],
        [
            "Several seaborn functions will automatically calculate both summary statistics and the error bars when given a full dataset. This chapter explains how you can control what the error bars show and why you might choose each of the options that seaborn affords.",
            "markdown"
        ],
        [
            "The error bars around an estimate of central tendency can show one of two general things: either the range of uncertainty about the estimate or the spread of the underlying data around it. These measures are related: given the same sample size, estimates will be more uncertain when data has a broader spread. But uncertainty will decrease as sample sizes grow, whereas spread will not.",
            "markdown"
        ],
        [
            "In seaborn, there are two approaches for constructing each kind of error bar. One approach is parametric, using a formula that relies on assumptions about the shape of the distribution. The other approach is nonparametric, using only the data that you provide.",
            "markdown"
        ],
        [
            "Your choice is made with the errorbar parameter, which exists for each function that does estimation as part of plotting. This parameter accepts the name of the method to use and, optionally, a parameter that controls the size of the interval. The choices can be defined in a 2D taxonomy that depends on what is shown and how it is constructed:",
            "markdown"
        ],
        [
            "You will note that the size parameter is defined differently for the parametric and nonparametric approaches. For parametric error bars, it is a scalar factor that is multiplied by the statistic defining the error (standard error or standard deviation). For nonparametric error bars, it is a percentile width. This is explained further for each specific approach below.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The errorbar API described here was introduced in seaborn v0.12. In prior versions, the only options were to show a bootstrap confidence interval or a standard deviation, with the choice controlled by the ci parameter (i.e., ci=&lt;size&gt; or ci=\"sd\").",
            "markdown"
        ],
        [
            "To compare the different parameterizations, we\u2019ll use the following helper function:",
            "markdown"
        ],
        [
            "def plot_errorbars(arg, **kws):\n    np.random.seed(sum(map(ord, \"error_bars\")))\n    x = np.random.normal(0, 1, 100)\n    f, axs = plt.subplots(2, figsize=(7, 2), sharex=True, layout=\"tight\")\n    sns.pointplot(x=x, errorbar=arg, **kws, capsize=.3, ax=axs[0])\n    sns.stripplot(x=x, jitter=.3, ax=axs[1])\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Statistical estimation and error bars->Measures of data spread": [
        [
            "Error bars that represent data spread present a compact display of the distribution, using three numbers where  would use 5 or more and  would use a complicated algorithm.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Statistical estimation and error bars->Measures of data spread->Standard deviation error bars": [
        [
            "Standard deviation error bars are the simplest to explain, because the standard deviation is a familiar statistic. It is the average distance from each data point to the sample mean. By default, errorbar=\"sd\" will draw error bars at +/- 1 sd around the estimate, but the range can be increased by passing a scaling size parameter. Note that, assuming normally-distributed data, ~68% of the data will lie within one standard deviation, ~95% will lie within two, and ~99.7% will lie within three:",
            "markdown"
        ],
        [
            "plot_errorbars(\"sd\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Statistical estimation and error bars->Measures of data spread->Percentile interval error bars": [
        [
            "Percentile intervals also represent the range where some amount of the data fall, but they do so by\ncomputing those percentiles directly from your sample. By default, errorbar=\"pi\" will show a 95% interval, ranging from the 2.5 to the 97.5 percentiles. You can choose a different range by passing a size parameter, e.g., to show the inter-quartile range:",
            "markdown"
        ],
        [
            "plot_errorbars((\"pi\", 50))\n",
            "code"
        ],
        [
            "The standard deviation error bars will always be symmetrical around the estimate. This can be a problem when the data are skewed, especially if there are natural bounds (e.g., if the data represent a quantity that can only be positive). In some cases, standard deviation error bars may extend to \u201cimpossible\u201d values. The nonparametric approach does not have this problem, because it can account for asymmetrical spread and will never extend beyond the range of the data.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Statistical estimation and error bars->Measures of estimate uncertainty": [
        [
            "If your data are a random sample from a larger population, then the mean (or other estimate) will be an imperfect measure of the true population average. Error bars that show estimate uncertainty try to represent the range of likely values for the true parameter.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Statistical estimation and error bars->Measures of estimate uncertainty->Standard error bars": [
        [
            "The standard error statistic is related to the standard deviation: in fact it is just the standard deviation divided by the square root of the sample size. The default, with errorbar=\"se\", draws an interval +/-1 standard error from the mean:",
            "markdown"
        ],
        [
            "plot_errorbars(\"se\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Statistical estimation and error bars->Measures of estimate uncertainty->Confidence interval error bars": [
        [
            "The nonparametric approach to representing uncertainty uses <em>bootstrapping</em>: a procedure where the dataset is randomly resampled with replacement a number of times, and the estimate is recalculated from each resample. This procedure creates a distribution of statistics approximating the distribution of values that you could have gotten for your estimate if you had a different sample.",
            "markdown"
        ],
        [
            "The confidence interval is constructed by taking a percentile interval of the <em>bootstrap distribution</em>. By default errorbar=\"ci\" draws a 95% confidence interval:",
            "markdown"
        ],
        [
            "plot_errorbars(\"ci\")\n",
            "code"
        ],
        [
            "The seaborn terminology is somewhat specific, because a confidence interval in statistics can be parametric or nonparametric. To draw a parametric confidence interval, you scale the standard error, using a formula similar to the one mentioned above. For example, an approximate 95% confidence interval can be constructed by taking the mean +/- two standard errors:",
            "markdown"
        ],
        [
            "plot_errorbars((\"se\", 2))\n",
            "code"
        ],
        [
            "The nonparametric bootstrap has advantages similar to those of the percentile interval: it will naturally adapt to skewed and bounded data in a way that a standard error interval cannot. It is also more general. While the standard error formula is specific to the mean, error bars can be computed using the bootstrap for any estimator:",
            "markdown"
        ],
        [
            "plot_errorbars(\"ci\", estimator=\"median\")\n",
            "code"
        ],
        [
            "Bootstrapping involves randomness, and the error bars will appear slightly different each time you run the code that creates them. A few parameters control this. One sets the number of iterations (n_boot): with more iterations, the resulting intervals will be more stable. The other sets the seed for the random number generator, which will ensure identical results:",
            "markdown"
        ],
        [
            "plot_errorbars(\"ci\", n_boot=5000, seed=10)\n",
            "code"
        ],
        [
            "Because of its iterative process, bootstrap intervals can be expensive to compute, especially for large datasets. But because uncertainty decreases with sample size, it may be more informative in that case to use an error bar that represents data spread.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Statistical estimation and error bars->Measures of estimate uncertainty->Custom error bars": [
        [
            "If these recipes are not sufficient, it is also possible to pass a generic function to the errorbar parameter. This function should take a vector and produce a pair of values representing the minimum and maximum points of the interval:",
            "markdown"
        ],
        [
            "plot_errorbars(lambda x: (x.min(), x.max()))\n",
            "code"
        ],
        [
            "(In practice, you could show the full range of the data with errorbar=(\"pi\", 100) rather than the custom function shown above).",
            "markdown"
        ],
        [
            "Note that seaborn functions cannot currently draw error bars from values that have been calculated externally, although matplotlib functions can be used to add such error bars to seaborn plots.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Statistical estimation and error bars->Error bars on regression fits": [
        [
            "The preceding discussion has focused on error bars shown around parameter estimates for aggregate data. Error bars also arise in seaborn when estimating regression models to visualize relationships. Here, the error bars will be represented by a \u201cband\u201d around the regression line:",
            "markdown"
        ],
        [
            "x = np.random.normal(0, 1, 50)\ny = x * 2 + np.random.normal(0, 2, size=x.size)\nsns.regplot(x=x, y=y)\n",
            "code"
        ],
        [
            "Currently, the error bars on a regression estimate are less flexible, only showing a confidence interval with a size set through ci=. This may change in the future.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Statistical estimation and error bars->Are error bars enough?": [
        [
            "You should always ask yourself whether it\u2019s best to use a plot that displays only a summary statistic and error bar. In many cases, it isn\u2019t.",
            "markdown"
        ],
        [
            "If you are interested in questions about summaries (such as whether the mean value differs between groups or increases over time), aggregation reduces the complexity of the plot and makes those inferences easier. But in doing so, it obscures valuable information about the underlying data points, such as the shape of the distributions and the presence of outliers.",
            "markdown"
        ],
        [
            "When analyzing your own data, don\u2019t be satisfied with summary statistics. Always look at the underlying distributions too. Sometimes, it can be helpful to combine both perspectives into the same figure. Many seaborn functions can help with this task, especially those discussed in the .",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Estimating regression fits": [
        [
            "Many datasets contain multiple quantitative variables, and the goal of an analysis is often to relate those variables to each other. We  functions that can accomplish this by showing the joint distribution of two variables. It can be very helpful, though, to use statistical models to estimate a simple relationship between two noisy sets of observations. The functions discussed in this chapter will do so through the common framework of linear regression.",
            "markdown"
        ],
        [
            "In the spirit of Tukey, the regression plots in seaborn are primarily intended to add a visual guide that helps to emphasize patterns in a dataset during exploratory data analyses. That is to say that seaborn is not itself a package for statistical analysis. To obtain quantitative measures related to the fit of regression models, you should use . The goal of seaborn, however, is to make exploring a dataset through visualization quick and easy, as doing so is just as (if not more) important than exploring a dataset through tables of statistics.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models": [
        [
            "The two functions that can be used to visualize a linear fit are  and .",
            "markdown"
        ],
        [
            "In the simplest invocation, both functions draw a scatterplot of two variables, x and y, and then fit the regression model y ~ x and plot the resulting regression line and a 95% confidence interval for that regression:",
            "markdown"
        ],
        [
            "tips = sns.load_dataset(\"tips\")\nsns.regplot(x=\"total_bill\", y=\"tip\", data=tips);\n",
            "code"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"tip\", data=tips);\n",
            "code"
        ],
        [
            "These functions draw similar plots, but  is an , and  is a figure-level function. Additionally,  accepts the x and y variables in a variety of formats including simple numpy arrays,  objects, or as references to variables in a  object passed to data. In contrast,  has data as a required parameter and the x and y variables must be specified as strings. Finally, only  has hue as a parameter.",
            "markdown"
        ],
        [
            "The core functionality is otherwise similar, though, so this tutorial will focus on :.",
            "markdown"
        ],
        [
            "It\u2019s possible to fit a linear regression when one of the variables takes discrete values, however, the simple scatterplot produced by this kind of dataset is often not optimal:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"size\", y=\"tip\", data=tips);\n",
            "code"
        ],
        [
            "One option is to add some random noise (\u201cjitter\u201d) to the discrete values to make the distribution of those values more clear. Note that jitter is applied only to the scatterplot data and does not influence the regression line fit itself:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_jitter=.05);\n",
            "code"
        ],
        [
            "A second option is to collapse over the observations in each discrete bin to plot an estimate of central tendency along with a confidence interval:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_estimator=np.mean);\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Estimating regression fits->Fitting different kinds of models": [
        [
            "The simple linear regression model used above is very simple to fit, however, it is not appropriate for some kinds of datasets. The  dataset shows a few examples where simple linear regression provides an identical estimate of a relationship where simple visual inspection clearly shows differences. For example, in the first case, the linear regression is a good model:",
            "markdown"
        ],
        [
            "anscombe = sns.load_dataset(\"anscombe\")\n",
            "code"
        ],
        [
            "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'I'\"),\n           ci=None, scatter_kws={\"s\": 80});\n",
            "code"
        ],
        [
            "The linear relationship in the second dataset is the same, but the plot clearly shows that this is not a good model:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'II'\"),\n           ci=None, scatter_kws={\"s\": 80});\n",
            "code"
        ],
        [
            "In the presence of these kind of higher-order relationships,  and  can fit a polynomial regression model to explore simple kinds of nonlinear trends in the dataset:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'II'\"),\n           order=2, ci=None, scatter_kws={\"s\": 80});\n",
            "code"
        ],
        [
            "A different problem is posed by \u201coutlier\u201d observations that deviate for some reason other than the main relationship under study:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'III'\"),\n           ci=None, scatter_kws={\"s\": 80});\n",
            "code"
        ],
        [
            "In the presence of outliers, it can be useful to fit a robust regression, which uses a different loss function to downweight relatively large residuals:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'III'\"),\n           robust=True, ci=None, scatter_kws={\"s\": 80});\n",
            "code"
        ],
        [
            "When the y variable is binary, simple linear regression also \u201cworks\u201d but provides implausible predictions:",
            "markdown"
        ],
        [
            "tips[\"big_tip\"] = (tips.tip / tips.total_bill) &gt; .15\nsns.lmplot(x=\"total_bill\", y=\"big_tip\", data=tips,\n           y_jitter=.03);\n",
            "code"
        ],
        [
            "The solution in this case is to fit a logistic regression, such that the regression line shows the estimated probability of y = 1 for a given value of x:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"big_tip\", data=tips,\n           logistic=True, y_jitter=.03);\n",
            "code"
        ],
        [
            "Note that the logistic regression estimate is considerably more computationally intensive (this is true of robust regression as well). As the confidence interval around the regression line is computed using a bootstrap procedure, you may wish to turn this off for faster iteration (using ci=None).",
            "markdown"
        ],
        [
            "An altogether different approach is to fit a nonparametric regression using a . This approach has the fewest assumptions, although it is computationally intensive and so currently confidence intervals are not computed at all:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"tip\", data=tips,\n           lowess=True, line_kws={\"color\": \"C1\"});\n",
            "code"
        ],
        [
            "The  function can be a useful tool for checking whether the simple regression model is appropriate for a dataset. It fits and removes a simple linear regression and then plots the residual values for each observation. Ideally, these values should be randomly scattered around y = 0:",
            "markdown"
        ],
        [
            "sns.residplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'I'\"),\n              scatter_kws={\"s\": 80});\n",
            "code"
        ],
        [
            "If there is structure in the residuals, it suggests that simple linear regression is not appropriate:",
            "markdown"
        ],
        [
            "sns.residplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'II'\"),\n              scatter_kws={\"s\": 80});\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Estimating regression fits->Conditioning on other variables": [
        [
            "The plots above show many ways to explore the relationship between a pair of variables. Often, however, a more interesting question is \u201chow does the relationship between these two variables change as a function of a third variable?\u201d This is where the main differences between  and  appear. While  always shows a single relationship,  combines  with  to show multiple fits using hue mapping or faceting.",
            "markdown"
        ],
        [
            "The best way to separate out a relationship is to plot both levels on the same axes and to use color to distinguish them:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips);\n",
            "code"
        ],
        [
            "Unlike , it\u2019s not possible to map a distinct variable to the style properties of the scatter plot, but you can redundantly code the hue variable with marker shape:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips,\n           markers=[\"o\", \"x\"], palette=\"Set1\");\n",
            "code"
        ],
        [
            "To add another variable, you can draw multiple \u201cfacets\u201d with each level of the variable appearing in the rows or columns of the grid:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", col=\"time\", data=tips);\n",
            "code"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\",\n           col=\"time\", row=\"sex\", data=tips, height=3);\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Estimating regression fits->Plotting a regression in other contexts": [
        [
            "A few other seaborn functions use  in the context of a larger, more complex plot. The first is the  function that we introduced in the . In addition to the plot styles previously discussed,  can use  to show the linear regression fit on the joint axes by passing kind=\"reg\":",
            "markdown"
        ],
        [
            "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\");\n",
            "code"
        ],
        [
            "Using the  function with kind=\"reg\" combines  and  to show the linear relationship between variables in a dataset. Take care to note how this is different from . In the figure below, the two axes don\u2019t show the same relationship conditioned on two levels of a third variable; rather,  is used to show multiple relationships between different pairings of the variables in a dataset:",
            "markdown"
        ],
        [
            "sns.pairplot(tips, x_vars=[\"total_bill\", \"size\"], y_vars=[\"tip\"],\n             height=5, aspect=.8, kind=\"reg\");\n",
            "code"
        ],
        [
            "Conditioning on an additional categorical variable is built into both of these functions using the hue parameter:",
            "markdown"
        ],
        [
            "sns.pairplot(tips, x_vars=[\"total_bill\", \"size\"], y_vars=[\"tip\"],\n             hue=\"smoker\", height=5, aspect=.8, kind=\"reg\");\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Building structured multi-plot grids": [
        [
            "When exploring multi-dimensional data, a useful approach is to draw multiple instances of the same plot on different subsets of your dataset. This technique is sometimes called either \u201clattice\u201d or \u201ctrellis\u201d plotting, and it is related to the idea of . It allows a viewer to quickly extract a large amount of information about a complex dataset. Matplotlib offers good support for making figures with multiple axes; seaborn builds on top of this to directly link the structure of the plot to the structure of your dataset.",
            "markdown"
        ],
        [
            "The  functions are built on top of the objects discussed in this chapter of the tutorial. In most cases, you will want to work with those functions. They take care of some important bookkeeping that synchronizes the multiple plots in each grid. This chapter explains how the underlying objects work, which may be useful for advanced applications.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples": [
        [
            "The  class is useful when you want to visualize the distribution of a variable or the relationship between multiple variables separately within subsets of your dataset. A  can be drawn with up to three dimensions: row, col, and hue. The first two have obvious correspondence with the resulting array of axes; think of the hue variable as a third dimension along a depth axis, where different levels are plotted with different colors.",
            "markdown"
        ],
        [
            "Each of , , , and  use this object internally, and they return the object when they are finished so that it can be used for further tweaking.",
            "markdown"
        ],
        [
            "The class is used by initializing a  object with a dataframe and the names of the variables that will form the row, column, or hue dimensions of the grid. These variables should be categorical or discrete, and then the data at each level of the variable will be used for a facet along that axis. For example, say we wanted to examine differences between lunch and dinner in the tips dataset:",
            "markdown"
        ],
        [
            "tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "code"
        ],
        [
            "Initializing the grid like this sets up the matplotlib figure and axes, but doesn\u2019t draw anything on them.",
            "markdown"
        ],
        [
            "The main approach for visualizing data on this grid is with the  method. Provide it with a plotting function and the name(s) of variable(s) in the dataframe to plot. Let\u2019s look at the distribution of tips in each of these subsets, using a histogram:",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "code"
        ],
        [
            "This function will draw the figure and annotate the axes, hopefully producing a finished plot in one step. To make a relational plot, just pass multiple variable names. You can also provide keyword arguments, which will be passed to the plotting function:",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(tips, col=\"sex\", hue=\"smoker\")\ng.map(sns.scatterplot, \"total_bill\", \"tip\", alpha=.7)\ng.add_legend()\n",
            "code"
        ],
        [
            "There are several options for controlling the look of the grid that can be passed to the class constructor.",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(tips, row=\"smoker\", col=\"time\", margin_titles=True)\ng.map(sns.regplot, \"size\", \"total_bill\", color=\".3\", fit_reg=False, x_jitter=.1)\n",
            "code"
        ],
        [
            "Note that margin_titles isn\u2019t formally supported by the matplotlib API, and may not work well in all cases. In particular, it currently can\u2019t be used with a legend that lies outside of the plot.",
            "markdown"
        ],
        [
            "The size of the figure is set by providing the height of <em>each</em> facet, along with the aspect ratio:",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(tips, col=\"day\", height=4, aspect=.5)\ng.map(sns.barplot, \"sex\", \"total_bill\", order=[\"Male\", \"Female\"])\n",
            "code"
        ],
        [
            "The default ordering of the facets is derived from the information in the DataFrame. If the variable used to define facets has a categorical type, then the order of the categories is used. Otherwise, the facets will be in the order of appearance of the category levels. It is possible, however, to specify an ordering of any facet dimension with the appropriate *_order parameter:",
            "markdown"
        ],
        [
            "ordered_days = tips.day.value_counts().index\ng = sns.FacetGrid(tips, row=\"day\", row_order=ordered_days,\n                  height=1.7, aspect=4,)\ng.map(sns.kdeplot, \"total_bill\")\n",
            "code"
        ],
        [
            "Any seaborn color palette (i.e., something that can be passed to ) can be provided. You can also use a dictionary that maps the names of values in the hue variable to valid matplotlib colors:",
            "markdown"
        ],
        [
            "pal = dict(Lunch=\"seagreen\", Dinner=\".7\")\ng = sns.FacetGrid(tips, hue=\"time\", palette=pal, height=5)\ng.map(sns.scatterplot, \"total_bill\", \"tip\", s=100, alpha=.5)\ng.add_legend()\n",
            "code"
        ],
        [
            "If you have many levels of one variable, you can plot it along the columns but \u201cwrap\u201d them so that they span multiple rows. When doing this, you cannot use a row variable.",
            "markdown"
        ],
        [
            "attend = sns.load_dataset(\"attention\").query(\"subject &lt;= 12\")\ng = sns.FacetGrid(attend, col=\"subject\", col_wrap=4, height=2, ylim=(0, 10))\ng.map(sns.pointplot, \"solutions\", \"score\", order=[1, 2, 3], color=\".3\", errorbar=None)\n",
            "code"
        ],
        [
            "Once you\u2019ve drawn a plot using  (which can be called multiple times), you may want to adjust some aspects of the plot. There are also a number of methods on the  object for manipulating the figure at a higher level of abstraction. The most general is , and there are other more specialized methods like , which respects the fact that interior facets do not have axis labels. For example:",
            "markdown"
        ],
        [
            "with sns.axes_style(\"white\"):\n    g = sns.FacetGrid(tips, row=\"sex\", col=\"smoker\", margin_titles=True, height=2.5)\ng.map(sns.scatterplot, \"total_bill\", \"tip\", color=\"#334488\")\ng.set_axis_labels(\"Total bill (US Dollars)\", \"Tip\")\ng.set(xticks=[10, 30, 50], yticks=[2, 6, 10])\ng.figure.subplots_adjust(wspace=.02, hspace=.02)\n",
            "code"
        ],
        [
            "For even more customization, you can  work directly with the underling matplotlib Figure and Axes objects, which are stored as member attributes at figure and axes_dict, respectively. When making a figure without row or column faceting, you can also use the ax attribute to directly access the single axes.",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(tips, col=\"smoker\", margin_titles=True, height=4)\ng.map(plt.scatter, \"total_bill\", \"tip\", color=\"#338844\", edgecolor=\"white\", s=50, lw=1)\nfor ax in g.axes_dict.values():\n    ax.axline((0, 0), slope=.2, c=\".2\", ls=\"--\", zorder=0)\ng.set(xlim=(0, 60), ylim=(0, 14))\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Building structured multi-plot grids->Using custom functions": [
        [
            "You\u2019re not limited to existing matplotlib and seaborn functions when using . However, to work properly, any function you use must follow a few rules:",
            "markdown"
        ],
        [
            "It must plot onto the \u201ccurrently active\u201d matplotlib Axes. This will be true of functions in the matplotlib.pyplot namespace, and you can call  to get a reference to the current Axes if you want to work directly with its methods.",
            "markdown"
        ],
        [
            "It must accept the data that it plots in positional arguments. Internally,  will pass a Series of data for each of the named positional arguments passed to .",
            "markdown"
        ],
        [
            "It must be able to accept color and label keyword arguments, and, ideally, it will do something useful with them. In most cases, it\u2019s easiest to catch a generic dictionary of **kwargs and pass it along to the underlying plotting function.",
            "markdown"
        ],
        [
            "Let\u2019s look at minimal example of a function you can plot with. This function will just take a single vector of data for each facet:",
            "markdown"
        ],
        [
            "from scipy import stats\ndef quantile_plot(x, **kwargs):\n    quantiles, xr = stats.probplot(x, fit=False)\n    plt.scatter(xr, quantiles, **kwargs)\n\ng = sns.FacetGrid(tips, col=\"sex\", height=4)\ng.map(quantile_plot, \"total_bill\")\n",
            "code"
        ],
        [
            "If we want to make a bivariate plot, you should write the function so that it accepts the x-axis variable first and the y-axis variable second:",
            "markdown"
        ],
        [
            "def qqplot(x, y, **kwargs):\n    _, xr = stats.probplot(x, fit=False)\n    _, yr = stats.probplot(y, fit=False)\n    plt.scatter(xr, yr, **kwargs)\n\ng = sns.FacetGrid(tips, col=\"smoker\", height=4)\ng.map(qqplot, \"total_bill\", \"tip\")\n",
            "code"
        ],
        [
            "Because  accepts color and label keyword arguments and does the right thing with them, we can add a hue facet without any difficulty:",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(tips, hue=\"time\", col=\"sex\", height=4)\ng.map(qqplot, \"total_bill\", \"tip\")\ng.add_legend()\n",
            "code"
        ],
        [
            "Sometimes, though, you\u2019ll want to map a function that doesn\u2019t work the way you expect with the color and label keyword arguments. In this case, you\u2019ll want to explicitly catch them and handle them in the logic of your custom function. For example, this approach will allow use to map , which otherwise does not play well with the  API:",
            "markdown"
        ],
        [
            "def hexbin(x, y, color, **kwargs):\n    cmap = sns.light_palette(color, as_cmap=True)\n    plt.hexbin(x, y, gridsize=15, cmap=cmap, **kwargs)\n\nwith sns.axes_style(\"dark\"):\n    g = sns.FacetGrid(tips, hue=\"time\", col=\"time\", height=4)\ng.map(hexbin, \"total_bill\", \"tip\", extent=[0, 50, 0, 10]);\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships": [
        [
            " also allows you to quickly draw a grid of small subplots using the same plot type to visualize data in each. In a , each row and column is assigned to a different variable, so the resulting plot shows each pairwise relationship in the dataset. This style of plot is sometimes called a \u201cscatterplot matrix\u201d, as this is the most common way to show each relationship, but  is not limited to scatterplots.",
            "markdown"
        ],
        [
            "It\u2019s important to understand the differences between a  and a . In the former, each facet shows the same relationship conditioned on different levels of other variables. In the latter, each plot shows a different relationship (although the upper and lower triangles will have mirrored plots). Using  can give you a very quick, very high-level summary of interesting relationships in your dataset.",
            "markdown"
        ],
        [
            "The basic usage of the class is very similar to . First you initialize the grid, then you pass plotting function to a map method and it will be called on each subplot. There is also a companion function,  that trades off some flexibility for faster plotting.",
            "markdown"
        ],
        [
            "iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "code"
        ],
        [
            "It\u2019s possible to plot a different function on the diagonal to show the univariate distribution of the variable in each column. Note that the axis ticks won\u2019t correspond to the count or density axis of this plot, though.",
            "markdown"
        ],
        [
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "code"
        ],
        [
            "A very common way to use this plot colors the observations by a separate categorical variable. For example, the iris dataset has four measurements for each of three different species of iris flowers so you can see how they differ.",
            "markdown"
        ],
        [
            "g = sns.PairGrid(iris, hue=\"species\")\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\ng.add_legend()\n",
            "code"
        ],
        [
            "By default every numeric column in the dataset is used, but you can focus on particular relationships if you want.",
            "markdown"
        ],
        [
            "g = sns.PairGrid(iris, vars=[\"sepal_length\", \"sepal_width\"], hue=\"species\")\ng.map(sns.scatterplot)\n",
            "code"
        ],
        [
            "It\u2019s also possible to use a different function in the upper and lower triangles to emphasize different aspects of the relationship.",
            "markdown"
        ],
        [
            "g = sns.PairGrid(iris)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw=3, legend=False)\n",
            "code"
        ],
        [
            "The square grid with identity relationships on the diagonal is actually just a special case, and you can plot with different variables in the rows and columns.",
            "markdown"
        ],
        [
            "g = sns.PairGrid(tips, y_vars=[\"tip\"], x_vars=[\"total_bill\", \"size\"], height=4)\ng.map(sns.regplot, color=\".3\")\ng.set(ylim=(-1, 11), yticks=[0, 5, 10])\n",
            "code"
        ],
        [
            "Of course, the aesthetic attributes are configurable. For instance, you can use a different palette (say, to show an ordering of the hue variable) and pass keyword arguments into the plotting functions.",
            "markdown"
        ],
        [
            "g = sns.PairGrid(tips, hue=\"size\", palette=\"GnBu_d\")\ng.map(plt.scatter, s=50, edgecolor=\"white\")\ng.add_legend()\n",
            "code"
        ],
        [
            " is flexible, but to take a quick look at a dataset, it can be easier to use . This function uses scatterplots and histograms by default, although a few other kinds will be added (currently, you can also plot regression plots on the off-diagonals and KDEs on the diagonal).",
            "markdown"
        ],
        [
            "sns.pairplot(iris, hue=\"species\", height=2.5)\n",
            "code"
        ],
        [
            "You can also control the aesthetics of the plot with keyword arguments, and it returns the  instance for further tweaking.",
            "markdown"
        ],
        [
            "g = sns.pairplot(iris, hue=\"species\", palette=\"Set2\", diag_kind=\"kde\", height=2.5)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Controlling figure aesthetics": [
        [
            "Drawing attractive figures is important. When making figures for yourself, as you explore a dataset, it\u2019s nice to have plots that are pleasant to look at. Visualizations are also central to communicating quantitative insights to an audience, and in that setting it\u2019s even more necessary to have figures that catch the attention and draw a viewer in.",
            "markdown"
        ],
        [
            "Matplotlib is highly customizable, but it can be hard to know what settings to tweak to achieve an attractive plot. Seaborn comes with a number of customized themes and a high-level interface for controlling the look of matplotlib figures.",
            "markdown"
        ],
        [
            "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n",
            "code"
        ],
        [
            "Let\u2019s define a simple function to plot some offset sine waves, which will help us see the different stylistic parameters we can tweak.",
            "markdown"
        ],
        [
            "def sinplot(n=10, flip=1):\n    x = np.linspace(0, 14, 100)\n    for i in range(1, n + 1):\n        plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip)\n",
            "code"
        ],
        [
            "This is what the plot looks like with matplotlib defaults:",
            "markdown"
        ],
        [
            "sinplot()\n",
            "code"
        ],
        [
            "To switch to seaborn defaults, simply call the  function.",
            "markdown"
        ],
        [
            "sns.set_theme()\nsinplot()\n",
            "code"
        ],
        [
            "(Note that in versions of seaborn prior to 0.8,  was called on import. On later versions, it must be explicitly invoked).",
            "markdown"
        ],
        [
            "Seaborn splits matplotlib parameters into two independent groups. The first group sets the aesthetic style of the plot, and the second scales various elements of the figure so that it can be easily incorporated into different contexts.",
            "markdown"
        ],
        [
            "The interface for manipulating these parameters are two pairs of functions. To control the style, use the  and  functions. To scale the plot, use the  and  functions. In both cases, the first function returns a dictionary of parameters and the second sets the matplotlib defaults.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Controlling figure aesthetics->Seaborn figure styles": [
        [
            "There are five preset seaborn themes: darkgrid, whitegrid, dark, white, and ticks. They are each suited to different applications and personal preferences. The default theme is darkgrid. As mentioned above, the grid helps the plot serve as a lookup table for quantitative information, and the white-on grey helps to keep the grid from competing with lines that represent data. The whitegrid theme is similar, but it is better suited to plots with heavy data elements:",
            "markdown"
        ],
        [
            "sns.set_style(\"whitegrid\")\ndata = np.random.normal(size=(20, 6)) + np.arange(6) / 2\nsns.boxplot(data=data);\n",
            "code"
        ],
        [
            "For many plots, (especially for settings like talks, where you primarily want to use figures to provide impressions of patterns in the data), the grid is less necessary.",
            "markdown"
        ],
        [
            "sns.set_style(\"dark\")\nsinplot()\n",
            "code"
        ],
        [
            "sns.set_style(\"white\")\nsinplot()\n",
            "code"
        ],
        [
            "Sometimes you might want to give a little extra structure to the plots, which is where ticks come in handy:",
            "markdown"
        ],
        [
            "sns.set_style(\"ticks\")\nsinplot()\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines": [
        [
            "Both the white and ticks styles can benefit from removing the top and right axes spines, which are not needed. The seaborn function  can be called to remove them:",
            "markdown"
        ],
        [
            "sinplot()\nsns.despine()\n",
            "code"
        ],
        [
            "Some plots benefit from offsetting the spines away from the data, which can also be done when calling . When the ticks don\u2019t cover the whole range of the axis, the trim parameter will limit the range of the surviving spines.",
            "markdown"
        ],
        [
            "f, ax = plt.subplots()\nsns.violinplot(data=data)\nsns.despine(offset=10, trim=True);\n",
            "code"
        ],
        [
            "You can also control which spines are removed with additional arguments to :",
            "markdown"
        ],
        [
            "sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Controlling figure aesthetics->Temporarily setting figure style": [
        [
            "Although it\u2019s easy to switch back and forth, you can also use the  function in a with statement to temporarily set plot parameters. This also allows you to make figures with differently-styled axes:",
            "markdown"
        ],
        [
            "f = plt.figure(figsize=(6, 6))\ngs = f.add_gridspec(2, 2)\n\nwith sns.axes_style(\"darkgrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    sinplot(6)\n\nwith sns.axes_style(\"white\"):\n    ax = f.add_subplot(gs[0, 1])\n    sinplot(6)\n\nwith sns.axes_style(\"ticks\"):\n    ax = f.add_subplot(gs[1, 0])\n    sinplot(6)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[1, 1])\n    sinplot(6)\n\nf.tight_layout()\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Controlling figure aesthetics->Overriding elements of the seaborn styles": [
        [
            "If you want to customize the seaborn styles, you can pass a dictionary of parameters to the rc argument of  and . Note that you can only override the parameters that are part of the style definition through this method. (However, the higher-level  function takes a dictionary of any matplotlib parameters).",
            "markdown"
        ],
        [
            "If you want to see what parameters are included, you can just call the function with no arguments, which will return the current settings:",
            "markdown"
        ],
        [
            "sns.axes_style()\n",
            "code"
        ],
        [
            "You can then set different versions of these parameters:",
            "markdown"
        ],
        [
            "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\nsinplot()\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Controlling figure aesthetics->Scaling plot elements": [
        [
            "A separate set of parameters control the scale of plot elements, which should let you use the same code to make plots that are suited for use in settings where larger or smaller plots are appropriate.",
            "markdown"
        ],
        [
            "First let\u2019s reset the default parameters by calling :",
            "markdown"
        ],
        [
            "sns.set_theme()\n",
            "code"
        ],
        [
            "The four preset contexts, in order of relative size, are paper, notebook, talk, and poster. The notebook style is the default, and was used in the plots above.",
            "markdown"
        ],
        [
            "sns.set_context(\"paper\")\nsinplot()\n",
            "code"
        ],
        [
            "sns.set_context(\"talk\")\nsinplot()\n",
            "code"
        ],
        [
            "sns.set_context(\"poster\")\nsinplot()\n",
            "code"
        ],
        [
            "Most of what you now know about the style functions should transfer to the context functions.",
            "markdown"
        ],
        [
            "You can call  with one of these names to set the parameters, and you can override the parameters by providing a dictionary of parameter values.",
            "markdown"
        ],
        [
            "You can also independently scale the size of the font elements when changing the context. (This option is also available through the top-level  function).",
            "markdown"
        ],
        [
            "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\nsinplot()\n",
            "code"
        ],
        [
            "Similarly, you can temporarily control the scale of figures nested under a with statement.",
            "markdown"
        ],
        [
            "Both the style and the context can be quickly configured with the  function. This function also sets the default color palette, but that will be covered in more detail in the  of the tutorial.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes": [
        [
            "Seaborn makes it easy to use colors that are well-suited to the characteristics of your data and your visualization goals. This chapter discusses both the general principles that should guide your choices and the tools in seaborn that help you quickly find the best solution for a given application.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->General principles for using color in plots->Components of color": [
        [
            "Because of the way our eyes work, a particular color can be defined using three components. We usually program colors in a computer by specifying their RGB values, which set the intensity of the red, green, and blue channels in a display. But for analyzing the perceptual attributes of a color, it\u2019s better to think in terms of <em>hue</em>, <em>saturation</em>, and <em>luminance</em> channels.",
            "markdown"
        ],
        [
            "Hue is the component that distinguishes \u201cdifferent colors\u201d in a non-technical sense. It\u2019s property of color that leads to first-order names like \u201cred\u201d and \u201cblue\u201d:",
            "markdown"
        ],
        [
            "Saturation (or chroma) is the <em>colorfulness</em>. Two colors with different hues will look more distinct when they have more saturation:",
            "markdown"
        ],
        [
            "And lightness corresponds to how much light is emitted (or reflected, for printed colors), ranging from black to white:",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->General principles for using color in plots->Vary hue to distinguish categories": [
        [
            "When you want to represent multiple categories in a plot, you typically should vary the color of the elements. Consider this simple example: in which of these two plots is it easier to count the number of triangular points?",
            "markdown"
        ],
        [
            "In the plot on the right, the orange triangles \u201cpop out\u201d, making it easy to distinguish them from the circles. This pop-out effect happens because our visual system prioritizes color differences.",
            "markdown"
        ],
        [
            "The blue and orange colors differ mostly in terms of their hue. Hue is useful for representing categories: most people can distinguish a moderate number of hues relatively easily, and points that have different hues but similar brightness or intensity seem equally important. It also makes plots easier to talk about. Consider this example:",
            "markdown"
        ],
        [
            "Most people would be able to quickly ascertain that there are five distinct categories in the plot on the left and, if asked to characterize the \u201cblue\u201d points, would be able to do so.",
            "markdown"
        ],
        [
            "With the plot on the right, where the points are all blue but vary in their luminance and saturation, it\u2019s harder to say how many unique categories are present. And how would we talk about a particular category? \u201cThe fairly-but-not-too-blue points?\u201d What\u2019s more, the gray dots seem to fade into the background, de-emphasizing them relative to the more intense blue dots. If the categories are equally important, this is a poor representation.",
            "markdown"
        ],
        [
            "So as a general rule, use hue variation to represent categories. With that said, here are few notes of caution. If you have more than a handful of colors in your plot, it can become difficult to keep in mind what each one means, unless there are pre-existing associations between the categories and the colors used to represent them. This makes your plot harder to interpret: rather than focusing on the data, a viewer will have to continually refer to the legend to make sense of what is shown. So you should strive not to make plots that are too complex. And be mindful that not everyone sees colors the same way. Varying both shape (or some other attribute) and color can help people with anomalous color vision understand your plots, and it can keep them (somewhat) interpretable if they are printed to black-and-white.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->General principles for using color in plots->Vary luminance to represent numbers": [
        [
            "On the other hand, hue variations are not well suited to representing numeric data. Consider this example, where we need colors to represent the counts in a bivariate histogram. On the left, we use a circular colormap, where gradual changes in the number of observation within each bin correspond to gradual changes in hue. On the right, we use a palette that uses brighter colors to represent bins with larger counts:",
            "markdown"
        ],
        [
            "With the hue-based palette, it\u2019s quite difficult to ascertain the shape of the bivariate distribution. In contrast, the luminance palette makes it much more clear that there are two prominent peaks.",
            "markdown"
        ],
        [
            "Varying luminance helps you see structure in data, and changes in luminance are more intuitively processed as changes in importance. But the plot on the right does not use a grayscale colormap. Its colorfulness makes it more interesting, and the subtle hue variation increases the perceptual distance between two values. As a result, small differences slightly easier to resolve.",
            "markdown"
        ],
        [
            "These examples show that color palette choices are about more than aesthetics: the colors you choose can reveal patterns in your data if used effectively or hide them if used poorly. There is not one optimal palette, but there are palettes that are better or worse for particular datasets and visualization approaches.",
            "markdown"
        ],
        [
            "And aesthetics do matter: the more that people want to look at your figures, the greater the chance that they will learn something from them. This is true even when you are making plots for yourself. During exploratory data analysis, you may generate many similar figures. Varying the color palettes will add a sense of novelty, which keeps you engaged and prepared to notice interesting features of your data.",
            "markdown"
        ],
        [
            "So how can you choose color palettes that both represent your data well and look attractive?",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Tools for choosing color palettes": [
        [
            "The most important function for working with color palettes is, aptly, . This function provides an interface to most of the possible ways that one can generate color palettes in seaborn. And it\u2019s used internally by any function that has a palette argument.",
            "markdown"
        ],
        [
            "The primary argument to  is usually a string: either the name of a specific palette or the name of a family and additional arguments to select a specific member. In the latter case,  will delegate to more specific function, such as . It\u2019s also possible to pass a list of colors specified any way that matplotlib accepts (an RGB tuple, a hex code, or a name in the X11 table). The return value is an object that wraps a list of RGB tuples with a few useful methods, such as conversion to hex codes and a rich HTML representation.",
            "markdown"
        ],
        [
            "Calling  with no arguments will return the current default color palette that matplotlib (and most seaborn functions) will use if colors are not otherwise specified. This default palette can be set with the corresponding  function, which calls  internally and accepts the same arguments.",
            "markdown"
        ],
        [
            "To motivate the different options that  provides, it will be useful to introduce a classification scheme for color palettes. Broadly, palettes fall into one of three categories:",
            "markdown"
        ],
        [
            "qualitative palettes, good for representing categorical data",
            "markdown"
        ],
        [
            "sequential palettes, good for representing numeric data",
            "markdown"
        ],
        [
            "diverging palettes, good for representing numeric data with a categorical boundary",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Qualitative color palettes": [
        [
            "Qualitative palettes are well-suited to representing categorical data because most of their variation is in the hue component. The default color palette in seaborn is a qualitative palette with ten distinct hues:",
            "markdown"
        ],
        [
            "sns.color_palette()\n",
            "code"
        ],
        [
            "These colors have the same ordering as the default matplotlib color palette, \"tab10\", but they are a bit less intense. Compare:",
            "markdown"
        ],
        [
            "sns.color_palette(\"tab10\")\n",
            "code"
        ],
        [
            "Seaborn in fact has six variations of matplotlib\u2019s palette, called deep, muted, pastel, bright, dark, and colorblind. These span a range of average luminance and saturation values:",
            "markdown"
        ],
        [
            "Many people find the moderated hues of the default \"deep\" palette to be aesthetically pleasing, but they are also less distinct. As a result, they may be more difficult to discriminate in some contexts, which is something to keep in mind when making publication graphics.  can be helpful for estimating how the seaborn color palettes perform when simulating different forms of colorblindess.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Qualitative color palettes->Using circular color systems": [
        [
            "When you have an arbitrary number of categories, the easiest approach to finding unique hues is to draw evenly-spaced colors in a circular color space (one where the hue changes while keeping the brightness and saturation constant). This is what most seaborn functions default to when they need to use more colors than are currently set in the default color cycle.",
            "markdown"
        ],
        [
            "The most common way to do this uses the hls color space, which is a simple transformation of RGB values. We saw this color palette before as a counterexample for how to plot a histogram:",
            "markdown"
        ],
        [
            "sns.color_palette(\"hls\", 8)\n",
            "code"
        ],
        [
            "Because of the way the human visual system works, colors that have the same luminance and saturation in terms of their RGB values won\u2019t necessarily look equally intense To remedy this, seaborn provides an interface to the  system (since renamed to HSLuv), which achieves less intensity variation as you rotate around the color wheel:",
            "markdown"
        ],
        [
            "sns.color_palette(\"husl\", 8)\n",
            "code"
        ],
        [
            "When seaborn needs a categorical palette with more colors than are available in the current default, it will use this approach.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Qualitative color palettes->Using categorical Color Brewer palettes": [
        [
            "Another source of visually pleasing categorical palettes comes from the  tool (which also has sequential and diverging palettes, as we\u2019ll see below).",
            "markdown"
        ],
        [
            "sns.color_palette(\"Set2\")\n",
            "code"
        ],
        [
            "Be aware that the qualitative Color Brewer palettes have different lengths, and the default behavior of  is to give you the full list:",
            "markdown"
        ],
        [
            "sns.color_palette(\"Paired\")\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Sequential color palettes": [
        [
            "The second major class of color palettes is called \u201csequential\u201d. This kind of mapping is appropriate when data range from relatively low or uninteresting values to relatively high or interesting values (or vice versa). As we saw above, the primary dimension of variation in a sequential palette is luminance. Some seaborn functions will default to a sequential palette when you are mapping numeric data. (For historical reasons, both categorical and numeric mappings are specified with the hue parameter in functions like  or , even though numeric mappings use color palettes with relatively little hue variation).",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Sequential color palettes->Perceptually uniform palettes": [
        [
            "Because they are intended to represent numeric values, the best sequential palettes will be <em>perceptually uniform</em>, meaning that the relative discriminability of two colors is proportional to the difference between the corresponding data values. Seaborn includes four perceptually uniform sequential colormaps: \"rocket\", \"mako\", \"flare\", and \"crest\". The first two have a very wide luminance range and are well suited for applications such as heatmaps, where colors fill the space they are plotted into:",
            "markdown"
        ],
        [
            "sns.color_palette(\"rocket\", as_cmap=True)\n",
            "code"
        ],
        [
            "sns.color_palette(\"mako\", as_cmap=True)\n",
            "code"
        ],
        [
            "Because the extreme values of these colormaps approach white, they are not well-suited for coloring elements such as lines or points: it will be difficult to discriminate important values against a white or gray background. The \u201cflare\u201d and \u201ccrest\u201d colormaps are a better choice for such plots. They have a more restricted range of luminance variations, which they compensate for with a slightly more pronounced variation in hue. The default direction of the luminance ramp is also reversed, so that smaller values have lighter colors:",
            "markdown"
        ],
        [
            "sns.color_palette(\"flare\", as_cmap=True)\n",
            "code"
        ],
        [
            "sns.color_palette(\"crest\", as_cmap=True)\n",
            "code"
        ],
        [
            "It is also possible to use the perceptually uniform colormaps provided by matplotlib, such as \"magma\" and \"viridis\":",
            "markdown"
        ],
        [
            "sns.color_palette(\"magma\", as_cmap=True)\n",
            "code"
        ],
        [
            "sns.color_palette(\"viridis\", as_cmap=True)\n",
            "code"
        ],
        [
            "As with the convention in matplotlib, every continuous colormap has a reversed version, which has the suffix \"_r\":",
            "markdown"
        ],
        [
            "sns.color_palette(\"rocket_r\", as_cmap=True)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Sequential color palettes->Discrete vs. continuous mapping": [
        [
            "One thing to be aware of is that seaborn can generate discrete values from sequential colormaps and, when doing so, it will not use the most extreme values. Compare the discrete version of \"rocket\" against the continuous version shown above:",
            "markdown"
        ],
        [
            "sns.color_palette(\"rocket\")\n",
            "code"
        ],
        [
            "Internally, seaborn uses the discrete version for categorical data and the continuous version when in numeric mapping mode. Discrete sequential colormaps can be well-suited for visualizing categorical data with an intrinsic ordering, especially if there is some hue variation.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Sequential color palettes->Sequential \u201ccubehelix\u201d palettes": [
        [
            "The perceptually uniform colormaps are difficult to programmatically generate, because they are not based on the RGB color space. The  system offers an RGB-based compromise: it generates sequential palettes with a linear increase or decrease in brightness and some continuous variation in hue. While not perfectly perceptually uniform, the resulting colormaps have many good properties. Importantly, many aspects of the design process are parameterizable.",
            "markdown"
        ],
        [
            "Matplotlib has the default cubehelix version built into it:",
            "markdown"
        ],
        [
            "sns.color_palette(\"cubehelix\", as_cmap=True)\n",
            "code"
        ],
        [
            "The default palette returned by the seaborn  function is a bit different from the matplotlib default in that it does not rotate as far around the hue wheel or cover as wide a range of intensities. It also reverses the luminance ramp:",
            "markdown"
        ],
        [
            "sns.cubehelix_palette(as_cmap=True)\n",
            "code"
        ],
        [
            "Other arguments to  control how the palette looks. The two main things you\u2019ll change are the start (a value between 0 and 3) and rot, or number of rotations (an arbitrary value, but usually between -1 and 1)",
            "markdown"
        ],
        [
            "sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True)\n",
            "code"
        ],
        [
            "The more you rotate, the more hue variation you will see:",
            "markdown"
        ],
        [
            "sns.cubehelix_palette(start=.5, rot=-.75, as_cmap=True)\n",
            "code"
        ],
        [
            "You can control both how dark and light the endpoints are and their order:",
            "markdown"
        ],
        [
            "sns.cubehelix_palette(start=2, rot=0, dark=0, light=.95, reverse=True, as_cmap=True)\n",
            "code"
        ],
        [
            "The  accepts a string code, starting with \"ch:\", for generating an arbitrary cubehelix palette. You can passs the names of parameters in the string:",
            "markdown"
        ],
        [
            "sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True)\n",
            "code"
        ],
        [
            "And for compactness, each parameter can be specified with its first letter:",
            "markdown"
        ],
        [
            "sns.color_palette(\"ch:s=-.2,r=.6\", as_cmap=True)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Sequential color palettes->Custom sequential palettes": [
        [
            "For a simpler interface to custom sequential palettes, you can use  or , which are both seeded with a single color and produce a palette that ramps either from light or dark desaturated values to that color:",
            "markdown"
        ],
        [
            "sns.light_palette(\"seagreen\", as_cmap=True)\n",
            "code"
        ],
        [
            "sns.dark_palette(\"#69d\", reverse=True, as_cmap=True)\n",
            "code"
        ],
        [
            "As with cubehelix palettes, you can also specify light or dark palettes through  or anywhere palette is accepted:",
            "markdown"
        ],
        [
            "sns.color_palette(\"light:b\", as_cmap=True)\n",
            "code"
        ],
        [
            "Reverse the colormap by adding \"_r\":",
            "markdown"
        ],
        [
            "sns.color_palette(\"dark:salmon_r\", as_cmap=True)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Sequential color palettes->Sequential Color Brewer palettes": [
        [
            "The Color Brewer library also has some good options for sequential palettes. They include palettes with one primary hue:",
            "markdown"
        ],
        [
            "sns.color_palette(\"Blues\", as_cmap=True)\n",
            "code"
        ],
        [
            "Along with multi-hue options:",
            "markdown"
        ],
        [
            "sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Diverging color palettes": [
        [
            "The third class of color palettes is called \u201cdiverging\u201d. These are used for data where both large low and high values are interesting and span a midpoint value (often 0) that should be demphasized. The rules for choosing good diverging palettes are similar to good sequential palettes, except now there should be two dominant hues in the colormap, one at (or near) each pole. It\u2019s also important that the starting values are of similar brightness and saturation.",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Diverging color palettes->Perceptually uniform diverging palettes": [
        [
            "Seaborn includes two perceptually uniform diverging palettes: \"vlag\" and \"icefire\". They both use blue and red at their poles, which many intuitively processes as \u201ccold\u201d and \u201chot\u201d:",
            "markdown"
        ],
        [
            "sns.color_palette(\"vlag\", as_cmap=True)\n",
            "code"
        ],
        [
            "sns.color_palette(\"icefire\", as_cmap=True)\n",
            "code"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Diverging color palettes->Custom diverging palettes": [
        [
            "You can also use the seaborn function  to create a custom colormap for diverging data. This function makes diverging palettes using the husl color system. You pass it two hues (in degrees) and, optionally, the lightness and saturation values for the extremes. Using husl means that the extreme values, and the resulting ramps to the midpoint, while not perfectly perceptually uniform, will be well-balanced:",
            "markdown"
        ],
        [
            "sns.diverging_palette(220, 20, as_cmap=True)\n",
            "code"
        ],
        [
            "This is convenient when you want to stray from the boring confines of cold-hot approaches:",
            "markdown"
        ],
        [
            "sns.diverging_palette(145, 300, s=60, as_cmap=True)\n",
            "code"
        ],
        [
            "It\u2019s also possible to make a palette where the midpoint is dark rather than light:",
            "markdown"
        ],
        [
            "sns.diverging_palette(250, 30, l=65, center=\"dark\", as_cmap=True)\n",
            "code"
        ],
        [
            "It\u2019s important to emphasize here that using red and green, while intuitive, .",
            "markdown"
        ]
    ],
    "seaborn->User guide and tutorial->Choosing color palettes->Diverging color palettes->Other diverging palettes": [
        [
            "There are a few other good diverging palettes built into matplotlib, including Color Brewer palettes:",
            "markdown"
        ],
        [
            "sns.color_palette(\"Spectral\", as_cmap=True)\n",
            "code"
        ],
        [
            "And the coolwarm palette, which has less contrast between the middle values and the extremes:",
            "markdown"
        ],
        [
            "sns.color_palette(\"coolwarm\", as_cmap=True)\n",
            "code"
        ],
        [
            "As you can see, there are many options for using color in your visualizations. Seaborn tries both to use good defaults and to offer a lot of flexibility.",
            "markdown"
        ],
        [
            "This discussion is only the beginning, and there are a number of good resources for learning more about techniques for using color in visualizations. One great example is this  from the NASA Earth Observatory. The matplotlib docs also have a  that illustrates some of the perceptual properties of their colormaps.",
            "markdown"
        ]
    ],
    "seaborn->API Overview->Overview of seaborn plotting functions": [
        [
            "Most of your interactions with seaborn will happen through a set of plotting functions. Later chapters in the tutorial will explore the specific features offered by each function. This chapter will introduce, at a high-level, the different kinds of functions that you will encounter.",
            "markdown"
        ]
    ],
    "seaborn->API Overview->Overview of seaborn plotting functions->Similar functions for similar tasks": [
        [
            "The seaborn namespace is flat; all of the functionality is accessible at the top level. But the code itself is hierarchically structured, with modules of functions that achieve similar visualization goals through different means. Most of the docs are structured around these modules: you\u2019ll encounter names like \u201crelational\u201d, \u201cdistributional\u201d, and \u201ccategorical\u201d.",
            "markdown"
        ],
        [
            "For example, the  defines functions that specialize in representing the distribution of datapoints. This includes familiar methods like the histogram:",
            "markdown"
        ],
        [
            "penguins = sns.load_dataset(\"penguins\")\nsns.histplot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\")\n",
            "code"
        ],
        [
            "Along with similar, but perhaps less familiar, options such as kernel density estimation:",
            "markdown"
        ],
        [
            "sns.kdeplot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\")\n",
            "code"
        ],
        [
            "Functions within a module share a lot of underlying code and offer similar features that may not be present in other components of the library (such as multiple=\"stack\" in the examples above). They are designed to facilitate switching between different visual representations as you explore a dataset, because different representations often have complementary strengths and weaknesses.",
            "markdown"
        ]
    ],
    "seaborn->API Overview->Overview of seaborn plotting functions->Figure-level vs. axes-level functions": [
        [
            "In addition to the different modules, there is a cross-cutting classification of seaborn functions as \u201caxes-level\u201d or \u201cfigure-level\u201d. The examples above are axes-level functions. They plot data onto a single matplotlib.pyplot.Axes object, which is the return value of the function.",
            "markdown"
        ],
        [
            "In contrast, figure-level functions interface with matplotlib through a seaborn object, usually a , that manages the figure. Each module has a single figure-level function, which offers a unitary interface to its various axes-level functions. The organization looks a bit like this:",
            "markdown"
        ],
        [
            "For example,  is the figure-level function for the distributions module. Its default behavior is to draw a histogram, using the same code as  behind the scenes:",
            "markdown"
        ],
        [
            "sns.displot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\")\n",
            "code"
        ],
        [
            "To draw a kernel density plot instead, using the same code as , select it using the kind parameter:",
            "markdown"
        ],
        [
            "sns.displot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\", kind=\"kde\")\n",
            "code"
        ],
        [
            "You\u2019ll notice that the figure-level plots look mostly like their axes-level counterparts, but there are a few differences. Notably, the legend is placed outside the plot. They also have a slightly different shape (more on that shortly).",
            "markdown"
        ],
        [
            "The most useful feature offered by the figure-level functions is that they can easily create figures with multiple subplots. For example, instead of stacking the three distributions for each species of penguins in the same axes, we can \u201cfacet\u201d them by plotting each distribution across the columns of the figure:",
            "markdown"
        ],
        [
            "sns.displot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", col=\"species\")\n",
            "code"
        ],
        [
            "The figure-level functions wrap their axes-level counterparts and pass the kind-specific keyword arguments (such as the bin size for a histogram) down to the underlying function. That means they are no less flexible, but there is a downside: the kind-specific parameters don\u2019t appear in the function signature or docstrings. Some of their features might be less discoverable, and you may need to look at two different pages of the documentation before understanding how to achieve a specific goal.",
            "markdown"
        ]
    ],
    "seaborn->API Overview->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Axes-level functions make self-contained plots": [
        [
            "The axes-level functions are written to act like drop-in replacements for matplotlib functions. While they add axis labels and legends automatically, they don\u2019t modify anything beyond the axes that they are drawn into. That means they can be composed into arbitrarily-complex matplotlib figures with predictable results.",
            "markdown"
        ],
        [
            "The axes-level functions call  internally, which hooks into the matplotlib state-machine interface so that they draw their plots on the \u201ccurrently-active\u201d axes. But they additionally accept an ax= argument, which integrates with the object-oriented interface and lets you specify exactly where each plot should go:",
            "markdown"
        ],
        [
            "f, axs = plt.subplots(1, 2, figsize=(8, 4), gridspec_kw=dict(width_ratios=[4, 3]))\nsns.scatterplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\", ax=axs[0])\nsns.histplot(data=penguins, x=\"species\", hue=\"species\", shrink=.8, alpha=.8, legend=False, ax=axs[1])\nf.tight_layout()\n",
            "code"
        ]
    ],
    "seaborn->API Overview->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Figure-level functions own their figure": [
        [
            "In contrast, figure-level functions cannot (easily) be composed with other plots. By design, they \u201cown\u201d their own figure, including its initialization, so there\u2019s no notion of using a figure-level function to draw a plot onto an existing axes. This constraint allows the figure-level functions to implement features such as putting the legend outside of the plot.",
            "markdown"
        ],
        [
            "Nevertheless, it is possible to go beyond what the figure-level functions offer by accessing the matplotlib axes on the object that they return and adding other elements to the plot that way:",
            "markdown"
        ],
        [
            "tips = sns.load_dataset(\"tips\")\ng = sns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\ng.ax.axline(xy1=(10, 2), slope=.2, color=\"b\", dashes=(5, 2))\n",
            "code"
        ]
    ],
    "seaborn->API Overview->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Customizing plots from a figure-level function": [
        [
            "The figure-level functions return a  instance, which has a few methods for customizing attributes of the plot in a way that is \u201csmart\u201d about the subplot organization. For example, you can change the labels on the external axes using a single line of code:",
            "markdown"
        ],
        [
            "g = sns.relplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", col=\"sex\")\ng.set_axis_labels(\"Flipper length (mm)\", \"Bill length (mm)\")\n",
            "code"
        ],
        [
            "While convenient, this does add a bit of extra complexity, as you need to remember that this method is not part of the matplotlib API and exists only when using a figure-level function.",
            "markdown"
        ]
    ],
    "seaborn->API Overview->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Specifying figure sizes": [
        [
            "To increase or decrease the size of a matplotlib plot, you set the width and height of the entire figure, either in the , while setting up the plot (e.g. with the figsize parameter of ), or by calling a method on the figure object (e.g. matplotlib.Figure.set_size_inches()). When using an axes-level function in seaborn, the same rules apply: the size of the plot is determined by the size of the figure it is part of and the axes layout in that figure.",
            "markdown"
        ],
        [
            "When using a figure-level function, there are several key differences. First, the functions themselves have parameters to control the figure size (although these are actually parameters of the underlying  that manages the figure). Second, these parameters, height and aspect, parameterize the size slightly differently than the width, height parameterization in matplotlib (using the seaborn parameters, width = height * aspect). Most importantly, the parameters correspond to the size of each <em>subplot</em>, rather than the size of the overall figure.",
            "markdown"
        ],
        [
            "To illustrate the difference between these approaches, here is the default output of  with one subplot:",
            "markdown"
        ],
        [
            "f, ax = plt.subplots()\n",
            "code"
        ],
        [
            "A figure with multiple columns will have the same overall size, but the axes will be squeezed horizontally to fit in the space:",
            "markdown"
        ],
        [
            "f, ax = plt.subplots(1, 2, sharey=True)\n",
            "code"
        ],
        [
            "In contrast, a plot created by a figure-level function will be square. To demonstrate that, let\u2019s set up an empty plot by using  directly. This happens behind the scenes in functions like , , or :",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(penguins)\n",
            "code"
        ],
        [
            "When additional columns are added, the figure itself will become wider, so that its subplots have the same size and shape:",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(penguins, col=\"sex\")\n",
            "code"
        ],
        [
            "And you can adjust the size and shape of each subplot without accounting for the total number of rows and columns in the figure:",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(penguins, col=\"sex\", height=3.5, aspect=.75)\n",
            "code"
        ],
        [
            "The upshot is that you can assign faceting variables without stopping to think about how you\u2019ll need to adjust the total figure size. A downside is that, when you do want to change the figure size, you\u2019ll need to remember that things work a bit differently than they do in matplotlib.",
            "markdown"
        ]
    ],
    "seaborn->API Overview->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Relative merits of figure-level functions": [
        [
            "Here is a summary of the pros and cons that we have discussed above:",
            "markdown"
        ],
        [
            "On balance, the figure-level functions add some additional complexity that can make things more confusing for beginners, but their distinct features give them additional power. The tutorial documentation mostly uses the figure-level functions, because they produce slightly cleaner plots, and we generally recommend their use for most applications. The one situation where they are not a good choice is when you need to make a complex, standalone figure that composes multiple different plot kinds. At this point, it\u2019s recommended to set up the figure using matplotlib directly and to fill in the individual components using axes-level functions.",
            "markdown"
        ]
    ],
    "seaborn->API Overview->Overview of seaborn plotting functions->Combining multiple views on the data": [
        [
            "Two important plotting functions in seaborn don\u2019t fit cleanly into the classification scheme discussed above. These functions,  and , employ multiple kinds of plots from different modules to represent multiple aspects of a dataset in a single figure. Both plots are figure-level functions and create figures with multiple subplots by default. But they use different objects to manage the figure:  and , respectively.",
            "markdown"
        ],
        [
            " plots the relationship or joint distribution of two variables while adding marginal axes that show the univariate distribution of each one separately:",
            "markdown"
        ],
        [
            "sns.jointplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\")\n",
            "code"
        ],
        [
            " is similar \u2014 it combines joint and marginal views \u2014 but rather than focusing on a single relationship, it visualizes every pairwise combination of variables simultaneously:",
            "markdown"
        ],
        [
            "sns.pairplot(data=penguins, hue=\"species\")\n",
            "code"
        ],
        [
            "Behind the scenes, these functions are using axes-level functions that you have already met ( and ), and they also have a kind parameter that lets you quickly swap in a different representation:",
            "markdown"
        ],
        [
            "sns.jointplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\", kind=\"hist\")\n",
            "code"
        ]
    ],
    "seaborn->API Overview->Data structures accepted by seaborn": [
        [
            "As a data visualization library, seaborn requires that you provide it with data. This chapter explains the various ways to accomplish that task. Seaborn supports several different dataset formats, and most functions accept data represented with objects from the  or  libraries as well as built-in Python types like lists and dictionaries. Understanding the usage patterns associated with these different options will help you quickly create useful visualizations for nearly any dataset.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "As of current writing (v0.11.0), the full breadth of options covered here are supported by only a subset of the modules in seaborn (namely, the  and  modules). The other modules offer much of the same flexibility, but have some exceptions (e.g.,  and  are limited to long-form data with named variables). The data-ingest code will be standardized over the next few release cycles, but until that point, be mindful of the specific documentation for each function if it is not doing what you expect with your dataset.",
            "markdown"
        ]
    ],
    "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data": [
        [
            "Most plotting functions in seaborn are oriented towards <em>vectors</em> of data. When plotting x against y, each variable should be a vector. Seaborn accepts data <em>sets</em> that have more than one vector organized in some tabular fashion. There is a fundamental distinction between \u201clong-form\u201d and \u201cwide-form\u201d data tables, and seaborn will treat each differently.",
            "markdown"
        ]
    ],
    "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data": [
        [
            "A long-form data table has the following characteristics:",
            "markdown"
        ],
        [
            "Each variable is a column",
            "markdown"
        ],
        [
            "Each observation is a row",
            "markdown"
        ],
        [
            "As a simple example, consider the \u201cflights\u201d dataset, which records the number of airline passengers who flew in each month from 1949 to 1960. This dataset has three variables (<em>year</em>, <em>month</em>, and number of <em>passengers</em>):",
            "markdown"
        ],
        [
            "flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "code"
        ],
        [
            "With long-form data, columns in the table are given roles in the plot by explicitly assigning them to one of the variables. For example, making a monthly plot of the number of passengers per year looks like this:",
            "markdown"
        ],
        [
            "sns.relplot(data=flights, x=\"year\", y=\"passengers\", hue=\"month\", kind=\"line\")\n",
            "code"
        ],
        [
            "The advantage of long-form data is that it lends itself well to this explicit specification of the plot. It can accommodate datasets of arbitrary complexity, so long as the variables and observations can be clearly defined. But this format takes some getting used to, because it is often not the model of the data that one has in their head.",
            "markdown"
        ]
    ],
    "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data": [
        [
            "For simple datasets, it is often more intuitive to think about data the way it might be viewed in a spreadsheet, where the columns and rows contain <em>levels</em> of different variables. For example, we can convert the flights dataset into a wide-form organization by  \u201cpivoting\u201d it so that each column has each month\u2019s time series over years:",
            "markdown"
        ],
        [
            "flights_wide = flights.pivot(index=\"year\", columns=\"month\", values=\"passengers\")\nflights_wide.head()\n",
            "code"
        ],
        [
            "Here we have the same three variables, but they are organized differently. The variables in this dataset are linked to the <em>dimensions</em> of the table, rather than to named fields. Each observation is defined by both the value at a cell in the table and the coordinates of that cell with respect to the row and column indices.",
            "markdown"
        ],
        [
            "With long-form data, we can access variables in the dataset by their name. That is not the case with wide-form data. Nevertheless, because there is a clear association between the dimensions of the table and the variable in the dataset, seaborn is able to assign those variables roles in the plot.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Seaborn treats the argument to data as wide form when neither x nor y are assigned.",
            "markdown"
        ],
        [
            "sns.relplot(data=flights_wide, kind=\"line\")\n",
            "code"
        ],
        [
            "This plot looks very  similar to the one before. Seaborn has assigned the index of the dataframe to x, the values of the dataframe to y, and it has drawn a separate line for each month. There is a notable difference between the two plots, however. When the dataset went through the \u201cpivot\u201d operation that converted it from long-form to wide-form, the information about what the values mean was lost. As a result, there is no y axis label. (The lines also have dashes here, because  has mapped the column variable to both the hue and style semantic so that the plot is more accessible. We didn\u2019t do that in the long-form case, but we could have by setting style=\"month\").",
            "markdown"
        ],
        [
            "Thus far, we did much less typing while using wide-form data and made nearly the same plot. This seems easier! But a big advantage of long-form data is that, once you have the data in the correct format, you no longer need to think about its <em>structure</em>. You can design your plots by thinking only about the variables contained within it. For example, to draw lines that represent the monthly time series for each year, simply reassign the variables:",
            "markdown"
        ],
        [
            "sns.relplot(data=flights, x=\"month\", y=\"passengers\", hue=\"year\", kind=\"line\")\n",
            "code"
        ],
        [
            "To achieve the same remapping with the wide-form dataset, we would need to transpose the table:",
            "markdown"
        ],
        [
            "sns.relplot(data=flights_wide.transpose(), kind=\"line\")\n",
            "code"
        ],
        [
            "(This example also illustrates another wrinkle, which is that seaborn currently considers the column variable in a wide-form dataset to be categorical regardless of its datatype, whereas, because the long-form variable is numeric, it is assigned a quantitative color palette and legend. This may change in the future).",
            "markdown"
        ],
        [
            "The absence of explicit variable assignments also means that each plot type needs to define a fixed mapping between the dimensions of the wide-form data and the roles in the plot. Because this natural mapping may vary across plot types, the results are less predictable when using wide-form data. For example, the  plots assign the <em>column</em> dimension of the table to x and then aggregate across the rows (ignoring the index):",
            "markdown"
        ],
        [
            "sns.catplot(data=flights_wide, kind=\"box\")\n",
            "code"
        ],
        [
            "When using pandas to represent wide-form data, you are limited to just a few variables (no more than three). This is because seaborn does not make use of multi-index information, which is how pandas represents additional variables in a tabular format. The  project offers labeled N-dimensional array objects, which can be considered a generalization of wide-form data to higher dimensions. At present, seaborn does not directly support objects from xarray, but they can be transformed into a long-form  using the to_pandas method and then plotted in seaborn like any other long-form data set.",
            "markdown"
        ],
        [
            "In summary, we can think of long-form and wide-form datasets as looking something like this:",
            "markdown"
        ]
    ],
    "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data": [
        [
            "Many datasets cannot be clearly interpreted using either long-form or wide-form rules. If datasets that are clearly long-form or wide-form are , we might say that these more ambiguous datasets are \u201cmessy\u201d. In a messy dataset, the variables are neither uniquely defined by the keys nor by the dimensions of the table. This often occurs with <em>repeated-measures</em> data, where it is natural to organize a table such that each row corresponds to the <em>unit</em> of data collection. Consider this simple dataset from a psychology experiment in which twenty subjects performed a memory task where they studied anagrams while their attention was either divided or focused:",
            "markdown"
        ],
        [
            "anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "code"
        ],
        [
            "The attention variable is <em>between-subjects</em>, but there is also a <em>within-subjects</em> variable: the number of possible solutions to the anagrams, which varied from 1 to 3. The dependent measure is a score of memory performance. These two variables (number and score) are jointly encoded across several columns. As a result, the whole dataset is neither clearly long-form nor clearly wide-form.",
            "markdown"
        ],
        [
            "How might we tell seaborn to plot the average score as a function of attention and number of solutions? We\u2019d first need to coerce the data into one of our two structures. Let\u2019s transform it to a tidy long-form table, such that each variable is a column and each row is an observation. We can use the method  to accomplish this task:",
            "markdown"
        ],
        [
            "anagrams_long = anagrams.melt(id_vars=[\"subidr\", \"attnr\"], var_name=\"solutions\", value_name=\"score\")\nanagrams_long.head()\n",
            "code"
        ],
        [
            "Now we can make the plot that we want:",
            "markdown"
        ],
        [
            "sns.catplot(data=anagrams_long, x=\"solutions\", y=\"score\", hue=\"attnr\", kind=\"point\")\n",
            "code"
        ]
    ],
    "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Further reading and take-home points": [
        [
            "For a longer discussion about tabular data structures, you could read the  paper by Hadley Whickham. Note that seaborn uses a slightly different set of concepts than are defined in the paper. While the paper associates tidyness with long-form structure, we have drawn a distinction between \u201ctidy wide-form\u201d data, where there is a clear mapping between variables in the dataset and the dimensions of the table, and \u201cmessy data\u201d, where no such mapping exists.",
            "markdown"
        ],
        [
            "The long-form structure has clear advantages. It allows you to create figures by explicitly assigning variables in the dataset to roles in plot, and you can do so with more than three variables. When possible, try to represent your data with a long-form structure when embarking on serious analysis. Most of the examples in the seaborn documentation will use long-form data. But in cases where it is more natural to keep the dataset wide, remember that seaborn can remain useful.",
            "markdown"
        ]
    ],
    "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing long-form data": [
        [
            "While long-form data has a precise definition, seaborn is fairly flexible in terms of how it is actually organized across the data structures in memory. The examples in the rest of the documentation will typically use  objects and reference variables in them by assigning names of their columns to the variables in the plot. But it is also possible to store vectors in a Python dictionary or a class that implements that interface:",
            "markdown"
        ],
        [
            "flights_dict = flights.to_dict()\nsns.relplot(data=flights_dict, x=\"year\", y=\"passengers\", hue=\"month\", kind=\"line\")\n",
            "code"
        ],
        [
            "Many pandas operations, such as the split-apply-combine operations of a group-by, will produce a dataframe where information has moved from the columns of the input dataframe to the index of the output. So long as the name is retained, you can still reference the data as normal:",
            "markdown"
        ],
        [
            "flights_avg = flights.groupby(\"year\").mean()\nsns.relplot(data=flights_avg, x=\"year\", y=\"passengers\", kind=\"line\")\n",
            "code"
        ],
        [
            "/var/folders/qk/cdrdfhfn5g554pnb30pp4ylr0000gn/T/ipykernel_77263/885836857.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  flights_avg = flights.groupby(\"year\").mean()\n",
            "code"
        ],
        [
            "Additionally, it\u2019s possible to pass vectors of data directly as arguments to x, y, and other plotting variables. If these vectors are pandas objects, the name attribute will be used to label the plot:",
            "markdown"
        ],
        [
            "year = flights_avg.index\npassengers = flights_avg[\"passengers\"]\nsns.relplot(x=year, y=passengers, kind=\"line\")\n",
            "code"
        ],
        [
            "Numpy arrays and other objects that implement the Python sequence interface work too, but if they don\u2019t have names, the plot will not be as informative without further tweaking:",
            "markdown"
        ],
        [
            "sns.relplot(x=year.to_numpy(), y=passengers.to_list(), kind=\"line\")\n",
            "code"
        ]
    ],
    "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data": [
        [
            "The options for passing wide-form data are even more flexible. As with long-form data, pandas objects are preferable because the name (and, in some cases, index) information can be used. But in essence, any format that can be viewed as a single vector or a collection of vectors can be passed to data, and a valid plot can usually be constructed.",
            "markdown"
        ],
        [
            "The example we saw above used a rectangular , which can be thought of as a collection of its columns. A dict or list of pandas objects will also work, but we\u2019ll lose the axis labels:",
            "markdown"
        ],
        [
            "flights_wide_list = [col for _, col in flights_wide.items()]\nsns.relplot(data=flights_wide_list, kind=\"line\")\n",
            "code"
        ],
        [
            "The vectors in a collection do not need to have the same length. If they have an index, it will be used to align them:",
            "markdown"
        ],
        [
            "two_series = [flights_wide.loc[:1955, \"Jan\"], flights_wide.loc[1952:, \"Aug\"]]\nsns.relplot(data=two_series, kind=\"line\")\n",
            "code"
        ],
        [
            "Whereas an ordinal index will be used for numpy arrays or simple Python sequences:",
            "markdown"
        ],
        [
            "two_arrays = [s.to_numpy() for s in two_series]\nsns.relplot(data=two_arrays, kind=\"line\")\n",
            "code"
        ],
        [
            "But a dictionary of such vectors will at least use the keys:",
            "markdown"
        ],
        [
            "two_arrays_dict = {s.name: s.to_numpy() for s in two_series}\nsns.relplot(data=two_arrays_dict, kind=\"line\")\n",
            "code"
        ],
        [
            "Rectangular numpy arrays are treated just like a dataframe without index information, so they are viewed as a collection of column vectors. Note that this is different from how numpy indexing operations work, where a single indexer will access a row. But it is consistent with how pandas would turn the array into a dataframe or how matplotlib would plot it:",
            "markdown"
        ],
        [
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface": [
        [
            "The seaborn.objects namespace was introduced in version 0.12 as a completely new interface for making seaborn plots. It offers a more consistent and flexible API, comprising a collection of composable classes for transforming and plotting data. In contrast to the existing seaborn functions, the new interface aims to support end-to-end plot specification and customization without dropping down to matplotlib (although it will remain possible to do so if necessary).",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The objects interface is currently experimental and incomplete. It is stable enough for serious use, but there certainly are some rough edges and missing features.",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Specifying a plot and mapping data": [
        [
            "The objects interface should be imported with the following convention:",
            "markdown"
        ],
        [
            "import seaborn.objects as so\n",
            "code"
        ],
        [
            "The seaborn.objects namespace will provide access to all of the relevant classes. The most important is . You specify plots by instantiating a  object and calling its methods. Let\u2019s see a simple example:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n    .add(so.Dot())\n)\n",
            "code"
        ],
        [
            "This code, which produces a scatter plot, should look reasonably familiar. Just as when using , we passed a tidy dataframe (penguins) and assigned two of its columns to the x and y coordinates of the plot. But instead of starting with the type of chart and then adding some data assignments, here we started with the data assignments and then added a graphical element.",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Specifying a plot and mapping data->Setting properties": [
        [
            "The  class is an example of a : an object that graphically represents data values. Each mark will have a number of properties that can be set to change its appearance:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n    .add(so.Dot(color=\"g\", pointsize=4))\n)\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Specifying a plot and mapping data->Mapping properties": [
        [
            "As with seaborn\u2019s functions, it is also possible to <em>map</em> data values to various graphical properties:",
            "markdown"
        ],
        [
            "(\n    so.Plot(\n        penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n        color=\"species\", pointsize=\"body_mass_g\",\n    )\n    .add(so.Dot())\n)\n",
            "code"
        ],
        [
            "While this basic functionality is not novel, an important difference from the function API is that properties are mapped using the same parameter names that would set them directly (instead of having hue vs. color, etc.). What matters is <em>where</em> the property is defined: passing a value when you initialize  will set it directly, whereas assigning a variable when you set up the  will <em>map</em> the corresponding data.",
            "markdown"
        ],
        [
            "Beyond this difference, the objects interface also allows a much wider range of mark properties to be mapped:",
            "markdown"
        ],
        [
            "(\n    so.Plot(\n        penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n        edgecolor=\"sex\", edgewidth=\"body_mass_g\",\n    )\n    .add(so.Dot(color=\".8\"))\n)\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Specifying a plot and mapping data->Defining groups": [
        [
            "The  mark represents each data point independently, so the assignment of a variable to a property only has the effect of changing each dot\u2019s appearance. For marks that group or connect observations, such as , it also determines the number of distinct graphical elements:",
            "markdown"
        ],
        [
            "(\n    so.Plot(healthexp, x=\"Year\", y=\"Life_Expectancy\", color=\"Country\")\n    .add(so.Line())\n)\n",
            "code"
        ],
        [
            "It is also possible to define a grouping without changing any visual properties, by using group:",
            "markdown"
        ],
        [
            "(\n    so.Plot(healthexp, x=\"Year\", y=\"Life_Expectancy\", group=\"Country\")\n    .add(so.Line())\n)\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Transforming data before plotting->Statistical transformation": [
        [
            "As with many seaborn functions, the objects interface supports statistical transformations. These are performed by  objects, such as :",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\")\n    .add(so.Bar(), so.Agg())\n)\n",
            "code"
        ],
        [
            "In the function interface, statistical transformations are possible with some visual representations (e.g. ) but not others (e.g. ). The objects interface more cleanly separates representation and transformation, allowing you to compose  and  objects:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\")\n    .add(so.Dot(pointsize=10), so.Agg())\n)\n",
            "code"
        ],
        [
            "When forming groups by mapping properties, the  transformation is applied to each group separately:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\", color=\"sex\")\n    .add(so.Dot(pointsize=10), so.Agg())\n)\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Transforming data before plotting->Resolving overplotting": [
        [
            "Some seaborn functions also have mechanisms that automatically resolve overplotting, as when  \u201cdodges\u201d bars once hue is assigned. The objects interface has less complex default behavior. Bars representing multiple groups will overlap by default:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\", color=\"sex\")\n    .add(so.Bar(), so.Agg())\n)\n",
            "code"
        ],
        [
            "Nevertheless, it is possible to compose the  mark with the  stat and a second transformation, implemented by :",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\", color=\"sex\")\n    .add(so.Bar(), so.Agg(), so.Dodge())\n)\n",
            "code"
        ],
        [
            "The  class is an example of a  transformation, which is like a  but only adjusts x and y coordinates. The  classes can be applied with any mark, and it\u2019s not necessary to use a  first:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\", color=\"sex\")\n    .add(so.Dot(), so.Dodge())\n)\n",
            "code"
        ],
        [
            "It\u2019s also possible to apply multiple  operations in sequence:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\", color=\"sex\")\n    .add(so.Dot(), so.Dodge(), so.Jitter(.3))\n)\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Transforming data before plotting->Creating variables through transformation": [
        [
            "The  stat requires both x and y to already be defined, but variables can also be <em>created</em> through statistical transformation. For example, the  stat requires only one of x <em>or</em> y to be defined, and it will create the other by counting observations:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"species\")\n    .add(so.Bar(), so.Hist())\n)\n",
            "code"
        ],
        [
            "The  stat will also create new x values (by binning) when given numeric data:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"flipper_length_mm\")\n    .add(so.Bars(), so.Hist())\n)\n",
            "code"
        ],
        [
            "Notice how we used , rather than  for the plot with the continuous x axis. These two marks are related, but  has different defaults and works better for continuous histograms. It also produces a different, more efficient matplotlib artist. You will find the pattern of singular/plural marks elsewhere. The plural version is typically optimized for cases with larger numbers of marks.",
            "markdown"
        ],
        [
            "Some transforms accept both x and y, but add <em>interval</em> data for each coordinate. This is particularly relevant for plotting error bars after aggregating:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"body_mass_g\", y=\"species\", color=\"sex\")\n    .add(so.Range(), so.Est(errorbar=\"sd\"), so.Dodge())\n    .add(so.Dot(), so.Agg(), so.Dodge())\n)\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Transforming data before plotting->Orienting marks and transforms": [
        [
            "When aggregating, dodging, and drawing a bar, the x and y variables are treated differently. Each operation has the concept of an <em>orientation</em>. The  tries to determine the orientation automatically based on the data types of the variables. For instance, if we flip the assignment of species and body_mass_g, we\u2019ll get the same plot, but oriented horizontally:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"body_mass_g\", y=\"species\", color=\"sex\")\n    .add(so.Bar(), so.Agg(), so.Dodge())\n)\n",
            "code"
        ],
        [
            "Sometimes, the correct orientation is ambiguous, as when both the x and y variables are numeric. In these cases, you can be explicit by passing the orient parameter to :",
            "markdown"
        ],
        [
            "(\n    so.Plot(tips, x=\"total_bill\", y=\"size\", color=\"time\")\n    .add(so.Bar(), so.Agg(), so.Dodge(), orient=\"y\")\n)\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Building and displaying the plot": [
        [
            "Each example thus far has produced a single subplot with a single kind of mark on it. But  does not limit you to this.",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Building and displaying the plot->Adding multiple layers": [
        [
            "More complex single-subplot graphics can be created by calling  repeatedly. Each time it is called, it defines a <em>layer</em> in the plot. For example, we may want to add a scatterplot (now using ) and then a regression fit:",
            "markdown"
        ],
        [
            "(\n    so.Plot(tips, x=\"total_bill\", y=\"tip\")\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit())\n)\n",
            "code"
        ],
        [
            "Variable mappings that are defined in the  constructor will be used for all layers:",
            "markdown"
        ],
        [
            "(\n    so.Plot(tips, x=\"total_bill\", y=\"tip\", color=\"time\")\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit())\n)\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Building and displaying the plot->Layer-specific mappings": [
        [
            "You can also define a mapping such that it is used only in a specific layer. This is accomplished by defining the mapping within the call to  for the relevant layer:",
            "markdown"
        ],
        [
            "(\n    so.Plot(tips, x=\"total_bill\", y=\"tip\")\n    .add(so.Dots(), color=\"time\")\n    .add(so.Line(color=\".2\"), so.PolyFit())\n)\n",
            "code"
        ],
        [
            "Alternatively, define the layer for the entire plot, but <em>remove</em> it from a specific layer by setting the variable to None:",
            "markdown"
        ],
        [
            "(\n    so.Plot(tips, x=\"total_bill\", y=\"tip\", color=\"time\")\n    .add(so.Dots())\n    .add(so.Line(color=\".2\"), so.PolyFit(), color=None)\n)\n",
            "code"
        ],
        [
            "To recap, there are three ways to specify the value of a mark property: (1) by mapping a variable in all layers, (2) by mapping a variable in a specific layer, and (3) by setting the property directy:",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Building and displaying the plot->Faceting and pairing subplots": [
        [
            "As with seaborn\u2019s figure-level functions (, , etc.), the  interface can also produce figures with multiple \u201cfacets\u201d, or subplots containing subsets of data. This is accomplished with the  method:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"flipper_length_mm\")\n    .facet(\"species\")\n    .add(so.Bars(), so.Hist())\n)\n",
            "code"
        ],
        [
            "Call  with the variables that should be used to define the columns and/or rows of the plot:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"flipper_length_mm\")\n    .facet(col=\"species\", row=\"sex\")\n    .add(so.Bars(), so.Hist())\n)\n",
            "code"
        ],
        [
            "You can facet using a variable with a larger number of levels by \u201cwrapping\u201d across the other dimension:",
            "markdown"
        ],
        [
            "(\n    so.Plot(healthexp, x=\"Year\", y=\"Life_Expectancy\")\n    .facet(col=\"Country\", wrap=3)\n    .add(so.Line())\n)\n",
            "code"
        ],
        [
            "All layers will be faceted unless you explicitly exclude them, which can\nbe useful for providing additional context on each subplot:",
            "markdown"
        ],
        [
            "(\n    so.Plot(healthexp, x=\"Year\", y=\"Life_Expectancy\")\n    .facet(\"Country\", wrap=3)\n    .add(so.Line(alpha=.3), group=\"Country\", col=None)\n    .add(so.Line(linewidth=3))\n)\n",
            "code"
        ],
        [
            "An alternate way to produce subplots is . Like , this draws all of the data on each subplot, using different variables for the x and/or y coordinates:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, y=\"body_mass_g\", color=\"species\")\n    .pair(x=[\"bill_length_mm\", \"bill_depth_mm\"])\n    .add(so.Dots())\n)\n",
            "code"
        ],
        [
            "You can combine faceting and pairing so long as the operations add subplots on opposite dimensions:",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, y=\"body_mass_g\", color=\"species\")\n    .pair(x=[\"bill_length_mm\", \"bill_depth_mm\"])\n    .facet(row=\"sex\")\n    .add(so.Dots())\n)\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Building and displaying the plot->Integrating with matplotlib": [
        [
            "There may be cases where you want multiple subplots to appear in a figure with a more complex structure than what  or  can provide. The current solution is to delegate figure setup to matplotlib and to supply the matplotlib object that  should use with the  method. This object can be either a , , or ; the latter is most useful for constructing bespoke subplot layouts:",
            "markdown"
        ],
        [
            "f = mpl.figure.Figure(figsize=(8, 4))\nsf1, sf2 = f.subfigures(1, 2)\n(\n    so.Plot(penguins, x=\"body_mass_g\", y=\"flipper_length_mm\")\n    .add(so.Dots())\n    .on(sf1)\n    .plot()\n)\n(\n    so.Plot(penguins, x=\"body_mass_g\")\n    .facet(row=\"sex\")\n    .add(so.Bars(), so.Hist())\n    .on(sf2)\n    .plot()\n)\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Building and displaying the plot->Building and displaying the plot": [
        [
            "An important thing to know is that  methods clone the object they are called on and return that clone instead of updating the object in place. This means that you can define a common plot spec and then produce several variations on it.",
            "markdown"
        ],
        [
            "So, take this basic specification:",
            "markdown"
        ],
        [
            "p = so.Plot(healthexp, \"Year\", \"Spending_USD\", color=\"Country\")\n",
            "code"
        ],
        [
            "We could use it to draw a line plot:",
            "markdown"
        ],
        [
            "p.add(so.Line())\n",
            "code"
        ],
        [
            "Or perhaps a stacked area plot:",
            "markdown"
        ],
        [
            "p.add(so.Area(), so.Stack())\n",
            "code"
        ],
        [
            "The  methods are fully declarative. Calling them updates the plot spec, but it doesn\u2019t actually do any plotting. One consequence of this is that methods can be called in any order, and many of them can be called multiple times.",
            "markdown"
        ],
        [
            "When does the plot actually get rendered?  is optimized for use in notebook environments. The rendering is automatically triggered when the  gets displayed in the Jupyter REPL. That\u2019s why we didn\u2019t see anything in the example above, where we defined a  but assigned it to p rather than letting it return out to the REPL.",
            "markdown"
        ],
        [
            "To see a plot in a notebook, either return it from the final line of a cell or call Jupyter\u2019s built-in display function on the object. The notebook integration bypasses  entirely, but you can use its figure-display machinery in other contexts by calling .",
            "markdown"
        ],
        [
            "You can also save the plot to a file (or buffer) by calling .",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Customizing the appearance": [
        [
            "The new interface aims to support a deep amount of customization through , reducing the need to switch gears and use matplotlib functionality directly. (But please be patient; not all of the features needed to achieve this goal have been implemented!)",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Customizing the appearance->Parameterizing scales": [
        [
            "All of the data-dependent properties are controlled by the concept of a  and the  method. This method accepts several different types of arguments. One possibility, which is closest to the use of scales in matplotlib, is to pass the name of a function that transforms the coordinates:",
            "markdown"
        ],
        [
            "(\n    so.Plot(diamonds, x=\"carat\", y=\"price\")\n    .add(so.Dots())\n    .scale(y=\"log\")\n)\n",
            "code"
        ],
        [
            " can also control the mappings for semantic properties like color. You can directly pass it any argument that you would pass to the palette parameter in seaborn\u2019s function interface:",
            "markdown"
        ],
        [
            "(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"clarity\")\n    .add(so.Dots())\n    .scale(color=\"flare\")\n)\n",
            "code"
        ],
        [
            "Another option is to provide a tuple of (min, max) values, controlling the range that the scale should map into. This works both for numeric properties and for colors:",
            "markdown"
        ],
        [
            "(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"clarity\", pointsize=\"carat\")\n    .add(so.Dots())\n    .scale(color=(\"#88c\", \"#555\"), pointsize=(2, 10))\n)\n",
            "code"
        ],
        [
            "For additional control, you can pass a  object. There are several different types of , each with appropriate parameters. For example,  lets you define the input domain (norm), the output range (values), and the function that maps between them (trans), while  allows you to specify an ordering:",
            "markdown"
        ],
        [
            "(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"carat\", marker=\"cut\")\n    .add(so.Dots())\n    .scale(\n        color=so.Continuous(\"crest\", norm=(0, 3), trans=\"sqrt\"),\n        marker=so.Nominal([\"o\", \"+\", \"x\"], order=[\"Ideal\", \"Premium\", \"Good\"]),\n    )\n)\n",
            "code"
        ],
        [
            "/Users/mwaskom/code/seaborn/seaborn/_core/properties.py:370: RuntimeWarning: invalid value encountered in cast\n  ixs = np.asarray(x, np.intp)\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Customizing the appearance->Customizing legends and ticks": [
        [
            "The  objects are also how you specify which values should appear as tick labels / in the legend, along with how they appear. For example, the  method lets you control the density or locations of the ticks, and the  method lets you modify the format:",
            "markdown"
        ],
        [
            "(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"carat\")\n    .add(so.Dots())\n    .scale(\n        x=so.Continuous().tick(every=0.5),\n        y=so.Continuous().label(like=\"${x:.0f}\"),\n        color=so.Continuous().tick(at=[1, 2, 3, 4]),\n    )\n)\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Customizing the appearance->Customizing limits, labels, and titles": [
        [
            " has a number of methods for simple customization, including , , and :",
            "markdown"
        ],
        [
            "(\n    so.Plot(penguins, x=\"body_mass_g\", y=\"species\", color=\"island\")\n    .facet(col=\"sex\")\n    .add(so.Dot(), so.Jitter(.5))\n    .share(x=False)\n    .limit(y=(2.5, -.5))\n    .label(\n        x=\"Body mass (g)\", y=\"\",\n        color=str.capitalize,\n        title=\"{} penguins\".format,\n    )\n)\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->The seaborn.objects interface->Customizing the appearance->Theme customization": [
        [
            "Finally,  supports data-independent theming through the  method. Currently, this method accepts a dictionary of matplotlib rc parameters. You can set them directly and/or pass a package of parameters from seaborn\u2019s theming functions:",
            "markdown"
        ],
        [
            "from seaborn import axes_style\nso.Plot().theme({**axes_style(\"whitegrid\"), \"grid.linestyle\": \":\"})\n",
            "code"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Coordinate properties->x, y, xmin, xmax, ymin, ymax": [
        [
            "Coordinate properties determine where a mark is drawn on a plot. Canonically, the x coordinate is the horizontal positon and the y coordinate is the vertical position. Some marks accept a span (i.e., min, max) parameterization for one or both variables. Others may accept x and y but also use a baseline parameter to show a span. The layer\u2019s orient parameter determines how this works.",
            "markdown"
        ],
        [
            "If a variable does not contain numeric data, its scale will apply a conversion so that data can be drawn on a screen. For instance, Nominal scales assign an integer index to each distinct category, and Temporal scales represent dates as the number of days from a reference \u201cepoch\u201d:",
            "markdown"
        ],
        [
            "A Continuous scale can also apply a nonlinear transform between data values and spatial positions:",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Color properties->color, fillcolor, edgecolor": [
        [
            "All marks can be given a color, and many distinguish between the color of the mark\u2019s \u201cedge\u201d and \u201cfill\u201d. Often, simply using color will set both, while the more-specific properties allow further control:",
            "markdown"
        ],
        [
            "When the color property is mapped, the default palette depends on the type of scale. Nominal scales use discrete, unordered hues, while continuous scales (including temporal ones) use a sequential gradient:",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The default continuous scale is subject to change in future releases to improve discriminability.",
            "markdown"
        ],
        [
            "Color scales are parameterized by the name of a palette, such as 'viridis', 'rocket', or 'deep'. Some palette names can include parameters, including simple gradients (e.g. 'dark:blue') or the cubehelix system (e.g. 'ch:start=.2,rot=-.4`). See the  for guidance on making an appropriate selection.",
            "markdown"
        ],
        [
            "Continuous scales can also be parameterized by a tuple of colors that the scale should interpolate between. When using a nominal scale, it is possible to provide either the name of the palette (which will be discretely-sampled, if necessary), a list of individual color values, or a dictionary directly mapping data values to colors.",
            "markdown"
        ],
        [
            "Individual colors may be specified . These include indexed references to the current color cycle ('C0'), single-letter shorthands ('b'), grayscale values ('.4'), RGB hex codes ('#4c72b0'), X11 color names ('seagreen'), and XKCD color survey names ('purpleish'):",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Color properties->alpha, fillalpha, edgealpha": [
        [
            "The alpha property determines the mark\u2019s opacity. Lowering the alpha can be helpful for representing density in the case of overplotting:",
            "markdown"
        ],
        [
            "Mapping the alpha property can also be useful even when marks do not overlap because it conveys a sense of importance and can be combined with a color scale to represent two variables. Moreover, colors with lower alpha appear less saturated, which can improve the appearance of larger filled marks (such as bars).",
            "markdown"
        ],
        [
            "As with color, some marks define separate edgealpha and fillalpha properties for additional control.",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Style properties->fill": [
        [
            "The fill property is relevant to marks with a distinction between the edge and interior and determines whether the interior is visible. It is a boolean state: fill can be set only to True or False:",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Style properties->marker": [
        [
            "The marker property is relevant for dot marks and some line marks. The API for specifying markers is very flexible, as detailed in the matplotlib API docs: .",
            "markdown"
        ],
        [
            "Markers can be specified using a number of simple string codes:",
            "markdown"
        ],
        [
            "They can also be programatically generated using a (num_sides, fill_style, angle) tuple:",
            "markdown"
        ],
        [
            "See the matplotlib docs for additional formats, including mathtex character codes ('$...$') and arrays of vertices.",
            "markdown"
        ],
        [
            "A marker property is always mapped with a nominal scale; there is no inherent ordering to the different shapes. If no scale is provided, the plot will programmatically generate a suitably large set of unique markers:",
            "markdown"
        ],
        [
            "While this ensures that the shapes are technically distinct, bear in mind that \u2014 in most cases \u2014\u00a0it will be difficult to tell the markers apart if more than a handful are used in a single plot.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The default marker scale is subject to change in future releases to improve discriminability.",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Style properties->linestyle, edgestyle": [
        [
            "The linestyle property is relevant to line marks, and the edgestyle propety is relevant to a number of marks with \u201cedges. Both properties determine the \u201cdashing\u201d of a line in terms of on-off segments.",
            "markdown"
        ],
        [
            "Dashes can be specified with a small number of shorthand codes ('-', '--', '-.', and ':') or programatically using (on, off, ...) tuples. In the tuple specification, the unit is equal to the linewidth:",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Size properties->pointsize": [
        [
            "The pointsize property is relevant to dot marks and to line marks that can show markers at individual data points. The units correspond to the diameter of the mark in points.",
            "markdown"
        ],
        [
            "The pointsize scales with the square root of the data by default so that magnitude is represented by diameter rather than area:",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Size properties->linewidth": [
        [
            "The linewidth property is relevant to line marks and determines their thickness. The value should be non-negative and has point units:",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Size properties->edgewidth": [
        [
            "The edgewidth property is akin to linewidth but applies to marks with an edge/fill rather than to lines. It also has a different default range when used in a scale. The units are the same:",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Size properties->stroke": [
        [
            "The stroke property is akin to edgewidth but applies when a dot mark is defined by its stroke rather than its fill. It also has a slightly different default scale range, but otherwise behaves similarly:",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Text properties->halign, valign": [
        [
            "The halign and valign properties control the <em>horizontal</em> and <em>vertical</em> alignment of text marks. The options for horizontal alignment are 'left', 'right', and 'center', while the options for vertical alignment are 'top', 'bottom', 'center', 'baseline', and 'center_baseline'.",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Text properties->fontsize": [
        [
            "The fontsize property controls the size of textual marks. The value has point units:",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Text properties->offset": [
        [
            "The offset property controls the spacing between a text mark and its anchor position. It applies when <em>not</em> using center alignment (i.e., when using left/right or top/bottom). The value has point units.",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Other properties->text": [
        [
            "The text property is used to set the content of a textual mark. It is always used literally (not mapped), and cast to string when necessary.",
            "markdown"
        ]
    ],
    "seaborn->Objects interface->Properties of Mark objects->Other properties->group": [
        [
            "The group property is special in that it does not change anything about the mark\u2019s appearance but defines additional data subsets that transforms should operate on independently.",
            "markdown"
        ]
    ],
    "seaborn->Plotting functions->Visualizing statistical relationships": [
        [
            "Statistical analysis is a process of understanding how variables in a dataset relate to each other and how those relationships depend on other variables. Visualization can be a core component of this process because, when data are visualized properly, the human visual system can see trends and patterns that indicate a relationship.",
            "markdown"
        ],
        [
            "We will discuss three seaborn functions in this tutorial. The one we will use most is . This is a  for visualizing statistical relationships using two common approaches: scatter plots and line plots.  combines a  with one of two axes-level functions:",
            "markdown"
        ],
        [
            " (with kind=\"scatter\"; the default)",
            "markdown"
        ],
        [
            " (with kind=\"line\")",
            "markdown"
        ],
        [
            "As we will see, these functions can be quite illuminating because they use simple and easily-understood representations of data that can nevertheless represent complex dataset structures. They can do so because they plot two-dimensional graphics that can be enhanced by mapping up to three additional variables using the semantics of hue, size, and style.",
            "markdown"
        ],
        [
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots": [
        [
            "The scatter plot is a mainstay of statistical visualization. It depicts the joint distribution of two variables using a cloud of points, where each point represents an observation in the dataset. This depiction allows the eye to infer a substantial amount of information about whether there is any meaningful relationship between them.",
            "markdown"
        ],
        [
            "There are several ways to draw a scatter plot in seaborn. The most basic, which should be used when both variables are numeric, is the  function. In the , we will see specialized tools for using scatterplots to visualize categorical data. The  is the default kind in  (it can also be forced by setting kind=\"scatter\"):",
            "markdown"
        ],
        [
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "code"
        ],
        [
            "While the points are plotted in two dimensions, another dimension can be added to the plot by coloring the points according to a third variable. In seaborn, this is referred to as using a \u201chue semantic\u201d, because the color of the point gains meaning:",
            "markdown"
        ],
        [
            "sns.relplot(data=tips, x=\"total_bill\", y=\"tip\", hue=\"smoker\")\n",
            "code"
        ],
        [
            "To emphasize the difference between the classes, and to improve accessibility, you can use a different marker style for each class:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", hue=\"smoker\", style=\"smoker\"\n)\n",
            "code"
        ],
        [
            "It\u2019s also possible to represent four variables by changing the hue and style of each point independently. But this should be done carefully, because the eye is much less sensitive to shape than to color:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", hue=\"smoker\", style=\"time\",\n)\n",
            "code"
        ],
        [
            "In the examples above, the hue semantic was categorical, so the default  was applied. If the hue semantic is numeric (specifically, if it can be cast to float), the default coloring switches to a sequential palette:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=tips, x=\"total_bill\", y=\"tip\", hue=\"size\",\n)\n",
            "code"
        ],
        [
            "In both cases, you can customize the color palette. There are many options for doing so. Here, we customize a sequential palette using the string interface to :",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\",\n    hue=\"size\", palette=\"ch:r=-.5,l=.75\"\n)\n",
            "code"
        ],
        [
            "The third kind of semantic variable changes the size of each point:",
            "markdown"
        ],
        [
            "sns.relplot(data=tips, x=\"total_bill\", y=\"tip\", size=\"size\")\n",
            "code"
        ],
        [
            "Unlike with , the literal value of the variable is not used to pick the area of the point. Instead, the range of values in data units is normalized into a range in area units. This range can be customized:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=tips, x=\"total_bill\", y=\"tip\",\n    size=\"size\", sizes=(15, 200)\n)\n",
            "code"
        ],
        [
            "More examples for customizing how the different semantics are used to show statistical relationships are shown in the  API examples.",
            "markdown"
        ]
    ],
    "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots": [
        [
            "Scatter plots are highly effective, but there is no universally optimal type of visualisation. Instead, the visual representation should be adapted for the specifics of the dataset and to the question you are trying to answer with the plot.",
            "markdown"
        ],
        [
            "With some datasets, you may want to understand changes in one variable as a function of time, or a similarly continuous variable. In this situation, a good choice is to draw a line plot. In seaborn, this can be accomplished by the  function, either directly or with  by setting kind=\"line\":",
            "markdown"
        ],
        [
            "dowjones = sns.load_dataset(\"dowjones\")\nsns.relplot(data=dowjones, x=\"Date\", y=\"Price\", kind=\"line\")\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty": [
        [
            "More complex datasets will have multiple measurements for the same value of the x variable. The default behavior in seaborn is to aggregate the multiple measurements at each x value by plotting the mean and the 95% confidence interval around the mean:",
            "markdown"
        ],
        [
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "code"
        ],
        [
            "The confidence intervals are computed using bootstrapping, which can be time-intensive for larger datasets. It\u2019s therefore possible to disable them:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", errorbar=None,\n)\n",
            "code"
        ],
        [
            "Another good option, especially with larger data, is to represent the spread of the distribution at each timepoint by plotting the standard deviation instead of a confidence interval:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", errorbar=\"sd\",\n)\n",
            "code"
        ],
        [
            "To turn off aggregation altogether, set the estimator parameter to None This might produce a strange effect when the data have multiple observations at each point.",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\",\n    estimator=None,\n)\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Plotting subsets of data with semantic mappings": [
        [
            "The  function has the same flexibility as : it can show up to three additional variables by modifying the hue, size, and style of the plot elements. It does so using the same API as , meaning that we don\u2019t need to stop and think about the parameters that control the look of lines vs. points in matplotlib.",
            "markdown"
        ],
        [
            "Using semantics in  will also determine how the data get aggregated. For example, adding a hue semantic with two levels splits the plot into two lines and error bands, coloring each to indicate which subset of the data they correspond to.",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"event\",\n)\n",
            "code"
        ],
        [
            "Adding a style semantic to a line plot changes the pattern of dashes in the line by default:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\",\n    hue=\"region\", style=\"event\",\n)\n",
            "code"
        ],
        [
            "But you can identify subsets by the markers used at each observation, either together with the dashes or instead of them:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"region\", style=\"event\",\n    dashes=False, markers=True,\n)\n",
            "code"
        ],
        [
            "As with scatter plots, be cautious about making line plots using multiple semantics. While sometimes informative, they can also be difficult to parse and interpret. But even when you are only examining changes across one additional variable, it can be useful to alter both the color and style of the lines. This can make the plot more accessible when printed to black-and-white or viewed by someone with color blindness:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"event\", style=\"event\",\n)\n",
            "code"
        ],
        [
            "When you are working with repeated measures data (that is, you have units that were sampled multiple times), you can also plot each sampling unit separately without distinguishing them through semantics. This avoids cluttering the legend:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri.query(\"event == 'stim'\"), kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"region\",\n    units=\"subject\", estimator=None,\n)\n",
            "code"
        ],
        [
            "The default colormap and handling of the legend in  also depends on whether the hue semantic is categorical or numeric:",
            "markdown"
        ],
        [
            "dots = sns.load_dataset(\"dots\").query(\"align == 'dots'\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    hue=\"coherence\", style=\"choice\",\n)\n",
            "code"
        ],
        [
            "It may happen that, even though the hue variable is numeric, it is poorly represented by a linear color scale. That\u2019s the case here, where the levels of the hue variable are logarithmically scaled. You can provide specific color values for each line by passing a list or dictionary:",
            "markdown"
        ],
        [
            "palette = sns.cubehelix_palette(light=.8, n_colors=6)\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    hue=\"coherence\", style=\"choice\", palette=palette,\n)\n",
            "code"
        ],
        [
            "Or you can alter how the colormap is normalized:",
            "markdown"
        ],
        [
            "from matplotlib.colors import LogNorm\npalette = sns.cubehelix_palette(light=.7, n_colors=6)\nsns.relplot(\n    data=dots.query(\"coherence &gt; 0\"), kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    hue=\"coherence\", style=\"choice\",\n    hue_norm=LogNorm(),\n)\n",
            "code"
        ],
        [
            "The third semantic, size, changes the width of the lines:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    size=\"coherence\", style=\"choice\",\n)\n",
            "code"
        ],
        [
            "While the size variable will typically be numeric, it\u2019s also possible to map a categorical variable with the width of the lines. Be cautious when doing so, because it will be difficult to distinguish much more than \u201cthick\u201d vs \u201cthin\u201d lines. However, dashes can be hard to perceive when lines have high-frequency variability, so using different widths may be more effective in that case:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    hue=\"coherence\", size=\"choice\", palette=palette,\n)\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Controlling sorting and orientation": [
        [
            "Because  assumes that you are most often trying to draw y as a function of x, the default behavior is to sort the data by the x values before plotting. However, this can be disabled:",
            "markdown"
        ],
        [
            "healthexp = sns.load_dataset(\"healthexp\").sort_values(\"Year\")\nsns.relplot(\n    data=healthexp, kind=\"line\",\n    x=\"Spending_USD\", y=\"Life_Expectancy\", hue=\"Country\",\n    sort=False\n)\n",
            "code"
        ],
        [
            "It\u2019s also possible to sort (and aggregate) along the y axis:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n     x=\"signal\", y=\"timepoint\", hue=\"event\",\n    orient=\"y\",\n)\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing statistical relationships->Showing multiple relationships with facets": [
        [
            "We\u2019ve emphasized in this tutorial that, while these functions <em>can</em> show several semantic variables at once, it\u2019s not always effective to do so. But what about when you do want to understand how a relationship between two variables depends on more than one other variable?",
            "markdown"
        ],
        [
            "The best approach may be to make more than one plot. Because  is based on the , this is easy to do. To show the influence of an additional variable, instead of assigning it to one of the semantic roles in the plot, use it to \u201cfacet\u201d the visualization. This means that you make multiple axes and plot subsets of the data on each of them:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", hue=\"smoker\", col=\"time\",\n)\n",
            "code"
        ],
        [
            "You can also show the influence of two variables this way: one by faceting on the columns and one by faceting on the rows. As you start adding more variables to the grid, you may want to decrease the figure size. Remember that the size  is parameterized by the height and aspect ratio of <em>each facet</em>:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"subject\",\n    col=\"region\", row=\"event\", height=3,\n    estimator=None\n)\n",
            "code"
        ],
        [
            "When you want to examine effects across many levels of a variable, it can be a good idea to facet that variable on the columns and then \u201cwrap\u201d the facets into the rows:",
            "markdown"
        ],
        [
            "sns.relplot(\n    data=fmri.query(\"region == 'frontal'\"), kind=\"line\",\n    x=\"timepoint\", y=\"signal\", hue=\"event\", style=\"event\",\n    col=\"subject\", col_wrap=5,\n    height=3, aspect=.75, linewidth=2.5,\n)\n",
            "code"
        ],
        [
            "These visualizations, which are sometimes called \u201clattice\u201d plots or \u201csmall-multiples\u201d, are very effective because they present the data in a format that makes it easy for the eye to detect both overall patterns and deviations from those patterns. While you should make use of the flexibility afforded by  and , always try to keep in mind that several simple plots are usually more effective than one complex plot.",
            "markdown"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data": [
        [
            "An early step in any effort to analyze or model data should be to understand how the variables are distributed. Techniques for distribution visualization can provide quick answers to many important questions. What range do the observations cover? What is their central tendency? Are they heavily skewed in one direction? Is there evidence for bimodality? Are there significant outliers? Do the answers to these questions vary across subsets defined by other variables?",
            "markdown"
        ],
        [
            "The  contains several functions designed to answer questions such as these. The axes-level functions are , , , and . They are grouped together within the figure-level , , and  functions.",
            "markdown"
        ],
        [
            "There are several different approaches to visualizing a distribution, and each has its relative advantages and drawbacks. It is important to understand these factors so that you can choose the best approach for your particular aim.",
            "markdown"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms": [
        [
            "Perhaps the most common approach to visualizing a distribution is the <em>histogram</em>. This is the default approach in , which uses the same underlying code as . A histogram is a bar plot where the axis representing the data variable is divided into a set of discrete bins and the count of observations falling within each bin is shown using the height of the corresponding bar:",
            "markdown"
        ],
        [
            "penguins = sns.load_dataset(\"penguins\")\nsns.displot(penguins, x=\"flipper_length_mm\")\n",
            "code"
        ],
        [
            "This plot immediately affords a few insights about the flipper_length_mm variable. For instance, we can see that the most common flipper length is about 195 mm, but the distribution appears bimodal, so this one number does not represent the data well.",
            "markdown"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size": [
        [
            "The size of the bins is an important parameter, and using the wrong bin size can mislead by obscuring important features of the data or by creating apparent features out of random variability. By default, / choose a default bin size based on the variance of the data and the number of observations. But you should not be over-reliant on such automatic approaches, because they depend on particular assumptions about the structure of your data. It is always advisable to check that your impressions of the distribution are consistent across different bin sizes. To choose the size directly, set the binwidth parameter:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", binwidth=3)\n",
            "code"
        ],
        [
            "In other circumstances, it may make more sense to specify the <em>number</em> of bins, rather than their size:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", bins=20)\n",
            "code"
        ],
        [
            "One example of a situation where defaults fail is when the variable takes a relatively small number of integer values. In that case, the default bin width may be too small, creating awkward gaps in the distribution:",
            "markdown"
        ],
        [
            "tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n",
            "code"
        ],
        [
            "One approach would be to specify the precise bin breaks by passing an array to bins:",
            "markdown"
        ],
        [
            "sns.displot(tips, x=\"size\", bins=[1, 2, 3, 4, 5, 6, 7])\n",
            "code"
        ],
        [
            "This can also be accomplished by setting discrete=True, which chooses bin breaks that represent the unique values in a dataset with bars that are centered on their corresponding value.",
            "markdown"
        ],
        [
            "sns.displot(tips, x=\"size\", discrete=True)\n",
            "code"
        ],
        [
            "It\u2019s also possible to visualize the distribution of a categorical variable using the logic of a histogram. Discrete bins are automatically set for categorical variables, but it may also be helpful to \u201cshrink\u201d the bars slightly to emphasize the categorical nature of the axis:",
            "markdown"
        ],
        [
            "sns.displot(tips, x=\"day\", shrink=.8)\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables": [
        [
            "Once you understand the distribution of a variable, the next step is often to ask whether features of that distribution differ across other variables in the dataset. For example, what accounts for the bimodal distribution of flipper lengths that we saw above?  and  provide support for conditional subsetting via the hue semantic. Assigning a variable to hue will draw a separate histogram for each of its unique values and distinguish them by color:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\")\n",
            "code"
        ],
        [
            "By default, the different histograms are \u201clayered\u201d on top of each other and, in some cases, they may be difficult to distinguish. One option is to change the visual representation of the histogram from a bar plot to a \u201cstep\u201d plot:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", element=\"step\")\n",
            "code"
        ],
        [
            "Alternatively, instead of layering each bar, they can be \u201cstacked\u201d, or moved vertically. In this plot, the outline of the full histogram will match the plot with only a single variable:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\")\n",
            "code"
        ],
        [
            "The stacked histogram emphasizes the part-whole relationship between the variables, but it can obscure other features (for example, it is difficult to determine the mode of the Adelie distribution. Another option is \u201cdodge\u201d the bars, which moves them horizontally and reduces their width. This ensures that there are no overlaps and that the bars remain comparable in terms of height. But it only works well when the categorical variable has a small number of levels:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"sex\", multiple=\"dodge\")\n",
            "code"
        ],
        [
            "Because  is a figure-level function and is drawn onto a , it is also possible to draw each individual distribution in a separate subplot by assigning the second variable to col or row rather than (or in addition to) hue. This represents the distribution of each subset well, but it makes it more difficult to draw direct comparisons:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", col=\"sex\")\n",
            "code"
        ],
        [
            "None of these approaches are perfect, and  we will soon see some alternatives to a histogram that are better-suited to the task of comparison.",
            "markdown"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics": [
        [
            "Before we do, another point to note is that, when the subsets have unequal numbers of observations, comparing their distributions in terms of counts may not be ideal. One solution is to <em>normalize</em> the counts using the stat parameter:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\")\n",
            "code"
        ],
        [
            "By default, however, the normalization is applied to the entire distribution, so this simply rescales the height of the bars. By setting common_norm=False, each subset will be normalized independently:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\", common_norm=False)\n",
            "code"
        ],
        [
            "Density normalization scales the bars so that their <em>areas</em> sum to 1. As a result, the density axis is not directly interpretable. Another option is to normalize the bars to that their <em>heights</em> sum to 1. This makes most sense when the variable is discrete, but  it is an option for all histograms:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"probability\")\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation": [
        [
            "A histogram aims to approximate the underlying probability density function that generated the data by binning and counting observations. Kernel density estimation (KDE) presents a different solution to the same problem. Rather than using discrete bins, a KDE plot smooths the observations with a Gaussian kernel, producing a continuous density estimate:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", kind=\"kde\")\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Choosing the smoothing bandwidth": [
        [
            "Much like with the bin size in the histogram, the ability of the KDE to accurately represent the data depends on the choice of smoothing bandwidth. An over-smoothed estimate might erase meaningful features, but an under-smoothed estimate can obscure the true shape within random noise. The easiest way to check the robustness of the estimate is to adjust the default bandwidth:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", kind=\"kde\", bw_adjust=.25)\n",
            "code"
        ],
        [
            "Note how the narrow bandwidth makes the bimodality much more apparent, but the curve is much less smooth. In contrast, a larger bandwidth obscures the bimodality almost completely:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", kind=\"kde\", bw_adjust=2)\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Conditioning on other variables": [
        [
            "As with histograms, if you assign a hue variable, a separate density estimate will be computed for each level of that variable:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", kind=\"kde\")\n",
            "code"
        ],
        [
            "In many cases, the layered KDE is easier to interpret than the layered histogram, so it is often a good choice for the task of comparison. Many of the same options for resolving multiple distributions apply to the KDE as well, however:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", kind=\"kde\", multiple=\"stack\")\n",
            "code"
        ],
        [
            "Note how the stacked plot filled in the area between each curve by default. It is also possible to fill in the curves for single or layered densities, although the default alpha value (opacity) will be different, so that the individual densities are easier to resolve.",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", kind=\"kde\", fill=True)\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls": [
        [
            "KDE plots have many advantages. Important features of the data are easy to discern (central tendency, bimodality, skew), and they afford easy comparisons between subsets. But there are also situations where KDE poorly represents the underlying data. This is because the logic of KDE assumes that the underlying distribution is smooth and unbounded. One way this assumption can fail is when a variable reflects a quantity that is naturally bounded. If there are observations lying close to the bound (for example, small values of a variable that cannot be negative), the KDE curve may extend to unrealistic values:",
            "markdown"
        ],
        [
            "sns.displot(tips, x=\"total_bill\", kind=\"kde\")\n",
            "code"
        ],
        [
            "This can be partially avoided with the cut parameter, which specifies how far the curve should extend beyond the extreme datapoints. But this influences only where the curve is drawn; the density estimate will still smooth over the range where no data can exist, causing it to be artificially low at the extremes of the distribution:",
            "markdown"
        ],
        [
            "sns.displot(tips, x=\"total_bill\", kind=\"kde\", cut=0)\n",
            "code"
        ],
        [
            "The KDE approach also fails for discrete data or when data are naturally continuous but specific values are over-represented. The important thing to keep in mind is that the KDE will <em>always show you a smooth curve</em>, even when the data themselves are not smooth. For example, consider this distribution of diamond weights:",
            "markdown"
        ],
        [
            "diamonds = sns.load_dataset(\"diamonds\")\nsns.displot(diamonds, x=\"carat\", kind=\"kde\")\n",
            "code"
        ],
        [
            "While the KDE suggests that there are peaks around specific values, the histogram reveals a much more jagged distribution:",
            "markdown"
        ],
        [
            "sns.displot(diamonds, x=\"carat\")\n",
            "code"
        ],
        [
            "As a compromise, it is possible to combine these two approaches. While in histogram mode,  (as with ) has the option of including the smoothed KDE curve (note kde=True, not kind=\"kde\"):",
            "markdown"
        ],
        [
            "sns.displot(diamonds, x=\"carat\", kde=True)\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data->Empirical cumulative distributions": [
        [
            "A third option for visualizing distributions computes the \u201cempirical cumulative distribution function\u201d (ECDF). This plot draws a monotonically-increasing curve through each datapoint such that the height of the curve reflects the proportion of observations with a smaller value:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", kind=\"ecdf\")\n",
            "code"
        ],
        [
            "The ECDF plot has two key advantages. Unlike the histogram or KDE, it directly represents each datapoint. That means there is no bin size or smoothing parameter to consider. Additionally, because the curve is monotonically increasing, it is well-suited for comparing multiple distributions:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", kind=\"ecdf\")\n",
            "code"
        ],
        [
            "The major downside to the ECDF plot is that it represents the shape of the distribution less intuitively than a histogram or density curve. Consider how the bimodality of flipper lengths is immediately apparent in the histogram, but to see it in the ECDF plot, you must look for varying slopes. Nevertheless, with practice, you can learn to answer all of the important questions about a distribution by examining the ECDF, and doing so can be a powerful approach.",
            "markdown"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions": [
        [
            "All of the examples so far have considered <em>univariate</em> distributions: distributions of a single variable, perhaps conditional on a second variable assigned to hue. Assigning a second variable to y, however, will plot a <em>bivariate</em> distribution:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "code"
        ],
        [
            "A bivariate histogram bins the data within rectangles that tile the plot and then shows the count of observations within each rectangle with the fill color (analogous to a ). Similarly, a bivariate KDE plot smoothes the (x, y) observations with a 2D Gaussian. The default representation then shows the <em>contours</em> of the 2D density:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\")\n",
            "code"
        ],
        [
            "Assigning a hue variable will plot multiple heatmaps or contour sets using different colors. For bivariate histograms, this will only work well if there is minimal overlap between the conditional distributions:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\")\n",
            "code"
        ],
        [
            "The contour approach of the bivariate KDE plot lends itself better to evaluating overlap, although a plot with too many contours can get busy:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", kind=\"kde\")\n",
            "code"
        ],
        [
            "Just as with univariate plots, the choice of bin size or smoothing bandwidth will determine how well the plot represents the underlying bivariate distribution. The same parameters apply, but they can be tuned for each variable by passing a pair of values:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5))\n",
            "code"
        ],
        [
            "To aid interpretation of the heatmap, add a colorbar to show the mapping between counts and color intensity:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5), cbar=True)\n",
            "code"
        ],
        [
            "The meaning of the bivariate density contours is less straightforward. Because the density is not directly interpretable, the contours are drawn at <em>iso-proportions</em> of the density, meaning that each curve shows a level set such that some proportion <em>p</em> of the density lies below it. The <em>p</em> values are evenly spaced, with the lowest level contolled by the thresh parameter and the number controlled by levels:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", thresh=.2, levels=4)\n",
            "code"
        ],
        [
            "The levels parameter also accepts a list of values, for more control:",
            "markdown"
        ],
        [
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", levels=[.01, .05, .1, .8])\n",
            "code"
        ],
        [
            "The bivariate histogram allows one or both variables to be discrete. Plotting one discrete and one continuous variable offers another way to compare conditional univariate distributions:",
            "markdown"
        ],
        [
            "sns.displot(diamonds, x=\"price\", y=\"clarity\", log_scale=(True, False))\n",
            "code"
        ],
        [
            "In contrast, plotting two discrete variables is an easy to way show the cross-tabulation of the observations:",
            "markdown"
        ],
        [
            "sns.displot(diamonds, x=\"color\", y=\"clarity\")\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings": [
        [
            "Several other figure-level plotting functions in seaborn make use of the  and  functions.",
            "markdown"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions": [
        [
            "The first is , which augments a bivariate relatonal or distribution plot with the marginal distributions of the two variables. By default,  represents the bivariate distribution using  and the marginal distributions using :",
            "markdown"
        ],
        [
            "sns.jointplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "code"
        ],
        [
            "Similar to , setting a different kind=\"kde\" in  will change both the joint and marginal plots the use :",
            "markdown"
        ],
        [
            "sns.jointplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\",\n    kind=\"kde\"\n)\n",
            "code"
        ],
        [
            " is a convenient interface to the  class, which offeres more flexibility when used directly:",
            "markdown"
        ],
        [
            "g = sns.JointGrid(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\ng.plot_joint(sns.histplot)\ng.plot_marginals(sns.boxplot)\n",
            "code"
        ],
        [
            "A less-obtrusive way to show marginal distributions uses a \u201crug\u201d plot, which adds a small tick on the edge of the plot to represent each individual observation. This is built into :",
            "markdown"
        ],
        [
            "sns.displot(\n    penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n    kind=\"kde\", rug=True\n)\n",
            "code"
        ],
        [
            "And the axes-level  function can be used to add rugs on the side of any other kind of plot:",
            "markdown"
        ],
        [
            "sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions": [
        [
            "The  function offers a similar blend of joint and marginal distributions. Rather than focusing on a single relationship, however,  uses a \u201csmall-multiple\u201d approach to visualize the univariate distribution of all variables in a dataset along with all of their pairwise relationships:",
            "markdown"
        ],
        [
            "sns.pairplot(penguins)\n",
            "code"
        ],
        [
            "As with /, using the underlying  directly will afford more flexibility with only a bit more typing:",
            "markdown"
        ],
        [
            "g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing categorical data": [
        [
            "In the  we saw how to use different visual representations to show the relationship between multiple variables in a dataset. In the examples, we focused on cases where the main relationship was between two numerical variables. If one of the main variables is \u201ccategorical\u201d (divided into discrete groups) it may be helpful to use a more specialized approach to visualization.",
            "markdown"
        ],
        [
            "In seaborn, there are several different ways to visualize a relationship involving categorical data. Similar to the relationship between  and either  or , there are two ways to make these plots. There are a number of axes-level functions for plotting categorical data in different ways and a figure-level interface, , that gives unified higher-level access to them.",
            "markdown"
        ],
        [
            "It\u2019s helpful to think of the different categorical plot kinds as belonging to three different families, which we\u2019ll discuss in detail below. They are:",
            "markdown"
        ],
        [
            "Categorical scatterplots:",
            "markdown"
        ],
        [
            " (with kind=\"strip\"; the default)",
            "markdown"
        ],
        [
            " (with kind=\"swarm\")",
            "markdown"
        ],
        [
            "Categorical distribution plots:",
            "markdown"
        ],
        [
            " (with kind=\"box\")",
            "markdown"
        ],
        [
            " (with kind=\"violin\")",
            "markdown"
        ],
        [
            " (with kind=\"boxen\")",
            "markdown"
        ],
        [
            "Categorical estimate plots:",
            "markdown"
        ],
        [
            " (with kind=\"point\")",
            "markdown"
        ],
        [
            " (with kind=\"bar\")",
            "markdown"
        ],
        [
            " (with kind=\"count\")",
            "markdown"
        ],
        [
            "These families represent the data using different levels of granularity. When deciding which to use, you\u2019ll have to think about the question that you want to answer. The unified API makes it easy to switch between different kinds and see your data from several perspectives.",
            "markdown"
        ],
        [
            "In this tutorial, we\u2019ll mostly focus on the figure-level interface, . Remember that this function is a higher-level interface each of the functions above, so we\u2019ll reference them when we show each kind of plot, keeping the more verbose kind-specific API documentation at hand.",
            "markdown"
        ]
    ],
    "seaborn->Plotting functions->Visualizing categorical data->Categorical scatterplots": [
        [
            "The default representation of the data in  uses a scatterplot. There are actually two different categorical scatter plots in seaborn. They take different approaches to resolving the main challenge in representing categorical data with a scatter plot, which is that all of the points belonging to one category would fall on the same position along the axis corresponding to the categorical variable. The approach used by , which is the default \u201ckind\u201d in  is to adjust the positions of points on the categorical axis with a small amount of random \u201cjitter\u201d:",
            "markdown"
        ],
        [
            "tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "code"
        ],
        [
            "The jitter parameter controls the magnitude of jitter or disables it altogether:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"day\", y=\"total_bill\", jitter=False)\n",
            "code"
        ],
        [
            "The second approach adjusts the points along the categorical axis using an algorithm that prevents them from overlapping. It can give a better representation of the distribution of observations, although it only works well for relatively small datasets. This kind of plot is sometimes called a \u201cbeeswarm\u201d and is drawn in seaborn by , which is activated by setting kind=\"swarm\" in :",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"swarm\")\n",
            "code"
        ],
        [
            "Similar to the relational plots, it\u2019s possible to add another dimension to a categorical plot by using a hue semantic. (The categorical plots do not currently support size or style semantics). Each different categorical plotting function handles the hue semantic differently. For the scatter plots, it is only necessary to change the color of the points:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\", kind=\"swarm\")\n",
            "code"
        ],
        [
            "Unlike with numerical data, it is not always obvious how to order the levels of the categorical variable along its axis. In general, the seaborn categorical plotting functions try to infer the order of categories from the data. If your data have a pandas Categorical datatype, then the default order of the categories can be set there. If the variable passed to the categorical axis looks numerical, the levels will be sorted. But the data are still treated as categorical and drawn at ordinal positions on the categorical axes (specifically, at 0, 1, \u2026) even when numbers are used to label them:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips.query(\"size != 3\"), x=\"size\", y=\"total_bill\")\n",
            "code"
        ],
        [
            "The other option for choosing a default ordering is to take the levels of the category as they appear in the dataset. The ordering can also be controlled on a plot-specific basis using the order parameter. This can be important when drawing multiple categorical plots in the same figure, which we\u2019ll see more of below:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"smoker\", y=\"tip\", order=[\"No\", \"Yes\"])\n",
            "code"
        ],
        [
            "We\u2019ve referred to the idea of \u201ccategorical axis\u201d. In these examples, that\u2019s always corresponded to the horizontal axis. But it\u2019s often helpful to put the categorical variable on the vertical axis (particularly when the category names are relatively long or there are many categories). To do this, swap the assignment of variables to axes:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"total_bill\", y=\"day\", hue=\"time\", kind=\"swarm\")\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions": [
        [
            "As the size of the dataset grows, categorical scatter plots become limited in the information they can provide about the distribution of values within each category. When this happens, there are several approaches for summarizing the distributional information in ways that facilitate easy comparisons across the category levels.",
            "markdown"
        ]
    ],
    "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Boxplots": [
        [
            "The first is the familiar . This kind of plot shows the three quartile values of the distribution along with extreme values. The \u201cwhiskers\u201d extend to points that lie within 1.5 IQRs of the lower and upper quartile, and then observations that fall outside this range are displayed independently. This means that each value in the boxplot corresponds to an actual observation in the data.",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"box\")\n",
            "code"
        ],
        [
            "When adding a hue semantic, the box for each level of the semantic variable is moved along the categorical axis so they don\u2019t overlap:",
            "markdown"
        ],
        [
            "sns.catplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"smoker\", kind=\"box\")\n",
            "code"
        ],
        [
            "This behavior is called \u201cdodging\u201d and is turned on by default because it is assumed that the semantic variable is nested within the main categorical variable. If that\u2019s not the case, you can disable the dodging:",
            "markdown"
        ],
        [
            "tips[\"weekend\"] = tips[\"day\"].isin([\"Sat\", \"Sun\"])\nsns.catplot(\n    data=tips, x=\"day\", y=\"total_bill\", hue=\"weekend\",\n    kind=\"box\", dodge=False,\n)\n",
            "code"
        ],
        [
            "A related function, , draws a plot that is similar to a box plot but optimized for showing more information about the shape of the distribution. It is best suited for larger datasets:",
            "markdown"
        ],
        [
            "diamonds = sns.load_dataset(\"diamonds\")\nsns.catplot(\n    data=diamonds.sort_values(\"color\"),\n    x=\"color\", y=\"price\", kind=\"boxen\",\n)\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Violinplots": [
        [
            "A different approach is a , which combines a boxplot with the kernel density estimation procedure described in the  tutorial:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=tips, x=\"total_bill\", y=\"day\", hue=\"sex\", kind=\"violin\",\n)\n",
            "code"
        ],
        [
            "This approach uses the kernel density estimate to provide a richer description of the distribution of values. Additionally, the quartile and whisker values from the boxplot are shown inside the violin. The downside is that, because the violinplot uses a KDE, there are some other parameters that may need tweaking, adding some complexity relative to the straightforward boxplot:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=tips, x=\"total_bill\", y=\"day\", hue=\"sex\",\n    kind=\"violin\", bw=.15, cut=0,\n)\n",
            "code"
        ],
        [
            "It\u2019s also possible to \u201csplit\u201d the violins when the hue parameter has only two levels, which can allow for a more efficient use of space:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\",\n    kind=\"violin\", split=True,\n)\n",
            "code"
        ],
        [
            "Finally, there are several options for the plot that is drawn on the interior of the violins, including ways to show each individual observation instead of the summary boxplot values:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\",\n    kind=\"violin\", inner=\"stick\", split=True, palette=\"pastel\",\n)\n",
            "code"
        ],
        [
            "It can also be useful to combine  or  with a box plot or violin plot to show each observation along with a summary of the distribution:",
            "markdown"
        ],
        [
            "g = sns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"violin\", inner=None)\nsns.swarmplot(data=tips, x=\"day\", y=\"total_bill\", color=\"k\", size=3, ax=g.ax)\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing categorical data->Estimating central tendency": [
        [
            "For other applications, rather than showing the distribution within each category, you might want to show an estimate of the central tendency of the values. Seaborn has two main ways to show this information. Importantly, the basic API for these functions is identical to that for the ones discussed above.",
            "markdown"
        ]
    ],
    "seaborn->Plotting functions->Visualizing categorical data->Estimating central tendency->Bar plots": [
        [
            "A familiar style of plot that accomplishes this goal is a bar plot. In seaborn, the  function operates on a full dataset and applies a function to obtain the estimate (taking the mean by default). When there are multiple observations in each category, it also uses bootstrapping to compute a confidence interval around the estimate, which is plotted using error bars:",
            "markdown"
        ],
        [
            "titanic = sns.load_dataset(\"titanic\")\nsns.catplot(data=titanic, x=\"sex\", y=\"survived\", hue=\"class\", kind=\"bar\")\n",
            "code"
        ],
        [
            "The default error bars show 95% confidence intervals, but (starting in\nv0.12), it is possible to select from a number of other representations:",
            "markdown"
        ],
        [
            "sns.catplot(data=titanic, x=\"age\", y=\"deck\", errorbar=(\"pi\", 95), kind=\"bar\")\n",
            "code"
        ],
        [
            "A special case for the bar plot is when you want to show the number of observations in each category rather than computing a statistic for a second variable. This is similar to a histogram over a categorical, rather than quantitative, variable. In seaborn, it\u2019s easy to do so with the  function:",
            "markdown"
        ],
        [
            "sns.catplot(data=titanic, x=\"deck\", kind=\"count\", palette=\"ch:.25\")\n",
            "code"
        ],
        [
            "Both  and  can be invoked with all of the options discussed above, along with others that are demonstrated in the detailed documentation for each function:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=titanic, y=\"deck\", hue=\"class\", kind=\"count\",\n    palette=\"pastel\", edgecolor=\".6\",\n)\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing categorical data->Estimating central tendency->Point plots": [
        [
            "An alternative style for visualizing the same information is offered by the  function. This function also encodes the value of the estimate with height on the other axis, but rather than showing a full bar, it plots the point estimate and confidence interval. Additionally,  connects points from the same hue category. This makes it easy to see how the main relationship is changing as a function of the hue semantic, because your eyes are quite good at picking up on differences of slopes:",
            "markdown"
        ],
        [
            "sns.catplot(data=titanic, x=\"sex\", y=\"survived\", hue=\"class\", kind=\"point\")\n",
            "code"
        ],
        [
            "While the categorical functions lack the style semantic of the relational functions, it can still be a good idea to vary the marker and/or linestyle along with the hue to make figures that are maximally accessible and reproduce well in black and white:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=titanic, x=\"class\", y=\"survived\", hue=\"sex\",\n    palette={\"male\": \"g\", \"female\": \"m\"},\n    markers=[\"^\", \"o\"], linestyles=[\"-\", \"--\"],\n    kind=\"point\"\n)\n",
            "code"
        ]
    ],
    "seaborn->Plotting functions->Visualizing categorical data->Showing additional dimensions": [
        [
            "Just like , the fact that  is built on a  means that it is easy to add faceting variables to visualize higher-dimensional relationships:",
            "markdown"
        ],
        [
            "sns.catplot(\n    data=tips, x=\"day\", y=\"total_bill\", hue=\"smoker\",\n    kind=\"swarm\", col=\"time\", aspect=.7,\n)\n",
            "code"
        ],
        [
            "For further customization of the plot, you can use the methods on the  object that it returns:",
            "markdown"
        ],
        [
            "g = sns.catplot(\n    data=titanic,\n    x=\"fare\", y=\"embark_town\", row=\"class\",\n    kind=\"box\", orient=\"h\",\n    sharex=False, margin_titles=True,\n    height=1.5, aspect=4,\n)\ng.set(xlabel=\"Fare\", ylabel=\"\")\ng.set_titles(row_template=\"{row_name} class\")\nfor ax in g.axes.flat:\n    ax.xaxis.set_major_formatter('${x:.0f}')\n",
            "code"
        ]
    ],
    "seaborn->Statistical operations->Statistical estimation and error bars": [
        [
            "Data visualization sometimes involves a step of aggregation or estimation, where multiple data points are reduced to a summary statistic such as the mean or median. When showing a summary statistic, it is usually appropriate to add <em>error bars</em>, which provide a visual cue about how well the summary represents the underlying data points.",
            "markdown"
        ],
        [
            "Several seaborn functions will automatically calculate both summary statistics and the error bars when given a full dataset. This chapter explains how you can control what the error bars show and why you might choose each of the options that seaborn affords.",
            "markdown"
        ],
        [
            "The error bars around an estimate of central tendency can show one of two general things: either the range of uncertainty about the estimate or the spread of the underlying data around it. These measures are related: given the same sample size, estimates will be more uncertain when data has a broader spread. But uncertainty will decrease as sample sizes grow, whereas spread will not.",
            "markdown"
        ],
        [
            "In seaborn, there are two approaches for constructing each kind of error bar. One approach is parametric, using a formula that relies on assumptions about the shape of the distribution. The other approach is nonparametric, using only the data that you provide.",
            "markdown"
        ],
        [
            "Your choice is made with the errorbar parameter, which exists for each function that does estimation as part of plotting. This parameter accepts the name of the method to use and, optionally, a parameter that controls the size of the interval. The choices can be defined in a 2D taxonomy that depends on what is shown and how it is constructed:",
            "markdown"
        ],
        [
            "You will note that the size parameter is defined differently for the parametric and nonparametric approaches. For parametric error bars, it is a scalar factor that is multiplied by the statistic defining the error (standard error or standard deviation). For nonparametric error bars, it is a percentile width. This is explained further for each specific approach below.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The errorbar API described here was introduced in seaborn v0.12. In prior versions, the only options were to show a bootstrap confidence interval or a standard deviation, with the choice controlled by the ci parameter (i.e., ci=&lt;size&gt; or ci=\"sd\").",
            "markdown"
        ],
        [
            "To compare the different parameterizations, we\u2019ll use the following helper function:",
            "markdown"
        ],
        [
            "def plot_errorbars(arg, **kws):\n    np.random.seed(sum(map(ord, \"error_bars\")))\n    x = np.random.normal(0, 1, 100)\n    f, axs = plt.subplots(2, figsize=(7, 2), sharex=True, layout=\"tight\")\n    sns.pointplot(x=x, errorbar=arg, **kws, capsize=.3, ax=axs[0])\n    sns.stripplot(x=x, jitter=.3, ax=axs[1])\n",
            "code"
        ]
    ],
    "seaborn->Statistical operations->Statistical estimation and error bars->Measures of data spread": [
        [
            "Error bars that represent data spread present a compact display of the distribution, using three numbers where  would use 5 or more and  would use a complicated algorithm.",
            "markdown"
        ]
    ],
    "seaborn->Statistical operations->Statistical estimation and error bars->Measures of data spread->Standard deviation error bars": [
        [
            "Standard deviation error bars are the simplest to explain, because the standard deviation is a familiar statistic. It is the average distance from each data point to the sample mean. By default, errorbar=\"sd\" will draw error bars at +/- 1 sd around the estimate, but the range can be increased by passing a scaling size parameter. Note that, assuming normally-distributed data, ~68% of the data will lie within one standard deviation, ~95% will lie within two, and ~99.7% will lie within three:",
            "markdown"
        ],
        [
            "plot_errorbars(\"sd\")\n",
            "code"
        ]
    ],
    "seaborn->Statistical operations->Statistical estimation and error bars->Measures of data spread->Percentile interval error bars": [
        [
            "Percentile intervals also represent the range where some amount of the data fall, but they do so by\ncomputing those percentiles directly from your sample. By default, errorbar=\"pi\" will show a 95% interval, ranging from the 2.5 to the 97.5 percentiles. You can choose a different range by passing a size parameter, e.g., to show the inter-quartile range:",
            "markdown"
        ],
        [
            "plot_errorbars((\"pi\", 50))\n",
            "code"
        ],
        [
            "The standard deviation error bars will always be symmetrical around the estimate. This can be a problem when the data are skewed, especially if there are natural bounds (e.g., if the data represent a quantity that can only be positive). In some cases, standard deviation error bars may extend to \u201cimpossible\u201d values. The nonparametric approach does not have this problem, because it can account for asymmetrical spread and will never extend beyond the range of the data.",
            "markdown"
        ]
    ],
    "seaborn->Statistical operations->Statistical estimation and error bars->Measures of estimate uncertainty": [
        [
            "If your data are a random sample from a larger population, then the mean (or other estimate) will be an imperfect measure of the true population average. Error bars that show estimate uncertainty try to represent the range of likely values for the true parameter.",
            "markdown"
        ]
    ],
    "seaborn->Statistical operations->Statistical estimation and error bars->Measures of estimate uncertainty->Standard error bars": [
        [
            "The standard error statistic is related to the standard deviation: in fact it is just the standard deviation divided by the square root of the sample size. The default, with errorbar=\"se\", draws an interval +/-1 standard error from the mean:",
            "markdown"
        ],
        [
            "plot_errorbars(\"se\")\n",
            "code"
        ]
    ],
    "seaborn->Statistical operations->Statistical estimation and error bars->Measures of estimate uncertainty->Confidence interval error bars": [
        [
            "The nonparametric approach to representing uncertainty uses <em>bootstrapping</em>: a procedure where the dataset is randomly resampled with replacement a number of times, and the estimate is recalculated from each resample. This procedure creates a distribution of statistics approximating the distribution of values that you could have gotten for your estimate if you had a different sample.",
            "markdown"
        ],
        [
            "The confidence interval is constructed by taking a percentile interval of the <em>bootstrap distribution</em>. By default errorbar=\"ci\" draws a 95% confidence interval:",
            "markdown"
        ],
        [
            "plot_errorbars(\"ci\")\n",
            "code"
        ],
        [
            "The seaborn terminology is somewhat specific, because a confidence interval in statistics can be parametric or nonparametric. To draw a parametric confidence interval, you scale the standard error, using a formula similar to the one mentioned above. For example, an approximate 95% confidence interval can be constructed by taking the mean +/- two standard errors:",
            "markdown"
        ],
        [
            "plot_errorbars((\"se\", 2))\n",
            "code"
        ],
        [
            "The nonparametric bootstrap has advantages similar to those of the percentile interval: it will naturally adapt to skewed and bounded data in a way that a standard error interval cannot. It is also more general. While the standard error formula is specific to the mean, error bars can be computed using the bootstrap for any estimator:",
            "markdown"
        ],
        [
            "plot_errorbars(\"ci\", estimator=\"median\")\n",
            "code"
        ],
        [
            "Bootstrapping involves randomness, and the error bars will appear slightly different each time you run the code that creates them. A few parameters control this. One sets the number of iterations (n_boot): with more iterations, the resulting intervals will be more stable. The other sets the seed for the random number generator, which will ensure identical results:",
            "markdown"
        ],
        [
            "plot_errorbars(\"ci\", n_boot=5000, seed=10)\n",
            "code"
        ],
        [
            "Because of its iterative process, bootstrap intervals can be expensive to compute, especially for large datasets. But because uncertainty decreases with sample size, it may be more informative in that case to use an error bar that represents data spread.",
            "markdown"
        ]
    ],
    "seaborn->Statistical operations->Statistical estimation and error bars->Measures of estimate uncertainty->Custom error bars": [
        [
            "If these recipes are not sufficient, it is also possible to pass a generic function to the errorbar parameter. This function should take a vector and produce a pair of values representing the minimum and maximum points of the interval:",
            "markdown"
        ],
        [
            "plot_errorbars(lambda x: (x.min(), x.max()))\n",
            "code"
        ],
        [
            "(In practice, you could show the full range of the data with errorbar=(\"pi\", 100) rather than the custom function shown above).",
            "markdown"
        ],
        [
            "Note that seaborn functions cannot currently draw error bars from values that have been calculated externally, although matplotlib functions can be used to add such error bars to seaborn plots.",
            "markdown"
        ]
    ],
    "seaborn->Statistical operations->Statistical estimation and error bars->Error bars on regression fits": [
        [
            "The preceding discussion has focused on error bars shown around parameter estimates for aggregate data. Error bars also arise in seaborn when estimating regression models to visualize relationships. Here, the error bars will be represented by a \u201cband\u201d around the regression line:",
            "markdown"
        ],
        [
            "x = np.random.normal(0, 1, 50)\ny = x * 2 + np.random.normal(0, 2, size=x.size)\nsns.regplot(x=x, y=y)\n",
            "code"
        ],
        [
            "Currently, the error bars on a regression estimate are less flexible, only showing a confidence interval with a size set through ci=. This may change in the future.",
            "markdown"
        ]
    ],
    "seaborn->Statistical operations->Statistical estimation and error bars->Are error bars enough?": [
        [
            "You should always ask yourself whether it\u2019s best to use a plot that displays only a summary statistic and error bar. In many cases, it isn\u2019t.",
            "markdown"
        ],
        [
            "If you are interested in questions about summaries (such as whether the mean value differs between groups or increases over time), aggregation reduces the complexity of the plot and makes those inferences easier. But in doing so, it obscures valuable information about the underlying data points, such as the shape of the distributions and the presence of outliers.",
            "markdown"
        ],
        [
            "When analyzing your own data, don\u2019t be satisfied with summary statistics. Always look at the underlying distributions too. Sometimes, it can be helpful to combine both perspectives into the same figure. Many seaborn functions can help with this task, especially those discussed in the .",
            "markdown"
        ]
    ],
    "seaborn->Statistical operations->Estimating regression fits": [
        [
            "Many datasets contain multiple quantitative variables, and the goal of an analysis is often to relate those variables to each other. We  functions that can accomplish this by showing the joint distribution of two variables. It can be very helpful, though, to use statistical models to estimate a simple relationship between two noisy sets of observations. The functions discussed in this chapter will do so through the common framework of linear regression.",
            "markdown"
        ],
        [
            "In the spirit of Tukey, the regression plots in seaborn are primarily intended to add a visual guide that helps to emphasize patterns in a dataset during exploratory data analyses. That is to say that seaborn is not itself a package for statistical analysis. To obtain quantitative measures related to the fit of regression models, you should use . The goal of seaborn, however, is to make exploring a dataset through visualization quick and easy, as doing so is just as (if not more) important than exploring a dataset through tables of statistics.",
            "markdown"
        ]
    ],
    "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models": [
        [
            "The two functions that can be used to visualize a linear fit are  and .",
            "markdown"
        ],
        [
            "In the simplest invocation, both functions draw a scatterplot of two variables, x and y, and then fit the regression model y ~ x and plot the resulting regression line and a 95% confidence interval for that regression:",
            "markdown"
        ],
        [
            "tips = sns.load_dataset(\"tips\")\nsns.regplot(x=\"total_bill\", y=\"tip\", data=tips);\n",
            "code"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"tip\", data=tips);\n",
            "code"
        ],
        [
            "These functions draw similar plots, but  is an , and  is a figure-level function. Additionally,  accepts the x and y variables in a variety of formats including simple numpy arrays,  objects, or as references to variables in a  object passed to data. In contrast,  has data as a required parameter and the x and y variables must be specified as strings. Finally, only  has hue as a parameter.",
            "markdown"
        ],
        [
            "The core functionality is otherwise similar, though, so this tutorial will focus on :.",
            "markdown"
        ],
        [
            "It\u2019s possible to fit a linear regression when one of the variables takes discrete values, however, the simple scatterplot produced by this kind of dataset is often not optimal:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"size\", y=\"tip\", data=tips);\n",
            "code"
        ],
        [
            "One option is to add some random noise (\u201cjitter\u201d) to the discrete values to make the distribution of those values more clear. Note that jitter is applied only to the scatterplot data and does not influence the regression line fit itself:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_jitter=.05);\n",
            "code"
        ],
        [
            "A second option is to collapse over the observations in each discrete bin to plot an estimate of central tendency along with a confidence interval:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_estimator=np.mean);\n",
            "code"
        ]
    ],
    "seaborn->Statistical operations->Estimating regression fits->Fitting different kinds of models": [
        [
            "The simple linear regression model used above is very simple to fit, however, it is not appropriate for some kinds of datasets. The  dataset shows a few examples where simple linear regression provides an identical estimate of a relationship where simple visual inspection clearly shows differences. For example, in the first case, the linear regression is a good model:",
            "markdown"
        ],
        [
            "anscombe = sns.load_dataset(\"anscombe\")\n",
            "code"
        ],
        [
            "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'I'\"),\n           ci=None, scatter_kws={\"s\": 80});\n",
            "code"
        ],
        [
            "The linear relationship in the second dataset is the same, but the plot clearly shows that this is not a good model:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'II'\"),\n           ci=None, scatter_kws={\"s\": 80});\n",
            "code"
        ],
        [
            "In the presence of these kind of higher-order relationships,  and  can fit a polynomial regression model to explore simple kinds of nonlinear trends in the dataset:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'II'\"),\n           order=2, ci=None, scatter_kws={\"s\": 80});\n",
            "code"
        ],
        [
            "A different problem is posed by \u201coutlier\u201d observations that deviate for some reason other than the main relationship under study:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'III'\"),\n           ci=None, scatter_kws={\"s\": 80});\n",
            "code"
        ],
        [
            "In the presence of outliers, it can be useful to fit a robust regression, which uses a different loss function to downweight relatively large residuals:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'III'\"),\n           robust=True, ci=None, scatter_kws={\"s\": 80});\n",
            "code"
        ],
        [
            "When the y variable is binary, simple linear regression also \u201cworks\u201d but provides implausible predictions:",
            "markdown"
        ],
        [
            "tips[\"big_tip\"] = (tips.tip / tips.total_bill) &gt; .15\nsns.lmplot(x=\"total_bill\", y=\"big_tip\", data=tips,\n           y_jitter=.03);\n",
            "code"
        ],
        [
            "The solution in this case is to fit a logistic regression, such that the regression line shows the estimated probability of y = 1 for a given value of x:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"big_tip\", data=tips,\n           logistic=True, y_jitter=.03);\n",
            "code"
        ],
        [
            "Note that the logistic regression estimate is considerably more computationally intensive (this is true of robust regression as well). As the confidence interval around the regression line is computed using a bootstrap procedure, you may wish to turn this off for faster iteration (using ci=None).",
            "markdown"
        ],
        [
            "An altogether different approach is to fit a nonparametric regression using a . This approach has the fewest assumptions, although it is computationally intensive and so currently confidence intervals are not computed at all:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"tip\", data=tips,\n           lowess=True, line_kws={\"color\": \"C1\"});\n",
            "code"
        ],
        [
            "The  function can be a useful tool for checking whether the simple regression model is appropriate for a dataset. It fits and removes a simple linear regression and then plots the residual values for each observation. Ideally, these values should be randomly scattered around y = 0:",
            "markdown"
        ],
        [
            "sns.residplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'I'\"),\n              scatter_kws={\"s\": 80});\n",
            "code"
        ],
        [
            "If there is structure in the residuals, it suggests that simple linear regression is not appropriate:",
            "markdown"
        ],
        [
            "sns.residplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'II'\"),\n              scatter_kws={\"s\": 80});\n",
            "code"
        ]
    ],
    "seaborn->Statistical operations->Estimating regression fits->Conditioning on other variables": [
        [
            "The plots above show many ways to explore the relationship between a pair of variables. Often, however, a more interesting question is \u201chow does the relationship between these two variables change as a function of a third variable?\u201d This is where the main differences between  and  appear. While  always shows a single relationship,  combines  with  to show multiple fits using hue mapping or faceting.",
            "markdown"
        ],
        [
            "The best way to separate out a relationship is to plot both levels on the same axes and to use color to distinguish them:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips);\n",
            "code"
        ],
        [
            "Unlike , it\u2019s not possible to map a distinct variable to the style properties of the scatter plot, but you can redundantly code the hue variable with marker shape:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips,\n           markers=[\"o\", \"x\"], palette=\"Set1\");\n",
            "code"
        ],
        [
            "To add another variable, you can draw multiple \u201cfacets\u201d with each level of the variable appearing in the rows or columns of the grid:",
            "markdown"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", col=\"time\", data=tips);\n",
            "code"
        ],
        [
            "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\",\n           col=\"time\", row=\"sex\", data=tips, height=3);\n",
            "code"
        ]
    ],
    "seaborn->Statistical operations->Estimating regression fits->Plotting a regression in other contexts": [
        [
            "A few other seaborn functions use  in the context of a larger, more complex plot. The first is the  function that we introduced in the . In addition to the plot styles previously discussed,  can use  to show the linear regression fit on the joint axes by passing kind=\"reg\":",
            "markdown"
        ],
        [
            "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\");\n",
            "code"
        ],
        [
            "Using the  function with kind=\"reg\" combines  and  to show the linear relationship between variables in a dataset. Take care to note how this is different from . In the figure below, the two axes don\u2019t show the same relationship conditioned on two levels of a third variable; rather,  is used to show multiple relationships between different pairings of the variables in a dataset:",
            "markdown"
        ],
        [
            "sns.pairplot(tips, x_vars=[\"total_bill\", \"size\"], y_vars=[\"tip\"],\n             height=5, aspect=.8, kind=\"reg\");\n",
            "code"
        ],
        [
            "Conditioning on an additional categorical variable is built into both of these functions using the hue parameter:",
            "markdown"
        ],
        [
            "sns.pairplot(tips, x_vars=[\"total_bill\", \"size\"], y_vars=[\"tip\"],\n             hue=\"smoker\", height=5, aspect=.8, kind=\"reg\");\n",
            "code"
        ]
    ],
    "seaborn->Multi-plot grids->Building structured multi-plot grids": [
        [
            "When exploring multi-dimensional data, a useful approach is to draw multiple instances of the same plot on different subsets of your dataset. This technique is sometimes called either \u201clattice\u201d or \u201ctrellis\u201d plotting, and it is related to the idea of . It allows a viewer to quickly extract a large amount of information about a complex dataset. Matplotlib offers good support for making figures with multiple axes; seaborn builds on top of this to directly link the structure of the plot to the structure of your dataset.",
            "markdown"
        ],
        [
            "The  functions are built on top of the objects discussed in this chapter of the tutorial. In most cases, you will want to work with those functions. They take care of some important bookkeeping that synchronizes the multiple plots in each grid. This chapter explains how the underlying objects work, which may be useful for advanced applications.",
            "markdown"
        ]
    ],
    "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples": [
        [
            "The  class is useful when you want to visualize the distribution of a variable or the relationship between multiple variables separately within subsets of your dataset. A  can be drawn with up to three dimensions: row, col, and hue. The first two have obvious correspondence with the resulting array of axes; think of the hue variable as a third dimension along a depth axis, where different levels are plotted with different colors.",
            "markdown"
        ],
        [
            "Each of , , , and  use this object internally, and they return the object when they are finished so that it can be used for further tweaking.",
            "markdown"
        ],
        [
            "The class is used by initializing a  object with a dataframe and the names of the variables that will form the row, column, or hue dimensions of the grid. These variables should be categorical or discrete, and then the data at each level of the variable will be used for a facet along that axis. For example, say we wanted to examine differences between lunch and dinner in the tips dataset:",
            "markdown"
        ],
        [
            "tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "code"
        ],
        [
            "Initializing the grid like this sets up the matplotlib figure and axes, but doesn\u2019t draw anything on them.",
            "markdown"
        ],
        [
            "The main approach for visualizing data on this grid is with the  method. Provide it with a plotting function and the name(s) of variable(s) in the dataframe to plot. Let\u2019s look at the distribution of tips in each of these subsets, using a histogram:",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "code"
        ],
        [
            "This function will draw the figure and annotate the axes, hopefully producing a finished plot in one step. To make a relational plot, just pass multiple variable names. You can also provide keyword arguments, which will be passed to the plotting function:",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(tips, col=\"sex\", hue=\"smoker\")\ng.map(sns.scatterplot, \"total_bill\", \"tip\", alpha=.7)\ng.add_legend()\n",
            "code"
        ],
        [
            "There are several options for controlling the look of the grid that can be passed to the class constructor.",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(tips, row=\"smoker\", col=\"time\", margin_titles=True)\ng.map(sns.regplot, \"size\", \"total_bill\", color=\".3\", fit_reg=False, x_jitter=.1)\n",
            "code"
        ],
        [
            "Note that margin_titles isn\u2019t formally supported by the matplotlib API, and may not work well in all cases. In particular, it currently can\u2019t be used with a legend that lies outside of the plot.",
            "markdown"
        ],
        [
            "The size of the figure is set by providing the height of <em>each</em> facet, along with the aspect ratio:",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(tips, col=\"day\", height=4, aspect=.5)\ng.map(sns.barplot, \"sex\", \"total_bill\", order=[\"Male\", \"Female\"])\n",
            "code"
        ],
        [
            "The default ordering of the facets is derived from the information in the DataFrame. If the variable used to define facets has a categorical type, then the order of the categories is used. Otherwise, the facets will be in the order of appearance of the category levels. It is possible, however, to specify an ordering of any facet dimension with the appropriate *_order parameter:",
            "markdown"
        ],
        [
            "ordered_days = tips.day.value_counts().index\ng = sns.FacetGrid(tips, row=\"day\", row_order=ordered_days,\n                  height=1.7, aspect=4,)\ng.map(sns.kdeplot, \"total_bill\")\n",
            "code"
        ],
        [
            "Any seaborn color palette (i.e., something that can be passed to ) can be provided. You can also use a dictionary that maps the names of values in the hue variable to valid matplotlib colors:",
            "markdown"
        ],
        [
            "pal = dict(Lunch=\"seagreen\", Dinner=\".7\")\ng = sns.FacetGrid(tips, hue=\"time\", palette=pal, height=5)\ng.map(sns.scatterplot, \"total_bill\", \"tip\", s=100, alpha=.5)\ng.add_legend()\n",
            "code"
        ],
        [
            "If you have many levels of one variable, you can plot it along the columns but \u201cwrap\u201d them so that they span multiple rows. When doing this, you cannot use a row variable.",
            "markdown"
        ],
        [
            "attend = sns.load_dataset(\"attention\").query(\"subject &lt;= 12\")\ng = sns.FacetGrid(attend, col=\"subject\", col_wrap=4, height=2, ylim=(0, 10))\ng.map(sns.pointplot, \"solutions\", \"score\", order=[1, 2, 3], color=\".3\", errorbar=None)\n",
            "code"
        ],
        [
            "Once you\u2019ve drawn a plot using  (which can be called multiple times), you may want to adjust some aspects of the plot. There are also a number of methods on the  object for manipulating the figure at a higher level of abstraction. The most general is , and there are other more specialized methods like , which respects the fact that interior facets do not have axis labels. For example:",
            "markdown"
        ],
        [
            "with sns.axes_style(\"white\"):\n    g = sns.FacetGrid(tips, row=\"sex\", col=\"smoker\", margin_titles=True, height=2.5)\ng.map(sns.scatterplot, \"total_bill\", \"tip\", color=\"#334488\")\ng.set_axis_labels(\"Total bill (US Dollars)\", \"Tip\")\ng.set(xticks=[10, 30, 50], yticks=[2, 6, 10])\ng.figure.subplots_adjust(wspace=.02, hspace=.02)\n",
            "code"
        ],
        [
            "For even more customization, you can  work directly with the underling matplotlib Figure and Axes objects, which are stored as member attributes at figure and axes_dict, respectively. When making a figure without row or column faceting, you can also use the ax attribute to directly access the single axes.",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(tips, col=\"smoker\", margin_titles=True, height=4)\ng.map(plt.scatter, \"total_bill\", \"tip\", color=\"#338844\", edgecolor=\"white\", s=50, lw=1)\nfor ax in g.axes_dict.values():\n    ax.axline((0, 0), slope=.2, c=\".2\", ls=\"--\", zorder=0)\ng.set(xlim=(0, 60), ylim=(0, 14))\n",
            "code"
        ]
    ],
    "seaborn->Multi-plot grids->Building structured multi-plot grids->Using custom functions": [
        [
            "You\u2019re not limited to existing matplotlib and seaborn functions when using . However, to work properly, any function you use must follow a few rules:",
            "markdown"
        ],
        [
            "It must plot onto the \u201ccurrently active\u201d matplotlib Axes. This will be true of functions in the matplotlib.pyplot namespace, and you can call  to get a reference to the current Axes if you want to work directly with its methods.",
            "markdown"
        ],
        [
            "It must accept the data that it plots in positional arguments. Internally,  will pass a Series of data for each of the named positional arguments passed to .",
            "markdown"
        ],
        [
            "It must be able to accept color and label keyword arguments, and, ideally, it will do something useful with them. In most cases, it\u2019s easiest to catch a generic dictionary of **kwargs and pass it along to the underlying plotting function.",
            "markdown"
        ],
        [
            "Let\u2019s look at minimal example of a function you can plot with. This function will just take a single vector of data for each facet:",
            "markdown"
        ],
        [
            "from scipy import stats\ndef quantile_plot(x, **kwargs):\n    quantiles, xr = stats.probplot(x, fit=False)\n    plt.scatter(xr, quantiles, **kwargs)\n\ng = sns.FacetGrid(tips, col=\"sex\", height=4)\ng.map(quantile_plot, \"total_bill\")\n",
            "code"
        ],
        [
            "If we want to make a bivariate plot, you should write the function so that it accepts the x-axis variable first and the y-axis variable second:",
            "markdown"
        ],
        [
            "def qqplot(x, y, **kwargs):\n    _, xr = stats.probplot(x, fit=False)\n    _, yr = stats.probplot(y, fit=False)\n    plt.scatter(xr, yr, **kwargs)\n\ng = sns.FacetGrid(tips, col=\"smoker\", height=4)\ng.map(qqplot, \"total_bill\", \"tip\")\n",
            "code"
        ],
        [
            "Because  accepts color and label keyword arguments and does the right thing with them, we can add a hue facet without any difficulty:",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(tips, hue=\"time\", col=\"sex\", height=4)\ng.map(qqplot, \"total_bill\", \"tip\")\ng.add_legend()\n",
            "code"
        ],
        [
            "Sometimes, though, you\u2019ll want to map a function that doesn\u2019t work the way you expect with the color and label keyword arguments. In this case, you\u2019ll want to explicitly catch them and handle them in the logic of your custom function. For example, this approach will allow use to map , which otherwise does not play well with the  API:",
            "markdown"
        ],
        [
            "def hexbin(x, y, color, **kwargs):\n    cmap = sns.light_palette(color, as_cmap=True)\n    plt.hexbin(x, y, gridsize=15, cmap=cmap, **kwargs)\n\nwith sns.axes_style(\"dark\"):\n    g = sns.FacetGrid(tips, hue=\"time\", col=\"time\", height=4)\ng.map(hexbin, \"total_bill\", \"tip\", extent=[0, 50, 0, 10]);\n",
            "code"
        ]
    ],
    "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships": [
        [
            " also allows you to quickly draw a grid of small subplots using the same plot type to visualize data in each. In a , each row and column is assigned to a different variable, so the resulting plot shows each pairwise relationship in the dataset. This style of plot is sometimes called a \u201cscatterplot matrix\u201d, as this is the most common way to show each relationship, but  is not limited to scatterplots.",
            "markdown"
        ],
        [
            "It\u2019s important to understand the differences between a  and a . In the former, each facet shows the same relationship conditioned on different levels of other variables. In the latter, each plot shows a different relationship (although the upper and lower triangles will have mirrored plots). Using  can give you a very quick, very high-level summary of interesting relationships in your dataset.",
            "markdown"
        ],
        [
            "The basic usage of the class is very similar to . First you initialize the grid, then you pass plotting function to a map method and it will be called on each subplot. There is also a companion function,  that trades off some flexibility for faster plotting.",
            "markdown"
        ],
        [
            "iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "code"
        ],
        [
            "It\u2019s possible to plot a different function on the diagonal to show the univariate distribution of the variable in each column. Note that the axis ticks won\u2019t correspond to the count or density axis of this plot, though.",
            "markdown"
        ],
        [
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "code"
        ],
        [
            "A very common way to use this plot colors the observations by a separate categorical variable. For example, the iris dataset has four measurements for each of three different species of iris flowers so you can see how they differ.",
            "markdown"
        ],
        [
            "g = sns.PairGrid(iris, hue=\"species\")\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\ng.add_legend()\n",
            "code"
        ],
        [
            "By default every numeric column in the dataset is used, but you can focus on particular relationships if you want.",
            "markdown"
        ],
        [
            "g = sns.PairGrid(iris, vars=[\"sepal_length\", \"sepal_width\"], hue=\"species\")\ng.map(sns.scatterplot)\n",
            "code"
        ],
        [
            "It\u2019s also possible to use a different function in the upper and lower triangles to emphasize different aspects of the relationship.",
            "markdown"
        ],
        [
            "g = sns.PairGrid(iris)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw=3, legend=False)\n",
            "code"
        ],
        [
            "The square grid with identity relationships on the diagonal is actually just a special case, and you can plot with different variables in the rows and columns.",
            "markdown"
        ],
        [
            "g = sns.PairGrid(tips, y_vars=[\"tip\"], x_vars=[\"total_bill\", \"size\"], height=4)\ng.map(sns.regplot, color=\".3\")\ng.set(ylim=(-1, 11), yticks=[0, 5, 10])\n",
            "code"
        ],
        [
            "Of course, the aesthetic attributes are configurable. For instance, you can use a different palette (say, to show an ordering of the hue variable) and pass keyword arguments into the plotting functions.",
            "markdown"
        ],
        [
            "g = sns.PairGrid(tips, hue=\"size\", palette=\"GnBu_d\")\ng.map(plt.scatter, s=50, edgecolor=\"white\")\ng.add_legend()\n",
            "code"
        ],
        [
            " is flexible, but to take a quick look at a dataset, it can be easier to use . This function uses scatterplots and histograms by default, although a few other kinds will be added (currently, you can also plot regression plots on the off-diagonals and KDEs on the diagonal).",
            "markdown"
        ],
        [
            "sns.pairplot(iris, hue=\"species\", height=2.5)\n",
            "code"
        ],
        [
            "You can also control the aesthetics of the plot with keyword arguments, and it returns the  instance for further tweaking.",
            "markdown"
        ],
        [
            "g = sns.pairplot(iris, hue=\"species\", palette=\"Set2\", diag_kind=\"kde\", height=2.5)\n",
            "code"
        ]
    ],
    "seaborn->Figure aesthetics->Controlling figure aesthetics": [
        [
            "Drawing attractive figures is important. When making figures for yourself, as you explore a dataset, it\u2019s nice to have plots that are pleasant to look at. Visualizations are also central to communicating quantitative insights to an audience, and in that setting it\u2019s even more necessary to have figures that catch the attention and draw a viewer in.",
            "markdown"
        ],
        [
            "Matplotlib is highly customizable, but it can be hard to know what settings to tweak to achieve an attractive plot. Seaborn comes with a number of customized themes and a high-level interface for controlling the look of matplotlib figures.",
            "markdown"
        ],
        [
            "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n",
            "code"
        ],
        [
            "Let\u2019s define a simple function to plot some offset sine waves, which will help us see the different stylistic parameters we can tweak.",
            "markdown"
        ],
        [
            "def sinplot(n=10, flip=1):\n    x = np.linspace(0, 14, 100)\n    for i in range(1, n + 1):\n        plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip)\n",
            "code"
        ],
        [
            "This is what the plot looks like with matplotlib defaults:",
            "markdown"
        ],
        [
            "sinplot()\n",
            "code"
        ],
        [
            "To switch to seaborn defaults, simply call the  function.",
            "markdown"
        ],
        [
            "sns.set_theme()\nsinplot()\n",
            "code"
        ],
        [
            "(Note that in versions of seaborn prior to 0.8,  was called on import. On later versions, it must be explicitly invoked).",
            "markdown"
        ],
        [
            "Seaborn splits matplotlib parameters into two independent groups. The first group sets the aesthetic style of the plot, and the second scales various elements of the figure so that it can be easily incorporated into different contexts.",
            "markdown"
        ],
        [
            "The interface for manipulating these parameters are two pairs of functions. To control the style, use the  and  functions. To scale the plot, use the  and  functions. In both cases, the first function returns a dictionary of parameters and the second sets the matplotlib defaults.",
            "markdown"
        ]
    ],
    "seaborn->Figure aesthetics->Controlling figure aesthetics->Seaborn figure styles": [
        [
            "There are five preset seaborn themes: darkgrid, whitegrid, dark, white, and ticks. They are each suited to different applications and personal preferences. The default theme is darkgrid. As mentioned above, the grid helps the plot serve as a lookup table for quantitative information, and the white-on grey helps to keep the grid from competing with lines that represent data. The whitegrid theme is similar, but it is better suited to plots with heavy data elements:",
            "markdown"
        ],
        [
            "sns.set_style(\"whitegrid\")\ndata = np.random.normal(size=(20, 6)) + np.arange(6) / 2\nsns.boxplot(data=data);\n",
            "code"
        ],
        [
            "For many plots, (especially for settings like talks, where you primarily want to use figures to provide impressions of patterns in the data), the grid is less necessary.",
            "markdown"
        ],
        [
            "sns.set_style(\"dark\")\nsinplot()\n",
            "code"
        ],
        [
            "sns.set_style(\"white\")\nsinplot()\n",
            "code"
        ],
        [
            "Sometimes you might want to give a little extra structure to the plots, which is where ticks come in handy:",
            "markdown"
        ],
        [
            "sns.set_style(\"ticks\")\nsinplot()\n",
            "code"
        ]
    ],
    "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines": [
        [
            "Both the white and ticks styles can benefit from removing the top and right axes spines, which are not needed. The seaborn function  can be called to remove them:",
            "markdown"
        ],
        [
            "sinplot()\nsns.despine()\n",
            "code"
        ],
        [
            "Some plots benefit from offsetting the spines away from the data, which can also be done when calling . When the ticks don\u2019t cover the whole range of the axis, the trim parameter will limit the range of the surviving spines.",
            "markdown"
        ],
        [
            "f, ax = plt.subplots()\nsns.violinplot(data=data)\nsns.despine(offset=10, trim=True);\n",
            "code"
        ],
        [
            "You can also control which spines are removed with additional arguments to :",
            "markdown"
        ],
        [
            "sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "code"
        ]
    ],
    "seaborn->Figure aesthetics->Controlling figure aesthetics->Temporarily setting figure style": [
        [
            "Although it\u2019s easy to switch back and forth, you can also use the  function in a with statement to temporarily set plot parameters. This also allows you to make figures with differently-styled axes:",
            "markdown"
        ],
        [
            "f = plt.figure(figsize=(6, 6))\ngs = f.add_gridspec(2, 2)\n\nwith sns.axes_style(\"darkgrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    sinplot(6)\n\nwith sns.axes_style(\"white\"):\n    ax = f.add_subplot(gs[0, 1])\n    sinplot(6)\n\nwith sns.axes_style(\"ticks\"):\n    ax = f.add_subplot(gs[1, 0])\n    sinplot(6)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[1, 1])\n    sinplot(6)\n\nf.tight_layout()\n",
            "code"
        ]
    ],
    "seaborn->Figure aesthetics->Controlling figure aesthetics->Overriding elements of the seaborn styles": [
        [
            "If you want to customize the seaborn styles, you can pass a dictionary of parameters to the rc argument of  and . Note that you can only override the parameters that are part of the style definition through this method. (However, the higher-level  function takes a dictionary of any matplotlib parameters).",
            "markdown"
        ],
        [
            "If you want to see what parameters are included, you can just call the function with no arguments, which will return the current settings:",
            "markdown"
        ],
        [
            "sns.axes_style()\n",
            "code"
        ],
        [
            "You can then set different versions of these parameters:",
            "markdown"
        ],
        [
            "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\nsinplot()\n",
            "code"
        ]
    ],
    "seaborn->Figure aesthetics->Controlling figure aesthetics->Scaling plot elements": [
        [
            "A separate set of parameters control the scale of plot elements, which should let you use the same code to make plots that are suited for use in settings where larger or smaller plots are appropriate.",
            "markdown"
        ],
        [
            "First let\u2019s reset the default parameters by calling :",
            "markdown"
        ],
        [
            "sns.set_theme()\n",
            "code"
        ],
        [
            "The four preset contexts, in order of relative size, are paper, notebook, talk, and poster. The notebook style is the default, and was used in the plots above.",
            "markdown"
        ],
        [
            "sns.set_context(\"paper\")\nsinplot()\n",
            "code"
        ],
        [
            "sns.set_context(\"talk\")\nsinplot()\n",
            "code"
        ],
        [
            "sns.set_context(\"poster\")\nsinplot()\n",
            "code"
        ],
        [
            "Most of what you now know about the style functions should transfer to the context functions.",
            "markdown"
        ],
        [
            "You can call  with one of these names to set the parameters, and you can override the parameters by providing a dictionary of parameter values.",
            "markdown"
        ],
        [
            "You can also independently scale the size of the font elements when changing the context. (This option is also available through the top-level  function).",
            "markdown"
        ],
        [
            "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\nsinplot()\n",
            "code"
        ],
        [
            "Similarly, you can temporarily control the scale of figures nested under a with statement.",
            "markdown"
        ],
        [
            "Both the style and the context can be quickly configured with the  function. This function also sets the default color palette, but that will be covered in more detail in the  of the tutorial.",
            "markdown"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes": [
        [
            "Seaborn makes it easy to use colors that are well-suited to the characteristics of your data and your visualization goals. This chapter discusses both the general principles that should guide your choices and the tools in seaborn that help you quickly find the best solution for a given application.",
            "markdown"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->General principles for using color in plots->Components of color": [
        [
            "Because of the way our eyes work, a particular color can be defined using three components. We usually program colors in a computer by specifying their RGB values, which set the intensity of the red, green, and blue channels in a display. But for analyzing the perceptual attributes of a color, it\u2019s better to think in terms of <em>hue</em>, <em>saturation</em>, and <em>luminance</em> channels.",
            "markdown"
        ],
        [
            "Hue is the component that distinguishes \u201cdifferent colors\u201d in a non-technical sense. It\u2019s property of color that leads to first-order names like \u201cred\u201d and \u201cblue\u201d:",
            "markdown"
        ],
        [
            "Saturation (or chroma) is the <em>colorfulness</em>. Two colors with different hues will look more distinct when they have more saturation:",
            "markdown"
        ],
        [
            "And lightness corresponds to how much light is emitted (or reflected, for printed colors), ranging from black to white:",
            "markdown"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->General principles for using color in plots->Vary hue to distinguish categories": [
        [
            "When you want to represent multiple categories in a plot, you typically should vary the color of the elements. Consider this simple example: in which of these two plots is it easier to count the number of triangular points?",
            "markdown"
        ],
        [
            "In the plot on the right, the orange triangles \u201cpop out\u201d, making it easy to distinguish them from the circles. This pop-out effect happens because our visual system prioritizes color differences.",
            "markdown"
        ],
        [
            "The blue and orange colors differ mostly in terms of their hue. Hue is useful for representing categories: most people can distinguish a moderate number of hues relatively easily, and points that have different hues but similar brightness or intensity seem equally important. It also makes plots easier to talk about. Consider this example:",
            "markdown"
        ],
        [
            "Most people would be able to quickly ascertain that there are five distinct categories in the plot on the left and, if asked to characterize the \u201cblue\u201d points, would be able to do so.",
            "markdown"
        ],
        [
            "With the plot on the right, where the points are all blue but vary in their luminance and saturation, it\u2019s harder to say how many unique categories are present. And how would we talk about a particular category? \u201cThe fairly-but-not-too-blue points?\u201d What\u2019s more, the gray dots seem to fade into the background, de-emphasizing them relative to the more intense blue dots. If the categories are equally important, this is a poor representation.",
            "markdown"
        ],
        [
            "So as a general rule, use hue variation to represent categories. With that said, here are few notes of caution. If you have more than a handful of colors in your plot, it can become difficult to keep in mind what each one means, unless there are pre-existing associations between the categories and the colors used to represent them. This makes your plot harder to interpret: rather than focusing on the data, a viewer will have to continually refer to the legend to make sense of what is shown. So you should strive not to make plots that are too complex. And be mindful that not everyone sees colors the same way. Varying both shape (or some other attribute) and color can help people with anomalous color vision understand your plots, and it can keep them (somewhat) interpretable if they are printed to black-and-white.",
            "markdown"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->General principles for using color in plots->Vary luminance to represent numbers": [
        [
            "On the other hand, hue variations are not well suited to representing numeric data. Consider this example, where we need colors to represent the counts in a bivariate histogram. On the left, we use a circular colormap, where gradual changes in the number of observation within each bin correspond to gradual changes in hue. On the right, we use a palette that uses brighter colors to represent bins with larger counts:",
            "markdown"
        ],
        [
            "With the hue-based palette, it\u2019s quite difficult to ascertain the shape of the bivariate distribution. In contrast, the luminance palette makes it much more clear that there are two prominent peaks.",
            "markdown"
        ],
        [
            "Varying luminance helps you see structure in data, and changes in luminance are more intuitively processed as changes in importance. But the plot on the right does not use a grayscale colormap. Its colorfulness makes it more interesting, and the subtle hue variation increases the perceptual distance between two values. As a result, small differences slightly easier to resolve.",
            "markdown"
        ],
        [
            "These examples show that color palette choices are about more than aesthetics: the colors you choose can reveal patterns in your data if used effectively or hide them if used poorly. There is not one optimal palette, but there are palettes that are better or worse for particular datasets and visualization approaches.",
            "markdown"
        ],
        [
            "And aesthetics do matter: the more that people want to look at your figures, the greater the chance that they will learn something from them. This is true even when you are making plots for yourself. During exploratory data analysis, you may generate many similar figures. Varying the color palettes will add a sense of novelty, which keeps you engaged and prepared to notice interesting features of your data.",
            "markdown"
        ],
        [
            "So how can you choose color palettes that both represent your data well and look attractive?",
            "markdown"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Tools for choosing color palettes": [
        [
            "The most important function for working with color palettes is, aptly, . This function provides an interface to most of the possible ways that one can generate color palettes in seaborn. And it\u2019s used internally by any function that has a palette argument.",
            "markdown"
        ],
        [
            "The primary argument to  is usually a string: either the name of a specific palette or the name of a family and additional arguments to select a specific member. In the latter case,  will delegate to more specific function, such as . It\u2019s also possible to pass a list of colors specified any way that matplotlib accepts (an RGB tuple, a hex code, or a name in the X11 table). The return value is an object that wraps a list of RGB tuples with a few useful methods, such as conversion to hex codes and a rich HTML representation.",
            "markdown"
        ],
        [
            "Calling  with no arguments will return the current default color palette that matplotlib (and most seaborn functions) will use if colors are not otherwise specified. This default palette can be set with the corresponding  function, which calls  internally and accepts the same arguments.",
            "markdown"
        ],
        [
            "To motivate the different options that  provides, it will be useful to introduce a classification scheme for color palettes. Broadly, palettes fall into one of three categories:",
            "markdown"
        ],
        [
            "qualitative palettes, good for representing categorical data",
            "markdown"
        ],
        [
            "sequential palettes, good for representing numeric data",
            "markdown"
        ],
        [
            "diverging palettes, good for representing numeric data with a categorical boundary",
            "markdown"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Qualitative color palettes": [
        [
            "Qualitative palettes are well-suited to representing categorical data because most of their variation is in the hue component. The default color palette in seaborn is a qualitative palette with ten distinct hues:",
            "markdown"
        ],
        [
            "sns.color_palette()\n",
            "code"
        ],
        [
            "These colors have the same ordering as the default matplotlib color palette, \"tab10\", but they are a bit less intense. Compare:",
            "markdown"
        ],
        [
            "sns.color_palette(\"tab10\")\n",
            "code"
        ],
        [
            "Seaborn in fact has six variations of matplotlib\u2019s palette, called deep, muted, pastel, bright, dark, and colorblind. These span a range of average luminance and saturation values:",
            "markdown"
        ],
        [
            "Many people find the moderated hues of the default \"deep\" palette to be aesthetically pleasing, but they are also less distinct. As a result, they may be more difficult to discriminate in some contexts, which is something to keep in mind when making publication graphics.  can be helpful for estimating how the seaborn color palettes perform when simulating different forms of colorblindess.",
            "markdown"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Qualitative color palettes->Using circular color systems": [
        [
            "When you have an arbitrary number of categories, the easiest approach to finding unique hues is to draw evenly-spaced colors in a circular color space (one where the hue changes while keeping the brightness and saturation constant). This is what most seaborn functions default to when they need to use more colors than are currently set in the default color cycle.",
            "markdown"
        ],
        [
            "The most common way to do this uses the hls color space, which is a simple transformation of RGB values. We saw this color palette before as a counterexample for how to plot a histogram:",
            "markdown"
        ],
        [
            "sns.color_palette(\"hls\", 8)\n",
            "code"
        ],
        [
            "Because of the way the human visual system works, colors that have the same luminance and saturation in terms of their RGB values won\u2019t necessarily look equally intense To remedy this, seaborn provides an interface to the  system (since renamed to HSLuv), which achieves less intensity variation as you rotate around the color wheel:",
            "markdown"
        ],
        [
            "sns.color_palette(\"husl\", 8)\n",
            "code"
        ],
        [
            "When seaborn needs a categorical palette with more colors than are available in the current default, it will use this approach.",
            "markdown"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Qualitative color palettes->Using categorical Color Brewer palettes": [
        [
            "Another source of visually pleasing categorical palettes comes from the  tool (which also has sequential and diverging palettes, as we\u2019ll see below).",
            "markdown"
        ],
        [
            "sns.color_palette(\"Set2\")\n",
            "code"
        ],
        [
            "Be aware that the qualitative Color Brewer palettes have different lengths, and the default behavior of  is to give you the full list:",
            "markdown"
        ],
        [
            "sns.color_palette(\"Paired\")\n",
            "code"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Sequential color palettes": [
        [
            "The second major class of color palettes is called \u201csequential\u201d. This kind of mapping is appropriate when data range from relatively low or uninteresting values to relatively high or interesting values (or vice versa). As we saw above, the primary dimension of variation in a sequential palette is luminance. Some seaborn functions will default to a sequential palette when you are mapping numeric data. (For historical reasons, both categorical and numeric mappings are specified with the hue parameter in functions like  or , even though numeric mappings use color palettes with relatively little hue variation).",
            "markdown"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Sequential color palettes->Perceptually uniform palettes": [
        [
            "Because they are intended to represent numeric values, the best sequential palettes will be <em>perceptually uniform</em>, meaning that the relative discriminability of two colors is proportional to the difference between the corresponding data values. Seaborn includes four perceptually uniform sequential colormaps: \"rocket\", \"mako\", \"flare\", and \"crest\". The first two have a very wide luminance range and are well suited for applications such as heatmaps, where colors fill the space they are plotted into:",
            "markdown"
        ],
        [
            "sns.color_palette(\"rocket\", as_cmap=True)\n",
            "code"
        ],
        [
            "sns.color_palette(\"mako\", as_cmap=True)\n",
            "code"
        ],
        [
            "Because the extreme values of these colormaps approach white, they are not well-suited for coloring elements such as lines or points: it will be difficult to discriminate important values against a white or gray background. The \u201cflare\u201d and \u201ccrest\u201d colormaps are a better choice for such plots. They have a more restricted range of luminance variations, which they compensate for with a slightly more pronounced variation in hue. The default direction of the luminance ramp is also reversed, so that smaller values have lighter colors:",
            "markdown"
        ],
        [
            "sns.color_palette(\"flare\", as_cmap=True)\n",
            "code"
        ],
        [
            "sns.color_palette(\"crest\", as_cmap=True)\n",
            "code"
        ],
        [
            "It is also possible to use the perceptually uniform colormaps provided by matplotlib, such as \"magma\" and \"viridis\":",
            "markdown"
        ],
        [
            "sns.color_palette(\"magma\", as_cmap=True)\n",
            "code"
        ],
        [
            "sns.color_palette(\"viridis\", as_cmap=True)\n",
            "code"
        ],
        [
            "As with the convention in matplotlib, every continuous colormap has a reversed version, which has the suffix \"_r\":",
            "markdown"
        ],
        [
            "sns.color_palette(\"rocket_r\", as_cmap=True)\n",
            "code"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Sequential color palettes->Discrete vs. continuous mapping": [
        [
            "One thing to be aware of is that seaborn can generate discrete values from sequential colormaps and, when doing so, it will not use the most extreme values. Compare the discrete version of \"rocket\" against the continuous version shown above:",
            "markdown"
        ],
        [
            "sns.color_palette(\"rocket\")\n",
            "code"
        ],
        [
            "Internally, seaborn uses the discrete version for categorical data and the continuous version when in numeric mapping mode. Discrete sequential colormaps can be well-suited for visualizing categorical data with an intrinsic ordering, especially if there is some hue variation.",
            "markdown"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Sequential color palettes->Sequential \u201ccubehelix\u201d palettes": [
        [
            "The perceptually uniform colormaps are difficult to programmatically generate, because they are not based on the RGB color space. The  system offers an RGB-based compromise: it generates sequential palettes with a linear increase or decrease in brightness and some continuous variation in hue. While not perfectly perceptually uniform, the resulting colormaps have many good properties. Importantly, many aspects of the design process are parameterizable.",
            "markdown"
        ],
        [
            "Matplotlib has the default cubehelix version built into it:",
            "markdown"
        ],
        [
            "sns.color_palette(\"cubehelix\", as_cmap=True)\n",
            "code"
        ],
        [
            "The default palette returned by the seaborn  function is a bit different from the matplotlib default in that it does not rotate as far around the hue wheel or cover as wide a range of intensities. It also reverses the luminance ramp:",
            "markdown"
        ],
        [
            "sns.cubehelix_palette(as_cmap=True)\n",
            "code"
        ],
        [
            "Other arguments to  control how the palette looks. The two main things you\u2019ll change are the start (a value between 0 and 3) and rot, or number of rotations (an arbitrary value, but usually between -1 and 1)",
            "markdown"
        ],
        [
            "sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True)\n",
            "code"
        ],
        [
            "The more you rotate, the more hue variation you will see:",
            "markdown"
        ],
        [
            "sns.cubehelix_palette(start=.5, rot=-.75, as_cmap=True)\n",
            "code"
        ],
        [
            "You can control both how dark and light the endpoints are and their order:",
            "markdown"
        ],
        [
            "sns.cubehelix_palette(start=2, rot=0, dark=0, light=.95, reverse=True, as_cmap=True)\n",
            "code"
        ],
        [
            "The  accepts a string code, starting with \"ch:\", for generating an arbitrary cubehelix palette. You can passs the names of parameters in the string:",
            "markdown"
        ],
        [
            "sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True)\n",
            "code"
        ],
        [
            "And for compactness, each parameter can be specified with its first letter:",
            "markdown"
        ],
        [
            "sns.color_palette(\"ch:s=-.2,r=.6\", as_cmap=True)\n",
            "code"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Sequential color palettes->Custom sequential palettes": [
        [
            "For a simpler interface to custom sequential palettes, you can use  or , which are both seeded with a single color and produce a palette that ramps either from light or dark desaturated values to that color:",
            "markdown"
        ],
        [
            "sns.light_palette(\"seagreen\", as_cmap=True)\n",
            "code"
        ],
        [
            "sns.dark_palette(\"#69d\", reverse=True, as_cmap=True)\n",
            "code"
        ],
        [
            "As with cubehelix palettes, you can also specify light or dark palettes through  or anywhere palette is accepted:",
            "markdown"
        ],
        [
            "sns.color_palette(\"light:b\", as_cmap=True)\n",
            "code"
        ],
        [
            "Reverse the colormap by adding \"_r\":",
            "markdown"
        ],
        [
            "sns.color_palette(\"dark:salmon_r\", as_cmap=True)\n",
            "code"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Sequential color palettes->Sequential Color Brewer palettes": [
        [
            "The Color Brewer library also has some good options for sequential palettes. They include palettes with one primary hue:",
            "markdown"
        ],
        [
            "sns.color_palette(\"Blues\", as_cmap=True)\n",
            "code"
        ],
        [
            "Along with multi-hue options:",
            "markdown"
        ],
        [
            "sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
            "code"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Diverging color palettes": [
        [
            "The third class of color palettes is called \u201cdiverging\u201d. These are used for data where both large low and high values are interesting and span a midpoint value (often 0) that should be demphasized. The rules for choosing good diverging palettes are similar to good sequential palettes, except now there should be two dominant hues in the colormap, one at (or near) each pole. It\u2019s also important that the starting values are of similar brightness and saturation.",
            "markdown"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Diverging color palettes->Perceptually uniform diverging palettes": [
        [
            "Seaborn includes two perceptually uniform diverging palettes: \"vlag\" and \"icefire\". They both use blue and red at their poles, which many intuitively processes as \u201ccold\u201d and \u201chot\u201d:",
            "markdown"
        ],
        [
            "sns.color_palette(\"vlag\", as_cmap=True)\n",
            "code"
        ],
        [
            "sns.color_palette(\"icefire\", as_cmap=True)\n",
            "code"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Diverging color palettes->Custom diverging palettes": [
        [
            "You can also use the seaborn function  to create a custom colormap for diverging data. This function makes diverging palettes using the husl color system. You pass it two hues (in degrees) and, optionally, the lightness and saturation values for the extremes. Using husl means that the extreme values, and the resulting ramps to the midpoint, while not perfectly perceptually uniform, will be well-balanced:",
            "markdown"
        ],
        [
            "sns.diverging_palette(220, 20, as_cmap=True)\n",
            "code"
        ],
        [
            "This is convenient when you want to stray from the boring confines of cold-hot approaches:",
            "markdown"
        ],
        [
            "sns.diverging_palette(145, 300, s=60, as_cmap=True)\n",
            "code"
        ],
        [
            "It\u2019s also possible to make a palette where the midpoint is dark rather than light:",
            "markdown"
        ],
        [
            "sns.diverging_palette(250, 30, l=65, center=\"dark\", as_cmap=True)\n",
            "code"
        ],
        [
            "It\u2019s important to emphasize here that using red and green, while intuitive, .",
            "markdown"
        ]
    ],
    "seaborn->Figure aesthetics->Choosing color palettes->Diverging color palettes->Other diverging palettes": [
        [
            "There are a few other good diverging palettes built into matplotlib, including Color Brewer palettes:",
            "markdown"
        ],
        [
            "sns.color_palette(\"Spectral\", as_cmap=True)\n",
            "code"
        ],
        [
            "And the coolwarm palette, which has less contrast between the middle values and the extremes:",
            "markdown"
        ],
        [
            "sns.color_palette(\"coolwarm\", as_cmap=True)\n",
            "code"
        ],
        [
            "As you can see, there are many options for using color in your visualizations. Seaborn tries both to use good defaults and to offer a lot of flexibility.",
            "markdown"
        ],
        [
            "This discussion is only the beginning, and there are a number of good resources for learning more about techniques for using color in visualizations. One great example is this  from the NASA Earth Observatory. The matplotlib docs also have a  that illustrates some of the perceptual properties of their colormaps.",
            "markdown"
        ]
    ],
    "scipy->Introduction": [
        [
            "Contents",
            "markdown"
        ],
        [
            "Introduction",
            "markdown"
        ],
        [
            "SciPy Organization",
            "markdown"
        ],
        [
            "Finding Documentation",
            "markdown"
        ],
        [
            "SciPy is a collection of mathematical algorithms and convenience\nfunctions built on the NumPy extension of Python. It adds\nsignificant power to the interactive Python session by providing the\nuser with high-level commands and classes for manipulating and\nvisualizing data. With SciPy, an interactive Python session\nbecomes a data-processing and system-prototyping environment rivaling\nsystems, such as MATLAB, IDL, Octave, R-Lab, and SciLab.",
            "markdown"
        ],
        [
            "The additional benefit of basing SciPy on Python is that this also makes a\npowerful programming language available for use in developing\nsophisticated programs and specialized applications. Scientific\napplications using SciPy benefit from the development of\nadditional modules in numerous niches of the software landscape by\ndevelopers across the world. Everything from parallel programming to\nweb and data-base subroutines and classes have been made available to\nthe Python programmer. All of this power is available in addition to\nthe mathematical libraries in SciPy.",
            "markdown"
        ],
        [
            "This tutorial will acquaint the first-time user of SciPy with some of its most\nimportant features. It assumes that the user has already installed the SciPy\npackage. Some general Python facility is also assumed, such as could be\nacquired by working through the Python distribution\u00e2\u0080\u0099s Tutorial. For further\nintroductory help the user is directed to the NumPy documentation.",
            "markdown"
        ]
    ],
    "scipy->Introduction->SciPy Organization": [
        [
            "SciPy is organized into subpackages covering different scientific\ncomputing domains. These are summarized in the following table:",
            "markdown"
        ],
        [
            "SciPy sub-packages need to be imported separately, for example:",
            "markdown"
        ],
        [
            "from scipy import linalg, optimize",
            "code"
        ],
        [
            "Because of their ubiquitousness, some of the functions in these\nsubpackages are also made available in the scipy namespace to ease\ntheir use in interactive sessions and programs. In addition, many\nbasic array functions from numpy are also available at the\ntop-level of the scipy package. Before looking at the\nsub-packages individually, we will first look at some of these common\nfunctions.",
            "markdown"
        ]
    ],
    "scipy->Introduction->Finding Documentation": [
        [
            "SciPy and NumPy have documentation versions in both HTML and PDF format\navailable at https://docs.scipy.org/, that cover nearly\nall available functionality. However, this documentation is still\nwork-in-progress and some parts may be incomplete or sparse. As\nwe are a volunteer organization and depend on the community for\ngrowth, your participation - everything from providing feedback to\nimproving the documentation and code - is welcome and actively\nencouraged.",
            "markdown"
        ],
        [
            "Python\u00e2\u0080\u0099s documentation strings are used in SciPy for on-line\ndocumentation. There are two methods for reading them and\ngetting help. One is Python\u00e2\u0080\u0099s command help in the pydoc\nmodule. Entering this command with no arguments (i.e. >>> help )\nlaunches an interactive help session that allows searching through the\nkeywords and modules available to all of Python. Secondly, running the command\n<em class=\"xref py py-obj\">help(obj) with an object as the argument displays that object\u00e2\u0080\u0099s calling\nsignature, and documentation string.",
            "markdown"
        ],
        [
            "The pydoc method of help is sophisticated but uses a pager to display\nthe text. Sometimes this can interfere with the terminal within which you are\nrunning the interactive session. A numpy/scipy-specific help system\nis also available under the command numpy.info. The signature and\ndocumentation string for the object passed to the help command are\nprinted to standard output (or to a writeable object passed as the\nthird argument). The second keyword argument of numpy.info defines\nthe maximum width of the line for printing. If a module is passed as\nthe argument to help then a list of the functions and classes defined\nin that module is printed. For example:",
            "markdown"
        ],
        [
            "np.info(optimize.fmin)\n fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None,\n      full_output=0, disp=1, retall=0, callback=None)\n\nMinimize a function using the downhill simplex algorithm.\n\nParameters\n----------\nfunc : callable func(x,*args)\n    The objective function to be minimized.\nx0 : ndarray\n    Initial guess.\nargs : tuple\n    Extra arguments passed to func, i.e. ``f(x,*args)``.\ncallback : callable\n    Called after each iteration, as callback(xk), where xk is the\n    current parameter vector.\n\nReturns\n-------\nxopt : ndarray\n    Parameter that minimizes function.\nfopt : float\n    Value of function at minimum: ``fopt = func(xopt)``.\niter : int\n    Number of iterations performed.\nfuncalls : int\n    Number of function calls made.\nwarnflag : int\n    1 : Maximum number of function evaluations made.\n    2 : Maximum number of iterations reached.\nallvecs : list\n    Solution at each iteration.\n\nOther parameters\n----------------\nxtol : float\n    Relative error in xopt acceptable for convergence.\nftol : number\n    Relative error in func(xopt) acceptable for convergence.\nmaxiter : int\n    Maximum number of iterations to perform.\nmaxfun : number\n    Maximum number of function evaluations to make.\nfull_output : bool\n    Set to True if fopt and warnflag outputs are desired.\ndisp : bool\n    Set to True to print convergence messages.\nretall : bool\n    Set to True to return list of solutions at each iteration.\n\nNotes\n-----\nUses a Nelder-Mead simplex algorithm to find the minimum of function of\none or more variables.",
            "code"
        ],
        [
            "Another useful command is dir,\nwhich can be used to look at the namespace of a module or package.",
            "markdown"
        ]
    ],
    "scipy->Special functions (scipy.special)": [
        [
            "The main feature of the scipy.special package is the definition of\nnumerous special functions of mathematical physics. Available\nfunctions include airy, elliptic, bessel, gamma, beta, hypergeometric,\nparabolic cylinder, mathieu, spheroidal wave, struve, and\nkelvin. There are also some low-level stats functions that are not\nintended for general use as an easier interface to these functions is\nprovided by the stats module. Most of these functions can take\narray arguments and return array results following the same\nbroadcasting rules as other math functions in Numerical Python. Many\nof these functions also accept complex numbers as input. For a\ncomplete list of the available functions with a one-line description\ntype >>> help(special). Each function also has its own\ndocumentation accessible using help.  If you don\u00e2\u0080\u0099t see a function you\nneed, consider writing it and contributing it to the library. You can\nwrite the function in either C, Fortran, or Python. Look in the source\ncode of the library for examples of each of these kinds of functions.",
            "markdown"
        ]
    ],
    "scipy->Special functions (scipy.special)->Bessel functions of real order(jv, jn_zeros)": [
        [
            "Bessel functions are a family of solutions to Bessel\u00e2\u0080\u0099s differential equation\nwith real or complex order alpha:\n\n\\[x^2 \\frac{d^2 y}{dx^2} + x \\frac{dy}{dx} + (x^2 - \\alpha^2)y = 0\\]",
            "markdown"
        ],
        [
            "Among other uses, these functions arise in wave propagation problems, such as\nthe vibrational modes of a thin drum head.  Here is an example of a circular\ndrum head anchored at the edge:",
            "markdown"
        ],
        [
            "from scipy import special\n import numpy as np\n def drumhead_height(n, k, distance, angle, t):\n...    kth_zero = special.jn_zeros(n, k)[-1]\n...    return np.cos(t) * np.cos(n*angle) * special.jn(n, distance*kth_zero)\n theta = np.r_[0:2*np.pi:50j]\n radius = np.r_[0:1:50j]\n x = np.array([r * np.cos(theta) for r in radius])\n y = np.array([r * np.sin(theta) for r in radius])\n z = np.array([drumhead_height(1, 1, r, theta, 0.5) for r in radius])",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n fig = plt.figure()\n ax = fig.add_axes(rect=(0, 0.05, 0.95, 0.95), projection='3d')\n ax.plot_surface(x, y, z, rstride=1, cstride=1, cmap='RdBu_r', vmin=-0.5, vmax=0.5)\n ax.set_xlabel('X')\n ax.set_ylabel('Y')\n ax.set_xticks(np.arange(-1, 1.1, 0.5))\n ax.set_yticks(np.arange(-1, 1.1, 0.5))\n ax.set_zlabel('Z')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates a 3-D representation of the vibrational modes on a drum head viewed at a three-quarter angle. A circular region on the X-Y plane is defined with a Z value of 0 around the edge. Within the circle a single smooth valley exists on the -X side and a smooth peak exists on the +X side. The image resembles a yin-yang at this angle.\"' class=\"plot-directive\" src=\"../_images/special-1.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Special functions (scipy.special)->Cython Bindings for Special Functions (scipy.special.cython_special)": [
        [
            "SciPy also offers Cython bindings for scalar, typed versions of many\nof the functions in special. The following Cython code gives a simple\nexample of how to use these functions:",
            "markdown"
        ],
        [
            "cimport scipy.special.cython_special as csc\n\ncdef:\n    double x = 1\n    double complex z = 1 + 1j\n    double si, ci, rgam\n    double complex cgam\n\nrgam = csc.gamma(x)\nprint(rgam)\ncgam = csc.gamma(z)\nprint(cgam)\ncsc.sici(x, &si, &ci)\nprint(si, ci)",
            "code"
        ],
        [
            "(See the Cython documentation for help with compiling Cython.) In\nthe example the function csc.gamma works essentially like its\nufunc counterpart gamma, though it takes C types as arguments\ninstead of NumPy arrays. Note, in particular, that the function is\noverloaded to support real and complex arguments; the correct variant\nis selected at compile time. The function csc.sici works slightly\ndifferently from sici; for the ufunc we could write ai, bi =\nsici(x), whereas in the Cython version multiple return values are\npassed as pointers. It might help to think of this as analogous to\ncalling a ufunc with an output array: sici(x, out=(si, ci)).",
            "markdown"
        ],
        [
            "There are two potential advantages to using the Cython bindings:",
            "markdown"
        ],
        [
            "they avoid Python function overhead",
            "markdown"
        ],
        [
            "they do not require the Python Global Interpreter Lock (GIL)",
            "markdown"
        ],
        [
            "The following sections discuss how to use these advantages to\npotentially speed up your code, though, of course, one should always\nprofile the code first to make sure putting in the extra effort will\nbe worth it.",
            "markdown"
        ]
    ],
    "scipy->Special functions (scipy.special)->Cython Bindings for Special Functions (scipy.special.cython_special)->Avoiding Python Function Overhead": [
        [
            "For the ufuncs in special, Python function overhead is avoided by\nvectorizing, that is, by passing an array to the function. Typically,\nthis approach works quite well, but sometimes it is more convenient to\ncall a special function on scalar inputs inside a loop, for example,\nwhen implementing your own ufunc. In this case, the Python function\noverhead can become significant. Consider the following example:",
            "markdown"
        ],
        [
            "import scipy.special as sc\ncimport scipy.special.cython_special as csc\n\ndef python_tight_loop():\n    cdef:\n        int n\n        double x = 1\n\n    for n in range(100):\n        sc.jv(n, x)\n\ndef cython_tight_loop():\n    cdef:\n        int n\n        double x = 1\n\n    for n in range(100):\n        csc.jv(n, x)",
            "code"
        ],
        [
            "On one computer python_tight_loop took about 131 microseconds to\nrun and cython_tight_loop took about 18.2 microseconds to\nrun. Obviously this example is contrived: one could just call\nspecial.jv(np.arange(100), 1) and get results just as fast as in\ncython_tight_loop. The point is that if Python function overhead\nbecomes significant in your code, then the Cython bindings might be\nuseful.",
            "markdown"
        ]
    ],
    "scipy->Special functions (scipy.special)->Cython Bindings for Special Functions (scipy.special.cython_special)->Releasing the GIL": [
        [
            "One often needs to evaluate a special function at many points, and\ntypically the evaluations are trivially parallelizable. Since the\nCython bindings do not require the GIL, it is easy to run them in\nparallel using Cython\u00e2\u0080\u0099s prange function. For example, suppose that\nwe wanted to compute the fundamental solution to the Helmholtz\nequation:\n\n\\[\\Delta_x G(x, y) + k^2G(x, y) = \\delta(x - y),\\]",
            "markdown"
        ],
        [
            "where \\(k\\) is the wavenumber and \\(\\delta\\) is the Dirac\ndelta function. It is known that in two dimensions the unique\n(radiating) solution is\n\n\\[G(x, y) = \\frac{i}{4}H_0^{(1)}(k|x - y|),\\]",
            "markdown"
        ],
        [
            "where \\(H_0^{(1)}\\) is the Hankel function of the first kind,\ni.e., the function hankel1. The following example shows how we could\ncompute this function in parallel:",
            "markdown"
        ],
        [
            "from libc.math cimport fabs\ncimport cython\nfrom cython.parallel cimport prange\n\nimport numpy as np\nimport scipy.special as sc\ncimport scipy.special.cython_special as csc\n\ndef serial_G(k, x, y):\n    return 0.25j*sc.hankel1(0, k*np.abs(x - y))\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ncdef void _parallel_G(double k, double[:,:] x, double[:,:] y,\n                      double complex[:,:] out) nogil:\n    cdef int i, j\n\n    for i in prange(x.shape[0]):\n        for j in range(y.shape[0]):\n            out[i,j] = 0.25j*csc.hankel1(0, k*fabs(x[i,j] - y[i,j]))\n\ndef parallel_G(k, x, y):\n    out = np.empty_like(x, dtype='complex128')\n    _parallel_G(k, x, y, out)\n    return out",
            "code"
        ],
        [
            "(For help with compiling parallel code in Cython see here.) If the\nabove Cython code is in a file test.pyx, then we can write an\ninformal benchmark which compares the parallel and serial versions of\nthe function:",
            "markdown"
        ],
        [
            "import timeit\n\nimport numpy as np\n\nfrom test import serial_G, parallel_G\n\ndef main():\n    k = 1\n    x, y = np.linspace(-100, 100, 1000), np.linspace(-100, 100, 1000)\n    x, y = np.meshgrid(x, y)\n\n    def serial():\n        serial_G(k, x, y)\n\n    def parallel():\n        parallel_G(k, x, y)\n\n    time_serial = timeit.timeit(serial, number=3)\n    time_parallel = timeit.timeit(parallel, number=3)\n    print(\"Serial method took {:.3} seconds\".format(time_serial))\n    print(\"Parallel method took {:.3} seconds\".format(time_parallel))\n\nif __name__ == \"__main__\":\n    main()",
            "code"
        ],
        [
            "On one quad-core computer the serial method took 1.29 seconds and the\nparallel method took 0.29 seconds.",
            "markdown"
        ]
    ],
    "scipy->Special functions (scipy.special)->Functions not in scipy.special": [
        [
            "Some functions are not included in special because they are\nstraightforward to implement with existing functions in NumPy and\nSciPy. To prevent reinventing the wheel, this section provides\nimplementations of several such functions, which hopefully illustrate\nhow to handle similar functions. In all examples NumPy is imported as\nnp and special is imported as sc.",
            "markdown"
        ],
        [
            "The binary entropy function:",
            "markdown"
        ],
        [
            "def binary_entropy(x):\n    return -(sc.xlogy(x, x) + sc.xlog1py(1 - x, -x))/np.log(2)",
            "code"
        ],
        [
            "A rectangular step function on [0, 1]:",
            "markdown"
        ],
        [
            "def step(x):\n    return 0.5*(np.sign(x) + np.sign(1 - x))",
            "code"
        ],
        [
            "Translating and scaling can be used to get an arbitrary step function.",
            "markdown"
        ],
        [
            "The ramp function:",
            "markdown"
        ],
        [
            "def ramp(x):\n    return np.maximum(0, x)",
            "code"
        ]
    ],
    "scipy->Integration (scipy.integrate)": [
        [
            "The scipy.integrate sub-package provides several integration\ntechniques including an ordinary differential equation integrator. An\noverview of the module is provided by the help command:",
            "markdown"
        ],
        [
            "help(integrate)\n Methods for Integrating Functions given function object.\n\n   quad          -- General purpose integration.\n   dblquad       -- General purpose double integration.\n   tplquad       -- General purpose triple integration.\n   fixed_quad    -- Integrate func(x) using Gaussian quadrature of order n.\n   quadrature    -- Integrate with given tolerance using Gaussian quadrature.\n   romberg       -- Integrate func using Romberg integration.\n\n Methods for Integrating Functions given fixed samples.\n\n   trapezoid            -- Use trapezoidal rule to compute integral.\n   cumulative_trapezoid -- Use trapezoidal rule to cumulatively compute integral.\n   simpson              -- Use Simpson's rule to compute integral from samples.\n   romb                 -- Use Romberg Integration to compute integral from\n                        -- (2**k + 1) evenly-spaced samples.\n\n   See the special module's orthogonal polynomials (special) for Gaussian\n      quadrature roots and weights for other weighting factors and regions.\n\n Interface to numerical integrators of ODE systems.\n\n   odeint        -- General integration of ordinary differential equations.\n   ode           -- Integrate ODE using VODE and ZVODE routines.",
            "code"
        ]
    ],
    "scipy->Integration (scipy.integrate)->General integration (quad)": [
        [
            "The function quad is provided to integrate a function of one\nvariable between two points. The points can be \\(\\pm\\infty\\)\n(\\(\\pm\\) inf) to indicate infinite limits. For example,\nsuppose you wish to integrate a bessel function jv(2.5, x) along\nthe interval \\([0, 4.5].\\)\n\n\\[I=\\int_{0}^{4.5}J_{2.5}\\left(x\\right)\\, dx.\\]",
            "markdown"
        ],
        [
            "This could be computed using quad:",
            "markdown"
        ],
        [
            "import scipy.integrate as integrate\n import scipy.special as special\n result = integrate.quad(lambda x: special.jv(2.5,x), 0, 4.5)\n result\n(1.1178179380783249, 7.8663172481899801e-09)",
            "code"
        ],
        [
            "from numpy import sqrt, sin, cos, pi\n I = sqrt(2/pi)*(18.0/27*sqrt(2)*cos(4.5) - 4.0/27*sqrt(2)*sin(4.5) +\n...                 sqrt(2*pi) * special.fresnel(3/sqrt(pi))[0])\n I\n1.117817938088701",
            "code"
        ],
        [
            "print(abs(result[0]-I))\n1.03761443881e-11",
            "code"
        ],
        [
            "The first argument to quad is a \u00e2\u0080\u009ccallable\u00e2\u0080\u009d Python object (i.e., a\nfunction, method, or class instance). Notice the use of a lambda-\nfunction in this case as the argument. The next two arguments are the\nlimits of integration. The return value is a tuple, with the first\nelement holding the estimated value of the integral and the second\nelement holding an upper bound on the error. Notice, that in this\ncase, the true value of this integral is\n\n\\[I=\\sqrt{\\frac{2}{\\pi}}\\left(\\frac{18}{27}\\sqrt{2}\\cos\\left(4.5\\right)-\\frac{4}{27}\\sqrt{2}\\sin\\left(4.5\\right)+\\sqrt{2\\pi}\\textrm{Si}\\left(\\frac{3}{\\sqrt{\\pi}}\\right)\\right),\\]",
            "markdown"
        ],
        [
            "where\n\n\\[\\textrm{Si}\\left(x\\right)=\\int_{0}^{x}\\sin\\left(\\frac{\\pi}{2}t^{2}\\right)\\, dt.\\]",
            "markdown"
        ],
        [
            "is the Fresnel sine integral. Note that the numerically-computed integral is\nwithin \\(1.04\\times10^{-11}\\) of the exact result \u00e2\u0080\u0094 well below the\nreported error bound.",
            "markdown"
        ],
        [
            "If the function to integrate takes additional parameters, they can be provided\nin the <em class=\"xref py py-obj\">args argument. Suppose that the following integral shall be calculated:\n\n\\[I(a,b)=\\int_{0}^{1} ax^2+b \\, dx.\\]",
            "markdown"
        ],
        [
            "This integral can be evaluated by using the following code:",
            "markdown"
        ],
        [
            "from scipy.integrate import quad\n def integrand(x, a, b):\n...     return a*x**2 + b\n...\n a = 2\n b = 1\n I = quad(integrand, 0, 1, args=(a,b))\n I\n(1.6666666666666667, 1.8503717077085944e-14)",
            "code"
        ],
        [
            "Infinite inputs are also allowed in quad by using \\(\\pm\\)\ninf as one of the arguments. For example, suppose that a numerical\nvalue for the exponential integral:\n\n\\[E_{n}\\left(x\\right)=\\int_{1}^{\\infty}\\frac{e^{-xt}}{t^{n}}\\, dt.\\]",
            "markdown"
        ],
        [
            "is desired (and the fact that this integral can be computed as\nspecial.expn(n,x) is forgotten). The functionality of the function\nspecial.expn can be replicated by defining a new function\nvec_expint based on the routine quad:",
            "markdown"
        ],
        [
            "from scipy.integrate import quad\n import numpy as np\n def integrand(t, n, x):\n...     return np.exp(-x*t) / t**n\n...",
            "code"
        ],
        [
            "def expint(n, x):\n...     return quad(integrand, 1, np.inf, args=(n, x))[0]\n...",
            "code"
        ],
        [
            "vec_expint = np.vectorize(expint)",
            "code"
        ],
        [
            "vec_expint(3, np.arange(1.0, 4.0, 0.5))\narray([ 0.1097,  0.0567,  0.0301,  0.0163,  0.0089,  0.0049])\n import scipy.special as special\n special.expn(3, np.arange(1.0,4.0,0.5))\narray([ 0.1097,  0.0567,  0.0301,  0.0163,  0.0089,  0.0049])",
            "code"
        ],
        [
            "The function which is integrated can even use the quad argument (though the\nerror bound may underestimate the error due to possible numerical error in the\nintegrand from the use of quad ). The integral in this case is\n\n\\[I_{n}=\\int_{0}^{\\infty}\\int_{1}^{\\infty}\\frac{e^{-xt}}{t^{n}}\\, dt\\, dx=\\frac{1}{n}.\\]",
            "markdown"
        ],
        [
            "result = quad(lambda x: expint(3, x), 0, np.inf)\n print(result)\n(0.33333333324560266, 2.8548934485373678e-09)",
            "code"
        ],
        [
            "I3 = 1.0/3.0\n print(I3)\n0.333333333333",
            "code"
        ],
        [
            "print(I3 - result[0])\n8.77306560731e-11",
            "code"
        ],
        [
            "This last example shows that multiple integration can be handled using\nrepeated calls to quad.",
            "markdown"
        ]
    ],
    "scipy->Integration (scipy.integrate)->General multiple integration (dblquad, tplquad, nquad)": [
        [
            "The mechanics for double and triple integration have been wrapped up into the\nfunctions dblquad and tplquad. These functions take the function\nto  integrate and four, or six arguments, respectively. The limits of all\ninner integrals need to be defined as functions.",
            "markdown"
        ],
        [
            "An example of using double integration to compute several values of\n\\(I_{n}\\) is shown below:",
            "markdown"
        ],
        [
            "from scipy.integrate import quad, dblquad\n def I(n):\n...     return dblquad(lambda t, x: np.exp(-x*t)/t**n, 0, np.inf, lambda x: 1, lambda x: np.inf)\n...",
            "code"
        ],
        [
            "print(I(4))\n(0.2500000000043577, 1.29830334693681e-08)\n print(I(3))\n(0.33333333325010883, 1.3888461883425516e-08)\n print(I(2))\n(0.4999999999985751, 1.3894083651858995e-08)",
            "code"
        ],
        [
            "As example for non-constant limits consider the integral\n\n\\[I=\\int_{y=0}^{1/2}\\int_{x=0}^{1-2y} x y \\, dx\\, dy=\\frac{1}{96}.\\]",
            "markdown"
        ],
        [
            "This integral can be evaluated using the expression below (Note the use of the\nnon-constant lambda functions for the upper limit of the inner integral):",
            "markdown"
        ],
        [
            "from scipy.integrate import dblquad\n area = dblquad(lambda x, y: x*y, 0, 0.5, lambda x: 0, lambda x: 1-2*x)\n area\n(0.010416666666666668, 1.1564823173178715e-16)",
            "code"
        ],
        [
            "For n-fold integration, scipy provides the function nquad. The\nintegration bounds are an iterable object: either a list of constant bounds,\nor a list of functions for the non-constant integration bounds. The order of\nintegration (and therefore the bounds) is from the innermost integral to the\noutermost one.",
            "markdown"
        ],
        [
            "The integral from above\n\n\\[I_{n}=\\int_{0}^{\\infty}\\int_{1}^{\\infty}\\frac{e^{-xt}}{t^{n}}\\, dt\\, dx=\\frac{1}{n}\\]",
            "markdown"
        ],
        [
            "can be calculated as",
            "markdown"
        ],
        [
            "from scipy import integrate\n N = 5\n def f(t, x):\n...    return np.exp(-x*t) / t**N\n...\n integrate.nquad(f, [[1, np.inf],[0, np.inf]])\n(0.20000000000002294, 1.2239614263187945e-08)",
            "code"
        ],
        [
            "Note that the order of arguments for <em class=\"xref py py-obj\">f must match the order of the\nintegration bounds; i.e., the inner integral with respect to \\(t\\) is on\nthe interval \\([1, \\infty]\\) and the outer integral with respect to\n\\(x\\) is on the interval \\([0, \\infty]\\).",
            "markdown"
        ],
        [
            "Non-constant integration bounds can be treated in a similar manner; the\nexample from above\n\n\\[I=\\int_{y=0}^{1/2}\\int_{x=0}^{1-2y} x y \\, dx\\, dy=\\frac{1}{96}.\\]",
            "markdown"
        ],
        [
            "can be evaluated by means of",
            "markdown"
        ],
        [
            "from scipy import integrate\n def f(x, y):\n...     return x*y\n...\n def bounds_y():\n...     return [0, 0.5]\n...\n def bounds_x(y):\n...     return [0, 1-2*y]\n...\n integrate.nquad(f, [bounds_x, bounds_y])\n(0.010416666666666668, 4.101620128472366e-16)",
            "code"
        ],
        [
            "which is the same result as before.",
            "markdown"
        ]
    ],
    "scipy->Integration (scipy.integrate)->Gaussian quadrature": [
        [
            "A few functions are also provided in order to perform simple Gaussian\nquadrature over a fixed interval. The first is fixed_quad, which\nperforms fixed-order Gaussian quadrature. The second function is\nquadrature, which performs Gaussian quadrature of multiple\norders until the difference in the integral estimate is beneath some\ntolerance supplied by the user. These functions both use the module\nscipy.special.orthogonal, which can calculate the roots and quadrature\nweights of a large variety of orthogonal polynomials (the polynomials\nthemselves are available as special functions returning instances of\nthe polynomial class \u00e2\u0080\u0094 e.g., special.legendre).",
            "markdown"
        ]
    ],
    "scipy->Integration (scipy.integrate)->Romberg Integration": [
        [
            "Romberg\u00e2\u0080\u0099s method [WPR] is another method for numerically evaluating an\nintegral. See the help function for romberg for further details.",
            "markdown"
        ]
    ],
    "scipy->Integration (scipy.integrate)->Integrating using Samples": [
        [
            "If the samples are equally-spaced and the number of samples available\nis \\(2^{k}+1\\) for some integer \\(k\\), then Romberg romb\nintegration can be used to obtain high-precision estimates of the\nintegral using the available samples. Romberg integration uses the\ntrapezoid rule at step-sizes related by a power of two and then\nperforms Richardson extrapolation on these estimates to approximate\nthe integral with a higher degree of accuracy.",
            "markdown"
        ],
        [
            "In case of arbitrary spaced samples, the two functions trapezoid\nand simpson are available. They are using Newton-Coates formulas\nof order 1 and 2 respectively to perform integration. The trapezoidal rule\napproximates the function as a straight line between adjacent points, while\nSimpson\u00e2\u0080\u0099s rule approximates the function between three adjacent points as a\nparabola.",
            "markdown"
        ],
        [
            "For an odd number of samples that are equally spaced Simpson\u00e2\u0080\u0099s rule is exact\nif the function is a polynomial of order 3 or less. If the samples are not\nequally spaced, then the result is exact only if the function is a polynomial\nof order 2 or less.",
            "markdown"
        ],
        [
            "import numpy as np\n def f1(x):\n...    return x**2\n...\n def f2(x):\n...    return x**3\n...\n x = np.array([1,3,4])\n y1 = f1(x)\n from scipy import integrate\n I1 = integrate.simpson(y1, x)\n print(I1)\n21.0",
            "code"
        ],
        [
            "This corresponds exactly to\n\n\\[\\int_{1}^{4} x^2 \\, dx = 21,\\]",
            "markdown"
        ],
        [
            "whereas integrating the second function",
            "markdown"
        ],
        [
            "y2 = f2(x)\n I2 = integrate.simpson(y2, x)\n print(I2)\n61.5",
            "code"
        ],
        [
            "does not correspond to\n\n\\[\\int_{1}^{4} x^3 \\, dx = 63.75\\]",
            "markdown"
        ],
        [
            "because the order of the polynomial in f2 is larger than two.",
            "markdown"
        ]
    ],
    "scipy->Integration (scipy.integrate)->Faster integration using low-level callback functions": [
        [
            "A user desiring reduced integration times may pass a C function\npointer through scipy.LowLevelCallable to quad, dblquad,\ntplquad or nquad and it will be integrated and return a result in\nPython.  The performance increase here arises from two factors.  The\nprimary improvement is faster function evaluation, which is provided\nby compilation of the function itself.  Additionally we have a speedup\nprovided by the removal of function calls between C and Python in\nquad.  This method may provide a speed improvements of ~2x for\ntrivial functions such as sine but can produce a much more noticeable\nimprovements (10x+) for more complex functions.  This feature then, is\ngeared towards a user with numerically intensive integrations willing\nto write a little C to reduce computation time significantly.",
            "markdown"
        ],
        [
            "The approach can be used, for example, via ctypes in a few simple steps:",
            "markdown"
        ],
        [
            "1.) Write an integrand function in C with the function signature\ndouble f(int n, double *x, void *user_data), where x is an\narray containing the point the function f is evaluated at, and user_data\nto arbitrary additional data you want to provide.",
            "markdown"
        ],
        [
            "/* testlib.c */\ndouble f(int n, double *x, void *user_data) {\n    double c = *(double *)user_data;\n    return c + x[0] - x[1] * x[2]; /* corresponds to c + x - y * z */\n}",
            "code"
        ],
        [
            "2.) Now compile this file to a shared/dynamic library (a quick search will help\nwith this as it is OS-dependent). The user must link any math libraries,\netc., used.  On linux this looks like:",
            "markdown"
        ],
        [
            "$ gcc -shared -fPIC -o testlib.so testlib.c",
            "code"
        ],
        [
            "The output library will be referred to as testlib.so, but it may have a\ndifferent file extension. A library has now been created that can be loaded\ninto Python with ctypes.",
            "markdown"
        ],
        [
            "3.) Load shared library into Python using ctypes and set restypes and\nargtypes - this allows SciPy to interpret the function correctly:",
            "markdown"
        ],
        [
            "import os, ctypes\nfrom scipy import integrate, LowLevelCallable\n\nlib = ctypes.CDLL(os.path.abspath('testlib.so'))\nlib.f.restype = ctypes.c_double\nlib.f.argtypes = (ctypes.c_int, ctypes.POINTER(ctypes.c_double), ctypes.c_void_p)\n\nc = ctypes.c_double(1.0)\nuser_data = ctypes.cast(ctypes.pointer(c), ctypes.c_void_p)\n\nfunc = LowLevelCallable(lib.f, user_data)",
            "code"
        ],
        [
            "The last void *user_data in the function is optional and can be omitted\n(both in the C function and ctypes argtypes) if not needed. Note that the\ncoordinates are passed in as an array of doubles rather than a separate argument.",
            "markdown"
        ],
        [
            "4.) Now integrate the library function as normally, here using nquad:",
            "markdown"
        ],
        [
            "integrate.nquad(func, [[0, 10], [-10, 0], [-1, 1]])\n(1200.0, 1.1102230246251565e-11)",
            "code"
        ],
        [
            "The Python tuple is returned as expected in a reduced amount of time.  All\noptional parameters can be used with this method including specifying\nsingularities, infinite bounds, etc.",
            "markdown"
        ]
    ],
    "scipy->Integration (scipy.integrate)->Ordinary differential equations (solve_ivp)": [
        [
            "Integrating a set of ordinary differential equations (ODEs) given\ninitial conditions is another useful example. The function\nsolve_ivp is available in SciPy for integrating a first-order\nvector differential equation:\n\n\\[\\frac{d\\mathbf{y}}{dt}=\\mathbf{f}\\left(\\mathbf{y},t\\right),\\]",
            "markdown"
        ],
        [
            "given initial conditions \\(\\mathbf{y}\\left(0\\right)=y_{0}\\), where\n\\(\\mathbf{y}\\) is a length \\(N\\) vector and \\(\\mathbf{f}\\)\nis a mapping from \\(\\mathcal{R}^{N}\\) to \\(\\mathcal{R}^{N}.\\)\nA higher-order ordinary differential equation can always be reduced to\na differential equation of this type by introducing intermediate\nderivatives into the \\(\\mathbf{y}\\) vector.",
            "markdown"
        ],
        [
            "For example, suppose it is desired to find the solution to the\nfollowing second-order differential equation:\n\n\\[\\frac{d^{2}w}{dz^{2}}-zw(z)=0\\]",
            "markdown"
        ],
        [
            "with initial conditions \\(w\\left(0\\right)=\\frac{1}{\\sqrt[3]{3^{2}}\\Gamma\\left(\\frac{2}{3}\\right)}\\) and \\(\\left.\\frac{dw}{dz}\\right|_{z=0}=-\\frac{1}{\\sqrt[3]{3}\\Gamma\\left(\\frac{1}{3}\\right)}.\\) It is known that the solution to this differential equation with these\nboundary conditions is the Airy function\n\n\\[w=\\textrm{Ai}\\left(z\\right),\\]",
            "markdown"
        ],
        [
            "which gives a means to check the integrator using special.airy.",
            "markdown"
        ],
        [
            "First, convert this ODE into standard form by setting\n\\(\\mathbf{y}=\\left[\\frac{dw}{dz},w\\right]\\) and \\(t=z\\). Thus,\nthe differential equation becomes\n\n\\[\\begin{split}\\frac{d\\mathbf{y}}{dt}=\\left[\\begin{array}{c} ty_{1}\\\\ y_{0}\\end{array}\\right]=\\left[\\begin{array}{cc} 0 & t\\\\ 1 & 0\\end{array}\\right]\\left[\\begin{array}{c} y_{0}\\\\ y_{1}\\end{array}\\right]=\\left[\\begin{array}{cc} 0 & t\\\\ 1 & 0\\end{array}\\right]\\mathbf{y}.\\end{split}\\]",
            "markdown"
        ],
        [
            "In other words,\n\n\\[\\mathbf{f}\\left(\\mathbf{y},t\\right)=\\mathbf{A}\\left(t\\right)\\mathbf{y}.\\]",
            "markdown"
        ],
        [
            "As an interesting reminder, if \\(\\mathbf{A}\\left(t\\right)\\)\ncommutes with \\(\\int_{0}^{t}\\mathbf{A}\\left(\\tau\\right)\\, d\\tau\\)\nunder matrix multiplication, then this linear differential equation\nhas an exact solution using the matrix exponential:\n\n\\[\\mathbf{y}\\left(t\\right)=\\exp\\left(\\int_{0}^{t}\\mathbf{A}\\left(\\tau\\right)d\\tau\\right)\\mathbf{y}\\left(0\\right),\\]",
            "markdown"
        ],
        [
            "However, in this case, \\(\\mathbf{A}\\left(t\\right)\\) and its integral do not commute.",
            "markdown"
        ],
        [
            "This differential equation can be solved using the function solve_ivp.\nIt requires the derivative, fprime, the time span <em class=\"xref py py-obj\">[t_start, t_end]\nand the initial conditions vector, y0, as input arguments and returns\nan object whose y field is an array with consecutive solution values as\ncolumns. The initial conditions are therefore given in the first output column.",
            "markdown"
        ],
        [
            "from scipy.integrate import solve_ivp\n from scipy.special import gamma, airy\n y1_0 = +1 / 3**(2/3) / gamma(2/3)\n y0_0 = -1 / 3**(1/3) / gamma(1/3)\n y0 = [y0_0, y1_0]\n def func(t, y):\n...     return [t*y[1],y[0]]\n...\n t_span = [0, 4]\n sol1 = solve_ivp(func, t_span, y0)\n print(\"sol1.t: {}\".format(sol1.t))\nsol1.t:    [0.         0.10097672 1.04643602 1.91060117 2.49872472 3.08684827\n 3.62692846 4.        ]",
            "code"
        ],
        [
            "As it can be seen solve_ivp determines its time steps automatically if not\nspecified otherwise. To compare the solution of solve_ivp with the <em class=\"xref py py-obj\">airy\nfunction the time vector created by solve_ivp is passed to the <em class=\"xref py py-obj\">airy function.",
            "markdown"
        ],
        [
            "print(\"sol1.y[1]: {}\".format(sol1.y[1]))\nsol1.y[1]: [0.35502805 0.328952   0.12801343 0.04008508 0.01601291 0.00623879\n 0.00356316 0.00405982]\n print(\"airy(sol.t)[0]:  {}\".format(airy(sol1.t)[0]))\nairy(sol.t)[0]: [0.35502805 0.328952   0.12804768 0.03995804 0.01575943 0.00562799\n 0.00201689 0.00095156]",
            "code"
        ],
        [
            "The solution of solve_ivp with its standard parameters shows a big deviation\nto the airy function. To minimize this deviation, relative and absolute\ntolerances can be used.",
            "markdown"
        ],
        [
            "rtol, atol = (1e-8, 1e-8)\n sol2 = solve_ivp(func, t_span, y0, rtol=rtol, atol=atol)\n print(\"sol2.y[1][::6]: {}\".format(sol2.y[1][0::6]))\nsol2.y[1][::6]: [0.35502805 0.19145234 0.06368989 0.0205917  0.00554734 0.00106409]\n print(\"airy(sol2.t)[0][::6]: {}\".format(airy(sol2.t)[0][::6]))\nairy(sol2.t)[0][::6]: [0.35502805 0.19145234 0.06368989 0.0205917  0.00554733 0.00106406]",
            "code"
        ],
        [
            "To specify user defined time points for the solution of solve_ivp, solve_ivp\noffers two possibilities that can also be used complementarily. By passing the <em class=\"xref py py-obj\">t_eval\noption to the function call solve_ivp returns the solutions of these time points\nof <em class=\"xref py py-obj\">t_eval in its output.",
            "markdown"
        ],
        [
            "import numpy as np\n t = np.linspace(0, 4, 100)\n sol3 = solve_ivp(func, t_span, y0, t_eval=t)",
            "code"
        ],
        [
            "If the jacobian matrix of function is known, it can be passed to the solve_ivp\nto achieve better results. Please be aware however that the default integration method\nRK45 does not support jacobian matrices and thereby another integration method has\nto be chosen. One of the integration methods that support a jacobian matrix is the for\nexample the Radau method of following example.",
            "markdown"
        ],
        [
            "def gradient(t, y):\n...     return [[0,t], [1,0]]\n sol4 = solve_ivp(func, t_span, y0, method='Radau', jac=gradient)",
            "code"
        ]
    ],
    "scipy->Integration (scipy.integrate)->Ordinary differential equations (solve_ivp)->Solving a system with a banded Jacobian matrix": [
        [
            "odeint can be told that the Jacobian is banded.  For a large\nsystem of differential equations that are known to be stiff, this\ncan improve performance significantly.",
            "markdown"
        ],
        [
            "As an example, we\u00e2\u0080\u0099ll solve the 1-D Gray-Scott partial\ndifferential equations using the method of lines [MOL].  The Gray-Scott equations\nfor the functions \\(u(x, t)\\) and \\(v(x, t)\\) on the interval\n\\(x \\in [0, L]\\) are\n\n\\[\\begin{split}\\begin{split}\n\\frac{\\partial u}{\\partial t} = D_u \\frac{\\partial^2 u}{\\partial x^2} - uv^2 + f(1-u) \\\\\n\\frac{\\partial v}{\\partial t} = D_v \\frac{\\partial^2 v}{\\partial x^2} + uv^2 - (f + k)v \\\\\n\\end{split}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(D_u\\) and \\(D_v\\) are the diffusion coefficients of the\ncomponents \\(u\\) and \\(v\\), respectively, and \\(f\\) and \\(k\\)\nare constants.  (For more information about the system, see\nhttp://groups.csail.mit.edu/mac/projects/amorphous/GrayScott/)",
            "markdown"
        ],
        [
            "We\u00e2\u0080\u0099ll assume Neumann (i.e., \u00e2\u0080\u009cno flux\u00e2\u0080\u009d) boundary conditions:\n\n\\[\\frac{\\partial u}{\\partial x}(0,t) = 0, \\quad\n\\frac{\\partial v}{\\partial x}(0,t) = 0, \\quad\n\\frac{\\partial u}{\\partial x}(L,t) = 0, \\quad\n\\frac{\\partial v}{\\partial x}(L,t) = 0\\]",
            "markdown"
        ],
        [
            "To apply the method of lines, we discretize the \\(x\\) variable by defining\nthe uniformly spaced grid of \\(N\\) points \\(\\left\\{x_0, x_1, \\ldots, x_{N-1}\\right\\}\\), with\n\\(x_0 = 0\\) and \\(x_{N-1} = L\\).\nWe define \\(u_j(t) \\equiv u(x_k, t)\\) and \\(v_j(t) \\equiv v(x_k, t)\\), and\nreplace the \\(x\\) derivatives with finite differences.  That is,\n\n\\[\\frac{\\partial^2 u}{\\partial x^2}(x_j, t) \\rightarrow\n    \\frac{u_{j-1}(t) - 2 u_{j}(t) + u_{j+1}(t)}{(\\Delta x)^2}\\]",
            "markdown"
        ],
        [
            "We then have a system of \\(2N\\) ordinary differential equations:\n\n(1)#\\[\\begin{split} \\begin{split}\n \\frac{du_j}{dt} = \\frac{D_u}{(\\Delta x)^2} \\left(u_{j-1} - 2 u_{j} + u_{j+1}\\right)\n       -u_jv_j^2 + f(1 - u_j) \\\\\n \\frac{dv_j}{dt} = \\frac{D_v}{(\\Delta x)^2} \\left(v_{j-1} - 2 v_{j} + v_{j+1}\\right)\n       + u_jv_j^2 - (f + k)v_j\n \\end{split}\\end{split}\\]",
            "markdown"
        ],
        [
            "For convenience, the \\((t)\\) arguments have been dropped.",
            "markdown"
        ],
        [
            "To enforce the boundary conditions, we introduce \u00e2\u0080\u009cghost\u00e2\u0080\u009d points\n\\(x_{-1}\\) and \\(x_N\\), and define \\(u_{-1}(t) \\equiv u_1(t)\\),\n\\(u_N(t) \\equiv u_{N-2}(t)\\); \\(v_{-1}(t)\\) and \\(v_N(t)\\)\nare defined analogously.",
            "markdown"
        ],
        [
            "Then\n\n(2)#\\[\\begin{split} \\begin{split}\n \\frac{du_0}{dt} = \\frac{D_u}{(\\Delta x)^2} \\left(2u_{1} - 2 u_{0}\\right)\n       -u_0v_0^2 + f(1 - u_0) \\\\\n \\frac{dv_0}{dt} = \\frac{D_v}{(\\Delta x)^2} \\left(2v_{1} - 2 v_{0}\\right)\n       + u_0v_0^2 - (f + k)v_0\n \\end{split}\\end{split}\\]",
            "markdown"
        ],
        [
            "and\n\n(3)#\\[\\begin{split} \\begin{split}\n \\frac{du_{N-1}}{dt} = \\frac{D_u}{(\\Delta x)^2} \\left(2u_{N-2} - 2 u_{N-1}\\right)\n       -u_{N-1}v_{N-1}^2 + f(1 - u_{N-1}) \\\\\n \\frac{dv_{N-1}}{dt} = \\frac{D_v}{(\\Delta x)^2} \\left(2v_{N-2} - 2 v_{N-1}\\right)\n       + u_{N-1}v_{N-1}^2 - (f + k)v_{N-1}\n \\end{split}\\end{split}\\]",
            "markdown"
        ],
        [
            "Our complete system of \\(2N\\) ordinary differential equations is (1)\nfor \\(k = 1, 2, \\ldots, N-2\\), along with (2) and (3).",
            "markdown"
        ],
        [
            "We can now starting implementing this system in code.  We must combine\n\\(\\{u_k\\}\\) and \\(\\{v_k\\}\\) into a single vector of length \\(2N\\).\nThe two obvious choices are\n\\(\\{u_0, u_1, \\ldots, u_{N-1}, v_0, v_1, \\ldots, v_{N-1}\\}\\)\nand\n\\(\\{u_0, v_0, u_1, v_1, \\ldots, u_{N-1}, v_{N-1}\\}\\).\nMathematically, it does not matter, but the choice affects how\nefficiently odeint can solve the system.  The reason is in how\nthe order affects the pattern of the nonzero elements of the Jacobian matrix.",
            "markdown"
        ],
        [
            "When the variables are ordered\nas \\(\\{u_0, u_1, \\ldots, u_{N-1}, v_0, v_1, \\ldots, v_{N-1}\\}\\),\nthe pattern of nonzero elements of the Jacobian matrix is\n\n\\[\\begin{split}\\begin{smallmatrix}\n   * & * & 0 & 0 & 0 & 0 & 0  &  * & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   * & * & * & 0 & 0 & 0 & 0  &  0 & * & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & * & * & * & 0 & 0 & 0  &  0 & 0 & * & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & * & * & * & 0 & 0  &  0 & 0 & 0 & * & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & * & * & * & 0  &  0 & 0 & 0 & 0 & * & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & * & * & *  &  0 & 0 & 0 & 0 & 0 & * & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & * & *  &  0 & 0 & 0 & 0 & 0 & 0 & * \\\\\n   * & 0 & 0 & 0 & 0 & 0 & 0  &  * & * & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & * & 0 & 0 & 0 & 0 & 0  &  * & * & * & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & * & 0 & 0 & 0 & 0  &  0 & * & * & * & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & * & 0 & 0 & 0  &  0 & 0 & * & * & * & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & * & 0 & 0  &  0 & 0 & 0 & * & * & * & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & * & 0  &  0 & 0 & 0 & 0 & * & * & * \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & *  &  0 & 0 & 0 & 0 & ) & * & * \\\\\n\\end{smallmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "The Jacobian pattern with variables interleaved\nas \\(\\{u_0, v_0, u_1, v_1, \\ldots, u_{N-1}, v_{N-1}\\}\\) is\n\n\\[\\begin{split}\\begin{smallmatrix}\n   * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * \\\\\n\\end{smallmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "In both cases, there are just five nontrivial diagonals, but\nwhen the variables are interleaved, the bandwidth is much\nsmaller.\nThat is, the main diagonal and the two diagonals immediately\nabove and the two immediately below the main diagonal\nare the nonzero diagonals.\nThis is important, because the inputs mu and ml\nof odeint are the upper and lower bandwidths of the\nJacobian matrix.  When the variables are interleaved,\nmu and ml are 2.  When the variables are stacked\nwith \\(\\{v_k\\}\\) following \\(\\{u_k\\}\\), the upper\nand lower bandwidths are \\(N\\).",
            "markdown"
        ],
        [
            "With that decision made, we can write the function that\nimplements the system of differential equations.",
            "markdown"
        ],
        [
            "First, we define the functions for the source and reaction\nterms of the system:",
            "markdown"
        ],
        [
            "def G(u, v, f, k):\n    return f * (1 - u) - u*v**2\n\ndef H(u, v, f, k):\n    return -(f + k) * v + u*v**2",
            "code"
        ],
        [
            "Next, we define the function that computes the right-hand side\nof the system of differential equations:",
            "markdown"
        ],
        [
            "def grayscott1d(y, t, f, k, Du, Dv, dx):\n    \"\"\"\n    Differential equations for the 1-D Gray-Scott equations.\n\n    The ODEs are derived using the method of lines.\n    \"\"\"\n    # The vectors u and v are interleaved in y.  We define\n    # views of u and v by slicing y.\n    u = y[::2]\n    v = y[1::2]\n\n    # dydt is the return value of this function.\n    dydt = np.empty_like(y)\n\n    # Just like u and v are views of the interleaved vectors\n    # in y, dudt and dvdt are views of the interleaved output\n    # vectors in dydt.\n    dudt = dydt[::2]\n    dvdt = dydt[1::2]\n\n    # Compute du/dt and dv/dt.  The end points and the interior points\n    # are handled separately.\n    dudt[0]    = G(u[0],    v[0],    f, k) + Du * (-2.0*u[0] + 2.0*u[1]) / dx**2\n    dudt[1:-1] = G(u[1:-1], v[1:-1], f, k) + Du * np.diff(u,2) / dx**2\n    dudt[-1]   = G(u[-1],   v[-1],   f, k) + Du * (- 2.0*u[-1] + 2.0*u[-2]) / dx**2\n    dvdt[0]    = H(u[0],    v[0],    f, k) + Dv * (-2.0*v[0] + 2.0*v[1]) / dx**2\n    dvdt[1:-1] = H(u[1:-1], v[1:-1], f, k) + Dv * np.diff(v,2) / dx**2\n    dvdt[-1]   = H(u[-1],   v[-1],   f, k) + Dv * (-2.0*v[-1] + 2.0*v[-2]) / dx**2\n\n    return dydt",
            "code"
        ],
        [
            "We won\u00e2\u0080\u0099t implement a function to compute the Jacobian, but we will tell\nodeint that the Jacobian matrix is banded.  This allows the underlying\nsolver (LSODA) to avoid computing values that it knows are zero.  For a large\nsystem, this improves the performance significantly, as demonstrated in the\nfollowing ipython session.",
            "markdown"
        ],
        [
            "First, we define the required inputs:",
            "markdown"
        ],
        [
            "In [30]: rng = np.random.default_rng()\n\nIn [31]: y0 = rng.standard_normal(5000)\n\nIn [32]: t = np.linspace(0, 50, 11)\n\nIn [33]: f = 0.024\n\nIn [34]: k = 0.055\n\nIn [35]: Du = 0.01\n\nIn [36]: Dv = 0.005\n\nIn [37]: dx = 0.025",
            "code"
        ],
        [
            "Time the computation without taking advantage of the banded structure\nof the Jacobian matrix:",
            "markdown"
        ],
        [
            "In [38]: %timeit sola = odeint(grayscott1d, y0, t, args=(f, k, Du, Dv, dx))\n1 loop, best of 3: 25.2 s per loop",
            "code"
        ],
        [
            "Now set ml=2 and mu=2, so odeint knows that the Jacobian matrix\nis banded:",
            "markdown"
        ],
        [
            "In [39]: %timeit solb = odeint(grayscott1d, y0, t, args=(f, k, Du, Dv, dx), ml=2, mu=2)\n10 loops, best of 3: 191 ms per loop",
            "code"
        ],
        [
            "That is quite a bit faster!",
            "markdown"
        ],
        [
            "Let\u00e2\u0080\u0099s ensure that they have computed the same result:",
            "markdown"
        ],
        [
            "In [41]: np.allclose(sola, solb)\nOut[41]: True",
            "code"
        ]
    ],
    "scipy->Integration (scipy.integrate)->Ordinary differential equations (solve_ivp)->References": [
        [
            "https://en.wikipedia.org/wiki/Romberg\u00e2\u0080\u0099s_method\n\n\n[MOL]",
            "markdown"
        ],
        [
            "https://en.wikipedia.org/wiki/Method_of_lines",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)": [
        [
            "Contents",
            "markdown"
        ],
        [
            "Optimization (scipy.optimize)",
            "markdown"
        ],
        [
            "Unconstrained minimization of multivariate scalar functions (minimize)",
            "markdown"
        ],
        [
            "Nelder-Mead Simplex algorithm (method='Nelder-Mead')",
            "markdown"
        ],
        [
            "Broyden-Fletcher-Goldfarb-Shanno algorithm (method='BFGS')",
            "markdown"
        ],
        [
            "Newton-Conjugate-Gradient algorithm (method='Newton-CG')",
            "markdown"
        ],
        [
            "Full Hessian example:",
            "markdown"
        ],
        [
            "Hessian product example:",
            "markdown"
        ],
        [
            "Trust-Region Newton-Conjugate-Gradient Algorithm (method='trust-ncg')",
            "markdown"
        ],
        [
            "Full Hessian example:",
            "markdown"
        ],
        [
            "Hessian product example:",
            "markdown"
        ],
        [
            "Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm (method='trust-krylov')",
            "markdown"
        ],
        [
            "Full Hessian example:",
            "markdown"
        ],
        [
            "Hessian product example:",
            "markdown"
        ],
        [
            "Trust-Region Nearly Exact Algorithm (method='trust-exact')",
            "markdown"
        ],
        [
            "Constrained minimization of multivariate scalar functions (minimize)",
            "markdown"
        ],
        [
            "Trust-Region Constrained Algorithm (method='trust-constr')",
            "markdown"
        ],
        [
            "Defining Bounds Constraints:",
            "markdown"
        ],
        [
            "Defining Linear Constraints:",
            "markdown"
        ],
        [
            "Defining Nonlinear Constraints:",
            "markdown"
        ],
        [
            "Solving the Optimization Problem:",
            "markdown"
        ],
        [
            "Sequential Least SQuares Programming (SLSQP) Algorithm (method='SLSQP')",
            "markdown"
        ],
        [
            "Global optimization",
            "markdown"
        ],
        [
            "Least-squares minimization (least_squares)",
            "markdown"
        ],
        [
            "Example of solving a fitting problem",
            "markdown"
        ],
        [
            "Further examples",
            "markdown"
        ],
        [
            "Univariate function minimizers (minimize_scalar)",
            "markdown"
        ],
        [
            "Unconstrained minimization (method='brent')",
            "markdown"
        ],
        [
            "Bounded minimization (method='bounded')",
            "markdown"
        ],
        [
            "Custom minimizers",
            "markdown"
        ],
        [
            "Root finding",
            "markdown"
        ],
        [
            "Scalar functions",
            "markdown"
        ],
        [
            "Fixed-point solving",
            "markdown"
        ],
        [
            "Sets of equations",
            "markdown"
        ],
        [
            "Root finding for large problems",
            "markdown"
        ],
        [
            "Still too slow? Preconditioning.",
            "markdown"
        ],
        [
            "Linear programming (linprog)",
            "markdown"
        ],
        [
            "Linear programming example",
            "markdown"
        ],
        [
            "Assignment problems",
            "markdown"
        ],
        [
            "Linear sum assignment problem example",
            "markdown"
        ],
        [
            "Mixed integer linear programming",
            "markdown"
        ],
        [
            "Knapsack problem example",
            "markdown"
        ],
        [
            "The scipy.optimize package provides several commonly used\noptimization algorithms. A detailed listing is available:\nscipy.optimize (can also be found by help(scipy.optimize)).",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)": [
        [
            "The minimize function provides a common interface to unconstrained\nand constrained minimization algorithms for multivariate scalar functions\nin scipy.optimize. To demonstrate the minimization function, consider the\nproblem of minimizing the Rosenbrock function of \\(N\\) variables:\n\n\\[f\\left(\\mathbf{x}\\right)=\\sum_{i=1}^{N-1}100\\left(x_{i+1}-x_{i}^{2}\\right)^{2}+\\left(1-x_{i}\\right)^{2}.\\]",
            "markdown"
        ],
        [
            "The minimum value of this function is 0 which is achieved when\n\\(x_{i}=1.\\)",
            "markdown"
        ],
        [
            "Note that the Rosenbrock function and its derivatives are included in\nscipy.optimize. The implementations shown in the following sections\nprovide examples of how to define an objective function as well as its\njacobian and hessian functions. Objective functions in scipy.optimize\nexpect a numpy array as their first parameter which is to be optimized\nand must return a float value. The exact calling signature must be\nf(x, *args) where x represents a numpy array and args\na tuple of additional arguments supplied to the objective function.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Nelder-Mead Simplex algorithm (method='Nelder-Mead')": [
        [
            "In the example below, the minimize routine is used\nwith the Nelder-Mead simplex algorithm (selected through the method\nparameter):",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy.optimize import minimize",
            "code"
        ],
        [
            "def rosen(x):\n...     \"\"\"The Rosenbrock function\"\"\"\n...     return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)",
            "code"
        ],
        [
            "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n res = minimize(rosen, x0, method='nelder-mead',\n...                options={'xatol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 339\n         Function evaluations: 571",
            "code"
        ],
        [
            "print(res.x)\n[1. 1. 1. 1. 1.]",
            "code"
        ],
        [
            "The simplex algorithm is probably the simplest way to minimize a fairly\nwell-behaved function. It requires only function evaluations and is a good\nchoice for simple minimization problems. However, because it does not use\nany gradient evaluations, it may take longer to find the minimum.",
            "markdown"
        ],
        [
            "Another optimization algorithm that needs only function calls to find\nthe minimum is Powell\u00e2\u0080\u0099s method available by setting method='powell' in\nminimize.",
            "markdown"
        ],
        [
            "To demonstrate how to supply additional arguments to an objective function,\nlet us minimize the Rosenbrock function with an additional scaling factor <em class=\"xref py py-obj\">a\nand an offset <em class=\"xref py py-obj\">b:\n\n\\[f\\left(\\mathbf{x}, a, b\\right)=\\sum_{i=1}^{N-1}a\\left(x_{i+1}-x_{i}^{2}\\right)^{2}+\\left(1-x_{i}\\right)^{2} + b.\\]",
            "markdown"
        ],
        [
            "Again using the minimize routine this can be solved by the following\ncode block for the example parameters <em class=\"xref py py-obj\">a=0.5 and <em class=\"xref py py-obj\">b=1.",
            "markdown"
        ],
        [
            "def rosen_with_args(x, a, b):\n...     \"\"\"The Rosenbrock function with additional arguments\"\"\"\n...     return sum(a*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0) + b",
            "code"
        ],
        [
            "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n res = minimize(rosen_with_args, x0, method='nelder-mead',\n...                args=(0.5, 1.), options={'xatol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 1.000000\n         Iterations: 319\n         Function evaluations: 525",
            "code"
        ],
        [
            "print(res.x)\n[1.         1.         1.         1.         0.99999999]",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Broyden-Fletcher-Goldfarb-Shanno algorithm (method='BFGS')": [
        [
            "In order to converge more quickly to the solution, this routine uses\nthe gradient of the objective function. If the gradient is not given\nby the user, then it is estimated using first-differences. The\nBroyden-Fletcher-Goldfarb-Shanno (BFGS) method typically requires\nfewer function calls than the simplex algorithm even when the gradient\nmust be estimated.",
            "markdown"
        ],
        [
            "To demonstrate this algorithm, the Rosenbrock function is again used.\nThe gradient of the Rosenbrock function is the vector:\n\n \\begin{eqnarray*} \\frac{\\partial f}{\\partial x_{j}} & = & \\sum_{i=1}^{N}200\\left(x_{i}-x_{i-1}^{2}\\right)\\left(\\delta_{i,j}-2x_{i-1}\\delta_{i-1,j}\\right)-2\\left(1-x_{i-1}\\right)\\delta_{i-1,j}.\\\\  & = & 200\\left(x_{j}-x_{j-1}^{2}\\right)-400x_{j}\\left(x_{j+1}-x_{j}^{2}\\right)-2\\left(1-x_{j}\\right).\\end{eqnarray*}",
            "markdown"
        ],
        [
            "This expression is valid for the interior derivatives. Special cases\nare\n\n \\begin{eqnarray*} \\frac{\\partial f}{\\partial x_{0}} & = & -400x_{0}\\left(x_{1}-x_{0}^{2}\\right)-2\\left(1-x_{0}\\right),\\\\ \\frac{\\partial f}{\\partial x_{N-1}} & = & 200\\left(x_{N-1}-x_{N-2}^{2}\\right).\\end{eqnarray*}",
            "markdown"
        ],
        [
            "A Python function which computes this gradient is constructed by the\ncode-segment:",
            "markdown"
        ],
        [
            "def rosen_der(x):\n...     xm = x[1:-1]\n...     xm_m1 = x[:-2]\n...     xm_p1 = x[2:]\n...     der = np.zeros_like(x)\n...     der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n...     der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n...     der[-1] = 200*(x[-1]-x[-2]**2)\n...     return der",
            "code"
        ],
        [
            "This gradient information is specified in the minimize function\nthrough the jac parameter as illustrated below.",
            "markdown"
        ],
        [
            "res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n...                options={'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 25                     # may vary\n         Function evaluations: 30\n         Gradient evaluations: 30\n res.x\narray([1., 1., 1., 1., 1.])",
            "code"
        ],
        [
            "Another way to supply gradient information is to write a single\nfunction which returns both the objective and the gradient: this is\nindicated by setting jac=True. In this case, the Python function\nto be optimized must return a tuple whose first value is the objective\nand whose second value represents the gradient. For this example, the\nobjective can be specified in the following way:",
            "markdown"
        ],
        [
            "def rosen_and_der(x):\n...     objective = sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n...     xm = x[1:-1]\n...     xm_m1 = x[:-2]\n...     xm_p1 = x[2:]\n...     der = np.zeros_like(x)\n...     der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n...     der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n...     der[-1] = 200*(x[-1]-x[-2]**2)\n...     return objective, der",
            "code"
        ],
        [
            "res = minimize(rosen_and_der, x0, method='BFGS', jac=True,\n...                options={'disp': True})\n         Current function value: 0.000000\n         Iterations: 25                     # may vary\n         Function evaluations: 30\n         Gradient evaluations: 30",
            "code"
        ],
        [
            "Supplying objective and gradient in a single function can help to avoid\nredundant computations and therefore speed up the optimization significantly.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Newton-Conjugate-Gradient algorithm (method='Newton-CG')": [
        [
            "Newton-Conjugate Gradient algorithm is a modified Newton\u00e2\u0080\u0099s\nmethod and uses a conjugate gradient algorithm to (approximately) invert\nthe local Hessian [NW].  Newton\u00e2\u0080\u0099s method is based on fitting the function\nlocally to a quadratic form:\n\n\\[f\\left(\\mathbf{x}\\right)\\approx f\\left(\\mathbf{x}_{0}\\right)+\\nabla f\\left(\\mathbf{x}_{0}\\right)\\cdot\\left(\\mathbf{x}-\\mathbf{x}_{0}\\right)+\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{x}_{0}\\right)^{T}\\mathbf{H}\\left(\\mathbf{x}_{0}\\right)\\left(\\mathbf{x}-\\mathbf{x}_{0}\\right).\\]",
            "markdown"
        ],
        [
            "where \\(\\mathbf{H}\\left(\\mathbf{x}_{0}\\right)\\) is a matrix of second-derivatives (the Hessian). If the Hessian is\npositive definite then the local minimum of this function can be found\nby setting the gradient of the quadratic form to zero, resulting in\n\n\\[\\mathbf{x}_{\\textrm{opt}}=\\mathbf{x}_{0}-\\mathbf{H}^{-1}\\nabla f.\\]",
            "markdown"
        ],
        [
            "The inverse of the Hessian is evaluated using the conjugate-gradient\nmethod. An example of employing this method to minimizing the\nRosenbrock function is given below. To take full advantage of the\nNewton-CG method, a function which computes the Hessian must be\nprovided. The Hessian matrix itself does not need to be constructed,\nonly a vector which is the product of the Hessian with an arbitrary\nvector needs to be available to the minimization routine. As a result,\nthe user can provide either a function to compute the Hessian matrix,\nor a function to compute the product of the Hessian with an arbitrary\nvector.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Newton-Conjugate-Gradient algorithm (method='Newton-CG')->Full Hessian example:": [
        [
            "The Hessian of the Rosenbrock function is\n\n \\begin{eqnarray*} H_{ij}=\\frac{\\partial^{2}f}{\\partial x_{i}\\partial x_{j}} & = & 200\\left(\\delta_{i,j}-2x_{i-1}\\delta_{i-1,j}\\right)-400x_{i}\\left(\\delta_{i+1,j}-2x_{i}\\delta_{i,j}\\right)-400\\delta_{i,j}\\left(x_{i+1}-x_{i}^{2}\\right)+2\\delta_{i,j},\\\\  & = & \\left(202+1200x_{i}^{2}-400x_{i+1}\\right)\\delta_{i,j}-400x_{i}\\delta_{i+1,j}-400x_{i-1}\\delta_{i-1,j},\\end{eqnarray*}",
            "markdown"
        ],
        [
            "if \\(i,j\\in\\left[1,N-2\\right]\\) with \\(i,j\\in\\left[0,N-1\\right]\\) defining the \\(N\\times N\\) matrix. Other non-zero entries of the matrix are\n\n \\begin{eqnarray*} \\frac{\\partial^{2}f}{\\partial x_{0}^{2}} & = & 1200x_{0}^{2}-400x_{1}+2,\\\\ \\frac{\\partial^{2}f}{\\partial x_{0}\\partial x_{1}}=\\frac{\\partial^{2}f}{\\partial x_{1}\\partial x_{0}} & = & -400x_{0},\\\\ \\frac{\\partial^{2}f}{\\partial x_{N-1}\\partial x_{N-2}}=\\frac{\\partial^{2}f}{\\partial x_{N-2}\\partial x_{N-1}} & = & -400x_{N-2},\\\\ \\frac{\\partial^{2}f}{\\partial x_{N-1}^{2}} & = & 200.\\end{eqnarray*}",
            "markdown"
        ],
        [
            "For example, the Hessian when \\(N=5\\) is\n\n\\[\\begin{split}\\mathbf{H}=\\begin{bmatrix} 1200x_{0}^{2}-400x_{1}+2 & -400x_{0} & 0 & 0 & 0\\\\ -400x_{0} & 202+1200x_{1}^{2}-400x_{2} & -400x_{1} & 0 & 0\\\\ 0 & -400x_{1} & 202+1200x_{2}^{2}-400x_{3} & -400x_{2} & 0\\\\ 0 &  & -400x_{2} & 202+1200x_{3}^{2}-400x_{4} & -400x_{3}\\\\ 0 & 0 & 0 & -400x_{3} & 200\\end{bmatrix}.\\end{split}\\]",
            "markdown"
        ],
        [
            "The code which computes this Hessian along with the code to minimize\nthe function using Newton-CG method is shown in the following example:",
            "markdown"
        ],
        [
            "def rosen_hess(x):\n...     x = np.asarray(x)\n...     H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n...     diagonal = np.zeros_like(x)\n...     diagonal[0] = 1200*x[0]**2-400*x[1]+2\n...     diagonal[-1] = 200\n...     diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n...     H = H + np.diag(diagonal)\n...     return H",
            "code"
        ],
        [
            "res = minimize(rosen, x0, method='Newton-CG',\n...                jac=rosen_der, hess=rosen_hess,\n...                options={'xtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 19                       # may vary\n         Function evaluations: 22\n         Gradient evaluations: 19\n         Hessian evaluations: 19\n res.x\narray([1.,  1.,  1.,  1.,  1.])",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Newton-Conjugate-Gradient algorithm (method='Newton-CG')->Hessian product example:": [
        [
            "For larger minimization problems, storing the entire Hessian matrix can\nconsume considerable time and memory. The Newton-CG algorithm only needs\nthe product of the Hessian times an arbitrary vector. As a result, the user\ncan supply code to compute this product rather than the full Hessian by\ngiving a hess function which take the minimization vector as the first\nargument and the arbitrary vector as the second argument (along with extra\narguments passed to the function to be minimized). If possible, using\nNewton-CG with the Hessian product option is probably the fastest way to\nminimize the function.",
            "markdown"
        ],
        [
            "In this case, the product of the Rosenbrock Hessian with an arbitrary\nvector is not difficult to compute. If \\(\\mathbf{p}\\) is the arbitrary\nvector, then \\(\\mathbf{H}\\left(\\mathbf{x}\\right)\\mathbf{p}\\) has\nelements:\n\n\\[\\begin{split}\\mathbf{H}\\left(\\mathbf{x}\\right)\\mathbf{p}=\\begin{bmatrix} \\left(1200x_{0}^{2}-400x_{1}+2\\right)p_{0}-400x_{0}p_{1}\\\\ \\vdots\\\\ -400x_{i-1}p_{i-1}+\\left(202+1200x_{i}^{2}-400x_{i+1}\\right)p_{i}-400x_{i}p_{i+1}\\\\ \\vdots\\\\ -400x_{N-2}p_{N-2}+200p_{N-1}\\end{bmatrix}.\\end{split}\\]",
            "markdown"
        ],
        [
            "Code which makes use of this Hessian product to minimize the\nRosenbrock function using minimize follows:",
            "markdown"
        ],
        [
            "def rosen_hess_p(x, p):\n...     x = np.asarray(x)\n...     Hp = np.zeros_like(x)\n...     Hp[0] = (1200*x[0]**2 - 400*x[1] + 2)*p[0] - 400*x[0]*p[1]\n...     Hp[1:-1] = -400*x[:-2]*p[:-2]+(202+1200*x[1:-1]**2-400*x[2:])*p[1:-1] \\\n...                -400*x[1:-1]*p[2:]\n...     Hp[-1] = -400*x[-2]*p[-2] + 200*p[-1]\n...     return Hp",
            "code"
        ],
        [
            "res = minimize(rosen, x0, method='Newton-CG',\n...                jac=rosen_der, hessp=rosen_hess_p,\n...                options={'xtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 20                    # may vary\n         Function evaluations: 23\n         Gradient evaluations: 20\n         Hessian evaluations: 44\n res.x\narray([1., 1., 1., 1., 1.])",
            "code"
        ],
        [
            "According to [NW] p. 170 the Newton-CG algorithm can be inefficient\nwhen the Hessian is ill-conditioned because of the poor quality search directions\nprovided by the method in those situations. The method trust-ncg,\naccording to the authors, deals more effectively with this problematic situation\nand will be described next.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Newton-Conjugate-Gradient Algorithm (method='trust-ncg')": [
        [
            "The Newton-CG method is a line search method: it finds a direction\nof search minimizing a quadratic approximation of the function and then uses\na line search algorithm to find the (nearly) optimal step size in that direction.\nAn alternative approach is to, first, fix the step size limit \\(\\Delta\\) and then find the\noptimal step \\(\\mathbf{p}\\) inside the given trust-radius by solving\nthe following quadratic subproblem:\n\n\\begin{eqnarray*}\n   \\min_{\\mathbf{p}} f\\left(\\mathbf{x}_{k}\\right)+\\nabla f\\left(\\mathbf{x}_{k}\\right)\\cdot\\mathbf{p}+\\frac{1}{2}\\mathbf{p}^{T}\\mathbf{H}\\left(\\mathbf{x}_{k}\\right)\\mathbf{p};&\\\\\n   \\text{subject to: } \\|\\mathbf{p}\\|\\le \\Delta.&\n \\end{eqnarray*}",
            "markdown"
        ],
        [
            "The solution is then updated \\(\\mathbf{x}_{k+1} = \\mathbf{x}_{k} + \\mathbf{p}\\) and\nthe trust-radius \\(\\Delta\\) is adjusted according to the degree of agreement of the quadratic\nmodel with the real function. This family of methods is known as trust-region methods.\nThe trust-ncg algorithm is a trust-region method that uses a conjugate gradient algorithm\nto solve the trust-region subproblem [NW].",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Newton-Conjugate-Gradient Algorithm (method='trust-ncg')->Full Hessian example:": [
        [
            "res = minimize(rosen, x0, method='trust-ncg',\n...                jac=rosen_der, hess=rosen_hess,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 20                    # may vary\n         Function evaluations: 21\n         Gradient evaluations: 20\n         Hessian evaluations: 19\n res.x\narray([1., 1., 1., 1., 1.])",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Newton-Conjugate-Gradient Algorithm (method='trust-ncg')->Hessian product example:": [
        [
            "res = minimize(rosen, x0, method='trust-ncg',\n...                jac=rosen_der, hessp=rosen_hess_p,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 20                    # may vary\n         Function evaluations: 21\n         Gradient evaluations: 20\n         Hessian evaluations: 0\n res.x\narray([1., 1., 1., 1., 1.])",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm (method='trust-krylov')": [
        [
            "Similar to the trust-ncg method, the trust-krylov method is a method\nsuitable for large-scale problems as it uses the hessian only as linear\noperator by means of matrix-vector products.\nIt solves the quadratic subproblem more accurately than the trust-ncg\nmethod.\n\n\\begin{eqnarray*}\n   \\min_{\\mathbf{p}} f\\left(\\mathbf{x}_{k}\\right)+\\nabla f\\left(\\mathbf{x}_{k}\\right)\\cdot\\mathbf{p}+\\frac{1}{2}\\mathbf{p}^{T}\\mathbf{H}\\left(\\mathbf{x}_{k}\\right)\\mathbf{p};&\\\\\n   \\text{subject to: } \\|\\mathbf{p}\\|\\le \\Delta.&\n \\end{eqnarray*}",
            "markdown"
        ],
        [
            "This method wraps the [TRLIB] implementation of the [GLTR] method solving\nexactly a trust-region subproblem restricted to a truncated Krylov subspace.\nFor indefinite problems it is usually better to use this method as it reduces\nthe number of nonlinear iterations at the expense of few more matrix-vector\nproducts per subproblem solve in comparison to the trust-ncg method.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm (method='trust-krylov')->Full Hessian example:": [
        [
            "res = minimize(rosen, x0, method='trust-krylov',\n...                jac=rosen_der, hess=rosen_hess,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 19                    # may vary\n         Function evaluations: 20\n         Gradient evaluations: 20\n         Hessian evaluations: 18\n res.x\narray([1., 1., 1., 1., 1.])",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm (method='trust-krylov')->Hessian product example:": [
        [
            "res = minimize(rosen, x0, method='trust-krylov',\n...                jac=rosen_der, hessp=rosen_hess_p,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 19                    # may vary\n         Function evaluations: 20\n         Gradient evaluations: 20\n         Hessian evaluations: 0\n res.x\narray([1., 1., 1., 1., 1.])\n\n\n\n\n[TRLIB]",
            "code"
        ],
        [
            "F. Lenders, C. Kirches, A. Potschka: \u00e2\u0080\u009ctrlib: A vector-free\nimplementation of the GLTR method for iterative solution of\nthe trust region problem\u00e2\u0080\u009d, arXiv:1611.04718\n\n\n[GLTR]",
            "markdown"
        ],
        [
            "N. Gould, S. Lucidi, M. Roma, P. Toint: \u00e2\u0080\u009cSolving the\nTrust-Region Subproblem using the Lanczos Method\u00e2\u0080\u009d,\nSIAM J. Optim., 9(2), 504\u00e2\u0080\u0093525, (1999).\nDOI:10.1137/S1052623497322735",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Nearly Exact Algorithm (method='trust-exact')": [
        [
            "All methods Newton-CG, trust-ncg and trust-krylov are suitable for dealing with\nlarge-scale problems (problems with thousands of variables). That is because the conjugate\ngradient algorithm approximately solve the trust-region subproblem (or invert the Hessian)\nby iterations without the explicit Hessian factorization. Since only the product of the Hessian\nwith an arbitrary vector is needed, the algorithm is specially suited for dealing\nwith sparse Hessians, allowing low storage requirements and significant time savings for\nthose sparse problems.",
            "markdown"
        ],
        [
            "For medium-size problems, for which the storage and factorization cost of the Hessian are not critical,\nit is possible to obtain a solution within fewer iteration by solving the trust-region subproblems\nalmost exactly. To achieve that, a certain nonlinear equations is solved iteratively for each quadratic\nsubproblem [CGT]. This solution requires usually 3 or 4 Cholesky factorizations of the\nHessian matrix. As the result, the method converges in fewer number of iterations\nand takes fewer evaluations of the objective function than the other implemented\ntrust-region methods. The Hessian product option is not supported by this algorithm. An\nexample using the Rosenbrock function follows:",
            "markdown"
        ],
        [
            "res = minimize(rosen, x0, method='trust-exact',\n...                jac=rosen_der, hess=rosen_hess,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 13                    # may vary\n         Function evaluations: 14\n         Gradient evaluations: 13\n         Hessian evaluations: 14\n res.x\narray([1., 1., 1., 1., 1.])\n\n\n\n\n[NW]\n(1,2,3)",
            "code"
        ],
        [
            "J. Nocedal, S.J. Wright \u00e2\u0080\u009cNumerical optimization.\u00e2\u0080\u009d\n2nd edition. Springer Science (2006).\n\n\n[CGT]",
            "markdown"
        ],
        [
            "Conn, A. R., Gould, N. I., & Toint, P. L.\n\u00e2\u0080\u009cTrust region methods\u00e2\u0080\u009d. Siam. (2000). pp. 169-200.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)": [
        [
            "The minimize function provides algorithms for constrained minimization,\nnamely 'trust-constr' ,  'SLSQP' and 'COBYLA'. They require the constraints\nto be defined using slightly different structures. The method 'trust-constr' requires\nthe  constraints to be defined as a sequence of objects LinearConstraint and\nNonlinearConstraint. Methods 'SLSQP' and 'COBYLA', on the other hand,\nrequire constraints to be defined  as a sequence of dictionaries, with keys\ntype, fun and jac.",
            "markdown"
        ],
        [
            "As an example let us consider the constrained minimization of the Rosenbrock function:\n\n  \\begin{eqnarray*} \\min_{x_0, x_1} & ~~100\\left(x_{1}-x_{0}^{2}\\right)^{2}+\\left(1-x_{0}\\right)^{2} &\\\\\n                  \\text{subject to: } & x_0 + 2 x_1 \\leq 1 & \\\\\n                                      & x_0^2 + x_1 \\leq 1  & \\\\\n                                      & x_0^2 - x_1 \\leq 1  & \\\\\n                                      & 2 x_0 + x_1 = 1 & \\\\\n                                      & 0 \\leq  x_0  \\leq 1 & \\\\\n                                      & -0.5 \\leq  x_1  \\leq 2.0. & \\end{eqnarray*}",
            "markdown"
        ],
        [
            "This optimization problem has the unique solution \\([x_0, x_1] = [0.4149,~ 0.1701]\\),\nfor which only the first and fourth constraints are active.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')": [
        [
            "The trust-region constrained method deals with constrained minimization problems of the form:\n\n  \\begin{eqnarray*} \\min_x & f(x) & \\\\\n       \\text{subject to: } & ~~~ c^l  \\leq c(x) \\leq c^u, &\\\\\n        &  x^l  \\leq x \\leq x^u. & \\end{eqnarray*}",
            "markdown"
        ],
        [
            "When \\(c^l_j = c^u_j\\) the method reads the \\(j\\)-th constraint as an\nequality constraint and deals with it accordingly. Besides that, one-sided constraint\ncan be specified by setting the upper or lower bound to np.inf with the appropriate sign.",
            "markdown"
        ],
        [
            "The implementation is based on [EQSQP] for equality-constraint problems and on [TRIP]\nfor problems with inequality constraints. Both are trust-region type algorithms suitable\nfor large-scale problems.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')->Defining Bounds Constraints:": [
        [
            "The bound constraints  \\(0 \\leq  x_0  \\leq 1\\) and \\(-0.5 \\leq  x_1  \\leq 2.0\\)\nare defined using a Bounds object.",
            "markdown"
        ],
        [
            "from scipy.optimize import Bounds\n bounds = Bounds([0, -0.5], [1.0, 2.0])",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')->Defining Linear Constraints:": [
        [
            "The constraints \\(x_0 + 2 x_1 \\leq 1\\)\nand \\(2 x_0 + x_1 = 1\\) can be written in the linear constraint standard format:\n\n  \\begin{equation*} \\begin{bmatrix}-\\infty \\\\1\\end{bmatrix} \\leq\n   \\begin{bmatrix} 1& 2 \\\\ 2& 1\\end{bmatrix}\n    \\begin{bmatrix} x_0 \\\\x_1\\end{bmatrix} \\leq\n     \\begin{bmatrix} 1 \\\\ 1\\end{bmatrix},\\end{equation*}",
            "markdown"
        ],
        [
            "and defined using a LinearConstraint object.",
            "markdown"
        ],
        [
            "from scipy.optimize import LinearConstraint\n linear_constraint = LinearConstraint([[1, 2], [2, 1]], [-np.inf, 1], [1, 1])",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')->Defining Nonlinear Constraints:": [
        [
            "The nonlinear constraint:\n\n  \\begin{equation*} c(x) =\n  \\begin{bmatrix} x_0^2 + x_1 \\\\ x_0^2 - x_1\\end{bmatrix}\n   \\leq\n   \\begin{bmatrix} 1 \\\\ 1\\end{bmatrix}, \\end{equation*}",
            "markdown"
        ],
        [
            "with Jacobian matrix:\n\n  \\begin{equation*} J(x) =\n  \\begin{bmatrix} 2x_0 & 1 \\\\ 2x_0 & -1\\end{bmatrix},\\end{equation*}",
            "markdown"
        ],
        [
            "and linear combination of the Hessians:\n\n  \\begin{equation*} H(x, v) = \\sum_{i=0}^1 v_i \\nabla^2 c_i(x) =\n  v_0\\begin{bmatrix} 2 & 0 \\\\ 0 & 0\\end{bmatrix} +\n  v_1\\begin{bmatrix} 2 & 0 \\\\ 0 & 0\\end{bmatrix},\n  \\end{equation*}",
            "markdown"
        ],
        [
            "is defined using a NonlinearConstraint object.",
            "markdown"
        ],
        [
            "def cons_f(x):\n...     return [x[0]**2 + x[1], x[0]**2 - x[1]]\n def cons_J(x):\n...     return [[2*x[0], 1], [2*x[0], -1]]\n def cons_H(x, v):\n...     return v[0]*np.array([[2, 0], [0, 0]]) + v[1]*np.array([[2, 0], [0, 0]])\n from scipy.optimize import NonlinearConstraint\n nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess=cons_H)",
            "code"
        ],
        [
            "Alternatively, it is also possible to define the Hessian \\(H(x, v)\\)\nas a sparse matrix,",
            "markdown"
        ],
        [
            "from scipy.sparse import csc_matrix\n def cons_H_sparse(x, v):\n...     return v[0]*csc_matrix([[2, 0], [0, 0]]) + v[1]*csc_matrix([[2, 0], [0, 0]])\n nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1,\n...                                            jac=cons_J, hess=cons_H_sparse)",
            "code"
        ],
        [
            "or as a LinearOperator object.",
            "markdown"
        ],
        [
            "from scipy.sparse.linalg import LinearOperator\n def cons_H_linear_operator(x, v):\n...     def matvec(p):\n...         return np.array([p[0]*2*(v[0]+v[1]), 0])\n...     return LinearOperator((2, 2), matvec=matvec)\n nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1,\n...                                           jac=cons_J, hess=cons_H_linear_operator)",
            "code"
        ],
        [
            "When the evaluation of the Hessian \\(H(x, v)\\)\nis difficult to implement or computationally infeasible, one may use HessianUpdateStrategy.\nCurrently available strategies are BFGS and SR1.",
            "markdown"
        ],
        [
            "from scipy.optimize import BFGS\n nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess=BFGS())",
            "code"
        ],
        [
            "Alternatively, the Hessian may be approximated using finite differences.",
            "markdown"
        ],
        [
            "nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess='2-point')",
            "code"
        ],
        [
            "The Jacobian of the constraints can be approximated by finite differences as well. In this case,\nhowever, the Hessian cannot be computed with finite differences and needs to\nbe provided by the user or defined using HessianUpdateStrategy.",
            "markdown"
        ],
        [
            "nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac='2-point', hess=BFGS())",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')->Solving the Optimization Problem:": [
        [
            "The optimization problem is solved using:",
            "markdown"
        ],
        [
            "x0 = np.array([0.5, 0])\n res = minimize(rosen, x0, method='trust-constr', jac=rosen_der, hess=rosen_hess,\n...                constraints=[linear_constraint, nonlinear_constraint],\n...                options={'verbose': 1}, bounds=bounds)\n# may vary\n`gtol` termination condition is satisfied.\nNumber of iterations: 12, function evaluations: 8, CG iterations: 7, optimality: 2.99e-09, constraint violation: 1.11e-16, execution time: 0.016 s.\n print(res.x)\n[0.41494531 0.17010937]",
            "code"
        ],
        [
            "When needed, the objective function Hessian can be defined using a LinearOperator object,",
            "markdown"
        ],
        [
            "def rosen_hess_linop(x):\n...     def matvec(p):\n...         return rosen_hess_p(x, p)\n...     return LinearOperator((2, 2), matvec=matvec)\n res = minimize(rosen, x0, method='trust-constr', jac=rosen_der, hess=rosen_hess_linop,\n...                constraints=[linear_constraint, nonlinear_constraint],\n...                options={'verbose': 1}, bounds=bounds)\n# may vary\n`gtol` termination condition is satisfied.\nNumber of iterations: 12, function evaluations: 8, CG iterations: 7, optimality: 2.99e-09, constraint violation: 1.11e-16, execution time: 0.018 s.\n print(res.x)\n[0.41494531 0.17010937]",
            "code"
        ],
        [
            "or a Hessian-vector product through the parameter hessp.",
            "markdown"
        ],
        [
            "res = minimize(rosen, x0, method='trust-constr', jac=rosen_der, hessp=rosen_hess_p,\n...                constraints=[linear_constraint, nonlinear_constraint],\n...                options={'verbose': 1}, bounds=bounds)\n# may vary\n`gtol` termination condition is satisfied.\nNumber of iterations: 12, function evaluations: 8, CG iterations: 7, optimality: 2.99e-09, constraint violation: 1.11e-16, execution time: 0.018 s.\n print(res.x)\n[0.41494531 0.17010937]",
            "code"
        ],
        [
            "Alternatively, the first and second derivatives of the objective function can be approximated.\nFor instance,  the Hessian can be approximated with SR1 quasi-Newton approximation\nand the gradient with finite differences.",
            "markdown"
        ],
        [
            "from scipy.optimize import SR1\n res = minimize(rosen, x0, method='trust-constr',  jac=\"2-point\", hess=SR1(),\n...                constraints=[linear_constraint, nonlinear_constraint],\n...                options={'verbose': 1}, bounds=bounds)\n# may vary\n`gtol` termination condition is satisfied.\nNumber of iterations: 12, function evaluations: 24, CG iterations: 7, optimality: 4.48e-09, constraint violation: 0.00e+00, execution time: 0.016 s.\n print(res.x)\n[0.41494531 0.17010937]\n\n\n\n\n[TRIP]",
            "code"
        ],
        [
            "Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\nAn interior point algorithm for large-scale nonlinear  programming.\nSIAM Journal on Optimization 9.4: 877-900.\n\n\n[EQSQP]",
            "markdown"
        ],
        [
            "Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\nimplementation of an algorithm for large-scale equality constrained\noptimization. SIAM Journal on Optimization 8.3: 682-706.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Sequential Least SQuares Programming (SLSQP) Algorithm (method='SLSQP')": [
        [
            "The SLSQP method deals with constrained minimization problems of the form:\n\n  \\begin{eqnarray*} \\min_x & f(x) \\\\\n       \\text{subject to: } & c_j(x) =  0  ,  &j \\in \\mathcal{E}\\\\\n         & c_j(x) \\geq 0  ,  &j \\in \\mathcal{I}\\\\\n        &  \\text{lb}_i  \\leq x_i \\leq \\text{ub}_i , &i = 1,...,N. \\end{eqnarray*}",
            "markdown"
        ],
        [
            "Where \\(\\mathcal{E}\\) or \\(\\mathcal{I}\\) are sets of indices\ncontaining equality and inequality constraints.",
            "markdown"
        ],
        [
            "Both linear and nonlinear constraints are defined as dictionaries with keys type, fun and jac.",
            "markdown"
        ],
        [
            "ineq_cons = {'type': 'ineq',\n...              'fun' : lambda x: np.array([1 - x[0] - 2*x[1],\n...                                          1 - x[0]**2 - x[1],\n...                                          1 - x[0]**2 + x[1]]),\n...              'jac' : lambda x: np.array([[-1.0, -2.0],\n...                                          [-2*x[0], -1.0],\n...                                          [-2*x[0], 1.0]])}\n eq_cons = {'type': 'eq',\n...            'fun' : lambda x: np.array([2*x[0] + x[1] - 1]),\n...            'jac' : lambda x: np.array([2.0, 1.0])}",
            "code"
        ],
        [
            "And the optimization problem is solved with:",
            "markdown"
        ],
        [
            "x0 = np.array([0.5, 0])\n res = minimize(rosen, x0, method='SLSQP', jac=rosen_der,\n...                constraints=[eq_cons, ineq_cons], options={'ftol': 1e-9, 'disp': True},\n...                bounds=bounds)\n# may vary\nOptimization terminated successfully.    (Exit mode 0)\n            Current function value: 0.342717574857755\n            Iterations: 5\n            Function evaluations: 6\n            Gradient evaluations: 5\n print(res.x)\n[0.41494475 0.1701105 ]",
            "code"
        ],
        [
            "Most of the options available for the method 'trust-constr' are not available\nfor 'SLSQP'.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Global optimization": [
        [
            "Global optimization aims to find the global minimum of a function within given\nbounds, in the presence of potentially many local minima. Typically, global\nminimizers efficiently search the parameter space, while using a local\nminimizer (e.g., minimize) under the hood.  SciPy contains a\nnumber of good global optimizers.  Here, we\u00e2\u0080\u0099ll use those on the same objective\nfunction, namely the (aptly named) eggholder function:",
            "markdown"
        ],
        [
            "def eggholder(x):\n...     return (-(x[1] + 47) * np.sin(np.sqrt(abs(x[0]/2 + (x[1]  + 47))))\n...             -x[0] * np.sin(np.sqrt(abs(x[0] - (x[1]  + 47)))))\n\n bounds = [(-512, 512), (-512, 512)]",
            "code"
        ],
        [
            "This function looks like an egg carton:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n from mpl_toolkits.mplot3d import Axes3D\n\n x = np.arange(-512, 513)\n y = np.arange(-512, 513)\n xgrid, ygrid = np.meshgrid(x, y)\n xy = np.stack([xgrid, ygrid])\n\n fig = plt.figure()\n ax = fig.add_subplot(111, projection='3d')\n ax.view_init(45, -45)\n ax.plot_surface(xgrid, ygrid, eggholder(xy), cmap='terrain')\n ax.set_xlabel('x')\n ax.set_ylabel('y')\n ax.set_zlabel('eggholder(x, y)')\n plt.show()\n\n\n<figure class=\"align-center\">\n<img alt=\"&quot;A 3-D plot shown from a three-quarter view. The function is very noisy with dozens of valleys and peaks. There is no clear min or max discernable from this view and it's not possible to see all the local peaks and valleys from this view.&quot;\" class=\"plot-directive\" src=\"../_images/optimize_global_2.png\"/>\n</figure>",
            "code"
        ],
        [
            "We now use the global optimizers to obtain the minimum and the function value\nat the minimum. We\u00e2\u0080\u0099ll store the results in a dictionary so we can compare\ndifferent optimization results later.",
            "markdown"
        ],
        [
            "from scipy import optimize\n results = dict()\n results['shgo'] = optimize.shgo(eggholder, bounds)\n results['shgo']\n     fun: -935.3379515604197  # may vary\n    funl: array([-935.33795156])\n message: 'Optimization terminated successfully.'\n    nfev: 42\n     nit: 2\n   nlfev: 37\n   nlhev: 0\n   nljev: 9\n success: True\n       x: array([439.48096952, 453.97740589])\n      xl: array([[439.48096952, 453.97740589]])",
            "code"
        ],
        [
            "results['DA'] = optimize.dual_annealing(eggholder, bounds)\n results['DA']\n     fun: -956.9182316237413  # may vary\n message: ['Maximum number of iteration reached']\n    nfev: 4091\n    nhev: 0\n     nit: 1000\n    njev: 0\n       x: array([482.35324114, 432.87892901])",
            "code"
        ],
        [
            "All optimizers return an OptimizeResult, which in addition to the solution\ncontains information on the number of function evaluations, whether the\noptimization was successful, and more.  For brevity, we won\u00e2\u0080\u0099t show the full\noutput of the other optimizers:",
            "markdown"
        ],
        [
            "results['DE'] = optimize.differential_evolution(eggholder, bounds)",
            "code"
        ],
        [
            "shgo has a second method, which returns all local minima rather than\nonly what it thinks is the global minimum:",
            "markdown"
        ],
        [
            "results['shgo_sobol'] = optimize.shgo(eggholder, bounds, n=200, iters=5,\n...                                       sampling_method='sobol')",
            "code"
        ],
        [
            "We\u00e2\u0080\u0099ll now plot all found minima on a heatmap of the function:",
            "markdown"
        ],
        [
            "fig = plt.figure()\n ax = fig.add_subplot(111)\n im = ax.imshow(eggholder(xy), interpolation='bilinear', origin='lower',\n...                cmap='gray')\n ax.set_xlabel('x')\n ax.set_ylabel('y')\n\n def plot_point(res, marker='o', color=None):\n...     ax.plot(512+res.x[0], 512+res.x[1], marker=marker, color=color, ms=10)\n\n plot_point(results['DE'], color='c')  # differential_evolution - cyan\n plot_point(results['DA'], color='w')  # dual_annealing.        - white\n\n # SHGO produces multiple minima, plot them all (with a smaller marker size)\n plot_point(results['shgo'], color='r', marker='+')\n plot_point(results['shgo_sobol'], color='r', marker='x')\n for i in range(results['shgo_sobol'].xl.shape[0]):\n...     ax.plot(512 + results['shgo_sobol'].xl[i, 0],\n...             512 + results['shgo_sobol'].xl[i, 1],\n...             'ro', ms=2)\n\n ax.set_xlim([-4, 514*2])\n ax.set_ylim([-4, 514*2])\n plt.show()\n\n\n<figure class=\"align-center\">\n<img alt=\"&quot;This X-Y plot is a heatmap with the Z value denoted with the lowest points as black and the highest values as white. The image resembles a chess board rotated 45 degrees but heavily smoothed. A red dot is located at many of the minima on the grid resulting from the SHGO optimizer. SHGO shows the global minima as a red X in the top right. A local minima found with dual annealing is a white circle marker in the top left. A different local minima found with basinhopping is a yellow marker in the top center. The code is plotting the differential evolution result as a cyan circle, but it is not visible on the plot. At a glance it's not clear which of these valleys is the true global minima.&quot;\" class=\"plot-directive\" src=\"../_images/optimize_global_1.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Least-squares minimization (least_squares)": [
        [
            "SciPy is capable of solving robustified bound-constrained nonlinear\nleast-squares problems:\n\n\\begin{align}\n&\\min_\\mathbf{x} \\frac{1}{2} \\sum_{i = 1}^m \\rho\\left(f_i(\\mathbf{x})^2\\right) \\\\\n&\\text{subject to }\\mathbf{lb} \\leq \\mathbf{x} \\leq \\mathbf{ub}\n\\end{align}",
            "markdown"
        ],
        [
            "Here \\(f_i(\\mathbf{x})\\) are smooth functions from\n\\(\\mathbb{R}^n\\) to \\(\\mathbb{R}\\), we refer to them as residuals.\nThe purpose of a scalar-valued function \\(\\rho(\\cdot)\\) is to reduce the\ninfluence of outlier residuals and contribute to robustness of the solution,\nwe refer to it as a loss function. A linear loss function gives a standard\nleast-squares problem. Additionally, constraints in a form of lower and upper\nbounds on some of \\(x_j\\) are allowed.",
            "markdown"
        ],
        [
            "All methods specific to least-squares minimization utilize a \\(m \\times n\\)\nmatrix of partial derivatives called Jacobian and defined as\n\\(J_{ij} = \\partial f_i / \\partial x_j\\). It is highly recommended to\ncompute this matrix analytically and pass it to least_squares,\notherwise, it will be estimated by finite differences, which takes a lot of\nadditional time and can be very inaccurate in hard cases.",
            "markdown"
        ],
        [
            "Function least_squares can be used for fitting a function\n\\(\\varphi(t; \\mathbf{x})\\) to empirical data \\(\\{(t_i, y_i), i = 0, \\ldots, m-1\\}\\).\nTo do this, one should simply precompute residuals as\n\\(f_i(\\mathbf{x}) = w_i (\\varphi(t_i; \\mathbf{x}) - y_i)\\), where \\(w_i\\)\nare weights assigned to each observation.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Least-squares minimization (least_squares)->Example of solving a fitting problem": [
        [
            "Here we consider an enzymatic reaction [1]. There are 11 residuals defined as\n\n\\[f_i(x) = \\frac{x_0 (u_i^2 + u_i x_1)}{u_i^2 + u_i x_2 + x_3} - y_i, \\quad i = 0, \\ldots, 10,\\]",
            "markdown"
        ],
        [
            "where \\(y_i\\) are measurement values and \\(u_i\\) are values of\nthe independent variable. The unknown vector of parameters is\n\\(\\mathbf{x} = (x_0, x_1, x_2, x_3)^T\\). As was said previously, it is\nrecommended to compute Jacobian matrix in a closed form:\n\n \\begin{align}\n &J_{i0} = \\frac{\\partial f_i}{\\partial x_0} = \\frac{u_i^2 + u_i x_1}{u_i^2 + u_i x_2 + x_3} \\\\\n &J_{i1} = \\frac{\\partial f_i}{\\partial x_1} = \\frac{u_i x_0}{u_i^2 + u_i x_2 + x_3} \\\\\n &J_{i2} = \\frac{\\partial f_i}{\\partial x_2} = -\\frac{x_0 (u_i^2 + u_i x_1) u_i}{(u_i^2 + u_i x_2 + x_3)^2} \\\\\n &J_{i3} = \\frac{\\partial f_i}{\\partial x_3} = -\\frac{x_0 (u_i^2 + u_i x_1)}{(u_i^2 + u_i x_2 + x_3)^2}\n \\end{align}",
            "markdown"
        ],
        [
            "We are going to use the \u00e2\u0080\u009chard\u00e2\u0080\u009d starting point defined in [2]. To find a\nphysically meaningful solution, avoid potential division by zero and assure\nconvergence to the global minimum we impose constraints\n\\(0 \\leq x_j \\leq 100, j = 0, 1, 2, 3\\).",
            "markdown"
        ],
        [
            "The code below implements least-squares estimation of \\(\\mathbf{x}\\) and\nfinally plots the original data and the fitted model function:",
            "markdown"
        ],
        [
            "from scipy.optimize import least_squares",
            "code"
        ],
        [
            "def model(x, u):\n...     return x[0] * (u ** 2 + x[1] * u) / (u ** 2 + x[2] * u + x[3])",
            "code"
        ],
        [
            "def fun(x, u, y):\n...     return model(x, u) - y",
            "code"
        ],
        [
            "def jac(x, u, y):\n...     J = np.empty((u.size, x.size))\n...     den = u ** 2 + x[2] * u + x[3]\n...     num = u ** 2 + x[1] * u\n...     J[:, 0] = num / den\n...     J[:, 1] = x[0] * u / den\n...     J[:, 2] = -x[0] * num * u / den ** 2\n...     J[:, 3] = -x[0] * num / den ** 2\n...     return J",
            "code"
        ],
        [
            "u = np.array([4.0, 2.0, 1.0, 5.0e-1, 2.5e-1, 1.67e-1, 1.25e-1, 1.0e-1,\n...               8.33e-2, 7.14e-2, 6.25e-2])\n y = np.array([1.957e-1, 1.947e-1, 1.735e-1, 1.6e-1, 8.44e-2, 6.27e-2,\n...               4.56e-2, 3.42e-2, 3.23e-2, 2.35e-2, 2.46e-2])\n x0 = np.array([2.5, 3.9, 4.15, 3.9])\n res = least_squares(fun, x0, jac=jac, bounds=(0, 100), args=(u, y), verbose=1)\n# may vary\n`ftol` termination condition is satisfied.\nFunction evaluations 130, initial cost 4.4383e+00, final cost 1.5375e-04, first-order optimality 4.92e-08.\n res.x\narray([ 0.19280596,  0.19130423,  0.12306063,  0.13607247])",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n u_test = np.linspace(0, 5)\n y_test = model(res.x, u_test)\n plt.plot(u, y, 'o', markersize=4, label='data')\n plt.plot(u_test, y_test, label='fitted model')\n plt.xlabel(\"u\")\n plt.ylabel(\"y\")\n plt.legend(loc='lower right')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code plots an X-Y time-series. The series starts in the lower left at (0, 0) and rapidly trends up to the maximum of 0.2 then flattens out. The fitted model is shown as a smooth orange trace and is well fit to the data.\"' class=\"plot-directive\" src=\"../_images/optimize-1.png\"/>\n</figure>\n<aside class=\"footnote-list brackets\">\n<aside class=\"footnote brackets\" id=\"id15\" role=\"note\">\n[1]",
            "code"
        ],
        [
            "J. Kowalik and J. F. Morrison, \u00e2\u0080\u009cAnalysis of kinetic data for allosteric enzyme reactions as\na nonlinear regression problem\u00e2\u0080\u009d, Math. Biosci., vol. 2, pp. 57-66, 1968.\n</aside>\n<aside class=\"footnote brackets\" id=\"id16\" role=\"note\">\n[2]",
            "markdown"
        ],
        [
            "Averick et al., \u00e2\u0080\u009cThe MINPACK-2 Test Problem Collection\u00e2\u0080\u009d.\n\n\n\n</aside>\n</aside>",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Least-squares minimization (least_squares)->Further examples": [
        [
            "Three interactive examples below illustrate usage of least_squares in\ngreater detail.",
            "markdown"
        ],
        [
            "Large-scale bundle adjustment in scipy\ndemonstrates large-scale capabilities of least_squares and how to\nefficiently compute finite difference approximation of sparse Jacobian.",
            "markdown"
        ],
        [
            "Robust nonlinear regression in scipy\nshows how to handle outliers with a robust loss function in a nonlinear\nregression.",
            "markdown"
        ],
        [
            "Solving a discrete boundary-value problem in scipy\nexamines how to solve a large system of equations and use bounds to achieve\ndesired properties of the solution.",
            "markdown"
        ],
        [
            "For the details about mathematical algorithms behind the implementation refer\nto documentation of least_squares.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Univariate function minimizers (minimize_scalar)": [
        [
            "Often only the minimum of an univariate function (i.e., a function that\ntakes a scalar as input) is needed. In these circumstances, other\noptimization techniques have been developed that can work faster. These are\naccessible from the minimize_scalar function, which proposes several\nalgorithms.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Univariate function minimizers (minimize_scalar)->Unconstrained minimization (method='brent')": [
        [
            "There are, actually, two methods that can be used to minimize an univariate\nfunction: brent and golden, but golden is included only for academic\npurposes and should rarely be used. These can be respectively selected\nthrough the <em class=\"xref py py-obj\">method parameter in minimize_scalar. The brent\nmethod uses Brent\u00e2\u0080\u0099s algorithm for locating a minimum. Optimally, a bracket\n(the bracket parameter) should be given which contains the minimum desired. A\nbracket is a triple \\(\\left( a, b, c \\right)\\) such that \\(f\n\\left( a \\right) > f \\left( b \\right) &lt; f \\left( c \\right)\\) and \\(a &lt;\nb &lt; c\\) . If this is not given, then alternatively two starting points can\nbe chosen and a bracket will be found from these points using a simple\nmarching algorithm. If these two starting points are not provided, <em class=\"xref py py-obj\">0 and\n<em class=\"xref py py-obj\">1 will be used (this may not be the right choice for your function and\nresult in an unexpected minimum being returned).",
            "markdown"
        ],
        [
            "Here is an example:",
            "markdown"
        ],
        [
            "from scipy.optimize import minimize_scalar\n f = lambda x: (x - 2) * (x + 1)**2\n res = minimize_scalar(f, method='brent')\n print(res.x)\n1.0",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Univariate function minimizers (minimize_scalar)->Bounded minimization (method='bounded')": [
        [
            "Very often, there are constraints that can be placed on the solution space\nbefore minimization occurs. The <em class=\"xref py py-obj\">bounded method in minimize_scalar\nis an example of a constrained minimization procedure that provides a\nrudimentary interval constraint for scalar functions. The interval\nconstraint allows the minimization to occur only between two fixed\nendpoints, specified using the mandatory <em class=\"xref py py-obj\">bounds parameter.",
            "markdown"
        ],
        [
            "For example, to find the minimum of \\(J_{1}\\left( x \\right)\\) near\n\\(x=5\\) , minimize_scalar can be called using the interval\n\\(\\left[ 4, 7 \\right]\\) as a constraint. The result is\n\\(x_{\\textrm{min}}=5.3314\\) :",
            "markdown"
        ],
        [
            "from scipy.special import j1\n res = minimize_scalar(j1, bounds=(4, 7), method='bounded')\n res.x\n5.33144184241",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Custom minimizers": [
        [
            "Sometimes, it may be useful to use a custom method as a (multivariate\nor univariate) minimizer, for example, when using some library wrappers\nof minimize (e.g., basinhopping).",
            "markdown"
        ],
        [
            "We can achieve that by, instead of passing a method name, passing\na callable (either a function or an object implementing a <em class=\"xref py py-obj\">__call__\nmethod) as the <em class=\"xref py py-obj\">method parameter.",
            "markdown"
        ],
        [
            "Let us consider an (admittedly rather virtual) need to use a trivial\ncustom multivariate minimization method that will just search the\nneighborhood in each dimension independently with a fixed step size:",
            "markdown"
        ],
        [
            "from scipy.optimize import OptimizeResult\n def custmin(fun, x0, args=(), maxfev=None, stepsize=0.1,\n...         maxiter=100, callback=None, **options):\n...     bestx = x0\n...     besty = fun(x0)\n...     funcalls = 1\n...     niter = 0\n...     improved = True\n...     stop = False\n...\n...     while improved and not stop and niter &lt; maxiter:\n...         improved = False\n...         niter += 1\n...         for dim in range(np.size(x0)):\n...             for s in [bestx[dim] - stepsize, bestx[dim] + stepsize]:\n...                 testx = np.copy(bestx)\n...                 testx[dim] = s\n...                 testy = fun(testx, *args)\n...                 funcalls += 1\n...                 if testy &lt; besty:\n...                     besty = testy\n...                     bestx = testx\n...                     improved = True\n...             if callback is not None:\n...                 callback(bestx)\n...             if maxfev is not None and funcalls = maxfev:\n...                 stop = True\n...                 break\n...\n...     return OptimizeResult(fun=besty, x=bestx, nit=niter,\n...                           nfev=funcalls, success=(niter  1))\n x0 = [1.35, 0.9, 0.8, 1.1, 1.2]\n res = minimize(rosen, x0, method=custmin, options=dict(stepsize=0.05))\n res.x\narray([1., 1., 1., 1., 1.])",
            "code"
        ],
        [
            "This will work just as well in case of univariate optimization:",
            "markdown"
        ],
        [
            "def custmin(fun, bracket, args=(), maxfev=None, stepsize=0.1,\n...         maxiter=100, callback=None, **options):\n...     bestx = (bracket[1] + bracket[0]) / 2.0\n...     besty = fun(bestx)\n...     funcalls = 1\n...     niter = 0\n...     improved = True\n...     stop = False\n...\n...     while improved and not stop and niter &lt; maxiter:\n...         improved = False\n...         niter += 1\n...         for testx in [bestx - stepsize, bestx + stepsize]:\n...             testy = fun(testx, *args)\n...             funcalls += 1\n...             if testy &lt; besty:\n...                 besty = testy\n...                 bestx = testx\n...                 improved = True\n...         if callback is not None:\n...             callback(bestx)\n...         if maxfev is not None and funcalls = maxfev:\n...             stop = True\n...             break\n...\n...     return OptimizeResult(fun=besty, x=bestx, nit=niter,\n...                           nfev=funcalls, success=(niter  1))\n def f(x):\n...    return (x - 2)**2 * (x + 2)**2\n res = minimize_scalar(f, bracket=(-3.5, 0), method=custmin,\n...                       options=dict(stepsize = 0.05))\n res.x\n-2.0",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Root finding->Scalar functions": [
        [
            "If one has a single-variable equation, there are multiple different root\nfinding algorithms that can be tried. Most of these algorithms require the\nendpoints of an interval in which a root is expected (because the function\nchanges signs). In general, brentq is the best choice, but the other\nmethods may be useful in certain circumstances or for academic purposes.\nWhen a bracket is not available, but one or more derivatives are available,\nthen newton (or halley, secant) may be applicable.\nThis is especially the case if the function is defined on a subset of the\ncomplex plane, and the bracketing methods cannot be used.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Root finding->Fixed-point solving": [
        [
            "A problem closely related to finding the zeros of a function is the\nproblem of finding a fixed point of a function. A fixed point of a\nfunction is the point at which evaluation of the function returns the\npoint: \\(g\\left(x\\right)=x.\\) Clearly, the fixed point of \\(g\\)\nis the root of \\(f\\left(x\\right)=g\\left(x\\right)-x.\\)\nEquivalently, the root of \\(f\\) is the fixed point of\n\\(g\\left(x\\right)=f\\left(x\\right)+x.\\) The routine\nfixed_point provides a simple iterative method using Aitkens\nsequence acceleration to estimate the fixed point of \\(g\\) given a\nstarting point.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Root finding->Sets of equations": [
        [
            "Finding a root of a set of non-linear equations can be achieved using the\nroot function. Several methods are available, amongst which hybr\n(the default) and lm, which, respectively, use the hybrid method of Powell\nand the Levenberg-Marquardt method from MINPACK.",
            "markdown"
        ],
        [
            "The following example considers the single-variable transcendental\nequation\n\n\\[x+2\\cos\\left(x\\right)=0,\\]",
            "markdown"
        ],
        [
            "a root of which can be found as follows:",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy.optimize import root\n def func(x):\n...     return x + 2 * np.cos(x)\n sol = root(func, 0.3)\n sol.x\narray([-1.02986653])\n sol.fun\narray([ -6.66133815e-16])",
            "code"
        ],
        [
            "Consider now a set of non-linear equations\n\n \\begin{eqnarray*}\n x_{0}\\cos\\left(x_{1}\\right) & = & 4,\\\\\n x_{0}x_{1}-x_{1} & = & 5.\n \\end{eqnarray*}",
            "markdown"
        ],
        [
            "We define the objective function so that it also returns the Jacobian and\nindicate this by setting the jac parameter to True. Also, the\nLevenberg-Marquardt solver is used here.",
            "markdown"
        ],
        [
            "def func2(x):\n...     f = [x[0] * np.cos(x[1]) - 4,\n...          x[1]*x[0] - x[1] - 5]\n...     df = np.array([[np.cos(x[1]), -x[0] * np.sin(x[1])],\n...                    [x[1], x[0] - 1]])\n...     return f, df\n sol = root(func2, [1, 1], jac=True, method='lm')\n sol.x\narray([ 6.50409711,  0.90841421])",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Root finding->Root finding for large problems": [
        [
            "Methods hybr and lm in root cannot deal with a very large\nnumber of variables (N), as they need to calculate and invert a dense N\nx N Jacobian matrix on every Newton step. This becomes rather inefficient\nwhen N grows.",
            "markdown"
        ],
        [
            "Consider, for instance, the following problem: we need to solve the\nfollowing integrodifferential equation on the square\n\\([0,1]\\times[0,1]\\):\n\n\\[(\\partial_x^2 + \\partial_y^2) P + 5 \\left(\\int_0^1\\int_0^1\\cosh(P)\\,dx\\,dy\\right)^2 = 0\\]",
            "markdown"
        ],
        [
            "with the boundary condition \\(P(x,1) = 1\\) on the upper edge and\n\\(P=0\\) elsewhere on the boundary of the square. This can be done\nby approximating the continuous function P by its values on a grid,\n\\(P_{n,m}\\approx{}P(n h, m h)\\), with a small grid spacing\nh. The derivatives and integrals can then be approximated; for\ninstance \\(\\partial_x^2 P(x,y)\\approx{}(P(x+h,y) - 2 P(x,y) +\nP(x-h,y))/h^2\\). The problem is then equivalent to finding the root of\nsome function residual(P), where P is a vector of length\n\\(N_x N_y\\).",
            "markdown"
        ],
        [
            "Now, because \\(N_x N_y\\) can be large, methods hybr or lm in\nroot will take a long time to solve this problem. The solution can,\nhowever, be found using one of the large-scale solvers, for example\nkrylov, broyden2, or anderson. These use what is known as the\ninexact Newton method, which instead of computing the Jacobian matrix\nexactly, forms an approximation for it.",
            "markdown"
        ],
        [
            "The problem we have can now be solved as follows:",
            "markdown"
        ],
        [
            "import numpy as np\nfrom scipy.optimize import root\nfrom numpy import cosh, zeros_like, mgrid, zeros\n\n# parameters\nnx, ny = 75, 75\nhx, hy = 1./(nx-1), 1./(ny-1)\n\nP_left, P_right = 0, 0\nP_top, P_bottom = 1, 0\n\ndef residual(P):\n   d2x = zeros_like(P)\n   d2y = zeros_like(P)\n\n   d2x[1:-1] = (P[2:]   - 2*P[1:-1] + P[:-2]) / hx/hx\n   d2x[0]    = (P[1]    - 2*P[0]    + P_left)/hx/hx\n   d2x[-1]   = (P_right - 2*P[-1]   + P[-2])/hx/hx\n\n   d2y[:,1:-1] = (P[:,2:] - 2*P[:,1:-1] + P[:,:-2])/hy/hy\n   d2y[:,0]    = (P[:,1]  - 2*P[:,0]    + P_bottom)/hy/hy\n   d2y[:,-1]   = (P_top   - 2*P[:,-1]   + P[:,-2])/hy/hy\n\n   return d2x + d2y + 5*cosh(P).mean()**2\n\n# solve\nguess = zeros((nx, ny), float)\nsol = root(residual, guess, method='krylov', options={'disp': True})\n#sol = root(residual, guess, method='broyden2', options={'disp': True, 'max_rank': 50})\n#sol = root(residual, guess, method='anderson', options={'disp': True, 'M': 10})\nprint('Residual: %g' % abs(residual(sol.x)).max())\n\n# visualize\nimport matplotlib.pyplot as plt\nx, y = mgrid[0:1:(nx*1j), 0:1:(ny*1j)]\nplt.pcolormesh(x, y, sol.x, shading='gouraud')\nplt.colorbar()\nplt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates a 2-D heatmap with Z values from 0 to 1. The graph resembles a smooth, dark blue-green, U shape, with an open yellow top. The right, bottom, and left edges have a value near zero and the top has a value close to 1. The center of the solution space has a value close to 0.8.\"' class=\"plot-directive\" src=\"../_images/optimize-2.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Root finding->Still too slow? Preconditioning.": [
        [
            "When looking for the zero of the functions \\(f_i({\\bf x}) = 0\\),\ni = 1, 2, \u00e2\u0080\u00a6, N, the krylov solver spends most of the\ntime inverting the Jacobian matrix,\n\n\\[J_{ij} = \\frac{\\partial f_i}{\\partial x_j} .\\]",
            "markdown"
        ],
        [
            "If you have an approximation for the inverse matrix\n\\(M\\approx{}J^{-1}\\), you can use it for preconditioning the\nlinear-inversion problem. The idea is that instead of solving\n\\(J{\\bf s}={\\bf y}\\) one solves \\(MJ{\\bf s}=M{\\bf y}\\): since\nmatrix \\(MJ\\) is \u00e2\u0080\u009ccloser\u00e2\u0080\u009d to the identity matrix than \\(J\\)\nis, the equation should be easier for the Krylov method to deal with.",
            "markdown"
        ],
        [
            "The matrix M can be passed to root with method krylov as an\noption options['jac_options']['inner_M']. It can be a (sparse) matrix\nor a scipy.sparse.linalg.LinearOperator instance.",
            "markdown"
        ],
        [
            "For the problem in the previous section, we note that the function to\nsolve consists of two parts: the first one is the application of the\nLaplace operator, \\([\\partial_x^2 + \\partial_y^2] P\\), and the second\nis the integral. We can actually easily compute the Jacobian corresponding\nto the Laplace operator part: we know that in 1-D\n\n\\[\\begin{split}\\partial_x^2 \\approx \\frac{1}{h_x^2} \\begin{pmatrix}\n-2 & 1 & 0 & 0 \\cdots \\\\\n1 & -2 & 1 & 0 \\cdots \\\\\n0 & 1 & -2 & 1 \\cdots \\\\\n\\ldots\n\\end{pmatrix}\n= h_x^{-2} L\\end{split}\\]",
            "markdown"
        ],
        [
            "so that the whole 2-D operator is represented by\n\n\\[J_1 = \\partial_x^2 + \\partial_y^2\n\\simeq\nh_x^{-2} L \\otimes I + h_y^{-2} I \\otimes L\\]",
            "markdown"
        ],
        [
            "The matrix \\(J_2\\) of the Jacobian corresponding to the integral\nis more difficult to calculate, and since all of it entries are\nnonzero, it will be difficult to invert. \\(J_1\\) on the other hand\nis a relatively simple matrix, and can be inverted by\nscipy.sparse.linalg.splu (or the inverse can be approximated by\nscipy.sparse.linalg.spilu). So we are content to take\n\\(M\\approx{}J_1^{-1}\\) and hope for the best.",
            "markdown"
        ],
        [
            "In the example below, we use the preconditioner \\(M=J_1^{-1}\\).",
            "markdown"
        ],
        [
            "import numpy as np\nfrom scipy.optimize import root\nfrom scipy.sparse import spdiags, kron\nfrom scipy.sparse.linalg import spilu, LinearOperator\nfrom numpy import cosh, zeros_like, mgrid, zeros, eye\n\n# parameters\nnx, ny = 75, 75\nhx, hy = 1./(nx-1), 1./(ny-1)\n\nP_left, P_right = 0, 0\nP_top, P_bottom = 1, 0\n\ndef get_preconditioner():\n    \"\"\"Compute the preconditioner M\"\"\"\n    diags_x = zeros((3, nx))\n    diags_x[0,:] = 1/hx/hx\n    diags_x[1,:] = -2/hx/hx\n    diags_x[2,:] = 1/hx/hx\n    Lx = spdiags(diags_x, [-1,0,1], nx, nx)\n\n    diags_y = zeros((3, ny))\n    diags_y[0,:] = 1/hy/hy\n    diags_y[1,:] = -2/hy/hy\n    diags_y[2,:] = 1/hy/hy\n    Ly = spdiags(diags_y, [-1,0,1], ny, ny)\n\n    J1 = kron(Lx, eye(ny)) + kron(eye(nx), Ly)\n\n    # Now we have the matrix `J_1`. We need to find its inverse `M` --\n    # however, since an approximate inverse is enough, we can use\n    # the *incomplete LU* decomposition\n\n    J1_ilu = spilu(J1)\n\n    # This returns an object with a method .solve() that evaluates\n    # the corresponding matrix-vector product. We need to wrap it into\n    # a LinearOperator before it can be passed to the Krylov methods:\n\n    M = LinearOperator(shape=(nx*ny, nx*ny), matvec=J1_ilu.solve)\n    return M\n\ndef solve(preconditioning=True):\n    \"\"\"Compute the solution\"\"\"\n    count = [0]\n\n    def residual(P):\n        count[0] += 1\n\n        d2x = zeros_like(P)\n        d2y = zeros_like(P)\n\n        d2x[1:-1] = (P[2:]   - 2*P[1:-1] + P[:-2])/hx/hx\n        d2x[0]    = (P[1]    - 2*P[0]    + P_left)/hx/hx\n        d2x[-1]   = (P_right - 2*P[-1]   + P[-2])/hx/hx\n\n        d2y[:,1:-1] = (P[:,2:] - 2*P[:,1:-1] + P[:,:-2])/hy/hy\n        d2y[:,0]    = (P[:,1]  - 2*P[:,0]    + P_bottom)/hy/hy\n        d2y[:,-1]   = (P_top   - 2*P[:,-1]   + P[:,-2])/hy/hy\n\n        return d2x + d2y + 5*cosh(P).mean()**2\n\n    # preconditioner\n    if preconditioning:\n        M = get_preconditioner()\n    else:\n        M = None\n\n    # solve\n    guess = zeros((nx, ny), float)\n\n    sol = root(residual, guess, method='krylov',\n               options={'disp': True,\n                        'jac_options': {'inner_M': M}})\n    print('Residual', abs(residual(sol.x)).max())\n    print('Evaluations', count[0])\n\n    return sol.x\n\ndef main():\n    sol = solve(preconditioning=True)\n\n    # visualize\n    import matplotlib.pyplot as plt\n    x, y = mgrid[0:1:(nx*1j), 0:1:(ny*1j)]\n    plt.clf()\n    plt.pcolor(x, y, sol)\n    plt.clim(0, 1)\n    plt.colorbar()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    main()",
            "code"
        ],
        [
            "Resulting run, first without preconditioning:",
            "markdown"
        ],
        [
            "0:  |F(x)| = 803.614; step 1; tol 0.000257947\n1:  |F(x)| = 345.912; step 1; tol 0.166755\n2:  |F(x)| = 139.159; step 1; tol 0.145657\n3:  |F(x)| = 27.3682; step 1; tol 0.0348109\n4:  |F(x)| = 1.03303; step 1; tol 0.00128227\n5:  |F(x)| = 0.0406634; step 1; tol 0.00139451\n6:  |F(x)| = 0.00344341; step 1; tol 0.00645373\n7:  |F(x)| = 0.000153671; step 1; tol 0.00179246\n8:  |F(x)| = 6.7424e-06; step 1; tol 0.00173256\nResidual 3.57078908664e-07\nEvaluations 317",
            "code"
        ],
        [
            "and then with preconditioning:",
            "markdown"
        ],
        [
            "0:  |F(x)| = 136.993; step 1; tol 7.49599e-06\n1:  |F(x)| = 4.80983; step 1; tol 0.00110945\n2:  |F(x)| = 0.195942; step 1; tol 0.00149362\n3:  |F(x)| = 0.000563597; step 1; tol 7.44604e-06\n4:  |F(x)| = 1.00698e-09; step 1; tol 2.87308e-12\nResidual 9.29603061195e-11\nEvaluations 77",
            "code"
        ],
        [
            "Using a preconditioner reduced the number of evaluations of the\nresidual function by a factor of 4. For problems where the\nresidual is expensive to compute, good preconditioning can be crucial\n\u00e2\u0080\u0094 it can even decide whether the problem is solvable in practice or\nnot.",
            "markdown"
        ],
        [
            "Preconditioning is an art, science, and industry. Here, we were lucky\nin making a simple choice that worked reasonably well, but there is a\nlot more depth to this topic than is shown here.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Linear programming (linprog)": [
        [
            "The function linprog can minimize a linear objective function\nsubject to linear equality and inequality constraints. This kind of\nproblem is well known as linear programming. Linear programming solves\nproblems of the following form:\n\n\\[\\begin{split}\\min_x \\ & c^T x \\\\\n\\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n& A_{eq} x = b_{eq},\\\\\n& l \\leq x \\leq u ,\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(x\\) is a vector of decision variables; \\(c\\), \\(b_{ub}\\),\n\\(b_{eq}\\), \\(l\\), and \\(u\\) are vectors; and \\(A_{ub}\\) and\n\\(A_{eq}\\) are matrices.",
            "markdown"
        ],
        [
            "In this tutorial, we will try to solve a typical linear programming\nproblem using linprog.",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Linear programming (linprog)->Linear programming example": [
        [
            "Consider the following simple linear programming problem:\n\n\\[\\begin{split}\\max_{x_1, x_2, x_3, x_4} \\ & 29x_1 + 45x_2 \\\\\n\\mbox{such that} \\\n& x_1 -x_2 -3x_3 \\leq 5\\\\\n& 2x_1 -3x_2 -7x_3 + 3x_4 \\geq 10\\\\\n& 2x_1 + 8x_2 + x_3 = 60\\\\\n& 4x_1 + 4x_2 + x_4 = 60\\\\\n& 0 \\leq x_0\\\\\n& 0 \\leq x_1 \\leq 5\\\\\n& x_2 \\leq 0.5\\\\\n& -3 \\leq x_3\\\\\\end{split}\\]",
            "markdown"
        ],
        [
            "We need some mathematical manipulations to convert the target problem to the form accepted by linprog.",
            "markdown"
        ],
        [
            "First of all, let\u00e2\u0080\u0099s consider the objective function.\nWe want to maximize the objective\nfunction, but linprog can only accept a minimization problem. This is easily remedied by converting the maximize\n\\(29x_1 + 45x_2\\) to minimizing \\(-29x_1 -45x_2\\). Also, \\(x_3, x_4\\) are not shown in the objective\nfunction. That means the weights corresponding with \\(x_3, x_4\\) are zero. So, the objective function can be\nconverted to:\n\n\\[\\min_{x_1, x_2, x_3, x_4} \\ -29x_1 -45x_2 + 0x_3 + 0x_4\\]",
            "markdown"
        ],
        [
            "If we define the vector of decision variables \\(x = [x_1, x_2, x_3, x_4]^T\\), the objective weights vector \\(c\\) of linprog in this problem\nshould be\n\n\\[c = [-29, -45, 0, 0]^T\\]",
            "markdown"
        ],
        [
            "Next, let\u00e2\u0080\u0099s consider the two inequality constraints. The first one is a \u00e2\u0080\u009cless than\u00e2\u0080\u009d inequality, so it is already in the form accepted by linprog.\nThe second one is a \u00e2\u0080\u009cgreater than\u00e2\u0080\u009d inequality, so we need to multiply both sides by \\(-1\\) to convert it to a \u00e2\u0080\u009cless than\u00e2\u0080\u009d inequality.\nExplicitly showing zero coefficients, we have:\n\n\\[\\begin{split}x_1 -x_2 -3x_3 + 0x_4  &\\leq 5\\\\\n-2x_1 + 3x_2 + 7x_3 - 3x_4 &\\leq -10\\\\\\end{split}\\]",
            "markdown"
        ],
        [
            "These equations can be converted to matrix form:\n\n\\[\\begin{split}A_{ub} x \\leq b_{ub}\\\\\\end{split}\\]",
            "markdown"
        ],
        [
            "where\n\n \\begin{equation*} A_{ub} =\n \\begin{bmatrix} 1 & -1 & -3 & 0 \\\\\n                 -2 & 3 & 7 & -3\n \\end{bmatrix}\n \\end{equation*}\n \\begin{equation*} b_{ub} =\n \\begin{bmatrix} 5 \\\\\n                 -10\n \\end{bmatrix}\n \\end{equation*}",
            "markdown"
        ],
        [
            "Next, let\u00e2\u0080\u0099s consider the two equality constraints. Showing zero weights explicitly, these are:\n\n\\[\\begin{split}2x_1 + 8x_2 + 1x_3 + 0x_4 &= 60\\\\\n4x_1 + 4x_2 + 0x_3 + 1x_4 &= 60\\\\\\end{split}\\]",
            "markdown"
        ],
        [
            "These equations can be converted to matrix form:\n\n\\[\\begin{split}A_{eq} x = b_{eq}\\\\\\end{split}\\]",
            "markdown"
        ],
        [
            "where\n\n \\begin{equation*} A_{eq} =\n \\begin{bmatrix} 2 & 8 & 1 & 0 \\\\\n                 4 & 4 & 0 & 1\n \\end{bmatrix}\n \\end{equation*}\n \\begin{equation*} b_{eq} =\n \\begin{bmatrix} 60 \\\\\n                 60\n \\end{bmatrix}\n \\end{equation*}",
            "markdown"
        ],
        [
            "Lastly, let\u00e2\u0080\u0099s consider the separate inequality constraints on individual decision variables, which are known as\n\u00e2\u0080\u009cbox constraints\u00e2\u0080\u009d or \u00e2\u0080\u009csimple bounds\u00e2\u0080\u009d. These constraints can be applied using the bounds argument of linprog.\nAs noted in the linprog documentation, the default value of bounds is (0, None), meaning that the\nlower bound on each decision variable is 0, and the upper bound on each decision variable is infinity:\nall the decision variables are non-negative. Our bounds are different, so we will need to specify the lower and upper bound on each\ndecision variable as a tuple and group these tuples into a list.",
            "markdown"
        ],
        [
            "Finally, we can solve the transformed problem using linprog.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy.optimize import linprog\n c = np.array([-29.0, -45.0, 0.0, 0.0])\n A_ub = np.array([[1.0, -1.0, -3.0, 0.0],\n...                 [-2.0, 3.0, 7.0, -3.0]])\n b_ub = np.array([5.0, -10.0])\n A_eq = np.array([[2.0, 8.0, 1.0, 0.0],\n...                 [4.0, 4.0, 0.0, 1.0]])\n b_eq = np.array([60.0, 60.0])\n x0_bounds = (0, None)\n x1_bounds = (0, 5.0)\n x2_bounds = (-np.inf, 0.5)  # +/- np.inf can be used instead of None\n x3_bounds = (-3.0, None)\n bounds = [x0_bounds, x1_bounds, x2_bounds, x3_bounds]\n result = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds)\n print(result.message)\nThe problem is infeasible. (HiGHS Status 8: model_status is Infeasible; primal_status is At lower/fixed bound)",
            "code"
        ],
        [
            "The result states that our problem is infeasible, meaning that there is no solution vector that satisfies all the\nconstraints. That doesn\u00e2\u0080\u0099t necessarily mean we did anything wrong; some problems truly are infeasible.\nSuppose, however, that we were to decide that our bound constraint on \\(x_1\\) was too tight and that it could be loosened\nto \\(0 \\leq x_1 \\leq 6\\). After adjusting our code x1_bounds = (0, 6) to reflect the change and executing it again:",
            "markdown"
        ],
        [
            "x1_bounds = (0, 6)\n bounds = [x0_bounds, x1_bounds, x2_bounds, x3_bounds]\n result = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds)\n print(result.message)\nOptimization terminated successfully. (HiGHS Status 7: Optimal)",
            "code"
        ],
        [
            "The result shows the optimization was successful.\nWe can check the objective value (result.fun) is same as \\(c^Tx\\):",
            "markdown"
        ],
        [
            "x = np.array(result.x)\n obj = result.fun\n print(c @ x)\n-505.97435889013434  # may vary\n print(obj)\n-505.97435889013434  # may vary",
            "code"
        ],
        [
            "We can also check that all constraints are satisfied within reasonable tolerances:",
            "markdown"
        ],
        [
            "print(b_ub - (A_ub @ x).flatten())  # this is equivalent to result.slack\n[ 6.52747190e-10, -2.26730279e-09]  # may vary\n print(b_eq - (A_eq @ x).flatten())  # this is equivalent to result.con\n[ 9.78840831e-09, 1.04662945e-08]]  # may vary\n print([0 &lt;= result.x[0], 0 &lt;= result.x[1] &lt;= 6.0, result.x[2] &lt;= 0.5, -3.0 &lt;= result.x[3]])\n[True, True, True, True]",
            "code"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Assignment problems->Linear sum assignment problem example": [
        [
            "Consider the problem of selecting students for a swimming medley relay team.\nWe have a table showing times for each swimming style of five students:",
            "markdown"
        ],
        [
            "We need to choose a student for each of the four swimming styles such that\nthe total relay time is minimized.\nThis is a typical linear sum assignment problem. We can use linear_sum_assignment to solve it.",
            "markdown"
        ],
        [
            "The linear sum assignment problem is one of the most famous combinatorial optimization problems.\nGiven a \u00e2\u0080\u009ccost matrix\u00e2\u0080\u009d \\(C\\), the problem is to choose",
            "markdown"
        ],
        [
            "exactly one element from each row",
            "markdown"
        ],
        [
            "without choosing more than one element from any column",
            "markdown"
        ],
        [
            "such that the sum of the chosen elements is minimized",
            "markdown"
        ],
        [
            "In other words, we need to assign each row to one column such that the sum of\nthe corresponding entries is minimized.",
            "markdown"
        ],
        [
            "Formally, let \\(X\\) be a boolean matrix where \\(X[i,j] = 1\\) iff row  \\(i\\) is assigned to column \\(j\\).\nThen the optimal assignment has cost\n\n\\[\\min \\sum_i \\sum_j C_{i,j} X_{i,j}\\]",
            "markdown"
        ],
        [
            "The first step is to define the cost matrix.\nIn this example, we want to assign each swimming style to a student.\nlinear_sum_assignment is able to assign each row of a cost matrix to a column.\nTherefore, to form the cost matrix, the table above needs to be transposed so that the rows\ncorrespond with swimming styles and the columns correspond with students:",
            "markdown"
        ],
        [
            "import numpy as np\n cost = np.array([[43.5, 45.5, 43.4, 46.5, 46.3],\n...                  [47.1, 42.1, 39.1, 44.1, 47.8],\n...                  [48.4, 49.6, 42.1, 44.5, 50.4],\n...                  [38.2, 36.8, 43.2, 41.2, 37.2]])",
            "code"
        ],
        [
            "We can solve the assignment problem with linear_sum_assignment:",
            "markdown"
        ],
        [
            "from scipy.optimize import linear_sum_assignment\n row_ind, col_ind = linear_sum_assignment(cost)",
            "code"
        ],
        [
            "The row_ind and col_ind are optimal assigned matrix indexes of the cost matrix:",
            "markdown"
        ],
        [
            "row_ind\narray([0, 1, 2, 3])\n col_ind\narray([0, 2, 3, 1])",
            "code"
        ],
        [
            "The optimal assignment is:",
            "markdown"
        ],
        [
            "styles = np.array([\"backstroke\", \"breaststroke\", \"butterfly\", \"freestyle\"])[row_ind]\n students = np.array([\"A\", \"B\", \"C\", \"D\", \"E\"])[col_ind]\n dict(zip(styles, students))\n{'backstroke': 'A', 'breaststroke': 'C', 'butterfly': 'D', 'freestyle': 'B'}",
            "code"
        ],
        [
            "The optimal total medley time is:",
            "markdown"
        ],
        [
            "cost[row_ind, col_ind].sum()\n163.89999999999998",
            "code"
        ],
        [
            "Note that this result is not the same as the sum of the minimum times for each swimming style:",
            "markdown"
        ],
        [
            "np.min(cost, axis=1).sum()\n161.39999999999998",
            "code"
        ],
        [
            "because student \u00e2\u0080\u009cC\u00e2\u0080\u009d is the best swimmer in both \u00e2\u0080\u009cbreaststroke\u00e2\u0080\u009d and \u00e2\u0080\u009cbutterfly\u00e2\u0080\u009d style.\nWe cannot assign student \u00e2\u0080\u009cC\u00e2\u0080\u009d to both styles, so we assigned student C to the \u00e2\u0080\u009cbreaststroke\u00e2\u0080\u009d style\nand D to the \u00e2\u0080\u009cbutterfly\u00e2\u0080\u009d style to minimize the total time.",
            "markdown"
        ],
        [
            "References",
            "markdown"
        ],
        [
            "Some further reading and related software, such as Newton-Krylov [KK],\nPETSc [PP], and PyAMG [AMG]:\n\n\n[KK]",
            "markdown"
        ],
        [
            "D.A. Knoll and D.E. Keyes, \u00e2\u0080\u009cJacobian-free Newton-Krylov methods\u00e2\u0080\u009d,\nJ. Comp. Phys. 193, 357 (2004). DOI:10.1016/j.jcp.2003.08.010\n\n\n[PP]",
            "markdown"
        ],
        [
            "PETSc https://www.mcs.anl.gov/petsc/ and its Python bindings\nhttps://bitbucket.org/petsc/petsc4py/\n\n\n[AMG]",
            "markdown"
        ],
        [
            "PyAMG (algebraic multigrid preconditioners/solvers)\nhttps://github.com/pyamg/pyamg/issues",
            "markdown"
        ]
    ],
    "scipy->Optimization (scipy.optimize)->Mixed integer linear programming->Knapsack problem example": [
        [
            "The knapsack problem is a well known combinatorial optimization problem.\nGiven a set of items, each with a size and a value, the problem is to choose\nthe items that maximize the total value under the condition that the total size\nis below a certain threshold.",
            "markdown"
        ],
        [
            "Formally, let",
            "markdown"
        ],
        [
            "\\(x_i\\) be a boolean variable that indicates whether item \\(i\\) is\nincluded in the knapsack,",
            "markdown"
        ],
        [
            "\\(n\\) be the total number of items,",
            "markdown"
        ],
        [
            "\\(v_i\\) be the value of item \\(i\\),",
            "markdown"
        ],
        [
            "\\(s_i\\) be the size of item \\(i\\), and",
            "markdown"
        ],
        [
            "\\(C\\) be the capacity of the knapsack.",
            "markdown"
        ],
        [
            "Then the problem is:\n\n\\[\\max \\sum_i^n  v_{i} x_{i}\\]\n\n\\[\\text{subject to} \\sum_i^n s_{i} x_{i} \\leq C,  x_{i} \\in {0, 1}\\]",
            "markdown"
        ],
        [
            "Although the objective function and inequality constraints are linear in the\ndecision variables \\(x_i\\), this differs from a typical linear\nprogramming problem in that the decision variables can only assume integer\nvalues.  Specifically, our decision variables can only be \\(0\\) or\n\\(1\\), so this is known as a binary integer linear program (BILP). Such\na problem falls within the larger class of mixed integer linear programs\n(MILPs), which we we can solve with milp.",
            "markdown"
        ],
        [
            "In our example, there are 8 items to choose from, and the size and value of\neach is specified as follows.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import optimize\n sizes = np.array([21, 11, 15, 9, 34, 25, 41, 52])\n values = np.array([22, 12, 16, 10, 35, 26, 42, 53])",
            "code"
        ],
        [
            "We need to constrain our eight decision variables to be binary. We do so\nby adding a Bounds: constraint to ensure that they lie between\n\\(0\\) and \\(1\\), and we apply \u00e2\u0080\u009cintegrality\u00e2\u0080\u009d constraints to ensure that\nthey are either \\(0\\) or \\(1\\).",
            "markdown"
        ],
        [
            "bounds = optimize.Bounds(0, 1)  # 0 &lt;= x_i &lt;= 1\n integrality = np.full_like(values, True)  # x_i are integers",
            "code"
        ],
        [
            "The knapsack capacity constraint is specified using LinearConstraint.",
            "markdown"
        ],
        [
            "capacity = 100\n constraints = optimize.LinearConstraint(A=sizes, lb=0, ub=capacity)",
            "code"
        ],
        [
            "If we are following the usual rules of linear algebra, the input A should\nbe a  two-dimensional matrix, and the lower and upper bounds lb and ub\nshould be one-dimensional vectors, but LinearConstraint is forgiving\nas long as the inputs can be broadcast to consistent shapes.",
            "markdown"
        ],
        [
            "Using the variables defined above, we can solve the knapsack problem using\nmilp. Note that milp minimizes the objective function, but we\nwant to maximize the total value, so we set <em class=\"xref py py-obj\">c to be negative of the values.",
            "markdown"
        ],
        [
            "from scipy.optimize import milp\n res = milp(c=-values, constraints=constraints,\n...            integrality=integrality, bounds=bounds)",
            "code"
        ],
        [
            "Let\u00e2\u0080\u0099s check the result:",
            "markdown"
        ],
        [
            "res.success\nTrue\n res.x\narray([1., 1., 0., 1., 1., 1., 0., 0.])",
            "code"
        ],
        [
            "This means that we should select the items 1, 2, 4, 5, 6 to optimize the total\nvalue under the size constraint. Note that this is different from we would have\nobtained had we solved the linear programming relaxation (without integrality\nconstraints) and attempted to round the decision variables.",
            "markdown"
        ],
        [
            "from scipy.optimize import milp\n res = milp(c=-values, constraints=constraints,\n...            integrality=False, bounds=bounds)\n res.x\narray([1.        , 1.        , 1.        , 1.        ,\n       0.55882353, 1.        , 0.        , 0.        ])",
            "code"
        ],
        [
            "If we were to round this solution up to\narray([1., 1., 1., 1., 1., 1., 0., 0.]), our knapsack would be over the\ncapacity constraint, whereas if we were to round down to\narray([1., 1., 1., 1., 0., 1., 0., 0.]), we would have a sub-optimal\nsolution.",
            "markdown"
        ],
        [
            "For more MILP tutorials, see the Jupyter notebooks on SciPy Cookbooks:",
            "markdown"
        ],
        [
            "Compressed Sensing l1 program",
            "markdown"
        ],
        [
            "Compressed Sensing l0 program",
            "markdown"
        ]
    ],
    "scipy->Interpolation (scipy.interpolate)": [
        [
            "There are several general facilities available in SciPy for interpolation and\nsmoothing for data in 1, 2, and higher dimensions. The choice of a specific\ninterpolation routine depends on the data: whether it is one-dimensional,\nis given on a structured grid, or is unstructured. One other factor is the\ndesired smoothness of the interpolator. In short, routines recommended for\ninterpolation can be summarized as follows:",
            "markdown"
        ],
        [
            "For data smoothing, functions are provided\nfor 1- and 2-D data using cubic splines, based on the FORTRAN library FITPACK.",
            "markdown"
        ],
        [
            "Additionally, routines are provided for interpolation / smoothing using\nradial basis functions with several kernels.",
            "markdown"
        ],
        [
            "Futher details are given in the links below.\n\n\n1-D interpolation\nPiecewise linear interpolation\nCubic splines\nMonotone interpolants\nInterpolation with B-splines\nParametric spline curves\nLegacy interface for 1-D interpolation (interp1d)\nMissing data\n\n\nPiecewise polynomials and splines\nManipulating PPoly objects\nB-splines: knots and coefficients\nB-spline basis elements\nDesign matrices in the B-spline basis\n\n\n\n\nSmoothing splines\nSpline smoothing in 1-D\nProcedural (splrep)\nObject-oriented (UnivariateSpline)\n\n\n2-D smoothing splines\nBivariate spline fitting of scattered data\nBivariate spline fitting of data on a grid\nBivariate spline fitting of data in spherical coordinates\n\n\n\n\nMultivariate data interpolation on a regular grid  (RegularGridInterpolator)\nUniformly spaced data\n\n\nScattered data interpolation (griddata)\nUsing radial basis functions for smoothing/interpolation\n1-D Example\n2-D Example\n\n\nExtrapolation tips and tricks\ninterp1d : replicate numpy.interp left and right fill values\nCubicSpline extend the boundary conditions\nManually implement the asymptotics\nThe setup\nUse the known asymptotics\n\n\nExrapolation in D > 1",
            "markdown"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)": [
        [
            "Contents",
            "markdown"
        ],
        [
            "Fourier Transforms (scipy.fft)",
            "markdown"
        ],
        [
            "Fast Fourier transforms",
            "markdown"
        ],
        [
            "1-D discrete Fourier transforms",
            "markdown"
        ],
        [
            "2- and N-D discrete Fourier transforms",
            "markdown"
        ],
        [
            "Discrete Cosine Transforms",
            "markdown"
        ],
        [
            "Type I DCT",
            "markdown"
        ],
        [
            "Type II DCT",
            "markdown"
        ],
        [
            "Type III DCT",
            "markdown"
        ],
        [
            "Type IV DCT",
            "markdown"
        ],
        [
            "DCT and IDCT",
            "markdown"
        ],
        [
            "Example",
            "markdown"
        ],
        [
            "Discrete Sine Transforms",
            "markdown"
        ],
        [
            "Type I DST",
            "markdown"
        ],
        [
            "Type II DST",
            "markdown"
        ],
        [
            "Type III DST",
            "markdown"
        ],
        [
            "Type IV DST",
            "markdown"
        ],
        [
            "DST and IDST",
            "markdown"
        ],
        [
            "Fast Hankel Transform",
            "markdown"
        ],
        [
            "References",
            "markdown"
        ],
        [
            "Fourier analysis is a method for expressing a function as a sum of periodic\ncomponents, and for recovering the signal from those components. When both\nthe function and its Fourier transform are replaced with discretized\ncounterparts, it is called the discrete Fourier transform (DFT). The DFT has\nbecome a mainstay of numerical computing in part because of a very fast\nalgorithm for computing it, called the Fast Fourier Transform (FFT), which was\nknown to Gauss (1805) and was brought to light in its current form by Cooley\nand Tukey [CT65]. Press et al. [NR07] provide an accessible introduction to\nFourier analysis and its applications.",
            "markdown"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Fast Fourier transforms->1-D discrete Fourier transforms": [
        [
            "The FFT <em class=\"xref py py-obj\">y[k] of length \\(N\\) of the length-\\(N\\) sequence <em class=\"xref py py-obj\">x[n] is\ndefined as\n\n\\[y[k] = \\sum_{n=0}^{N-1} e^{-2 \\pi j \\frac{k n}{N} } x[n] \\, ,\\]",
            "markdown"
        ],
        [
            "and the inverse transform is defined as follows\n\n\\[x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} e^{2 \\pi j \\frac{k n}{N} } y[k] \\, .\\]",
            "markdown"
        ],
        [
            "These transforms can be calculated by means of fft and ifft,\nrespectively, as shown in the following example.",
            "markdown"
        ],
        [
            "from scipy.fft import fft, ifft\n import numpy as np\n x = np.array([1.0, 2.0, 1.0, -1.0, 1.5])\n y = fft(x)\n y\narray([ 4.5       +0.j        ,  2.08155948-1.65109876j,\n       -1.83155948+1.60822041j, -1.83155948-1.60822041j,\n        2.08155948+1.65109876j])\n yinv = ifft(y)\n yinv\narray([ 1.0+0.j,  2.0+0.j,  1.0+0.j, -1.0+0.j,  1.5+0.j])",
            "code"
        ],
        [
            "From the definition of the FFT it can be seen that\n\n\\[y[0] = \\sum_{n=0}^{N-1} x[n] \\, .\\]",
            "markdown"
        ],
        [
            "In the example",
            "markdown"
        ],
        [
            "np.sum(x)\n4.5",
            "code"
        ],
        [
            "which corresponds to \\(y[0]\\). For N even, the elements\n\\(y[1]...y[N/2-1]\\) contain the positive-frequency terms, and the elements\n\\(y[N/2]...y[N-1]\\) contain the negative-frequency terms, in order of\ndecreasingly negative frequency. For N odd, the elements\n\\(y[1]...y[(N-1)/2]\\) contain the positive-frequency terms, and the\nelements \\(y[(N+1)/2]...y[N-1]\\) contain the negative-frequency terms, in\norder of decreasingly negative frequency.",
            "markdown"
        ],
        [
            "In case the sequence x is real-valued, the values of \\(y[n]\\) for positive\nfrequencies is the conjugate of the values \\(y[n]\\) for negative\nfrequencies (because the spectrum is symmetric). Typically, only the FFT\ncorresponding to positive frequencies is plotted.",
            "markdown"
        ],
        [
            "The example plots the FFT of the sum of two sines.",
            "markdown"
        ],
        [
            "from scipy.fft import fft, fftfreq\n import numpy as np\n # Number of sample points\n N = 600\n # sample spacing\n T = 1.0 / 800.0\n x = np.linspace(0.0, N*T, N, endpoint=False)\n y = np.sin(50.0 * 2.0*np.pi*x) + 0.5*np.sin(80.0 * 2.0*np.pi*x)\n yf = fft(y)\n xf = fftfreq(N, T)[:N//2]\n import matplotlib.pyplot as plt\n plt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot showing amplitude on the Y axis vs frequency on the X axis. A single blue trace has an amplitude of zero all the way across with the exception of two peaks. The taller first peak is at 50 Hz with a second peak at 80 Hz.\"' class=\"plot-directive\" src=\"../_images/fft-1.png\"/>\n</figure>",
            "code"
        ],
        [
            "The FFT input signal is inherently truncated. This truncation can be modeled\nas multiplication of an infinite signal with a rectangular window function. In\nthe spectral domain this multiplication becomes convolution of the signal\nspectrum with the window function spectrum, being of form \\(\\sin(x)/x\\).\nThis convolution is the cause of an effect called spectral leakage (see\n[WPW]). Windowing the signal with a dedicated window function helps mitigate\nspectral leakage. The example below uses a Blackman window from scipy.signal\nand shows the effect of windowing (the zero component of the FFT has been\ntruncated for illustrative purposes).",
            "markdown"
        ],
        [
            "from scipy.fft import fft, fftfreq\n import numpy as np\n # Number of sample points\n N = 600\n # sample spacing\n T = 1.0 / 800.0\n x = np.linspace(0.0, N*T, N, endpoint=False)\n y = np.sin(50.0 * 2.0*np.pi*x) + 0.5*np.sin(80.0 * 2.0*np.pi*x)\n yf = fft(y)\n from scipy.signal import blackman\n w = blackman(N)\n ywf = fft(y*w)\n xf = fftfreq(N, T)[:N//2]\n import matplotlib.pyplot as plt\n plt.semilogy(xf[1:N//2], 2.0/N * np.abs(yf[1:N//2]), '-b')\n plt.semilogy(xf[1:N//2], 2.0/N * np.abs(ywf[1:N//2]), '-r')\n plt.legend(['FFT', 'FFT w. window'])\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y log-linear plot with amplitude on the Y axis vs frequency on the X axis. The first trace is the FFT with two peaks at 50 and 80 Hz and a noise floor around an amplitude of 1e-2. The second trace is the windowed FFT and has the same two peaks but the noise floor is much lower around an amplitude of 1e-7 due to the window function.\"' class=\"plot-directive\" src=\"../_images/fft-2.png\"/>\n</figure>",
            "code"
        ],
        [
            "In case the sequence x is complex-valued, the spectrum is no longer symmetric.\nTo simplify working with the FFT functions, scipy provides the following two\nhelper functions.",
            "markdown"
        ],
        [
            "The function fftfreq returns the FFT sample frequency points.",
            "markdown"
        ],
        [
            "from scipy.fft import fftfreq\n freq = fftfreq(8, 0.125)\n freq\narray([ 0., 1., 2., 3., -4., -3., -2., -1.])",
            "code"
        ],
        [
            "In a similar spirit, the function fftshift allows swapping the lower\nand upper halves of a vector, so that it becomes suitable for display.",
            "markdown"
        ],
        [
            "from scipy.fft import fftshift\n x = np.arange(8)\n fftshift(x)\narray([4, 5, 6, 7, 0, 1, 2, 3])",
            "code"
        ],
        [
            "The example below plots the FFT of two complex exponentials; note the\nasymmetric spectrum.",
            "markdown"
        ],
        [
            "from scipy.fft import fft, fftfreq, fftshift\n import numpy as np\n # number of signal points\n N = 400\n # sample spacing\n T = 1.0 / 800.0\n x = np.linspace(0.0, N*T, N, endpoint=False)\n y = np.exp(50.0 * 1.j * 2.0*np.pi*x) + 0.5*np.exp(-80.0 * 1.j * 2.0*np.pi*x)\n yf = fft(y)\n xf = fftfreq(N, T)\n xf = fftshift(xf)\n yplot = fftshift(yf)\n import matplotlib.pyplot as plt\n plt.plot(xf, 1.0/N * np.abs(yplot))\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with amplitude on the Y axis vs frequency on the X axis. The trace is zero-valued across the plot except for two sharp peaks at -80 and 50 Hz. The 50 Hz peak on the right is twice as tall.\"' class=\"plot-directive\" src=\"../_images/fft-3.png\"/>\n</figure>",
            "code"
        ],
        [
            "The function rfft calculates the FFT of a real sequence and outputs the\ncomplex FFT coefficients \\(y[n]\\) for only half of the frequency range. The\nremaining negative frequency components are implied by the Hermitian symmetry of\nthe FFT for a real input (y[n] = conj(y[-n])). In case of N being even:\n\\([Re(y[0]) + 0j, y[1], ..., Re(y[N/2]) + 0j]\\); in case of N being odd\n\\([Re(y[0]) + 0j, y[1], ..., y[N/2]\\). The terms shown explicitly as\n\\(Re(y[k]) + 0j\\) are restricted to be purely real since, by the hermitian\nproperty, they are their own complex conjugate.",
            "markdown"
        ],
        [
            "The corresponding function irfft calculates the IFFT of the FFT\ncoefficients with this special ordering.",
            "markdown"
        ],
        [
            "from scipy.fft import fft, rfft, irfft\n x = np.array([1.0, 2.0, 1.0, -1.0, 1.5, 1.0])\n fft(x)\narray([ 5.5 +0.j        ,  2.25-0.4330127j , -2.75-1.29903811j,\n        1.5 +0.j        , -2.75+1.29903811j,  2.25+0.4330127j ])\n yr = rfft(x)\n yr\narray([ 5.5 +0.j        ,  2.25-0.4330127j , -2.75-1.29903811j,\n        1.5 +0.j        ])\n irfft(yr)\narray([ 1. ,  2. ,  1. , -1. ,  1.5,  1. ])\n x = np.array([1.0, 2.0, 1.0, -1.0, 1.5])\n fft(x)\narray([ 4.5       +0.j        ,  2.08155948-1.65109876j,\n       -1.83155948+1.60822041j, -1.83155948-1.60822041j,\n        2.08155948+1.65109876j])\n yr = rfft(x)\n yr\narray([ 4.5       +0.j        ,  2.08155948-1.65109876j,\n        -1.83155948+1.60822041j])",
            "code"
        ],
        [
            "Notice that the rfft of odd and even length signals are of the same shape.\nBy default, irfft assumes the output signal should be of even length. And\nso, for odd signals, it will give the wrong result:",
            "markdown"
        ],
        [
            "irfft(yr)\narray([ 1.70788987,  2.40843925, -0.37366961,  0.75734049])",
            "code"
        ],
        [
            "To recover the original odd-length signal, we <strong>must</strong> pass the output shape by\nthe <em class=\"xref py py-obj\">n parameter.",
            "markdown"
        ],
        [
            "irfft(yr, n=len(x))\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Fast Fourier transforms->2- and N-D discrete Fourier transforms": [
        [
            "The functions fft2 and ifft2 provide 2-D FFT and\nIFFT, respectively. Similarly, fftn and ifftn provide\nN-D FFT, and IFFT, respectively.",
            "markdown"
        ],
        [
            "For real-input signals, similarly to rfft, we have the functions\nrfft2 and irfft2 for 2-D real transforms;\nrfftn and irfftn for N-D real transforms.",
            "markdown"
        ],
        [
            "The example below demonstrates a 2-D IFFT and plots the resulting\n(2-D) time-domain signals.",
            "markdown"
        ],
        [
            "from scipy.fft import ifftn\n import matplotlib.pyplot as plt\n import matplotlib.cm as cm\n import numpy as np\n N = 30\n f, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, sharex='col', sharey='row')\n xf = np.zeros((N,N))\n xf[0, 5] = 1\n xf[0, N-5] = 1\n Z = ifftn(xf)\n ax1.imshow(xf, cmap=cm.Reds)\n ax4.imshow(np.real(Z), cmap=cm.gray)\n xf = np.zeros((N, N))\n xf[5, 0] = 1\n xf[N-5, 0] = 1\n Z = ifftn(xf)\n ax2.imshow(xf, cmap=cm.Reds)\n ax5.imshow(np.real(Z), cmap=cm.gray)\n xf = np.zeros((N, N))\n xf[5, 10] = 1\n xf[N-5, N-10] = 1\n Z = ifftn(xf)\n ax3.imshow(xf, cmap=cm.Reds)\n ax6.imshow(np.real(Z), cmap=cm.gray)\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates six heatmaps arranged in a 2x3 grid. The top row shows mostly blank canvases with the exception of two tiny red peaks on each image. The bottom row shows the real-part of the inverse FFT of each image above it. The first column has two dots arranged horizontally in the top image and in the bottom image a smooth grayscale plot of 5 black vertical stripes representing the 2-D time domain signal. The second column has two dots arranged vertically in the top image and in the bottom image a smooth grayscale plot of 5 horizontal black stripes representing the 2-D time domain signal. In the last column the top image has two dots diagonally located; the corresponding image below has perhaps 20 black stripes at a 60 degree angle.\"' class=\"plot-directive\" src=\"../_images/fft-4.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Discrete Cosine Transforms": [
        [
            "SciPy provides a DCT with the function dct and a corresponding IDCT\nwith the function idct. There are 8 types of the DCT [WPC], [Mak];\nhowever, only the first 4 types are implemented in scipy. \u00e2\u0080\u009cThe\u00e2\u0080\u009d DCT generally\nrefers to DCT type 2, and \u00e2\u0080\u009cthe\u00e2\u0080\u009d Inverse DCT generally refers to DCT type 3. In\naddition, the DCT coefficients can be normalized differently (for most types,\nscipy provides None and ortho). Two parameters of the dct/idct\nfunction calls allow setting the DCT type and coefficient normalization.",
            "markdown"
        ],
        [
            "For a single dimension array x, dct(x, norm=\u00e2\u0080\u0099ortho\u00e2\u0080\u0099) is equal to\nMATLAB dct(x).",
            "markdown"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->Type I DCT": [
        [
            "SciPy uses the following definition of the unnormalized DCT-I\n(norm=None):\n\n\\[y[k] = x_0 + (-1)^k x_{N-1} + 2\\sum_{n=1}^{N-2} x[n]\n\\cos\\left(\\frac{\\pi nk}{N-1}\\right),\n\\qquad 0 \\le k &lt; N.\\]",
            "markdown"
        ],
        [
            "Note that the DCT-I is only supported for input size > 1.",
            "markdown"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->Type II DCT": [
        [
            "SciPy uses the following definition of the unnormalized DCT-II\n(norm=None):\n\n\\[y[k] = 2 \\sum_{n=0}^{N-1} x[n] \\cos \\left({\\pi(2n+1)k \\over 2N} \\right)\n\\qquad 0 \\le k &lt; N.\\]",
            "markdown"
        ],
        [
            "In case of the normalized DCT (norm='ortho'), the DCT coefficients\n\\(y[k]\\) are multiplied by a scaling factor <em class=\"xref py py-obj\">f:\n\n\\[\\begin{split}f = \\begin{cases} \\sqrt{1/(4N)}, & \\text{if $k = 0$} \\\\    \\sqrt{1/(2N)},\n& \\text{otherwise} \\end{cases} \\, .\\end{split}\\]",
            "markdown"
        ],
        [
            "In this case, the DCT \u00e2\u0080\u009cbase functions\u00e2\u0080\u009d \\(\\phi_k[n] = 2 f \\cos\n\\left({\\pi(2n+1)k \\over 2N} \\right)\\) become orthonormal:\n\n\\[\\sum_{n=0}^{N-1} \\phi_k[n] \\phi_l[n] = \\delta_{lk}.\\]",
            "markdown"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->Type III DCT": [
        [
            "SciPy uses the following definition of the unnormalized DCT-III\n(norm=None):\n\n\\[y[k] = x_0 + 2 \\sum_{n=1}^{N-1} x[n] \\cos\\left({\\pi n(2k+1) \\over 2N}\\right)\n\\qquad 0 \\le k &lt; N,\\]",
            "markdown"
        ],
        [
            "or, for norm='ortho':\n\n\\[y[k] = {x_0\\over\\sqrt{N}} + {2\\over\\sqrt{N}} \\sum_{n=1}^{N-1} x[n]\n\\cos\\left({\\pi n(2k+1) \\over 2N}\\right) \\qquad 0 \\le k &lt; N.\\]",
            "markdown"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->Type IV DCT": [
        [
            "SciPy uses the following definition of the unnormalized DCT-IV\n(norm=None):\n\n\\[y[k] = 2 \\sum_{n=0}^{N-1} x[n] \\cos\\left({\\pi (2n+1)(2k+1) \\over 4N}\\right)\n\\qquad 0 \\le k &lt; N,\\]",
            "markdown"
        ],
        [
            "or, for norm='ortho':\n\n\\[y[k] = \\sqrt{2\\over N}\\sum_{n=0}^{N-1} x[n] \\cos\\left({\\pi (2n+1)(2k+1) \\over 4N}\\right)\n\\qquad 0 \\le k &lt; N\\]",
            "markdown"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->DCT and IDCT": [
        [
            "The (unnormalized) DCT-III is the inverse of the (unnormalized) DCT-II, up to a\nfactor of <em class=\"xref py py-obj\">2N. The orthonormalized DCT-III is exactly the inverse of the\northonormalized DCT- II. The function idct performs the mappings between\nthe DCT and IDCT types, as well as the correct normalization.",
            "markdown"
        ],
        [
            "The following example shows the relation between DCT and IDCT for different\ntypes and normalizations.",
            "markdown"
        ],
        [
            "from scipy.fft import dct, idct\n x = np.array([1.0, 2.0, 1.0, -1.0, 1.5])",
            "code"
        ],
        [
            "The DCT-II and DCT-III are each other\u00e2\u0080\u0099s inverses, so for an orthonormal transform\nwe return back to the original signal.",
            "markdown"
        ],
        [
            "dct(dct(x, type=2, norm='ortho'), type=3, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ],
        [
            "Doing the same under default normalization, however, we pick up an extra scaling\nfactor of \\(2N=10\\) since the forward transform is unnormalized.",
            "markdown"
        ],
        [
            "dct(dct(x, type=2), type=3)\narray([ 10.,  20.,  10., -10.,  15.])",
            "code"
        ],
        [
            "For this reason, we should use the function idct using the same type for both,\ngiving a correctly normalized result.",
            "markdown"
        ],
        [
            "# Normalized inverse: no scaling factor\n idct(dct(x, type=2), type=2)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ],
        [
            "Analogous results can be seen for the DCT-I, which is its own inverse up to a\nfactor of \\(2(N-1)\\).",
            "markdown"
        ],
        [
            "dct(dct(x, type=1, norm='ortho'), type=1, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])\n # Unnormalized round-trip via DCT-I: scaling factor 2*(N-1) = 8\n dct(dct(x, type=1), type=1)\narray([ 8. ,  16.,  8. , -8. ,  12.])\n # Normalized inverse: no scaling factor\n idct(dct(x, type=1), type=1)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ],
        [
            "And for the DCT-IV, which is also its own inverse up to a factor of \\(2N\\).",
            "markdown"
        ],
        [
            "dct(dct(x, type=4, norm='ortho'), type=4, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])\n # Unnormalized round-trip via DCT-IV: scaling factor 2*N = 10\n dct(dct(x, type=4), type=4)\narray([ 10.,  20.,  10., -10.,  15.])\n # Normalized inverse: no scaling factor\n idct(dct(x, type=4), type=4)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->Example": [
        [
            "The DCT exhibits the \u00e2\u0080\u009cenergy compaction property\u00e2\u0080\u009d, meaning that for many\nsignals only the first few DCT coefficients have significant magnitude.\nZeroing out the other coefficients leads to a small reconstruction error, a\nfact which is exploited in lossy signal compression (e.g. JPEG compression).",
            "markdown"
        ],
        [
            "The example below shows a signal x and two reconstructions (\\(x_{20}\\) and\n\\(x_{15}\\)) from the signal\u00e2\u0080\u0099s DCT coefficients. The signal \\(x_{20}\\)\nis reconstructed from the first 20 DCT coefficients, \\(x_{15}\\) is\nreconstructed from the first 15 DCT coefficients. It can be seen that the\nrelative error of using 20 coefficients is still very small (~0.1%), but\nprovides a five-fold compression rate.",
            "markdown"
        ],
        [
            "from scipy.fft import dct, idct\n import matplotlib.pyplot as plt\n N = 100\n t = np.linspace(0,20,N, endpoint=False)\n x = np.exp(-t/3)*np.cos(2*t)\n y = dct(x, norm='ortho')\n window = np.zeros(N)\n window[:20] = 1\n yr = idct(y*window, norm='ortho')\n sum(abs(x-yr)**2) / sum(abs(x)**2)\n0.0009872817275276098\n plt.plot(t, x, '-bx')\n plt.plot(t, yr, 'ro')\n window = np.zeros(N)\n window[:15] = 1\n yr = idct(y*window, norm='ortho')\n sum(abs(x-yr)**2) / sum(abs(x)**2)\n0.06196643004256714\n plt.plot(t, yr, 'g+')\n plt.legend(['x', '$x_{20}$', '$x_{15}$'])\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot showing amplitude on the Y axis and time on the X axis. The first blue trace is the original signal and starts at amplitude 1 and oscillates down to 0 amplitude over the duration of the plot resembling a frequency chirp. The second red trace is the x_20 reconstruction using the DCT and closely follows the original signal in the high amplitude region but it is unclear to the right side of the plot. The third green trace is the x_15 reconstruction using the DCT and is less precise than the x_20 reconstruction but still similar to x.\"' class=\"plot-directive\" src=\"../_images/fft-5.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Discrete Sine Transforms": [
        [
            "SciPy provides a DST [Mak] with the function dst and a corresponding IDST\nwith the function idst.",
            "markdown"
        ],
        [
            "There are, theoretically, 8 types of the DST for different combinations of\neven/odd boundary conditions and boundary offsets [WPS], only the first 4\ntypes are implemented in scipy.",
            "markdown"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Discrete Sine Transforms->Type I DST": [
        [
            "DST-I assumes the input is odd around n=-1 and n=N. SciPy uses the following\ndefinition of the unnormalized DST-I (norm=None):\n\n\\[y[k] = 2\\sum_{n=0}^{N-1} x[n]  \\sin\\left( \\pi {(n+1) (k+1)}\\over{N+1}\n\\right), \\qquad 0 \\le k &lt; N.\\]",
            "markdown"
        ],
        [
            "Note also that the DST-I is only supported for input size > 1. The\n(unnormalized) DST-I is its own inverse, up to a factor of <em class=\"xref py py-obj\">2(N+1).",
            "markdown"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Discrete Sine Transforms->Type II DST": [
        [
            "DST-II assumes the input is odd around n=-1/2 and even around n=N. SciPy uses\nthe following definition of the unnormalized DST-II (norm=None):\n\n\\[y[k] = 2 \\sum_{n=0}^{N-1} x[n]  \\sin\\left( {\\pi (n+1/2)(k+1)} \\over N\n\\right), \\qquad 0 \\le k &lt; N.\\]",
            "markdown"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Discrete Sine Transforms->Type III DST": [
        [
            "DST-III assumes the input is odd around n=-1 and even around n=N-1. SciPy uses\nthe following definition of the unnormalized DST-III (norm=None):\n\n\\[y[k] = (-1)^k x[N-1] + 2 \\sum_{n=0}^{N-2} x[n] \\sin \\left( {\\pi\n(n+1)(k+1/2)} \\over N \\right), \\qquad 0 \\le k &lt; N.\\]",
            "markdown"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Discrete Sine Transforms->Type IV DST": [
        [
            "SciPy uses the following definition of the unnormalized DST-IV\n(norm=None):\n\n\\[y[k] = 2 \\sum_{n=0}^{N-1} x[n] \\sin\\left({\\pi (2n+1)(2k+1) \\over 4N}\\right)\n\\qquad 0 \\le k &lt; N,\\]",
            "markdown"
        ],
        [
            "or, for norm='ortho':\n\n\\[y[k] = \\sqrt{2\\over N}\\sum_{n=0}^{N-1} x[n] \\sin\\left({\\pi (2n+1)(2k+1) \\over 4N}\\right)\n\\qquad 0 \\le k &lt; N,\\]",
            "markdown"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Discrete Sine Transforms->DST and IDST": [
        [
            "The following example shows the relation between DST and IDST for\ndifferent types and normalizations.",
            "markdown"
        ],
        [
            "from scipy.fft import dst, idst\n x = np.array([1.0, 2.0, 1.0, -1.0, 1.5])",
            "code"
        ],
        [
            "The DST-II and DST-III are each other\u00e2\u0080\u0099s inverses, so for an orthonormal transform\nwe return back to the original signal.",
            "markdown"
        ],
        [
            "dst(dst(x, type=2, norm='ortho'), type=3, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ],
        [
            "Doing the same under default normalization, however, we pick up an extra scaling\nfactor of \\(2N=10\\) since the forward transform is unnormalized.",
            "markdown"
        ],
        [
            "dst(dst(x, type=2), type=3)\narray([ 10.,  20.,  10., -10.,  15.])",
            "code"
        ],
        [
            "For this reason, we should use the function idst using the same type for both,\ngiving a correctly normalized result.",
            "markdown"
        ],
        [
            "idst(dst(x, type=2), type=2)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ],
        [
            "Analogous results can be seen for the DST-I, which is its own inverse up to a\nfactor of \\(2(N-1)\\).",
            "markdown"
        ],
        [
            "dst(dst(x, type=1, norm='ortho'), type=1, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])\n  # scaling factor 2*(N+1) = 12\n dst(dst(x, type=1), type=1)\narray([ 12.,  24.,  12., -12.,  18.])\n  # no scaling factor\n idst(dst(x, type=1), type=1)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ],
        [
            "And for the DST-IV, which is also its own inverse up to a factor of \\(2N\\).",
            "markdown"
        ],
        [
            "dst(dst(x, type=4, norm='ortho'), type=4, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])\n  # scaling factor 2*N = 10\n dst(dst(x, type=4), type=4)\narray([ 10.,  20.,  10., -10.,  15.])\n  # no scaling factor\n idst(dst(x, type=4), type=4)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->Fast Hankel Transform": [
        [
            "SciPy provides the functions fht and ifht to perform the Fast\nHankel Transform (FHT) and its inverse (IFHT) on logarithmically-spaced input\narrays.",
            "markdown"
        ],
        [
            "The FHT is the discretised version of the continuous Hankel transform defined\nby [Ham00]\n\n\\[A(k) = \\int_{0}^{\\infty} \\! a(r) \\, J_{\\mu}(kr) \\, k \\, dr \\;,\\]",
            "markdown"
        ],
        [
            "with \\(J_{\\mu}\\) the Bessel function of order \\(\\mu\\). Under a change\nof variables \\(r \\to \\log r\\), \\(k \\to \\log k\\), this becomes\n\n\\[A(e^{\\log k})\n= \\int_{0}^{\\infty} \\! a(e^{\\log r}) \\, J_{\\mu}(e^{\\log k + \\log r})\n                                    \\, e^{\\log k + \\log r} \\, d{\\log r}\\]",
            "markdown"
        ],
        [
            "which is a convolution in logarithmic space. The FHT algorithm uses the FFT\nto perform this convolution on discrete input data.",
            "markdown"
        ],
        [
            "Care must be taken to minimise numerical ringing due to the circular nature\nof FFT convolution. To ensure that the low-ringing condition [Ham00] holds,\nthe output array can be slightly shifted by an offset computed using the\nfhtoffset function.",
            "markdown"
        ]
    ],
    "scipy->Fourier Transforms (scipy.fft)->References": [
        [
            "Cooley, James W., and John W. Tukey, 1965, \u00e2\u0080\u009cAn algorithm for the\nmachine calculation of complex Fourier series,\u00e2\u0080\u009d Math. Comput.\n19: 297-301.\n\n\n[NR07]",
            "markdown"
        ],
        [
            "Press, W., Teukolsky, S., Vetterline, W.T., and Flannery, B.P.,\n2007, Numerical Recipes: The Art of Scientific Computing, ch.\n12-13.  Cambridge Univ. Press, Cambridge, UK.\n\n\n[Mak]\n(1,2)",
            "markdown"
        ],
        [
            "J. Makhoul, 1980, \u00e2\u0080\u0098A Fast Cosine Transform in One and Two Dimensions\u00e2\u0080\u0099,\n<em class=\"xref py py-obj\">IEEE Transactions on acoustics, speech and signal processing\nvol. 28(1), pp. 27-34, DOI:10.1109/TASSP.1980.1163351\n\n\n[Ham00]\n(1,2)",
            "markdown"
        ],
        [
            "A. J. S. Hamilton, 2000, \u00e2\u0080\u009cUncorrelated modes of the non-linear power\nspectrum\u00e2\u0080\u009d, MNRAS, 312, 257. DOI:10.1046/j.1365-8711.2000.03071.x\n\n\n[WPW]",
            "markdown"
        ],
        [
            "https://en.wikipedia.org/wiki/Window_function\n\n\n[WPC]",
            "markdown"
        ],
        [
            "https://en.wikipedia.org/wiki/Discrete_cosine_transform\n\n\n[WPS]",
            "markdown"
        ],
        [
            "https://en.wikipedia.org/wiki/Discrete_sine_transform",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)": [
        [
            "The signal processing toolbox currently contains some filtering\nfunctions, a limited set of filter design tools, and a few B-spline\ninterpolation algorithms for 1- and 2-D data. While the\nB-spline algorithms could technically be placed under the\ninterpolation category, they are included here because they only work\nwith equally-spaced data and make heavy use of filter-theory and\ntransfer-function formalism to provide a fast B-spline transform. To\nunderstand this section, you will need to understand that a signal in\nSciPy is an array of real or complex numbers.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->B-splines": [
        [
            "A B-spline is an approximation of a continuous function over a finite-\ndomain in terms of B-spline coefficients and knot points. If the knot-\npoints are equally spaced with spacing \\(\\Delta x\\), then the B-spline\napproximation to a 1-D function is the finite-basis expansion.\n\n\\[y\\left(x\\right)\\approx\\sum_{j}c_{j}\\beta^{o}\\left(\\frac{x}{\\Delta x}-j\\right).\\]",
            "markdown"
        ],
        [
            "In two dimensions with knot-spacing \\(\\Delta x\\) and \\(\\Delta y\\), the\nfunction representation is\n\n\\[z\\left(x,y\\right)\\approx\\sum_{j}\\sum_{k}c_{jk}\\beta^{o}\\left(\\frac{x}{\\Delta x}-j\\right)\\beta^{o}\\left(\\frac{y}{\\Delta y}-k\\right).\\]",
            "markdown"
        ],
        [
            "In these expressions, \\(\\beta^{o}\\left(\\cdot\\right)\\) is the space-limited\nB-spline basis function of order \\(o\\). The requirement of equally-spaced\nknot-points and equally-spaced data points, allows the development of fast\n(inverse-filtering) algorithms for determining the coefficients, \\(c_{j}\\),\nfrom sample-values, \\(y_{n}\\). Unlike the general spline interpolation\nalgorithms, these algorithms can quickly find the spline coefficients for large\nimages.",
            "markdown"
        ],
        [
            "The advantage of representing a set of samples via B-spline basis\nfunctions is that continuous-domain operators (derivatives, re-\nsampling, integral, etc.), which assume that the data samples are drawn\nfrom an underlying continuous function, can be computed with relative\nease from the spline coefficients. For example, the second derivative\nof a spline is\n\n\\[y{}^{\\prime\\prime}\\left(x\\right)=\\frac{1}{\\Delta x^{2}}\\sum_{j}c_{j}\\beta^{o\\prime\\prime}\\left(\\frac{x}{\\Delta x}-j\\right).\\]",
            "markdown"
        ],
        [
            "Using the property of B-splines that\n\n\\[\\frac{d^{2}\\beta^{o}\\left(w\\right)}{dw^{2}}=\\beta^{o-2}\\left(w+1\\right)-2\\beta^{o-2}\\left(w\\right)+\\beta^{o-2}\\left(w-1\\right),\\]",
            "markdown"
        ],
        [
            "it can be seen that\n\n\\[y^{\\prime\\prime}\\left(x\\right)=\\frac{1}{\\Delta x^{2}}\\sum_{j}c_{j}\\left[\\beta^{o-2}\\left(\\frac{x}{\\Delta x}-j+1\\right)-2\\beta^{o-2}\\left(\\frac{x}{\\Delta x}-j\\right)+\\beta^{o-2}\\left(\\frac{x}{\\Delta x}-j-1\\right)\\right].\\]",
            "markdown"
        ],
        [
            "If \\(o=3\\), then at the sample points:\n\n \\begin{eqnarray*} \\Delta x^{2}\\left.y^{\\prime}\\left(x\\right)\\right|_{x=n\\Delta x} & = & \\sum_{j}c_{j}\\delta_{n-j+1}-2c_{j}\\delta_{n-j}+c_{j}\\delta_{n-j-1},\\\\  & = & c_{n+1}-2c_{n}+c_{n-1}.\\end{eqnarray*}",
            "markdown"
        ],
        [
            "Thus, the second-derivative signal can be easily calculated from the spline\nfit. If desired, smoothing splines can be found to make the second derivative\nless sensitive to random errors.",
            "markdown"
        ],
        [
            "The savvy reader will have already noticed that the data samples are related\nto the knot coefficients via a convolution operator, so that simple\nconvolution with the sampled B-spline function recovers the original data from\nthe spline coefficients. The output of convolutions can change depending on\nhow the boundaries are handled (this becomes increasingly more important as the\nnumber of dimensions in the dataset increases). The algorithms relating to\nB-splines in the signal-processing subpackage assume mirror-symmetric\nboundary conditions. Thus, spline coefficients are computed based on that\nassumption, and data-samples can be recovered exactly from the spline\ncoefficients by assuming them to be mirror-symmetric also.",
            "markdown"
        ],
        [
            "Currently the package provides functions for determining second- and third-\norder cubic spline coefficients from equally-spaced samples in one and two\ndimensions (qspline1d, qspline2d, cspline1d,\ncspline2d). The package also supplies a function ( bspline )\nfor evaluating the B-spline basis function, \\(\\beta^{o}\\left(x\\right)\\) for\narbitrary order and \\(x.\\) For large \\(o\\), the B-spline basis\nfunction can be approximated well by a zero-mean Gaussian function with\nstandard-deviation equal to \\(\\sigma_{o}=\\left(o+1\\right)/12\\) :\n\n\\[\\beta^{o}\\left(x\\right)\\approx\\frac{1}{\\sqrt{2\\pi\\sigma_{o}^{2}}}\\exp\\left(-\\frac{x^{2}}{2\\sigma_{o}}\\right).\\]",
            "markdown"
        ],
        [
            "A function to compute this Gaussian for arbitrary \\(x\\) and \\(o\\) is\nalso available ( gauss_spline ). The following code and figure use\nspline-filtering to compute an edge-image (the second derivative of a smoothed\nspline) of a raccoon\u00e2\u0080\u0099s face, which is an array returned by the command scipy.datasets.face.\nThe command sepfir2d was used to apply a separable 2-D FIR\nfilter with mirror-symmetric boundary conditions to the spline coefficients.\nThis function is ideally-suited for reconstructing samples from spline\ncoefficients and is faster than convolve2d, which convolves arbitrary\n2-D filters and allows for choosing mirror-symmetric boundary\nconditions.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import signal, datasets\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "image = datasets.face(gray=True).astype(np.float32)\n derfilt = np.array([1.0, -2, 1.0], dtype=np.float32)\n ck = signal.cspline2d(image, 8.0)\n deriv = (signal.sepfir2d(ck, derfilt, [1]) +\n...          signal.sepfir2d(ck, [1], derfilt))",
            "code"
        ],
        [
            "Alternatively, we could have done:",
            "markdown"
        ],
        [
            "laplacian = np.array([[0,1,0], [1,-4,1], [0,1,0]], dtype=np.float32)\nderiv2 = signal.convolve2d(ck,laplacian,mode='same',boundary='symm')",
            "code"
        ],
        [
            "plt.figure()\n plt.imshow(image)\n plt.gray()\n plt.title('Original image')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is a normal grayscale photo of a raccoon climbing on a palm plant. The second plot has the 2-D spline filter applied to the photo and is completely grey except the edges of the photo have been emphasized, especially on the raccoon fur and palm fronds.\"' class=\"plot-directive\" src=\"../_images/signal-1_00_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "plt.figure()\n plt.imshow(deriv)\n plt.gray()\n plt.title('Output of spline edge filter')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is a normal grayscale photo of a raccoon climbing on a palm plant. The second plot has the 2-D spline filter applied to the photo and is completely grey except the edges of the photo have been emphasized, especially on the raccoon fur and palm fronds.\"' class=\"plot-directive\" src=\"../_images/signal-1_01_00.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering": [
        [
            "Filtering is a generic name for any system that modifies an input\nsignal in some way. In SciPy, a signal can be thought of as a NumPy\narray. There are different kinds of filters for different kinds of\noperations. There are two broad kinds of filtering operations: linear\nand non-linear. Linear filters can always be reduced to multiplication\nof the flattened NumPy array by an appropriate matrix resulting in\nanother flattened NumPy array. Of course, this is not usually the best\nway to compute the filter, as the matrices and vectors involved may be\nhuge. For example, filtering a \\(512 \\times 512\\) image with this\nmethod would require multiplication of a \\(512^2 \\times 512^2\\)\nmatrix with a \\(512^2\\) vector. Just trying to store the\n\\(512^2 \\times 512^2\\) matrix using a standard NumPy array would\nrequire \\(68,719,476,736\\) elements. At 4 bytes per element this\nwould require \\(256\\textrm{GB}\\) of memory. In most applications,\nmost of the elements of this matrix are zero and a different method\nfor computing the output of the filter is employed.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Convolution/Correlation": [
        [
            "Many linear filters also have the property of shift-invariance. This\nmeans that the filtering operation is the same at different locations\nin the signal and it implies that the filtering matrix can be\nconstructed from knowledge of one row (or column) of the matrix alone.\nIn this case, the matrix multiplication can be accomplished using\nFourier transforms.",
            "markdown"
        ],
        [
            "Let \\(x\\left[n\\right]\\) define a 1-D signal indexed by the\ninteger \\(n.\\) Full convolution of two 1-D signals can be\nexpressed as\n\n\\[y\\left[n\\right]=\\sum_{k=-\\infty}^{\\infty}x\\left[k\\right]h\\left[n-k\\right].\\]",
            "markdown"
        ],
        [
            "This equation can only be implemented directly if we limit the\nsequences to finite-support sequences that can be stored in a\ncomputer, choose \\(n=0\\) to be the starting point of both\nsequences, let \\(K+1\\) be that value for which\n\\(x\\left[n\\right]=0\\) for all \\(n\\geq K+1\\) and \\(M+1\\) be\nthat value for which \\(h\\left[n\\right]=0\\) for all \\(n\\geq M+1\\),\nthen the discrete convolution expression is\n\n\\[y\\left[n\\right]=\\sum_{k=\\max\\left(n-M,0\\right)}^{\\min\\left(n,K\\right)}x\\left[k\\right]h\\left[n-k\\right].\\]",
            "markdown"
        ],
        [
            "For convenience, assume \\(K\\geq M.\\) Then, more explicitly, the output of\nthis operation is\n\n \\begin{eqnarray*} y\\left[0\\right] & = & x\\left[0\\right]h\\left[0\\right]\\\\ y\\left[1\\right] & = & x\\left[0\\right]h\\left[1\\right]+x\\left[1\\right]h\\left[0\\right]\\\\ y\\left[2\\right] & = & x\\left[0\\right]h\\left[2\\right]+x\\left[1\\right]h\\left[1\\right]+x\\left[2\\right]h\\left[0\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ y\\left[M\\right] & = & x\\left[0\\right]h\\left[M\\right]+x\\left[1\\right]h\\left[M-1\\right]+\\cdots+x\\left[M\\right]h\\left[0\\right]\\\\ y\\left[M+1\\right] & = & x\\left[1\\right]h\\left[M\\right]+x\\left[2\\right]h\\left[M-1\\right]+\\cdots+x\\left[M+1\\right]h\\left[0\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ y\\left[K\\right] & = & x\\left[K-M\\right]h\\left[M\\right]+\\cdots+x\\left[K\\right]h\\left[0\\right]\\\\ y\\left[K+1\\right] & = & x\\left[K+1-M\\right]h\\left[M\\right]+\\cdots+x\\left[K\\right]h\\left[1\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ y\\left[K+M-1\\right] & = & x\\left[K-1\\right]h\\left[M\\right]+x\\left[K\\right]h\\left[M-1\\right]\\\\ y\\left[K+M\\right] & = & x\\left[K\\right]h\\left[M\\right].\\end{eqnarray*}",
            "markdown"
        ],
        [
            "Thus, the full discrete convolution of two finite sequences of lengths\n\\(K+1\\) and \\(M+1\\), respectively, results in a finite sequence of length\n\\(K+M+1=\\left(K+1\\right)+\\left(M+1\\right)-1.\\)",
            "markdown"
        ],
        [
            "1-D convolution is implemented in SciPy with the function\nconvolve. This function takes as inputs the signals \\(x,\\)\n\\(h\\), and two optional flags \u00e2\u0080\u0098mode\u00e2\u0080\u0099 and \u00e2\u0080\u0098method\u00e2\u0080\u0099, and returns the signal\n\\(y.\\)",
            "markdown"
        ],
        [
            "The first optional flag, \u00e2\u0080\u0098mode\u00e2\u0080\u0099, allows for the specification of which part of the\noutput signal to return. The default value of \u00e2\u0080\u0098full\u00e2\u0080\u0099 returns the entire signal.\nIf the flag has a value of \u00e2\u0080\u0098same\u00e2\u0080\u0099, then only the middle \\(K\\) values are\nreturned, starting at \\(y\\left[\\left\\lfloor \\frac{M-1}{2}\\right\\rfloor\n\\right]\\), so that the output has the same length as the first input. If the flag\nhas a value of \u00e2\u0080\u0098valid\u00e2\u0080\u0099, then only the middle\n\\(K-M+1=\\left(K+1\\right)-\\left(M+1\\right)+1\\) output values are returned,\nwhere \\(z\\) depends on all of the values of the smallest input from\n\\(h\\left[0\\right]\\) to \\(h\\left[M\\right].\\) In other words, only the\nvalues \\(y\\left[M\\right]\\) to \\(y\\left[K\\right]\\) inclusive are\nreturned.",
            "markdown"
        ],
        [
            "The second optional flag, \u00e2\u0080\u0098method\u00e2\u0080\u0099, determines how the convolution is computed,\neither through the Fourier transform approach with fftconvolve or\nthrough the direct method. By default, it selects the expected faster method.\nThe Fourier transform method has order \\(O(N\\log N)\\), while the direct\nmethod has order \\(O(N^2)\\). Depending on the big O constant and the value\nof \\(N\\), one of these two methods may be faster. The default value, \u00e2\u0080\u0098auto\u00e2\u0080\u0099,\nperforms a rough calculation and chooses the expected faster method, while the\nvalues \u00e2\u0080\u0098direct\u00e2\u0080\u0099 and \u00e2\u0080\u0098fft\u00e2\u0080\u0099 force computation with the other two methods.",
            "markdown"
        ],
        [
            "The code below shows a simple example for convolution of 2 sequences:",
            "markdown"
        ],
        [
            "x = np.array([1.0, 2.0, 3.0])\n h = np.array([0.0, 1.0, 0.0, 0.0, 0.0])\n signal.convolve(x, h)\narray([ 0.,  1.,  2.,  3.,  0.,  0.,  0.])\n signal.convolve(x, h, 'same')\narray([ 2.,  3.,  0.])",
            "code"
        ],
        [
            "This same function convolve can actually take N-D\narrays as inputs and will return the N-D convolution of the\ntwo arrays, as is shown in the code example below. The same input flags are\navailable for that case as well.",
            "markdown"
        ],
        [
            "x = np.array([[1., 1., 0., 0.], [1., 1., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]])\n h = np.array([[1., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 0.]])\n signal.convolve(x, h)\narray([[ 1.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 1.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]])",
            "code"
        ],
        [
            "Correlation is very similar to convolution except that the minus sign\nbecomes a plus sign. Thus,\n\n\\[w\\left[n\\right]=\\sum_{k=-\\infty}^{\\infty}y\\left[k\\right]x\\left[n+k\\right],\\]",
            "markdown"
        ],
        [
            "is the (cross) correlation of the signals \\(y\\) and \\(x.\\) For\nfinite-length signals with \\(y\\left[n\\right]=0\\) outside of the range\n\\(\\left[0,K\\right]\\) and \\(x\\left[n\\right]=0\\) outside of the range\n\\(\\left[0,M\\right],\\) the summation can simplify to\n\n\\[w\\left[n\\right]=\\sum_{k=\\max\\left(0,-n\\right)}^{\\min\\left(K,M-n\\right)}y\\left[k\\right]x\\left[n+k\\right].\\]",
            "markdown"
        ],
        [
            "Assuming again that \\(K\\geq M\\), this is\n\n \\begin{eqnarray*} w\\left[-K\\right] & = & y\\left[K\\right]x\\left[0\\right]\\\\ w\\left[-K+1\\right] & = & y\\left[K-1\\right]x\\left[0\\right]+y\\left[K\\right]x\\left[1\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ w\\left[M-K\\right] & = & y\\left[K-M\\right]x\\left[0\\right]+y\\left[K-M+1\\right]x\\left[1\\right]+\\cdots+y\\left[K\\right]x\\left[M\\right]\\\\ w\\left[M-K+1\\right] & = & y\\left[K-M-1\\right]x\\left[0\\right]+\\cdots+y\\left[K-1\\right]x\\left[M\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ w\\left[-1\\right] & = & y\\left[1\\right]x\\left[0\\right]+y\\left[2\\right]x\\left[1\\right]+\\cdots+y\\left[M+1\\right]x\\left[M\\right]\\\\ w\\left[0\\right] & = & y\\left[0\\right]x\\left[0\\right]+y\\left[1\\right]x\\left[1\\right]+\\cdots+y\\left[M\\right]x\\left[M\\right]\\\\ w\\left[1\\right] & = & y\\left[0\\right]x\\left[1\\right]+y\\left[1\\right]x\\left[2\\right]+\\cdots+y\\left[M-1\\right]x\\left[M\\right]\\\\ w\\left[2\\right] & = & y\\left[0\\right]x\\left[2\\right]+y\\left[1\\right]x\\left[3\\right]+\\cdots+y\\left[M-2\\right]x\\left[M\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ w\\left[M-1\\right] & = & y\\left[0\\right]x\\left[M-1\\right]+y\\left[1\\right]x\\left[M\\right]\\\\ w\\left[M\\right] & = & y\\left[0\\right]x\\left[M\\right].\\end{eqnarray*}",
            "markdown"
        ],
        [
            "The SciPy function correlate implements this operation. Equivalent\nflags are available for this operation to return the full \\(K+M+1\\) length\nsequence (\u00e2\u0080\u0098full\u00e2\u0080\u0099) or a sequence with the same size as the largest sequence\nstarting at \\(w\\left[-K+\\left\\lfloor \\frac{M-1}{2}\\right\\rfloor \\right]\\)\n(\u00e2\u0080\u0098same\u00e2\u0080\u0099) or a sequence where the values depend on all the values of the\nsmallest sequence (\u00e2\u0080\u0098valid\u00e2\u0080\u0099). This final option returns the \\(K-M+1\\)\nvalues \\(w\\left[M-K\\right]\\) to \\(w\\left[0\\right]\\) inclusive.",
            "markdown"
        ],
        [
            "The function correlate can also take arbitrary N-D arrays as input\nand return the N-D convolution of the two arrays on output.",
            "markdown"
        ],
        [
            "When \\(N=2,\\) correlate and/or convolve can be used\nto construct arbitrary image filters to perform actions such as blurring,\nenhancing, and edge-detection for an image.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import signal, datasets\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "image = datasets.face(gray=True)\n w = np.zeros((50, 50))\n w[0][0] = 1.0\n w[49][25] = 1.0\n image_new = signal.fftconvolve(image, w)",
            "code"
        ],
        [
            "plt.figure()\n plt.imshow(image)\n plt.gray()\n plt.title('Original image')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is the familiar photo of a raccoon climbing on a palm. The second plot has the FIR filter applied and has the two copies of the photo superimposed due to the twin peaks manually set in the filter kernel definition.\"' class=\"plot-directive\" src=\"../_images/signal-2_00_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "plt.figure()\n plt.imshow(image_new)\n plt.gray()\n plt.title('Filtered image')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is the familiar photo of a raccoon climbing on a palm. The second plot has the FIR filter applied and has the two copies of the photo superimposed due to the twin peaks manually set in the filter kernel definition.\"' class=\"plot-directive\" src=\"../_images/signal-2_01_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "Calculating the convolution in the time domain as above is mainly used for\nfiltering when one of the signals is much smaller than the other ( \\(K\\gg\nM\\) ), otherwise linear filtering is more efficiently calculated in the\nfrequency domain provided by the function fftconvolve. By default,\nconvolve estimates the fastest method using choose_conv_method.",
            "markdown"
        ],
        [
            "If the filter function \\(w[n,m]\\) can be factored according to\n\n\\[h[n, m] = h_1[n] h_2[m],\\]",
            "markdown"
        ],
        [
            "convolution can be calculated by means of the function sepfir2d. As an\nexample, we consider a Gaussian filter gaussian\n\n\\[h[n, m] \\propto e^{-x^2-y^2} = e^{-x^2} e^{-y^2},\\]",
            "markdown"
        ],
        [
            "which is often used for blurring.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import signal, datasets\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "image = np.asarray(datasets.ascent(), np.float64)\n w = signal.windows.gaussian(51, 10.0)\n image_new = signal.sepfir2d(image, w, w)",
            "code"
        ],
        [
            "plt.figure()\n plt.imshow(image)\n plt.gray()\n plt.title('Original image')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt=\"&quot;This code displays two plots. The first plot is a grayscale photo of two people climbing a wooden staircase taken from below. The second plot has the 2-D gaussian FIR window applied and appears very blurry. You can still tell it's a photo but the subject is ambiguous.&quot;\" class=\"plot-directive\" src=\"../_images/signal-3_00_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "plt.figure()\n plt.imshow(image_new)\n plt.gray()\n plt.title('Filtered image')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt=\"&quot;This code displays two plots. The first plot is a grayscale photo of two people climbing a wooden staircase taken from below. The second plot has the 2-D gaussian FIR window applied and appears very blurry. You can still tell it's a photo but the subject is ambiguous.&quot;\" class=\"plot-directive\" src=\"../_images/signal-3_01_00.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Difference-equation filtering": [
        [
            "A general class of linear 1-D filters (that includes convolution\nfilters) are filters described by the difference equation\n\n\\[\\sum_{k=0}^{N}a_{k}y\\left[n-k\\right]=\\sum_{k=0}^{M}b_{k}x\\left[n-k\\right],\\]",
            "markdown"
        ],
        [
            "where \\(x\\left[n\\right]\\) is the input sequence and\n\\(y\\left[n\\right]\\) is the output sequence. If we assume initial rest so\nthat \\(y\\left[n\\right]=0\\) for \\(n&lt;0\\), then this kind of filter can\nbe implemented using convolution. However, the convolution filter sequence\n\\(h\\left[n\\right]\\) could be infinite if \\(a_{k}\\neq0\\) for\n\\(k\\geq1.\\) In addition, this general class of linear filter allows\ninitial conditions to be placed on \\(y\\left[n\\right]\\) for \\(n&lt;0\\)\nresulting in a filter that cannot be expressed using convolution.",
            "markdown"
        ],
        [
            "The difference equation filter can be thought of as finding\n\\(y\\left[n\\right]\\) recursively in terms of its previous values\n\n\\[a_{0}y\\left[n\\right]=-a_{1}y\\left[n-1\\right]-\\cdots-a_{N}y\\left[n-N\\right]+\\cdots+b_{0}x\\left[n\\right]+\\cdots+b_{M}x\\left[n-M\\right].\\]",
            "markdown"
        ],
        [
            "Often, \\(a_{0}=1\\) is chosen for normalization. The implementation in SciPy\nof this general difference equation filter is a little more complicated than\nwould be implied by the previous equation. It is implemented so that only one\nsignal needs to be delayed. The actual implementation equations are (assuming\n\\(a_{0}=1\\) ):\n\n \\begin{eqnarray*} y\\left[n\\right] & = & b_{0}x\\left[n\\right]+z_{0}\\left[n-1\\right]\\\\ z_{0}\\left[n\\right] & = & b_{1}x\\left[n\\right]+z_{1}\\left[n-1\\right]-a_{1}y\\left[n\\right]\\\\ z_{1}\\left[n\\right] & = & b_{2}x\\left[n\\right]+z_{2}\\left[n-1\\right]-a_{2}y\\left[n\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ z_{K-2}\\left[n\\right] & = & b_{K-1}x\\left[n\\right]+z_{K-1}\\left[n-1\\right]-a_{K-1}y\\left[n\\right]\\\\ z_{K-1}\\left[n\\right] & = & b_{K}x\\left[n\\right]-a_{K}y\\left[n\\right],\\end{eqnarray*}",
            "markdown"
        ],
        [
            "where \\(K=\\max\\left(N,M\\right).\\) Note that \\(b_{K}=0\\) if \\(K>M\\)\nand \\(a_{K}=0\\) if \\(K>N.\\) In this way, the output at time \\(n\\)\ndepends only on the input at time \\(n\\) and the value of \\(z_{0}\\) at\nthe previous time. This can always be calculated as long as the \\(K\\)\nvalues \\(z_{0}\\left[n-1\\right]\\ldots z_{K-1}\\left[n-1\\right]\\) are\ncomputed and stored at each time step.",
            "markdown"
        ],
        [
            "The difference-equation filter is called using the command lfilter in\nSciPy. This command takes as inputs the vector \\(b,\\) the vector,\n\\(a,\\) a signal \\(x\\) and returns the vector \\(y\\) (the same\nlength as \\(x\\) ) computed using the equation given above. If \\(x\\) is\nN-D, then the filter is computed along the axis provided.\nIf desired, initial conditions providing the values of\n\\(z_{0}\\left[-1\\right]\\) to \\(z_{K-1}\\left[-1\\right]\\) can be provided\nor else it will be assumed that they are all zero. If initial conditions are\nprovided, then the final conditions on the intermediate variables are also\nreturned. These could be used, for example, to restart the calculation in the\nsame state.",
            "markdown"
        ],
        [
            "Sometimes, it is more convenient to express the initial conditions in terms of\nthe signals \\(x\\left[n\\right]\\) and \\(y\\left[n\\right].\\) In other\nwords, perhaps you have the values of \\(x\\left[-M\\right]\\) to\n\\(x\\left[-1\\right]\\) and the values of \\(y\\left[-N\\right]\\) to\n\\(y\\left[-1\\right]\\) and would like to determine what values of\n\\(z_{m}\\left[-1\\right]\\) should be delivered as initial conditions to the\ndifference-equation filter. It is not difficult to show that, for \\(0\\leq\nm&lt;K,\\)\n\n\\[z_{m}\\left[n\\right]=\\sum_{p=0}^{K-m-1}\\left(b_{m+p+1}x\\left[n-p\\right]-a_{m+p+1}y\\left[n-p\\right]\\right).\\]",
            "markdown"
        ],
        [
            "Using this formula, we can find the initial-condition vector\n\\(z_{0}\\left[-1\\right]\\) to \\(z_{K-1}\\left[-1\\right]\\) given initial\nconditions on \\(y\\) (and \\(x\\) ). The command lfiltic performs\nthis function.",
            "markdown"
        ],
        [
            "As an example, consider the following system:\n\n\\[y[n] = \\frac{1}{2} x[n] + \\frac{1}{4} x[n-1] + \\frac{1}{3} y[n-1]\\]",
            "markdown"
        ],
        [
            "The code calculates the signal \\(y[n]\\) for a given signal \\(x[n]\\);\nfirst for initial conditions \\(y[-1] = 0\\) (default case), then for\n\\(y[-1] = 2\\) by means of lfiltic.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import signal",
            "code"
        ],
        [
            "x = np.array([1., 0., 0., 0.])\n b = np.array([1.0/2, 1.0/4])\n a = np.array([1.0, -1.0/3])\n signal.lfilter(b, a, x)\narray([0.5, 0.41666667, 0.13888889, 0.0462963])\n zi = signal.lfiltic(b, a, y=[2.])\n signal.lfilter(b, a, x, zi=zi)\n(array([ 1.16666667,  0.63888889,  0.21296296,  0.07098765]), array([0.02366]))",
            "code"
        ],
        [
            "Note that the output signal \\(y[n]\\) has the same length as the length as\nthe input signal \\(x[n]\\).",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Difference-equation filtering->Analysis of Linear Systems": [
        [
            "Linear system described a linear-difference equation can be fully described by\nthe coefficient vectors \\(a\\) and \\(b\\) as was done above; an alternative\nrepresentation is to provide a factor \\(k\\), \\(N_z\\) zeros \\(z_k\\)\nand \\(N_p\\) poles \\(p_k\\), respectively, to describe the system by\nmeans of its transfer function \\(H(z)\\), according to\n\n\\[H(z) = k \\frac{ (z-z_1)(z-z_2)...(z-z_{N_z})}{ (z-p_1)(z-p_2)...(z-p_{N_p})}.\\]",
            "markdown"
        ],
        [
            "This alternative representation can be obtained with the scipy function\ntf2zpk; the inverse is provided by zpk2tf.",
            "markdown"
        ],
        [
            "For the above example we have",
            "markdown"
        ],
        [
            "b = np.array([1.0/2, 1.0/4])\n a = np.array([1.0, -1.0/3])\n signal.tf2zpk(b, a)\n(array([-0.5]), array([ 0.33333333]), 0.5)",
            "code"
        ],
        [
            "i.e., the system has a zero at \\(z=-1/2\\) and a pole at \\(z=1/3\\).",
            "markdown"
        ],
        [
            "The scipy function freqz allows calculation of the frequency response\nof a system described by the coefficients \\(a_k\\) and \\(b_k\\). See the\nhelp of the freqz function for a comprehensive example.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design": [
        [
            "Time-discrete filters can be classified into finite response (FIR) filters and\ninfinite response (IIR) filters. FIR filters can provide a linear phase\nresponse, whereas IIR filters cannot. SciPy provides functions\nfor designing both types of filters.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter": [
        [
            "The function firwin designs filters according to the window method.\nDepending on the provided arguments, the function returns different filter\ntypes (e.g., low-pass, band-pass\u00e2\u0080\u00a6).",
            "markdown"
        ],
        [
            "The example below designs a low-pass and a band-stop filter, respectively.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "b1 = signal.firwin(40, 0.5)\n b2 = signal.firwin(41, [0.3, 0.8])\n w1, h1 = signal.freqz(b1)\n w2, h2 = signal.freqz(b2)",
            "code"
        ],
        [
            "plt.title('Digital filter frequency response')\n plt.plot(w1, 20*np.log10(np.abs(h1)), 'b')\n plt.plot(w2, 20*np.log10(np.abs(h2)), 'r')\n plt.ylabel('Amplitude Response (dB)')\n plt.xlabel('Frequency (rad/sample)')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays an X-Y plot with the amplitude response on the Y axis vs frequency on the X axis. The first (low-pass) trace in blue starts with a pass-band at 0 dB and curves down around halfway through with some ripple in the stop-band about 80 dB down. The second (band-stop) trace in red starts and ends at 0 dB, but the middle third is down about 60 dB from the peak with some ripple where the filter would supress a signal.\"' class=\"plot-directive\" src=\"../_images/signal-4.png\"/>\n</figure>",
            "code"
        ],
        [
            "Note that firwin uses, per default, a normalized frequency defined such\nthat the value \\(1\\) corresponds to the Nyquist frequency, whereas the\nfunction freqz is defined such that the value \\(\\pi\\) corresponds\nto the Nyquist frequency.",
            "markdown"
        ],
        [
            "The function firwin2 allows design of almost arbitrary frequency\nresponses by specifying an array of corner frequencies and corresponding\ngains, respectively.",
            "markdown"
        ],
        [
            "The example below designs a filter with such an arbitrary amplitude response.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "b = signal.firwin2(150, [0.0, 0.3, 0.6, 1.0], [1.0, 2.0, 0.5, 0.0])\n w, h = signal.freqz(b)",
            "code"
        ],
        [
            "plt.title('Digital filter frequency response')\n plt.plot(w, np.abs(h))\n plt.title('Digital filter frequency response')\n plt.ylabel('Amplitude Response')\n plt.xlabel('Frequency (rad/sample)')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays an X-Y plot with amplitude response on the Y axis vs frequency on the X axis. A single trace forms a shape similar to a heartbeat signal.\"' class=\"plot-directive\" src=\"../_images/signal-5.png\"/>\n</figure>",
            "code"
        ],
        [
            "Note the linear scaling of the y-axis and the different definition of the\nNyquist frequency in firwin2 and freqz (as explained above).",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->IIR Filter": [
        [
            "SciPy provides two functions to directly design IIR iirdesign and\niirfilter, where the filter type (e.g., elliptic) is passed as an\nargument and several more filter design functions for specific filter types,\ne.g., ellip.",
            "markdown"
        ],
        [
            "The example below designs an elliptic low-pass filter with defined pass-band\nand stop-band ripple, respectively. Note the much lower filter order (order 4)\ncompared with the FIR filters from the examples above in order to reach the same\nstop-band attenuation of \\(\\approx 60\\) dB.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "b, a = signal.iirfilter(4, Wn=0.2, rp=5, rs=60, btype='lowpass', ftype='ellip')\n w, h = signal.freqz(b, a)",
            "code"
        ],
        [
            "plt.title('Digital filter frequency response')\n plt.plot(w, 20*np.log10(np.abs(h)))\n plt.title('Digital filter frequency response')\n plt.ylabel('Amplitude Response [dB]')\n plt.xlabel('Frequency (rad/sample)')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with amplitude response on the Y axis vs Frequency on the X axis. A single trace shows a smooth low-pass filter with the left third passband near 0 dB. The right two-thirds are about 60 dB down with two sharp narrow valleys dipping down to -100 dB.\"' class=\"plot-directive\" src=\"../_images/signal-6.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->Filter Coefficients": [
        [
            "Filter coefficients can be stored in several different formats:",
            "markdown"
        ],
        [
            "\u00e2\u0080\u0098ba\u00e2\u0080\u0099 or \u00e2\u0080\u0098tf\u00e2\u0080\u0099 = transfer function coefficients",
            "markdown"
        ],
        [
            "\u00e2\u0080\u0098zpk\u00e2\u0080\u0099 = zeros, poles, and overall gain",
            "markdown"
        ],
        [
            "\u00e2\u0080\u0098ss\u00e2\u0080\u0099 = state-space system representation",
            "markdown"
        ],
        [
            "\u00e2\u0080\u0098sos\u00e2\u0080\u0099 = transfer function coefficients of second-order sections",
            "markdown"
        ],
        [
            "Functions, such as tf2zpk and zpk2ss, can convert between them.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->Filter Coefficients->Transfer function representation": [
        [
            "The ba or tf format is a 2-tuple (b, a) representing a transfer\nfunction, where <em class=\"xref py py-obj\">b is a length M+1 array of coefficients of the <em class=\"xref py py-obj\">M-order\nnumerator polynomial, and <em class=\"xref py py-obj\">a is a length N+1 array of coefficients of the\n<em class=\"xref py py-obj\">N-order denominator, as positive, descending powers of the transfer function\nvariable. So the tuple of \\(b = [b_0, b_1, ..., b_M]\\) and\n\\(a =[a_0, a_1, ..., a_N]\\) can represent an analog filter of the form:\n\n\\[H(s) = \\frac\n{b_0 s^M + b_1 s^{(M-1)} + \\cdots + b_M}\n{a_0 s^N + a_1 s^{(N-1)} + \\cdots + a_N}\n= \\frac\n{\\sum_{i=0}^M b_i s^{(M-i)}}\n{\\sum_{i=0}^N a_i s^{(N-i)}}\\]",
            "markdown"
        ],
        [
            "or a discrete-time filter of the form:\n\n\\[H(z) = \\frac\n{b_0 z^M + b_1 z^{(M-1)} + \\cdots + b_M}\n{a_0 z^N + a_1 z^{(N-1)} + \\cdots + a_N}\n= \\frac\n{\\sum_{i=0}^M b_i z^{(M-i)}}\n{\\sum_{i=0}^N a_i z^{(N-i)}}.\\]",
            "markdown"
        ],
        [
            "This \u00e2\u0080\u009cpositive powers\u00e2\u0080\u009d form is found more commonly in controls\nengineering.  If <em class=\"xref py py-obj\">M and <em class=\"xref py py-obj\">N are equal (which is true for all filters\ngenerated by the bilinear transform), then this happens to be equivalent\nto the \u00e2\u0080\u009cnegative powers\u00e2\u0080\u009d discrete-time form preferred in DSP:\n\n\\[H(z) = \\frac\n{b_0 + b_1 z^{-1} + \\cdots + b_M z^{-M}}\n{a_0 + a_1 z^{-1} + \\cdots + a_N z^{-N}}\n= \\frac\n{\\sum_{i=0}^M b_i z^{-i}}\n{\\sum_{i=0}^N a_i z^{-i}}.\\]",
            "markdown"
        ],
        [
            "Although this is true for common filters, remember that this is not true\nin the general case. If <em class=\"xref py py-obj\">M and <em class=\"xref py py-obj\">N are not equal, the discrete-time\ntransfer function coefficients must first be converted to the \u00e2\u0080\u009cpositive\npowers\u00e2\u0080\u009d form before finding the poles and zeros.",
            "markdown"
        ],
        [
            "This representation suffers from numerical error at higher orders, so other\nformats are preferred when possible.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->Filter Coefficients->Zeros and poles representation": [
        [
            "The zpk format is a 3-tuple (z, p, k), where <em class=\"xref py py-obj\">z is an <em class=\"xref py py-obj\">M-length\narray of the complex zeros of the transfer function\n\\(z = [z_0, z_1, ..., z_{M-1}]\\), <em class=\"xref py py-obj\">p is an <em class=\"xref py py-obj\">N-length array of the\ncomplex poles of the transfer function \\(p = [p_0, p_1, ..., p_{N-1}]\\),\nand <em class=\"xref py py-obj\">k is a scalar gain.  These represent the digital transfer function:\n\n\\[H(z) = k \\cdot \\frac\n{(z - z_0) (z - z_1) \\cdots (z - z_{(M-1)})}\n{(z - p_0) (z - p_1) \\cdots (z - p_{(N-1)})}\n= k \\frac\n{\\prod_{i=0}^{M-1} (z - z_i)}\n{\\prod_{i=0}^{N-1} (z - p_i)}\\]",
            "markdown"
        ],
        [
            "or the analog transfer function:\n\n\\[H(s) = k \\cdot \\frac\n{(s - z_0) (s - z_1) \\cdots (s - z_{(M-1)})}\n{(s - p_0) (s - p_1) \\cdots (s - p_{(N-1)})}\n= k \\frac\n{\\prod_{i=0}^{M-1} (s - z_i)}\n{\\prod_{i=0}^{N-1} (s - p_i)}.\\]",
            "markdown"
        ],
        [
            "Although the sets of roots are stored as ordered NumPy arrays, their ordering\ndoes not matter: ([-1, -2], [-3, -4], 1) is the same filter as\n([-2, -1], [-4, -3], 1).",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->Filter Coefficients->State-space system representation": [
        [
            "The ss format is a 4-tuple of arrays (A, B, C, D) representing the\nstate-space of an <em class=\"xref py py-obj\">N-order digital/discrete-time system of the form:\n\n\\[\\begin{split}\\mathbf{x}[k+1] = A \\mathbf{x}[k] + B \\mathbf{u}[k]\\\\\n\\mathbf{y}[k] = C \\mathbf{x}[k] + D \\mathbf{u}[k]\\end{split}\\]",
            "markdown"
        ],
        [
            "or a continuous/analog system of the form:\n\n\\[\\begin{split}\\dot{\\mathbf{x}}(t) = A \\mathbf{x}(t) + B \\mathbf{u}(t)\\\\\n\\mathbf{y}(t) = C \\mathbf{x}(t) + D \\mathbf{u}(t),\\end{split}\\]",
            "markdown"
        ],
        [
            "with <em class=\"xref py py-obj\">P inputs, <em class=\"xref py py-obj\">Q outputs and <em class=\"xref py py-obj\">N state variables, where:",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">x is the state vector",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">y is the output vector of length <em class=\"xref py py-obj\">Q",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">u is the input vector of length <em class=\"xref py py-obj\">P",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">A is the state matrix, with shape (N, N)",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">B is the input matrix with shape (N, P)",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">C is the output matrix with shape (Q, N)",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">D is the feedthrough or feedforward matrix with shape (Q, P).  (In\ncases where the system does not have a direct feedthrough, all values in\n<em class=\"xref py py-obj\">D are zero.)",
            "markdown"
        ],
        [
            "State-space is the most general representation and the only one that allows\nfor multiple-input, multiple-output (MIMO) systems. There are multiple\nstate-space representations for a given transfer function. Specifically, the\n\u00e2\u0080\u009ccontrollable canonical form\u00e2\u0080\u009d and \u00e2\u0080\u009cobservable canonical form\u00e2\u0080\u009d have the same\ncoefficients as the tf representation, and, therefore, suffer from the same\nnumerical errors.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->Filter Coefficients->Second-order sections representation": [
        [
            "The sos format is a single 2-D array of shape (n_sections, 6),\nrepresenting a sequence of second-order transfer functions which, when\ncascaded in series, realize a higher-order filter with minimal numerical\nerror. Each row corresponds to a second-order tf representation, with\nthe first three columns providing the numerator coefficients and the last\nthree providing the denominator coefficients:\n\n\\[[b_0, b_1, b_2, a_0, a_1, a_2]\\]",
            "markdown"
        ],
        [
            "The coefficients are typically normalized, such that \\(a_0\\) is always 1.\nThe section order is usually not important with floating-point computation;\nthe filter output will be the same, regardless of the order.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->Filter transformations": [
        [
            "The IIR filter design functions first generate a prototype analog low-pass filter\nwith a normalized cutoff frequency of 1 rad/sec. This is then transformed into\nother frequencies and band types using the following substitutions:",
            "markdown"
        ],
        [
            "Here, \\(\\omega_0\\) is the new cutoff or center frequency, and\n\\(\\mathrm{BW}\\) is the bandwidth.  These preserve symmetry on a logarithmic\nfrequency axis.",
            "markdown"
        ],
        [
            "To convert the transformed analog filter into a digital filter, the\nbilinear transform is used, which makes the following substitution:\n\n\\[s \\rightarrow \\frac{2}{T} \\frac{z - 1}{z + 1},\\]",
            "markdown"
        ],
        [
            "where T is the sampling time (the inverse of the sampling frequency).",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Other filters": [
        [
            "The signal processing package provides many more filters as well.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Other filters->Median Filter": [
        [
            "A median filter is commonly applied when noise is markedly non-Gaussian or\nwhen it is desired to preserve edges. The median filter works by sorting all\nof the array pixel values in a rectangular region surrounding the point of\ninterest. The sample median of this list of neighborhood pixel values is used\nas the value for the output array. The sample median is the middle-array value\nin a sorted list of neighborhood values. If there are an even number of\nelements in the neighborhood, then the average of the middle two values is\nused as the median. A general purpose median filter that works on\nN-D arrays is medfilt. A specialized version that works\nonly for 2-D arrays is available as medfilt2d.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Other filters->Order Filter": [
        [
            "A median filter is a specific example of a more general class of filters\ncalled order filters. To compute the output at a particular pixel, all order\nfilters use the array values in a region surrounding that pixel. These array\nvalues are sorted and then one of them is selected as the output value. For\nthe median filter, the sample median of the list of array values is used as\nthe output. A general-order filter allows the user to select which of the\nsorted values will be used as the output. So, for example, one could choose to\npick the maximum in the list or the minimum. The order filter takes an\nadditional argument besides the input array and the region mask that specifies\nwhich of the elements in the sorted list of neighbor array values should be\nused as the output. The command to perform an order filter is\norder_filter.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Other filters->Wiener filter": [
        [
            "The Wiener filter is a simple deblurring filter for denoising images. This is\nnot the Wiener filter commonly described in image-reconstruction problems but,\ninstead, it is a simple, local-mean filter. Let \\(x\\) be the input signal,\nthen the output is\n\n\\[\\begin{split}y=\\left\\{ \\begin{array}{cc} \\frac{\\sigma^{2}}{\\sigma_{x}^{2}}m_{x}+\\left(1-\\frac{\\sigma^{2}}{\\sigma_{x}^{2}}\\right)x & \\sigma_{x}^{2}\\geq\\sigma^{2},\\\\ m_{x} & \\sigma_{x}^{2}&lt;\\sigma^{2},\\end{array}\\right.\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(m_{x}\\) is the local estimate of the mean and\n\\(\\sigma_{x}^{2}\\) is the local estimate of the variance. The window for\nthese estimates is an optional input parameter (default is \\(3\\times3\\) ).\nThe parameter \\(\\sigma^{2}\\) is a threshold noise parameter. If\n\\(\\sigma\\) is not given, then it is estimated as the average of the local\nvariances.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Other filters->Hilbert filter": [
        [
            "The Hilbert transform constructs the complex-valued analytic signal\nfrom a real signal. For example, if \\(x=\\cos\\omega n\\), then\n\\(y=\\textrm{hilbert}\\left(x\\right)\\) would return (except near the\nedges) \\(y=\\exp\\left(j\\omega n\\right).\\) In the frequency domain,\nthe hilbert transform performs\n\n\\[Y=X\\cdot H,\\]",
            "markdown"
        ],
        [
            "where \\(H\\) is \\(2\\) for positive frequencies, \\(0\\) for negative\nfrequencies, and \\(1\\) for zero-frequencies.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Filtering->Analog Filter Design": [
        [
            "The functions iirdesign, iirfilter, and the filter design\nfunctions for specific filter types (e.g., ellip) all have a flag\n<em class=\"xref py py-obj\">analog, which allows the design of analog filters as well.",
            "markdown"
        ],
        [
            "The example below designs an analog (IIR) filter, obtains via tf2zpk\nthe poles and zeros and plots them in the complex s-plane. The zeros at\n\\(\\omega \\approx 150\\) and \\(\\omega \\approx 300\\) can be clearly seen\nin the amplitude response.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "b, a = signal.iirdesign(wp=100, ws=200, gpass=2.0, gstop=40., analog=True)\n w, h = signal.freqs(b, a)",
            "code"
        ],
        [
            "plt.title('Analog filter frequency response')\n plt.plot(w, 20*np.log10(np.abs(h)))\n plt.ylabel('Amplitude Response [dB]')\n plt.xlabel('Frequency')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is an IIR filter response as an X-Y plot with amplitude response on the Y axis vs frequency on the X axis. The low-pass filter shown has a passband from 0 to 100 Hz with 0 dB response and a stop-band from about 175 Hz to 1 KHz about 40 dB down. There are two sharp discontinuities in the filter near 175 Hz and 300 Hz. The second plot is an X-Y showing the transfer function in the complex plane. The Y axis is real-valued an the X axis is complex-valued. The filter has four zeros near [300+0j, 175+0j, -175+0j, -300+0j] shown as blue X markers. The filter also has four poles near [50-30j, -50-30j, 100-8j, -100-8j] shown as red dots.\"' class=\"plot-directive\" src=\"../_images/signal-7_00_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "z, p, k = signal.tf2zpk(b, a)",
            "code"
        ],
        [
            "plt.plot(np.real(z), np.imag(z), 'ob', markerfacecolor='none')\n plt.plot(np.real(p), np.imag(p), 'xr')\n plt.legend(['Zeros', 'Poles'], loc=2)",
            "code"
        ],
        [
            "plt.title('Pole / Zero Plot')\n plt.xlabel('Real')\n plt.ylabel('Imaginary')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is an IIR filter response as an X-Y plot with amplitude response on the Y axis vs frequency on the X axis. The low-pass filter shown has a passband from 0 to 100 Hz with 0 dB response and a stop-band from about 175 Hz to 1 KHz about 40 dB down. There are two sharp discontinuities in the filter near 175 Hz and 300 Hz. The second plot is an X-Y showing the transfer function in the complex plane. The Y axis is real-valued an the X axis is complex-valued. The filter has four zeros near [300+0j, 175+0j, -175+0j, -300+0j] shown as blue X markers. The filter also has four poles near [50-30j, -50-30j, 100-8j, -100-8j] shown as red dots.\"' class=\"plot-directive\" src=\"../_images/signal-7_01_00.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Periodogram Measurements": [
        [
            "The scipy function periodogram provides a method to estimate the\nspectral density using the periodogram method.",
            "markdown"
        ],
        [
            "The example below calculates the periodogram of a sine signal in white\nGaussian noise.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "fs = 10e3\n N = 1e5\n amp = 2*np.sqrt(2)\n freq = 1270.0\n noise_power = 0.001 * fs / 2\n time = np.arange(N) / fs\n x = amp*np.sin(2*np.pi*freq*time)\n x += np.random.normal(scale=np.sqrt(noise_power), size=time.shape)",
            "code"
        ],
        [
            "f, Pper_spec = signal.periodogram(x, fs, 'flattop', scaling='spectrum')",
            "code"
        ],
        [
            "plt.semilogy(f, Pper_spec)\n plt.xlabel('frequency [Hz]')\n plt.ylabel('PSD')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays a single X-Y log-linear plot with the power spectral density on the Y axis vs frequency on the X axis. A single blue trace shows a noise floor with a power level of 1e-3 with a single peak at 1270 Hz up to a power of 1. The noise floor measurements appear noisy and oscillate down to 1e-7.\"' class=\"plot-directive\" src=\"../_images/signal-8.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Spectral Analysis using Welch\u00e2\u0080\u0099s Method": [
        [
            "An improved method, especially with respect to noise immunity, is Welch\u00e2\u0080\u0099s\nmethod, which is implemented by the scipy function welch.",
            "markdown"
        ],
        [
            "The example below estimates the spectrum using Welch\u00e2\u0080\u0099s method and uses the\nsame parameters as the example above. Note the much smoother noise floor of\nthe spectrogram.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "fs = 10e3\n N = 1e5\n amp = 2*np.sqrt(2)\n freq = 1270.0\n noise_power = 0.001 * fs / 2\n time = np.arange(N) / fs\n x = amp*np.sin(2*np.pi*freq*time)\n x += np.random.normal(scale=np.sqrt(noise_power), size=time.shape)",
            "code"
        ],
        [
            "f, Pwelch_spec = signal.welch(x, fs, scaling='spectrum')",
            "code"
        ],
        [
            "plt.semilogy(f, Pwelch_spec)\n plt.xlabel('frequency [Hz]')\n plt.ylabel('PSD')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays a single X-Y log-linear plot with the power spectral density on the Y axis vs frequency on the X axis. A single blue trace shows a smooth noise floor at a power level of 6e-2 with a single peak up to a power level of 2 at 1270 Hz.\"' class=\"plot-directive\" src=\"../_images/signal-9.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Lomb-Scargle Periodograms (lombscargle)": [
        [
            "Least-squares spectral analysis (LSSA) [1] [2] is a method of estimating a frequency\nspectrum, based on a least-squares fit of sinusoids to data samples, similar\nto Fourier analysis. Fourier analysis, the most used spectral method in\nscience, generally boosts long-periodic noise in long-gapped records; LSSA\nmitigates such problems.",
            "markdown"
        ],
        [
            "The Lomb-Scargle method performs spectral analysis on unevenly-sampled data and\nis known to be a powerful way to find, and test the significance of, weak\nperiodic signals.",
            "markdown"
        ],
        [
            "For a time series comprising \\(N_{t}\\) measurements \\(X_{j}\\equiv\nX(t_{j})\\) sampled at times \\(t_{j}\\), where \\((j = 1, \\ldots, N_{t})\\),\nassumed to have been scaled and shifted, such that its mean is zero and its\nvariance is unity, the normalized Lomb-Scargle periodogram at frequency\n\\(f\\) is\n\n\\[P_{n}(f) \\frac{1}{2}\\left\\{\\frac{\\left[\\sum_{j}^{N_{t}}X_{j}\\cos\\omega(t_{j}-\\tau)\\right]^{2}}{\\sum_{j}^{N_{t}}\\cos^{2}\\omega(t_{j}-\\tau)}+\\frac{\\left[\\sum_{j}^{N_{t}}X_{j}\\sin\\omega(t_{j}-\\tau)\\right]^{2}}{\\sum_{j}^{N_{t}}\\sin^{2}\\omega(t_{j}-\\tau)}\\right\\}.\\]",
            "markdown"
        ],
        [
            "Here, \\(\\omega \\equiv 2\\pi f\\) is the angular frequency. The frequency-dependent\ntime offset \\(\\tau\\) is given by\n\n\\[\\tan 2\\omega\\tau = \\frac{\\sum_{j}^{N_{t}}\\sin 2\\omega t_{j}}{\\sum_{j}^{N_{t}}\\cos 2\\omega t_{j}}.\\]",
            "markdown"
        ],
        [
            "The lombscargle function calculates the periodogram using a slightly\nmodified algorithm due to Townsend [3], which allows the periodogram to be\ncalculated using only a single pass through the input arrays for each\nfrequency.",
            "markdown"
        ],
        [
            "The equation is refactored as:\n\n\\[P_{n}(f) = \\frac{1}{2}\\left[\\frac{(c_{\\tau}XC + s_{\\tau}XS)^{2}}{c_{\\tau}^{2}CC + 2c_{\\tau}s_{\\tau}CS + s_{\\tau}^{2}SS} + \\frac{(c_{\\tau}XS - s_{\\tau}XC)^{2}}{c_{\\tau}^{2}SS - 2c_{\\tau}s_{\\tau}CS + s_{\\tau}^{2}CC}\\right]\\]",
            "markdown"
        ],
        [
            "and\n\n\\[\\tan 2\\omega\\tau = \\frac{2CS}{CC-SS}.\\]",
            "markdown"
        ],
        [
            "Here,\n\n\\[c_{\\tau} = \\cos\\omega\\tau,\\qquad s_{\\tau} = \\sin\\omega\\tau,\\]",
            "markdown"
        ],
        [
            "while the sums are\n\n\\[\\begin{split}XC &= \\sum_{j}^{N_{t}} X_{j}\\cos\\omega t_{j}\\\\\nXS &= \\sum_{j}^{N_{t}} X_{j}\\sin\\omega t_{j}\\\\\nCC &= \\sum_{j}^{N_{t}} \\cos^{2}\\omega t_{j}\\\\\nSS &= \\sum_{j}^{N_{t}} \\sin^{2}\\omega t_{j}\\\\\nCS &= \\sum_{j}^{N_{t}} \\cos\\omega t_{j}\\sin\\omega t_{j}.\\end{split}\\]",
            "markdown"
        ],
        [
            "This requires \\(N_{f}(2N_{t}+3)\\) trigonometric function evaluations\ngiving a factor of \\(\\sim 2\\) speed increase over the straightforward\nimplementation.",
            "markdown"
        ]
    ],
    "scipy->Signal Processing (scipy.signal)->Detrend": [
        [
            "SciPy provides the function detrend to remove a constant or linear\ntrend in a data series in order to see effect of higher order.",
            "markdown"
        ],
        [
            "The example below removes the constant and linear trend of a second-order\npolynomial time series and plots the remaining signal components.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "t = np.linspace(-10, 10, 20)\n y = 1 + t + 0.01*t**2\n yconst = signal.detrend(y, type='constant')\n ylin = signal.detrend(y, type='linear')",
            "code"
        ],
        [
            "plt.plot(t, y, '-rx')\n plt.plot(t, yconst, '-bo')\n plt.plot(t, ylin, '-k+')\n plt.grid()\n plt.legend(['signal', 'const. detrend', 'linear detrend'])\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with no units. A red trace corresponding to the original signal curves from the bottom left to the top right. A blue trace has the constant detrend applied and is below the red trace with zero Y offset. The last black trace has the linear detrend applied and is almost flat from left to right highlighting the curve of the original signal. This last trace has an average slope of zero and looks very different.\"' class=\"plot-directive\" src=\"../_images/signal-10.png\"/>\n</figure>",
            "code"
        ],
        [
            "References",
            "markdown"
        ],
        [
            "Some further reading and related software:\n<aside class=\"footnote-list brackets\">\n<aside class=\"footnote brackets\" id=\"id4\" role=\"note\">\n[1]",
            "markdown"
        ],
        [
            "N.R. Lomb \u00e2\u0080\u009cLeast-squares frequency analysis of unequally spaced\ndata\u00e2\u0080\u009d, Astrophysics and Space Science, vol 39, pp. 447-462, 1976\n</aside>\n<aside class=\"footnote brackets\" id=\"id5\" role=\"note\">\n[2]",
            "markdown"
        ],
        [
            "J.D. Scargle \u00e2\u0080\u009cStudies in astronomical time series analysis. II -\nStatistical aspects of spectral analysis of unevenly spaced data\u00e2\u0080\u009d,\nThe Astrophysical Journal, vol 263, pp. 835-853, 1982\n</aside>\n<aside class=\"footnote brackets\" id=\"id6\" role=\"note\">\n[3]",
            "markdown"
        ],
        [
            "R.H.D. Townsend, \u00e2\u0080\u009cFast calculation of the Lomb-Scargle\nperiodogram using graphics processing units.\u00e2\u0080\u009d, The Astrophysical\nJournal Supplement Series, vol 191, pp. 247-253, 2010\n</aside>\n</aside>",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)": [
        [
            "When SciPy is built using the optimized ATLAS LAPACK and BLAS\nlibraries, it has very fast linear algebra capabilities. If you dig\ndeep enough, all of the raw LAPACK and BLAS libraries are available\nfor your use for even more speed. In this section, some easier-to-use\ninterfaces to these routines are described.",
            "markdown"
        ],
        [
            "All of these linear algebra routines expect an object that can be\nconverted into a 2-D array. The output of these routines is\nalso a 2-D array.",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->scipy.linalg vs numpy.linalg": [
        [
            "scipy.linalg contains all the functions in\nnumpy.linalg.\nplus some other more advanced ones not contained in numpy.linalg.",
            "markdown"
        ],
        [
            "Another advantage of using scipy.linalg over numpy.linalg is that\nit is always compiled with BLAS/LAPACK support, while for numpy this is\noptional. Therefore, the scipy version might be faster depending on how\nnumpy was installed.",
            "markdown"
        ],
        [
            "Therefore, unless you don\u00e2\u0080\u0099t want to add scipy as a dependency to\nyour numpy program, use scipy.linalg instead of numpy.linalg.",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->numpy.matrix vs 2-D numpy.ndarray": [
        [
            "The classes that represent matrices, and basic operations, such as\nmatrix multiplications and transpose are a part of numpy.\nFor convenience, we summarize the differences between numpy.matrix\nand numpy.ndarray here.",
            "markdown"
        ],
        [
            "numpy.matrix is matrix class that has a more convenient interface\nthan numpy.ndarray for matrix operations. This class supports, for\nexample, MATLAB-like creation syntax via the semicolon, has matrix\nmultiplication as default for the * operator, and contains I\nand T members that serve as shortcuts for inverse and transpose:",
            "markdown"
        ],
        [
            "import numpy as np\n A = np.mat('[1 2;3 4]')\n A\nmatrix([[1, 2],\n        [3, 4]])\n A.I\nmatrix([[-2. ,  1. ],\n        [ 1.5, -0.5]])\n b = np.mat('[5 6]')\n b\nmatrix([[5, 6]])\n b.T\nmatrix([[5],\n        [6]])\n A*b.T\nmatrix([[17],\n        [39]])",
            "code"
        ],
        [
            "Despite its convenience, the use of the numpy.matrix class is\ndiscouraged, since it adds nothing that cannot be accomplished\nwith 2-D numpy.ndarray objects, and may lead to a confusion of which class\nis being used. For example, the above code can be rewritten as:",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A = np.array([[1,2],[3,4]])\n A\narray([[1, 2],\n      [3, 4]])\n linalg.inv(A)\narray([[-2. ,  1. ],\n      [ 1.5, -0.5]])\n b = np.array([[5,6]]) #2D array\n b\narray([[5, 6]])\n b.T\narray([[5],\n      [6]])\n A*b #not matrix multiplication!\narray([[ 5, 12],\n      [15, 24]])\n A.dot(b.T) #matrix multiplication\narray([[17],\n      [39]])\n b = np.array([5,6]) #1D array\n b\narray([5, 6])\n b.T  #not matrix transpose!\narray([5, 6])\n A.dot(b)  #does not matter for multiplication\narray([17, 39])",
            "code"
        ],
        [
            "scipy.linalg operations can be applied equally to\nnumpy.matrix or to 2D numpy.ndarray objects.",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Basic routines->Finding the inverse": [
        [
            "The inverse of a matrix \\(\\mathbf{A}\\) is the matrix\n\\(\\mathbf{B}\\), such that \\(\\mathbf{AB}=\\mathbf{I}\\), where\n\\(\\mathbf{I}\\) is the identity matrix consisting of ones down the\nmain diagonal.  Usually, \\(\\mathbf{B}\\) is denoted\n\\(\\mathbf{B}=\\mathbf{A}^{-1}\\) . In SciPy, the matrix inverse of\nthe NumPy array, A, is obtained using linalg.inv (A), or\nusing A.I if A is a Matrix. For example, let\n\n\\[\\begin{split}\\mathbf{A} = \\left[\\begin{array}{ccc} 1 & 3 & 5\\\\ 2 & 5 & 1\\\\ 2 & 3 & 8\\end{array}\\right],\\end{split}\\]",
            "markdown"
        ],
        [
            "then\n\n\\[\\begin{split}\\mathbf{A^{-1}} = \\frac{1}{25}\n    \\left[\\begin{array}{ccc} -37 & 9 & 22 \\\\\n                              14 & 2 & -9 \\\\\n                              4 & -3 & 1\n          \\end{array}\\right] = %\n     \\left[\\begin{array}{ccc} -1.48 & 0.36 & 0.88  \\\\\n                               0.56 & 0.08 & -0.36 \\\\\n                               0.16 & -0.12 & 0.04\n           \\end{array}\\right].\\end{split}\\]",
            "markdown"
        ],
        [
            "The following example demonstrates this computation in SciPy",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A = np.array([[1,3,5],[2,5,1],[2,3,8]])\n A\narray([[1, 3, 5],\n      [2, 5, 1],\n      [2, 3, 8]])\n linalg.inv(A)\narray([[-1.48,  0.36,  0.88],\n      [ 0.56,  0.08, -0.36],\n      [ 0.16, -0.12,  0.04]])\n A.dot(linalg.inv(A)) #double check\narray([[  1.00000000e+00,  -1.11022302e-16,  -5.55111512e-17],\n      [  3.05311332e-16,   1.00000000e+00,   1.87350135e-16],\n      [  2.22044605e-16,  -1.11022302e-16,   1.00000000e+00]])",
            "code"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Basic routines->Solving a linear system": [
        [
            "Solving linear systems of equations is straightforward using the scipy\ncommand linalg.solve. This command expects an input matrix and\na right-hand side vector. The solution vector is then computed. An\noption for entering a symmetric matrix is offered, which can speed up\nthe processing when applicable. As an example, suppose it is desired\nto solve the following simultaneous equations:\n\n \\begin{eqnarray*} x + 3y + 5z & = & 10 \\\\\n                   2x + 5y + z & = & 8  \\\\\n                   2x + 3y + 8z & = & 3\n \\end{eqnarray*}",
            "markdown"
        ],
        [
            "We could find the solution vector using a matrix inverse:\n\n\\[\\begin{split}\\left[\\begin{array}{c} x\\\\ y\\\\ z\\end{array}\\right]=\\left[\\begin{array}{ccc} 1 & 3 & 5\\\\ 2 & 5 & 1\\\\ 2 & 3 & 8\\end{array}\\right]^{-1}\\left[\\begin{array}{c} 10\\\\ 8\\\\ 3\\end{array}\\right]=\\frac{1}{25}\\left[\\begin{array}{c} -232\\\\ 129\\\\ 19\\end{array}\\right]=\\left[\\begin{array}{c} -9.28\\\\ 5.16\\\\ 0.76\\end{array}\\right].\\end{split}\\]",
            "markdown"
        ],
        [
            "However, it is better to use the linalg.solve command, which can be\nfaster and more numerically stable. In this case, it, however, gives the\nsame answer as shown in the following example:",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A = np.array([[1, 2], [3, 4]])\n A\narray([[1, 2],\n      [3, 4]])\n b = np.array([[5], [6]])\n b\narray([[5],\n      [6]])\n linalg.inv(A).dot(b)  # slow\narray([[-4. ],\n      [ 4.5]])\n A.dot(linalg.inv(A).dot(b)) - b  # check\narray([[  8.88178420e-16],\n      [  2.66453526e-15]])\n np.linalg.solve(A, b)  # fast\narray([[-4. ],\n      [ 4.5]])\n A.dot(np.linalg.solve(A, b)) - b  # check\narray([[ 0.],\n      [ 0.]])",
            "code"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Basic routines->Finding the determinant": [
        [
            "The determinant of a square matrix \\(\\mathbf{A}\\) is often denoted\n\\(\\left|\\mathbf{A}\\right|\\) and is a quantity often used in linear\nalgebra. Suppose \\(a_{ij}\\) are the elements of the matrix\n\\(\\mathbf{A}\\) and let \\(M_{ij}=\\left|\\mathbf{A}_{ij}\\right|\\)\nbe the determinant of the matrix left by removing the\n\\(i^{\\textrm{th}}\\) row and \\(j^{\\textrm{th}}\\) column from\n\\(\\mathbf{A}\\) . Then, for any row \\(i,\\)\n\n\\[\\left|\\mathbf{A}\\right|=\\sum_{j}\\left(-1\\right)^{i+j}a_{ij}M_{ij}.\\]",
            "markdown"
        ],
        [
            "This is a recursive way to define the determinant, where the base case\nis defined by accepting that the determinant of a \\(1\\times1\\) matrix is the only matrix element. In SciPy the determinant can be\ncalculated with linalg.det. For example, the determinant of\n\n\\[\\begin{split}\\mathbf{A=}\\left[\\begin{array}{ccc} 1 & 3 & 5\\\\ 2 & 5 & 1\\\\ 2 & 3 & 8\\end{array}\\right]\\end{split}\\]",
            "markdown"
        ],
        [
            "is\n\n \\begin{eqnarray*} \\left|\\mathbf{A}\\right| & = & 1\\left|\\begin{array}{cc} 5 & 1\\\\ 3 & 8\\end{array}\\right|-3\\left|\\begin{array}{cc} 2 & 1\\\\ 2 & 8\\end{array}\\right|+5\\left|\\begin{array}{cc} 2 & 5\\\\ 2 & 3\\end{array}\\right|\\\\  & = & 1\\left(5\\cdot8-3\\cdot1\\right)-3\\left(2\\cdot8-2\\cdot1\\right)+5\\left(2\\cdot3-2\\cdot5\\right)=-25.\\end{eqnarray*}.",
            "markdown"
        ],
        [
            "In SciPy, this is computed as shown in this example:",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A = np.array([[1,2],[3,4]])\n A\narray([[1, 2],\n      [3, 4]])\n linalg.det(A)\n-2.0",
            "code"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Basic routines->Computing norms": [
        [
            "Matrix and vector norms can also be computed with SciPy. A wide range\nof norm definitions are available using different parameters to the\norder argument of linalg.norm. This function takes a rank-1\n(vectors) or a rank-2 (matrices) array and an optional order argument\n(default is 2). Based on these inputs, a vector or matrix norm of the\nrequested order is computed.",
            "markdown"
        ],
        [
            "For vector x, the order parameter can be any real number including\ninf or -inf. The computed norm is\n\n\\[\\begin{split}\\left\\Vert \\mathbf{x}\\right\\Vert =\\left\\{ \\begin{array}{cc} \\max\\left|x_{i}\\right| & \\textrm{ord}=\\textrm{inf}\\\\ \\min\\left|x_{i}\\right| & \\textrm{ord}=-\\textrm{inf}\\\\ \\left(\\sum_{i}\\left|x_{i}\\right|^{\\textrm{ord}}\\right)^{1/\\textrm{ord}} & \\left|\\textrm{ord}\\right|&lt;\\infty.\\end{array}\\right.\\end{split}\\]",
            "markdown"
        ],
        [
            "For matrix \\(\\mathbf{A}\\), the only valid values for norm are \\(\\pm2,\\pm1,\\) \\(\\pm\\) inf, and \u00e2\u0080\u0098fro\u00e2\u0080\u0099 (or \u00e2\u0080\u0098f\u00e2\u0080\u0099) Thus,\n\n\\[\\begin{split}\\left\\Vert \\mathbf{A}\\right\\Vert =\\left\\{ \\begin{array}{cc} \\max_{i}\\sum_{j}\\left|a_{ij}\\right| & \\textrm{ord}=\\textrm{inf}\\\\ \\min_{i}\\sum_{j}\\left|a_{ij}\\right| & \\textrm{ord}=-\\textrm{inf}\\\\ \\max_{j}\\sum_{i}\\left|a_{ij}\\right| & \\textrm{ord}=1\\\\ \\min_{j}\\sum_{i}\\left|a_{ij}\\right| & \\textrm{ord}=-1\\\\ \\max\\sigma_{i} & \\textrm{ord}=2\\\\ \\min\\sigma_{i} & \\textrm{ord}=-2\\\\ \\sqrt{\\textrm{trace}\\left(\\mathbf{A}^{H}\\mathbf{A}\\right)} & \\textrm{ord}=\\textrm{'fro'}\\end{array}\\right.\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(\\sigma_{i}\\) are the singular values of \\(\\mathbf{A}\\).",
            "markdown"
        ],
        [
            "Examples:",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A=np.array([[1,2],[3,4]])\n A\narray([[1, 2],\n      [3, 4]])\n linalg.norm(A)\n5.4772255750516612\n linalg.norm(A,'fro') # frobenius norm is the default\n5.4772255750516612\n linalg.norm(A,1) # L1 norm (max column sum)\n6\n linalg.norm(A,-1)\n4\n linalg.norm(A,np.inf) # L inf norm (max row sum)\n7",
            "code"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Basic routines->Solving linear least-squares problems and pseudo-inverses": [
        [
            "Linear least-squares problems occur in many branches of applied\nmathematics. In this problem, a set of linear scaling coefficients is\nsought that allows a model to fit the data. In particular, it is assumed\nthat data \\(y_{i}\\) is related to data \\(\\mathbf{x}_{i}\\)\nthrough a set of coefficients \\(c_{j}\\) and model functions\n\\(f_{j}\\left(\\mathbf{x}_{i}\\right)\\) via the model\n\n\\[y_{i}=\\sum_{j}c_{j}f_{j}\\left(\\mathbf{x}_{i}\\right)+\\epsilon_{i},\\]",
            "markdown"
        ],
        [
            "where \\(\\epsilon_{i}\\) represents uncertainty in the data. The\nstrategy of least squares is to pick the coefficients \\(c_{j}\\) to\nminimize\n\n\\[J\\left(\\mathbf{c}\\right)=\\sum_{i}\\left|y_{i}-\\sum_{j}c_{j}f_{j}\\left(x_{i}\\right)\\right|^{2}.\\]",
            "markdown"
        ],
        [
            "Theoretically, a global minimum will occur when\n\n\\[\\frac{\\partial J}{\\partial c_{n}^{*}}=0=\\sum_{i}\\left(y_{i}-\\sum_{j}c_{j}f_{j}\\left(x_{i}\\right)\\right)\\left(-f_{n}^{*}\\left(x_{i}\\right)\\right)\\]",
            "markdown"
        ],
        [
            "or\n\n \\begin{eqnarray*} \\sum_{j}c_{j}\\sum_{i}f_{j}\\left(x_{i}\\right)f_{n}^{*}\\left(x_{i}\\right) & = & \\sum_{i}y_{i}f_{n}^{*}\\left(x_{i}\\right)\\\\ \\mathbf{A}^{H}\\mathbf{Ac} & = & \\mathbf{A}^{H}\\mathbf{y}\\end{eqnarray*},",
            "markdown"
        ],
        [
            "where\n\n\\[\\left\\{ \\mathbf{A}\\right\\} _{ij}=f_{j}\\left(x_{i}\\right).\\]",
            "markdown"
        ],
        [
            "When \\(\\mathbf{A^{H}A}\\) is invertible, then\n\n\\[\\mathbf{c}=\\left(\\mathbf{A}^{H}\\mathbf{A}\\right)^{-1}\\mathbf{A}^{H}\\mathbf{y}=\\mathbf{A}^{\\dagger}\\mathbf{y},\\]",
            "markdown"
        ],
        [
            "where \\(\\mathbf{A}^{\\dagger}\\) is called the pseudo-inverse of\n\\(\\mathbf{A}.\\) Notice that using this definition of\n\\(\\mathbf{A}\\) the model can be written\n\n\\[\\mathbf{y}=\\mathbf{Ac}+\\boldsymbol{\\epsilon}.\\]",
            "markdown"
        ],
        [
            "The command linalg.lstsq will solve the linear least-squares\nproblem for \\(\\mathbf{c}\\) given \\(\\mathbf{A}\\) and\n\\(\\mathbf{y}\\) . In addition, linalg.pinv will find\n\\(\\mathbf{A}^{\\dagger}\\) given \\(\\mathbf{A}.\\)",
            "markdown"
        ],
        [
            "The following example and figure demonstrate the use of\nlinalg.lstsq and linalg.pinv for solving a data-fitting\nproblem. The data shown below were generated using the model:\n\n\\[y_{i}=c_{1}e^{-x_{i}}+c_{2}x_{i},\\]",
            "markdown"
        ],
        [
            "where \\(x_{i}=0.1i\\) for \\(i=1\\ldots10\\) , \\(c_{1}=5\\),\nand \\(c_{2}=4.\\) Noise is added to \\(y_{i}\\) and the\ncoefficients \\(c_{1}\\) and \\(c_{2}\\) are estimated using\nlinear least squares.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n import matplotlib.pyplot as plt\n rng = np.random.default_rng()",
            "code"
        ],
        [
            "c1, c2 = 5.0, 2.0\n i = np.r_[1:11]\n xi = 0.1*i\n yi = c1*np.exp(-xi) + c2*xi\n zi = yi + 0.05 * np.max(yi) * rng.standard_normal(len(yi))",
            "code"
        ],
        [
            "A = np.c_[np.exp(-xi)[:, np.newaxis], xi[:, np.newaxis]]\n c, resid, rank, sigma = linalg.lstsq(A, zi)",
            "code"
        ],
        [
            "xi2 = np.r_[0.1:1.0:100j]\n yi2 = c[0]*np.exp(-xi2) + c[1]*xi2",
            "code"
        ],
        [
            "plt.plot(xi,zi,'x',xi2,yi2)\n plt.axis([0,1.1,3.0,5.5])\n plt.xlabel('$x_i$')\n plt.title('Data fitting with linalg.lstsq')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/linalg-1.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Basic routines->Generalized inverse": [
        [
            "The generalized inverse is calculated using the command\nlinalg.pinv. Let \\(\\mathbf{A}\\) be an\n\\(M\\times N\\) matrix, then if \\(M>N\\), the generalized\ninverse is\n\n\\[\\mathbf{A}^{\\dagger}=\\left(\\mathbf{A}^{H}\\mathbf{A}\\right)^{-1}\\mathbf{A}^{H},\\]",
            "markdown"
        ],
        [
            "while if \\(M&lt;N\\) matrix, the generalized inverse is\n\n\\[\\mathbf{A}^{\\#}=\\mathbf{A}^{H}\\left(\\mathbf{A}\\mathbf{A}^{H}\\right)^{-1}.\\]",
            "markdown"
        ],
        [
            "In the case that \\(M=N\\), then\n\n\\[\\mathbf{A}^{\\dagger}=\\mathbf{A}^{\\#}=\\mathbf{A}^{-1},\\]",
            "markdown"
        ],
        [
            "as long as \\(\\mathbf{A}\\) is invertible.",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Decompositions": [
        [
            "In many applications, it is useful to decompose a matrix using other\nrepresentations. There are several decompositions supported by SciPy.",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Decompositions->Eigenvalues and eigenvectors": [
        [
            "The eigenvalue-eigenvector problem is one of the most commonly\nemployed linear algebra operations. In one popular form, the\neigenvalue-eigenvector problem is to find for some square matrix\n\\(\\mathbf{A}\\) scalars \\(\\lambda\\) and corresponding vectors\n\\(\\mathbf{v}\\), such that\n\n\\[\\mathbf{Av}=\\lambda\\mathbf{v}.\\]",
            "markdown"
        ],
        [
            "For an \\(N\\times N\\) matrix, there are \\(N\\) (not necessarily\ndistinct) eigenvalues \u00e2\u0080\u0094 roots of the (characteristic) polynomial\n\n\\[\\left|\\mathbf{A}-\\lambda\\mathbf{I}\\right|=0.\\]",
            "markdown"
        ],
        [
            "The eigenvectors, \\(\\mathbf{v}\\), are also sometimes called right\neigenvectors to distinguish them from another set of left eigenvectors\nthat satisfy\n\n\\[\\mathbf{v}_{L}^{H}\\mathbf{A}=\\lambda\\mathbf{v}_{L}^{H}\\]",
            "markdown"
        ],
        [
            "or\n\n\\[\\mathbf{A}^{H}\\mathbf{v}_{L}=\\lambda^{*}\\mathbf{v}_{L}.\\]",
            "markdown"
        ],
        [
            "With its default optional arguments, the command linalg.eig\nreturns \\(\\lambda\\) and \\(\\mathbf{v}.\\) However, it can also\nreturn \\(\\mathbf{v}_{L}\\) and just \\(\\lambda\\) by itself (\nlinalg.eigvals returns just \\(\\lambda\\) as well).",
            "markdown"
        ],
        [
            "In addition, linalg.eig can also solve the more general eigenvalue problem\n\n \\begin{eqnarray*} \\mathbf{Av} & = & \\lambda\\mathbf{Bv}\\\\ \\mathbf{A}^{H}\\mathbf{v}_{L} & = & \\lambda^{*}\\mathbf{B}^{H}\\mathbf{v}_{L}\\end{eqnarray*}",
            "markdown"
        ],
        [
            "for square matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}.\\) The\nstandard eigenvalue problem is an example of the general eigenvalue\nproblem for \\(\\mathbf{B}=\\mathbf{I}.\\) When a generalized\neigenvalue problem can be solved, it provides a decomposition of\n\\(\\mathbf{A}\\) as\n\n\\[\\mathbf{A}=\\mathbf{BV}\\boldsymbol{\\Lambda}\\mathbf{V}^{-1},\\]",
            "markdown"
        ],
        [
            "where \\(\\mathbf{V}\\) is the collection of eigenvectors into\ncolumns and \\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix of\neigenvalues.",
            "markdown"
        ],
        [
            "By definition, eigenvectors are only defined up to a constant scale\nfactor. In SciPy, the scaling factor for the eigenvectors is chosen so\nthat \\(\\left\\Vert \\mathbf{v}\\right\\Vert\n^{2}=\\sum_{i}v_{i}^{2}=1.\\)",
            "markdown"
        ],
        [
            "As an example, consider finding the eigenvalues and eigenvectors of\nthe matrix\n\n\\[\\begin{split}\\mathbf{A}=\\left[\\begin{array}{ccc} 1 & 5 & 2\\\\ 2 & 4 & 1\\\\ 3 & 6 & 2\\end{array}\\right].\\end{split}\\]",
            "markdown"
        ],
        [
            "The characteristic polynomial is\n\n \\begin{eqnarray*} \\left|\\mathbf{A}-\\lambda\\mathbf{I}\\right| & = & \\left(1-\\lambda\\right)\\left[\\left(4-\\lambda\\right)\\left(2-\\lambda\\right)-6\\right]-\\\\  &  & 5\\left[2\\left(2-\\lambda\\right)-3\\right]+2\\left[12-3\\left(4-\\lambda\\right)\\right]\\\\  & = & -\\lambda^{3}+7\\lambda^{2}+8\\lambda-3.\\end{eqnarray*}",
            "markdown"
        ],
        [
            "The roots of this polynomial are the eigenvalues of \\(\\mathbf{A}\\):\n\n \\begin{eqnarray*} \\lambda_{1} & = & 7.9579\\\\ \\lambda_{2} & = & -1.2577\\\\ \\lambda_{3} & = & 0.2997.\\end{eqnarray*}",
            "markdown"
        ],
        [
            "The eigenvectors corresponding to each eigenvalue can be found using\nthe original equation. The eigenvectors associated with these\neigenvalues can then be found.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A = np.array([[1, 2], [3, 4]])\n la, v = linalg.eig(A)\n l1, l2 = la\n print(l1, l2)   # eigenvalues\n(-0.3722813232690143+0j) (5.372281323269014+0j)\n print(v[:, 0])   # first eigenvector\n[-0.82456484  0.56576746]\n print(v[:, 1])   # second eigenvector\n[-0.41597356 -0.90937671]\n print(np.sum(abs(v**2), axis=0))  # eigenvectors are unitary\n[1. 1.]\n v1 = np.array(v[:, 0]).T\n print(linalg.norm(A.dot(v1) - l1*v1))  # check the computation\n3.23682852457e-16",
            "code"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Decompositions->Singular value decomposition": [
        [
            "Singular value decomposition (SVD) can be thought of as an extension of\nthe eigenvalue problem to matrices that are not square. Let\n\\(\\mathbf{A}\\) be an \\(M\\times N\\) matrix with \\(M\\) and\n\\(N\\) arbitrary. The matrices \\(\\mathbf{A}^{H}\\mathbf{A}\\) and\n\\(\\mathbf{A}\\mathbf{A}^{H}\\) are square hermitian matrices [1] of\nsize \\(N\\times N\\) and \\(M\\times M\\), respectively. It is known\nthat the eigenvalues of square hermitian matrices are real and\nnon-negative. In addition, there are at most\n\\(\\min\\left(M,N\\right)\\) identical non-zero eigenvalues of\n\\(\\mathbf{A}^{H}\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^{H}.\\)\nDefine these positive eigenvalues as \\(\\sigma_{i}^{2}.\\) The\nsquare-root of these are called singular values of \\(\\mathbf{A}.\\)\nThe eigenvectors of \\(\\mathbf{A}^{H}\\mathbf{A}\\) are collected by\ncolumns into an \\(N\\times N\\) unitary [2] matrix\n\\(\\mathbf{V}\\), while the eigenvectors of\n\\(\\mathbf{A}\\mathbf{A}^{H}\\) are collected by columns in the\nunitary matrix \\(\\mathbf{U}\\), the singular values are collected\nin an \\(M\\times N\\) zero matrix\n\\(\\mathbf{\\boldsymbol{\\Sigma}}\\) with main diagonal entries set to\nthe singular values. Then\n\n\\[\\mathbf{A=U}\\boldsymbol{\\Sigma}\\mathbf{V}^{H}\\]",
            "markdown"
        ],
        [
            "is the singular value decomposition of \\(\\mathbf{A}.\\) Every\nmatrix has a singular value decomposition. Sometimes, the singular\nvalues are called the spectrum of \\(\\mathbf{A}.\\) The command\nlinalg.svd will return \\(\\mathbf{U}\\) ,\n\\(\\mathbf{V}^{H}\\), and \\(\\sigma_{i}\\) as an array of the\nsingular values. To obtain the matrix \\(\\boldsymbol{\\Sigma}\\), use\nlinalg.diagsvd. The following example illustrates the use of\nlinalg.svd:",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A = np.array([[1,2,3],[4,5,6]])\n A\narray([[1, 2, 3],\n      [4, 5, 6]])\n M,N = A.shape\n U,s,Vh = linalg.svd(A)\n Sig = linalg.diagsvd(s,M,N)\n U, Vh = U, Vh\n U\narray([[-0.3863177 , -0.92236578],\n      [-0.92236578,  0.3863177 ]])\n Sig\narray([[ 9.508032  ,  0.        ,  0.        ],\n      [ 0.        ,  0.77286964,  0.        ]])\n Vh\narray([[-0.42866713, -0.56630692, -0.7039467 ],\n      [ 0.80596391,  0.11238241, -0.58119908],\n      [ 0.40824829, -0.81649658,  0.40824829]])\n U.dot(Sig.dot(Vh)) #check computation\narray([[ 1.,  2.,  3.],\n      [ 4.,  5.,  6.]])\n\n\n<aside class=\"footnote-list brackets\">\n<aside class=\"footnote brackets\" id=\"id3\" role=\"note\">\n[1]",
            "code"
        ],
        [
            "A hermitian matrix \\(\\mathbf{D}\\) satisfies \\(\\mathbf{D}^{H}=\\mathbf{D}.\\)\n</aside>\n<aside class=\"footnote brackets\" id=\"id4\" role=\"note\">\n[2]",
            "markdown"
        ],
        [
            "A unitary matrix \\(\\mathbf{D}\\) satisfies \\(\\mathbf{D}^{H}\\mathbf{D}=\\mathbf{I}=\\mathbf{D}\\mathbf{D}^{H}\\) so that \\(\\mathbf{D}^{-1}=\\mathbf{D}^{H}.\\)\n</aside>\n</aside>",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Decompositions->LU decomposition": [
        [
            "The LU decomposition finds a representation for the \\(M\\times N\\)\nmatrix \\(\\mathbf{A}\\) as\n\n\\[\\mathbf{A}=\\mathbf{P}\\,\\mathbf{L}\\,\\mathbf{U},\\]",
            "markdown"
        ],
        [
            "where \\(\\mathbf{P}\\) is an \\(M\\times M\\) permutation matrix (a\npermutation of the rows of the identity matrix), \\(\\mathbf{L}\\) is\nin \\(M\\times K\\) lower triangular or trapezoidal matrix (\n\\(K=\\min\\left(M,N\\right)\\)) with unit-diagonal, and\n\\(\\mathbf{U}\\) is an upper triangular or trapezoidal matrix. The\nSciPy command for this decomposition is linalg.lu.",
            "markdown"
        ],
        [
            "Such a decomposition is often useful for solving many simultaneous\nequations where the left-hand side does not change but the right-hand\nside does. For example, suppose we are going to solve\n\n\\[\\mathbf{A}\\mathbf{x}_{i}=\\mathbf{b}_{i}\\]",
            "markdown"
        ],
        [
            "for many different \\(\\mathbf{b}_{i}\\). The LU decomposition allows this to be written as\n\n\\[\\mathbf{PLUx}_{i}=\\mathbf{b}_{i}.\\]",
            "markdown"
        ],
        [
            "Because \\(\\mathbf{L}\\) is lower-triangular, the equation can be\nsolved for \\(\\mathbf{U}\\mathbf{x}_{i}\\) and, finally,\n\\(\\mathbf{x}_{i}\\) very rapidly using forward- and\nback-substitution. An initial time spent factoring \\(\\mathbf{A}\\)\nallows for very rapid solution of similar systems of equations in the\nfuture. If the intent for performing LU decomposition is for solving\nlinear systems, then the command linalg.lu_factor should be used\nfollowed by repeated applications of the command\nlinalg.lu_solve to solve the system for each new\nright-hand side.",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Decompositions->Cholesky decomposition": [
        [
            "Cholesky decomposition is a special case of LU decomposition\napplicable to Hermitian positive definite matrices. When\n\\(\\mathbf{A}=\\mathbf{A}^{H}\\) and\n\\(\\mathbf{x}^{H}\\mathbf{Ax}\\geq0\\) for all \\(\\mathbf{x}\\),\nthen decompositions of \\(\\mathbf{A}\\) can be found so that\n\n \\begin{eqnarray*} \\mathbf{A} & = & \\mathbf{U}^{H}\\mathbf{U}\\\\ \\mathbf{A} & = & \\mathbf{L}\\mathbf{L}^{H}\\end{eqnarray*},",
            "markdown"
        ],
        [
            "where \\(\\mathbf{L}\\) is lower triangular and \\(\\mathbf{U}\\) is\nupper triangular. Notice that \\(\\mathbf{L}=\\mathbf{U}^{H}.\\) The\ncommand linalg.cholesky computes the Cholesky\nfactorization. For using the Cholesky factorization to solve systems of\nequations, there are also linalg.cho_factor and\nlinalg.cho_solve routines that work similarly to their LU\ndecomposition counterparts.",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Decompositions->QR decomposition": [
        [
            "The QR decomposition (sometimes called a polar decomposition) works\nfor any \\(M\\times N\\) array and finds an \\(M\\times M\\) unitary\nmatrix \\(\\mathbf{Q}\\) and an \\(M\\times N\\) upper-trapezoidal\nmatrix \\(\\mathbf{R}\\), such that\n\n\\[\\mathbf{A=QR}.\\]",
            "markdown"
        ],
        [
            "Notice that if the SVD of \\(\\mathbf{A}\\) is known, then the QR decomposition can be found.\n\n\\[\\mathbf{A}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{H}=\\mathbf{QR}\\]",
            "markdown"
        ],
        [
            "implies that \\(\\mathbf{Q}=\\mathbf{U}\\) and\n\\(\\mathbf{R}=\\boldsymbol{\\Sigma}\\mathbf{V}^{H}.\\) Note, however,\nthat in SciPy independent algorithms are used to find QR and SVD\ndecompositions. The command for QR decomposition is linalg.qr.",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Decompositions->Schur decomposition": [
        [
            "For a square \\(N\\times N\\) matrix, \\(\\mathbf{A}\\), the Schur\ndecomposition finds (not necessarily unique) matrices\n\\(\\mathbf{T}\\) and \\(\\mathbf{Z}\\), such that\n\n\\[\\mathbf{A}=\\mathbf{ZT}\\mathbf{Z}^{H},\\]",
            "markdown"
        ],
        [
            "where \\(\\mathbf{Z}\\) is a unitary matrix and \\(\\mathbf{T}\\) is\neither upper triangular or quasi upper triangular, depending on whether\nor not a real Schur form or complex Schur form is requested.  For a\nreal Schur form both \\(\\mathbf{T}\\) and \\(\\mathbf{Z}\\) are\nreal-valued when \\(\\mathbf{A}\\) is real-valued. When\n\\(\\mathbf{A}\\) is a real-valued matrix, the real Schur form is only\nquasi upper triangular because \\(2\\times2\\) blocks extrude from\nthe main diagonal corresponding to any complex-valued\neigenvalues. The command linalg.schur finds the Schur\ndecomposition, while the command linalg.rsf2csf converts\n\\(\\mathbf{T}\\) and \\(\\mathbf{Z}\\) from a real Schur form to a\ncomplex Schur form. The Schur form is especially useful in calculating\nfunctions of matrices.",
            "markdown"
        ],
        [
            "The following example illustrates the Schur decomposition:",
            "markdown"
        ],
        [
            "from scipy import linalg\n A = np.mat('[1 3 2; 1 4 5; 2 3 6]')\n T, Z = linalg.schur(A)\n T1, Z1 = linalg.schur(A, 'complex')\n T2, Z2 = linalg.rsf2csf(T, Z)\n T\narray([[ 9.90012467,  1.78947961, -0.65498528],\n       [ 0.        ,  0.54993766, -1.57754789],\n       [ 0.        ,  0.51260928,  0.54993766]])\n T2\narray([[ 9.90012467+0.00000000e+00j, -0.32436598+1.55463542e+00j,\n        -0.88619748+5.69027615e-01j],\n       [ 0.        +0.00000000e+00j,  0.54993766+8.99258408e-01j,\n         1.06493862+3.05311332e-16j],\n       [ 0.        +0.00000000e+00j,  0.        +0.00000000e+00j,\n         0.54993766-8.99258408e-01j]])\n abs(T1 - T2) # different\narray([[  1.06604538e-14,   2.06969555e+00,   1.69375747e+00],  # may vary\n       [  0.00000000e+00,   1.33688556e-15,   4.74146496e-01],\n       [  0.00000000e+00,   0.00000000e+00,   1.13220977e-15]])\n abs(Z1 - Z2) # different\narray([[ 0.06833781,  0.88091091,  0.79568503],    # may vary\n       [ 0.11857169,  0.44491892,  0.99594171],\n       [ 0.12624999,  0.60264117,  0.77257633]])\n T, Z, T1, Z1, T2, Z2 = map(np.mat,(T,Z,T1,Z1,T2,Z2))\n abs(A - Z*T*Z.H)  # same\nmatrix([[  5.55111512e-16,   1.77635684e-15,   2.22044605e-15],\n        [  0.00000000e+00,   3.99680289e-15,   8.88178420e-16],\n        [  1.11022302e-15,   4.44089210e-16,   3.55271368e-15]])\n abs(A - Z1*T1*Z1.H)  # same\nmatrix([[  4.26993904e-15,   6.21793362e-15,   8.00007092e-15],\n        [  5.77945386e-15,   6.21798014e-15,   1.06653681e-14],\n        [  7.16681444e-15,   8.90271058e-15,   1.77635764e-14]])\n abs(A - Z2*T2*Z2.H)  # same\nmatrix([[  6.02594127e-16,   1.77648931e-15,   2.22506907e-15],\n        [  2.46275555e-16,   3.99684548e-15,   8.91642616e-16],\n        [  8.88225111e-16,   8.88312432e-16,   4.44104848e-15]])",
            "code"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Decompositions->Interpolative decomposition": [
        [
            "scipy.linalg.interpolative contains routines for computing the\ninterpolative decomposition (ID) of a matrix. For a matrix \\(A\n\\in \\mathbb{C}^{m \\times n}\\) of rank \\(k \\leq \\min \\{ m, n \\}\\)\nthis is a factorization\n\n\\[A \\Pi =\n\\begin{bmatrix}\n A \\Pi_{1} & A \\Pi_{2}\n\\end{bmatrix} =\nA \\Pi_{1}\n\\begin{bmatrix}\n I & T\n\\end{bmatrix},\\]",
            "markdown"
        ],
        [
            "where \\(\\Pi = [\\Pi_{1}, \\Pi_{2}]\\) is a permutation matrix with\n\\(\\Pi_{1} \\in \\{ 0, 1 \\}^{n \\times k}\\), i.e., \\(A \\Pi_{2} =\nA \\Pi_{1} T\\). This can equivalently be written as \\(A = BP\\),\nwhere \\(B = A \\Pi_{1}\\) and \\(P = [I, T] \\Pi^{\\mathsf{T}}\\)\nare the skeleton and interpolation matrices, respectively.",
            "markdown"
        ],
        [
            "See also",
            "markdown"
        ],
        [
            "scipy.linalg.interpolative \u00e2\u0080\u0094 for more information.",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Matrix functions": [
        [
            "Consider the function \\(f\\left(x\\right)\\) with Taylor series expansion\n\n\\[f\\left(x\\right)=\\sum_{k=0}^{\\infty}\\frac{f^{\\left(k\\right)}\\left(0\\right)}{k!}x^{k}.\\]",
            "markdown"
        ],
        [
            "A matrix function can be defined using this Taylor series for the\nsquare matrix \\(\\mathbf{A}\\) as\n\n\\[f\\left(\\mathbf{A}\\right)=\\sum_{k=0}^{\\infty}\\frac{f^{\\left(k\\right)}\\left(0\\right)}{k!}\\mathbf{A}^{k}.\\]",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "While this serves as a useful representation of a matrix function, it is\nrarely the best way to calculate a matrix function. In particular, if the\nmatrix is not diagonalizable, results may be innacurate.",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Matrix functions->Exponential and logarithm functions": [
        [
            "The matrix exponential is one of the more common matrix functions.\nThe preferred method for implementing the matrix exponential is to use\nscaling and a Pad\u00c3\u00a9 approximation for \\(e^{x}\\). This algorithm is\nimplemented as linalg.expm.",
            "markdown"
        ],
        [
            "The inverse of the matrix exponential is the matrix logarithm defined\nas the inverse of the matrix exponential:\n\n\\[\\mathbf{A}\\equiv\\exp\\left(\\log\\left(\\mathbf{A}\\right)\\right).\\]",
            "markdown"
        ],
        [
            "The matrix logarithm can be obtained with linalg.logm.",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Matrix functions->Trigonometric functions": [
        [
            "The trigonometric functions, \\(\\sin\\), \\(\\cos\\), and\n\\(\\tan\\), are implemented for matrices in linalg.sinm,\nlinalg.cosm, and linalg.tanm, respectively. The matrix\nsine and cosine can be defined using Euler\u00e2\u0080\u0099s identity as\n\n \\begin{eqnarray*} \\sin\\left(\\mathbf{A}\\right) & = & \\frac{e^{j\\mathbf{A}}-e^{-j\\mathbf{A}}}{2j}\\\\ \\cos\\left(\\mathbf{A}\\right) & = & \\frac{e^{j\\mathbf{A}}+e^{-j\\mathbf{A}}}{2}.\\end{eqnarray*}",
            "markdown"
        ],
        [
            "The tangent is\n\n\\[\\tan\\left(x\\right)=\\frac{\\sin\\left(x\\right)}{\\cos\\left(x\\right)}=\\left[\\cos\\left(x\\right)\\right]^{-1}\\sin\\left(x\\right)\\]",
            "markdown"
        ],
        [
            "and so the matrix tangent is defined as\n\n\\[\\left[\\cos\\left(\\mathbf{A}\\right)\\right]^{-1}\\sin\\left(\\mathbf{A}\\right).\\]",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Matrix functions->Hyperbolic trigonometric functions": [
        [
            "The hyperbolic trigonometric functions, \\(\\sinh\\), \\(\\cosh\\),\nand \\(\\tanh\\), can also be defined for matrices using the familiar\ndefinitions:\n\n \\begin{eqnarray*} \\sinh\\left(\\mathbf{A}\\right) & = & \\frac{e^{\\mathbf{A}}-e^{-\\mathbf{A}}}{2}\\\\ \\cosh\\left(\\mathbf{A}\\right) & = & \\frac{e^{\\mathbf{A}}+e^{-\\mathbf{A}}}{2}\\\\ \\tanh\\left(\\mathbf{A}\\right) & = & \\left[\\cosh\\left(\\mathbf{A}\\right)\\right]^{-1}\\sinh\\left(\\mathbf{A}\\right).\\end{eqnarray*}",
            "markdown"
        ],
        [
            "These matrix functions can be found using linalg.sinhm,\nlinalg.coshm, and linalg.tanhm.",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Matrix functions->Arbitrary function": [
        [
            "Finally, any arbitrary function that takes one complex number and\nreturns a complex number can be called as a matrix function using the\ncommand linalg.funm. This command takes the matrix and an\narbitrary Python function. It then implements an algorithm from Golub\nand Van Loan\u00e2\u0080\u0099s book \u00e2\u0080\u009cMatrix Computations\u00e2\u0080\u009d to compute the function applied\nto the matrix using a Schur decomposition.  Note that the function\nneeds to accept complex numbers as input in order to work with this\nalgorithm. For example, the following code computes the zeroth-order\nBessel function applied to a matrix.",
            "markdown"
        ],
        [
            "from scipy import special, linalg\n rng = np.random.default_rng()\n A = rng.random((3, 3))\n B = linalg.funm(A, lambda x: special.jv(0, x))\n A\narray([[0.06369197, 0.90647174, 0.98024544],\n       [0.68752227, 0.5604377 , 0.49142032],\n       [0.86754578, 0.9746787 , 0.37932682]])\n B\narray([[ 0.6929219 , -0.29728805, -0.15930896],\n       [-0.16226043,  0.71967826, -0.22709386],\n       [-0.19945564, -0.33379957,  0.70259022]])\n linalg.eigvals(A)\narray([ 1.94835336+0.j, -0.72219681+0.j, -0.22270006+0.j])\n special.jv(0, linalg.eigvals(A))\narray([0.25375345+0.j, 0.87379738+0.j, 0.98763955+0.j])\n linalg.eigvals(B)\narray([0.25375345+0.j, 0.87379738+0.j, 0.98763955+0.j])",
            "code"
        ],
        [
            "Note how, by virtue of how matrix analytic functions are defined,\nthe Bessel function has acted on the matrix eigenvalues.",
            "markdown"
        ]
    ],
    "scipy->Linear Algebra (scipy.linalg)->Special matrices": [
        [
            "SciPy and NumPy provide several functions for creating special matrices\nthat are frequently used in engineering and science.",
            "markdown"
        ],
        [
            "For examples of the use of these functions, see their respective docstrings.",
            "markdown"
        ]
    ],
    "scipy->Sparse eigenvalue problems with ARPACK->Introduction": [
        [
            "ARPACK [1] is a Fortran package which provides routines for quickly finding a few\neigenvalues/eigenvectors of large sparse matrices. In order to find these\nsolutions, it requires only left-multiplication by the matrix in question.\nThis operation is performed through a reverse-communication interface. The\nresult of this structure is that ARPACK is able to find eigenvalues and\neigenvectors of any linear function mapping a vector to a vector.",
            "markdown"
        ],
        [
            "All of the functionality provided in ARPACK is contained within the two\nhigh-level interfaces scipy.sparse.linalg.eigs and\nscipy.sparse.linalg.eigsh. eigs\nprovides interfaces for finding the\neigenvalues/vectors of real or complex nonsymmetric square matrices, while\neigsh provides interfaces for real-symmetric or complex-hermitian\nmatrices.",
            "markdown"
        ]
    ],
    "scipy->Sparse eigenvalue problems with ARPACK->Basic functionality": [
        [
            "ARPACK can solve either standard eigenvalue problems of the form\n\n\\[A \\mathbf{x} = \\lambda \\mathbf{x}\\]",
            "markdown"
        ],
        [
            "or general eigenvalue problems of the form\n\n\\[A \\mathbf{x} = \\lambda M \\mathbf{x}.\\]",
            "markdown"
        ],
        [
            "The power of ARPACK is that it can compute only a specified subset of\neigenvalue/eigenvector pairs. This is accomplished through the keyword\nwhich. The following values of which are available:",
            "markdown"
        ],
        [
            "which = 'LM' : Eigenvalues with largest magnitude (eigs, eigsh),\nthat is, largest eigenvalues in the euclidean norm of complex numbers.",
            "markdown"
        ],
        [
            "which = 'SM' : Eigenvalues with smallest magnitude (eigs, eigsh),\nthat is, smallest eigenvalues in the euclidean norm of complex numbers.",
            "markdown"
        ],
        [
            "which = 'LR' : Eigenvalues with largest real part (eigs).",
            "markdown"
        ],
        [
            "which = 'SR' : Eigenvalues with smallest real part (eigs).",
            "markdown"
        ],
        [
            "which = 'LI' : Eigenvalues with largest imaginary part (eigs).",
            "markdown"
        ],
        [
            "which = 'SI' : Eigenvalues with smallest imaginary part (eigs).",
            "markdown"
        ],
        [
            "which = 'LA' : Eigenvalues with largest algebraic value (eigsh),\nthat is, largest eigenvalues inclusive of any negative sign.",
            "markdown"
        ],
        [
            "which = 'SA' : Eigenvalues with smallest algebraic value (eigsh),\nthat is, smallest eigenvalues inclusive of any negative sign.",
            "markdown"
        ],
        [
            "which = 'BE' : Eigenvalues from both ends of the spectrum (eigsh).",
            "markdown"
        ],
        [
            "Note that ARPACK is generally better at finding extremal eigenvalues, that\nis, eigenvalues with large magnitudes. In particular, using which = 'SM'\nmay lead to slow execution time and/or anomalous results. A better approach\nis to use shift-invert mode.",
            "markdown"
        ]
    ],
    "scipy->Sparse eigenvalue problems with ARPACK->Shift-invert mode": [
        [
            "Shift-invert mode relies on the following observation. For the generalized\neigenvalue problem\n\n\\[A \\mathbf{x} = \\lambda M \\mathbf{x},\\]",
            "markdown"
        ],
        [
            "it can be shown that\n\n\\[(A - \\sigma M)^{-1} M \\mathbf{x} = \\nu \\mathbf{x},\\]",
            "markdown"
        ],
        [
            "where\n\n\\[\\nu = \\frac{1}{\\lambda - \\sigma}.\\]",
            "markdown"
        ]
    ],
    "scipy->Sparse eigenvalue problems with ARPACK->Examples": [
        [
            "Imagine you\u00e2\u0080\u0099d like to find the smallest and largest eigenvalues and the\ncorresponding eigenvectors for a large matrix. ARPACK can handle many\nforms of input: dense matrices ,such as numpy.ndarray instances, sparse\nmatrices, such as scipy.sparse.csr_matrix, or a general linear operator\nderived from scipy.sparse.linalg.LinearOperator. For this example, for\nsimplicity, we\u00e2\u0080\u0099ll construct a symmetric, positive-definite matrix.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy.linalg import eig, eigh\n from scipy.sparse.linalg import eigs, eigsh\n np.set_printoptions(suppress=True)\n rng = np.random.default_rng()\n\n X = rng.random((100, 100)) - 0.5\n X = np.dot(X, X.T)  # create a symmetric matrix",
            "code"
        ],
        [
            "We now have a symmetric matrix X, with which to test the routines. First,\ncompute a standard eigenvalue decomposition using eigh:",
            "markdown"
        ],
        [
            "evals_all, evecs_all = eigh(X)",
            "code"
        ],
        [
            "As the dimension of X grows, this routine becomes very slow. Especially,\nif only a few eigenvectors and eigenvalues are needed, ARPACK can be a\nbetter option. First let\u00e2\u0080\u0099s compute the largest eigenvalues (which = 'LM')\nof X and compare them to the known results:",
            "markdown"
        ],
        [
            "evals_large, evecs_large = eigsh(X, 3, which='LM')\n print(evals_all[-3:])\n[29.22435321 30.05590784 30.58591252]\n print(evals_large)\n[29.22435321 30.05590784 30.58591252]\n print(np.dot(evecs_large.T, evecs_all[:,-3:]))\narray([[-1.  0.  0.],       # may vary (signs)\n       [ 0.  1.  0.],\n       [-0.  0. -1.]])",
            "code"
        ],
        [
            "The results are as expected. ARPACK recovers the desired eigenvalues and they\nmatch the previously known results. Furthermore, the eigenvectors are\northogonal, as we\u00e2\u0080\u0099d expect. Now, let\u00e2\u0080\u0099s attempt to solve for the eigenvalues\nwith smallest magnitude:",
            "markdown"
        ],
        [
            "evals_small, evecs_small = eigsh(X, 3, which='SM')\nTraceback (most recent call last):       # may vary (convergence)\n...\nscipy.sparse.linalg._eigen.arpack.arpack.ArpackNoConvergence:\nARPACK error -1: No convergence (1001 iterations, 0/3 eigenvectors converged)",
            "code"
        ],
        [
            "Oops. We see that, as mentioned above, ARPACK is not quite as adept at\nfinding small eigenvalues. There are a few ways this problem can be\naddressed. We could increase the tolerance (tol) to lead to faster\nconvergence:",
            "markdown"
        ],
        [
            "evals_small, evecs_small = eigsh(X, 3, which='SM', tol=1E-2)\n evals_all[:3]\narray([0.00053181, 0.00298319, 0.01387821])\n evals_small\narray([0.00053181, 0.00298319, 0.01387821])\n np.dot(evecs_small.T, evecs_all[:,:3])\narray([[ 0.99999999  0.00000024 -0.00000049],    # may vary (signs)\n       [-0.00000023  0.99999999  0.00000056],\n       [ 0.00000031 -0.00000037  0.99999852]])",
            "code"
        ],
        [
            "This works, but we lose the precision in the results. Another option is\nto increase the maximum number of iterations (maxiter) from 1000 to 5000:",
            "markdown"
        ],
        [
            "evals_small, evecs_small = eigsh(X, 3, which='SM', maxiter=5000)\n evals_all[:3]\narray([0.00053181, 0.00298319, 0.01387821])\n evals_small\narray([0.00053181, 0.00298319, 0.01387821])\n np.dot(evecs_small.T, evecs_all[:,:3])\narray([[ 1.  0.  0.],           # may vary (signs)\n       [-0.  1.  0.],\n       [ 0.  0. -1.]])",
            "code"
        ],
        [
            "We get the results we\u00e2\u0080\u0099d hoped for, but the computation time is much longer.\nFortunately, ARPACK contains a mode that allows a quick determination of\nnon-external eigenvalues: shift-invert mode. As mentioned above, this\nmode involves transforming the eigenvalue problem to an equivalent problem\nwith different eigenvalues. In this case, we hope to find eigenvalues near\nzero, so we\u00e2\u0080\u0099ll choose sigma = 0. The transformed eigenvalues will\nthen satisfy \\(\\nu = 1/(\\lambda - \\sigma) = 1/\\lambda\\), so our\nsmall eigenvalues \\(\\lambda\\) become large eigenvalues \\(\\nu\\).",
            "markdown"
        ],
        [
            "evals_small, evecs_small = eigsh(X, 3, sigma=0, which='LM')\n evals_all[:3]\narray([0.00053181, 0.00298319, 0.01387821])\n evals_small\narray([0.00053181, 0.00298319, 0.01387821])\n np.dot(evecs_small.T, evecs_all[:,:3])\narray([[ 1.  0.  0.],    # may vary (signs)\n       [ 0. -1. -0.],\n       [-0. -0.  1.]])",
            "code"
        ],
        [
            "We get the results we were hoping for, with much less computational time.\nNote that the transformation from \\(\\nu \\to \\lambda\\) takes place\nentirely in the background. The user need not worry about the details.",
            "markdown"
        ],
        [
            "The shift-invert mode provides more than just a fast way to obtain a few\nsmall eigenvalues. Say, you\ndesire to find internal eigenvalues and eigenvectors, e.g., those nearest to\n\\(\\lambda = 1\\). Simply set sigma = 1 and ARPACK will take care of\nthe rest:",
            "markdown"
        ],
        [
            "evals_mid, evecs_mid = eigsh(X, 3, sigma=1, which='LM')\n i_sort = np.argsort(abs(1. / (1 - evals_all)))[-3:]\n evals_all[i_sort]\narray([0.94164107, 1.05464515, 0.99090277])\n evals_mid\narray([0.94164107, 0.99090277, 1.05464515])\n print(np.dot(evecs_mid.T, evecs_all[:,i_sort]))\narray([[-0.  1.  0.],     # may vary (signs)\n       [-0. -0.  1.],\n       [ 1.  0.  0.]]",
            "code"
        ],
        [
            "The eigenvalues come out in a different order, but they\u00e2\u0080\u0099re all there.\nNote that the shift-invert mode requires the internal solution of a matrix\ninverse. This is taken care of automatically by eigsh and eigs,\nbut the operation can also be specified by the user. See the docstring of\nscipy.sparse.linalg.eigsh and\nscipy.sparse.linalg.eigs for details.",
            "markdown"
        ]
    ],
    "scipy->Sparse eigenvalue problems with ARPACK->Use of LinearOperator": [
        [
            "We consider now the case where you\u00e2\u0080\u0099d like to avoid creating a dense matrix\nand use scipy.sparse.linalg.LinearOperator instead. Our first\nlinear operator applies element-wise multiplication between the input vector\nand a vector \\(\\mathbf{d}\\) provided by the user to the operator itself.\nThis operator mimics a diagonal matrix with the elements of \\(\\mathbf{d}\\)\nalong the main diagonal and it has the main benefit that the forward and\nadjoint operations are simple element-wise multiplications other\nthan matrix-vector multiplications. For a diagonal matrix, we expect the\neigenvalues to be equal to the elements along the main diagonal, in this case\n\\(\\mathbf{d}\\). The eigenvalues and eigenvectors obtained with eigsh\nare compared to those obtained by using eigh when applied to\nthe dense matrix:",
            "markdown"
        ],
        [
            "from scipy.sparse.linalg import LinearOperator\n class Diagonal(LinearOperator):\n...     def __init__(self, diag, dtype='float32'):\n...         self.diag = diag\n...         self.shape = (len(self.diag), len(self.diag))\n...         self.dtype = np.dtype(dtype)\n...     def _matvec(self, x):\n...         return self.diag*x\n...     def _rmatvec(self, x):\n...         return self.diag*x",
            "code"
        ],
        [
            "N = 100\n rng = np.random.default_rng()\n d = rng.normal(0, 1, N).astype(np.float64)\n D = np.diag(d)\n Dop = Diagonal(d, dtype=np.float64)",
            "code"
        ],
        [
            "evals_all, evecs_all = eigh(D)\n evals_large, evecs_large = eigsh(Dop, 3, which='LA', maxiter=1e3)\n evals_all[-3:]\narray([1.53092498, 1.77243671, 2.00582508])\n evals_large\narray([1.53092498, 1.77243671, 2.00582508])\n print(np.dot(evecs_large.T, evecs_all[:,-3:]))\narray([[-1.  0.  0.],     # may vary (signs)\n       [-0. -1.  0.],\n       [ 0.  0. -1.]]",
            "code"
        ],
        [
            "In this case, we have created a quick and easy Diagonal operator.\nThe external library PyLops provides\nsimilar capabilities in the Diagonal operator,\nas well as several other operators.",
            "markdown"
        ],
        [
            "Finally, we consider a linear operator that mimics the application of a\nfirst-derivative stencil. In this case, the operator is equivalent to a real\nnonsymmetric matrix. Once again, we compare the estimated eigenvalues\nand eigenvectors with those from a dense matrix that applies the\nsame first derivative to an input signal:",
            "markdown"
        ],
        [
            "class FirstDerivative(LinearOperator):\n...     def __init__(self, N, dtype='float32'):\n...         self.N = N\n...         self.shape = (self.N, self.N)\n...         self.dtype = np.dtype(dtype)\n...     def _matvec(self, x):\n...         y = np.zeros(self.N, self.dtype)\n...         y[1:-1] = (0.5*x[2:]-0.5*x[0:-2])\n...         return y\n...     def _rmatvec(self, x):\n...         y = np.zeros(self.N, self.dtype)\n...         y[0:-2] = y[0:-2] - (0.5*x[1:-1])\n...         y[2:] = y[2:] + (0.5*x[1:-1])\n...         return y",
            "code"
        ],
        [
            "N = 21\n D = np.diag(0.5*np.ones(N-1), k=1) - np.diag(0.5*np.ones(N-1), k=-1)\n D[0] = D[-1] = 0 # take away edge effects\n Dop = FirstDerivative(N, dtype=np.float64)",
            "code"
        ],
        [
            "evals_all, evecs_all = eig(D)\n evals_large, evecs_large = eigs(Dop, 4, which='LI')\n evals_all_imag = evals_all.imag\n isort_imag = np.argsort(np.abs(evals_all_imag))\n evals_all_imag = evals_all_imag[isort_imag]\n evals_large_imag = evals_large.imag\n isort_imag = np.argsort(np.abs(evals_large_imag))\n evals_large_imag = evals_large_imag[isort_imag]\n evals_all_imag[-4:]\narray([-0.95105652, 0.95105652, -0.98768834, 0.98768834])\n evals_large_imag\narray([0.95105652, -0.95105652, 0.98768834, -0.98768834])",
            "code"
        ],
        [
            "Note that the eigenvalues of this operator are all imaginary. Moreover,\nthe keyword which='LI' of scipy.sparse.linalg.eigs produces\nthe eigenvalues with largest absolute imaginary part (both\npositive and negative). Again, a more advanced implementation of the\nfirst-derivative operator is available in the\nPyLops library under the name of\nFirstDerivative\noperator.",
            "markdown"
        ]
    ],
    "scipy->Sparse eigenvalue problems with ARPACK->References": [
        [
            "http://www.caam.rice.edu/software/ARPACK/\n</aside>\n</aside>",
            "markdown"
        ]
    ],
    "scipy->Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders": [
        [
            "A Word Ladder is a word game\ninvented by Lewis Carroll, in which players find paths between words by\nswitching one letter at a time. For example, one can link \u00e2\u0080\u009cape\u00e2\u0080\u009d and \u00e2\u0080\u009cman\u00e2\u0080\u009d\nin the following way:\n\n\\[{\\rm ape \\to apt \\to ait \\to bit \\to big \\to bag \\to mag \\to man}\\]",
            "markdown"
        ],
        [
            "Note that each step involves changing just one letter of the word. This is\njust one possible path from \u00e2\u0080\u009cape\u00e2\u0080\u009d to \u00e2\u0080\u009cman\u00e2\u0080\u009d, but is it the shortest possible\npath? If we desire to find the shortest word-ladder path between two given\nwords, the sparse graph submodule can help.",
            "markdown"
        ],
        [
            "First, we need a list of valid words. Many operating systems have such a list\nbuilt in. For example, on linux, a word list can often be found at one of the\nfollowing locations:",
            "markdown"
        ],
        [
            "/usr/share/dict\n/var/lib/dict",
            "code"
        ],
        [
            "Another easy source for words are the Scrabble word lists available at various\nsites around the internet (search with your favorite search engine). We\u00e2\u0080\u0099ll\nfirst create this list. The system word lists consist of a file with one\nword per line. The following should be modified to use the particular word\nlist you have available:",
            "markdown"
        ],
        [
            "word_list = open('/usr/share/dict/words').readlines()\n word_list = map(str.strip, word_list)",
            "code"
        ],
        [
            "We want to look at words of length 3, so let\u00e2\u0080\u0099s select just those words of the\ncorrect length. We\u00e2\u0080\u0099ll also eliminate words which start with upper-case\n(proper nouns) or contain non-alphanumeric characters, like apostrophes and\nhyphens. Finally, we\u00e2\u0080\u0099ll make sure everything is lower-case for comparison\nlater:",
            "markdown"
        ],
        [
            "word_list = [word for word in word_list if len(word) == 3]\n word_list = [word for word in word_list if word[0].islower()]\n word_list = [word for word in word_list if word.isalpha()]\n word_list = list(map(str.lower, word_list))\n len(word_list)\n586    # may vary",
            "code"
        ],
        [
            "Now we have a list of 586 valid three-letter words (the exact number may\nchange depending on the particular list used). Each of these words will\nbecome a node in our graph, and we will create edges connecting the nodes\nassociated with each pair of words which differs by only one letter.",
            "markdown"
        ],
        [
            "There are efficient ways to do this, and inefficient ways to do this. To\ndo this as efficiently as possible, we\u00e2\u0080\u0099re going to use some sophisticated\nnumpy array manipulation:",
            "markdown"
        ],
        [
            "import numpy as np\n word_list = np.asarray(word_list)\n word_list.dtype   # these are unicode characters in Python 3\ndtype('&lt;U3')\n word_list.sort()  # sort for quick searching later",
            "code"
        ],
        [
            "We have an array where each entry is three unicode characters long. We\u00e2\u0080\u0099d like\nto find all pairs where exactly one character is different. We\u00e2\u0080\u0099ll start by\nconverting each word to a 3-D vector:",
            "markdown"
        ],
        [
            "word_bytes = np.ndarray((word_list.size, word_list.itemsize),\n...                         dtype='uint8',\n...                         buffer=word_list.data)\n # each unicode character is four bytes long. We only need first byte\n # we know that there are three characters in each word\n word_bytes = word_bytes[:, ::word_list.itemsize//3]\n word_bytes.shape\n(586, 3)    # may vary",
            "code"
        ],
        [
            "Now, we\u00e2\u0080\u0099ll use the\nHamming distance\nbetween each point to determine which pairs of words are connected.\nThe Hamming distance measures the fraction of entries between two vectors\nwhich differ: any two words with a Hamming distance equal to \\(1/N\\),\nwhere \\(N\\) is the number of letters, are connected in the word ladder:",
            "markdown"
        ],
        [
            "from scipy.spatial.distance import pdist, squareform\n from scipy.sparse import csr_matrix\n hamming_dist = pdist(word_bytes, metric='hamming')\n # there are three characters in each word\n graph = csr_matrix(squareform(hamming_dist &lt; 1.5 / 3))",
            "code"
        ],
        [
            "When comparing the distances, we don\u00e2\u0080\u0099t use an equality because this can be\nunstable for floating point values. The inequality produces the desired\nresult, as long as no two entries of the word list are identical. Now, that our\ngraph is set up, we\u00e2\u0080\u0099ll use a shortest path search to find the path between\nany two words in the graph:",
            "markdown"
        ],
        [
            "i1 = word_list.searchsorted('ape')\n i2 = word_list.searchsorted('man')\n word_list[i1]\n'ape'\n word_list[i2]\n'man'",
            "code"
        ],
        [
            "We need to check that these match, because if the words are not in the list,\nthat will not be the case. Now, all we need is to find the shortest path\nbetween these two indices in the graph. We\u00e2\u0080\u0099ll use\nDijkstra\u00e2\u0080\u0099s algorithm,\nbecause it allows us to find the path for just one node:",
            "markdown"
        ],
        [
            "from scipy.sparse.csgraph import dijkstra\n distances, predecessors = dijkstra(graph, indices=i1,\n...                                    return_predecessors=True)\n print(distances[i2])\n5.0    # may vary",
            "code"
        ],
        [
            "So we see that the shortest path between \u00e2\u0080\u009cape\u00e2\u0080\u009d and \u00e2\u0080\u009cman\u00e2\u0080\u009d contains only\nfive steps. We can use the predecessors returned by the algorithm to\nreconstruct this path:",
            "markdown"
        ],
        [
            "path = []\n i = i2\n while i != i1:\n...     path.append(word_list[i])\n...     i = predecessors[i]\n path.append(word_list[i1])\n print(path[::-1])\n['ape', 'apt', 'opt', 'oat', 'mat', 'man']    # may vary",
            "code"
        ],
        [
            "This is three fewer links than our initial example: the path from \u00e2\u0080\u009cape\u00e2\u0080\u009d to \u00e2\u0080\u009cman\u00e2\u0080\u009d\nis only five steps.",
            "markdown"
        ],
        [
            "Using other tools in the module, we can answer other questions. For example,\nare there three-letter words which are not linked in a word ladder? This\nis a question of connected components in the graph:",
            "markdown"
        ],
        [
            "from scipy.sparse.csgraph import connected_components\n N_components, component_list = connected_components(graph)\n print(N_components)\n15    # may vary",
            "code"
        ],
        [
            "In this particular sample of three-letter words, there are 15 connected\ncomponents: that is, 15 distinct sets of words with no paths between the\nsets. How many words are there in each of these sets? We can learn this from\nthe list of components:",
            "markdown"
        ],
        [
            "[np.sum(component_list == i) for i in range(N_components)]\n[571, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]    # may vary",
            "code"
        ],
        [
            "There is one large connected set and 14 smaller ones. Let\u00e2\u0080\u0099s look at the\nwords in the smaller ones:",
            "markdown"
        ],
        [
            "[list(word_list[np.nonzero(component_list == i)]) for i in range(1, N_components)]\n[['aha'],    # may vary\n ['chi'],\n ['ebb'],\n ['ems', 'emu'],\n ['gnu'],\n ['ism'],\n ['khz'],\n ['nth'],\n ['ova'],\n ['qua'],\n ['ugh'],\n ['ups'],\n ['urn'],\n ['use']]",
            "code"
        ],
        [
            "These are all the three-letter words which do not connect to others via a word\nladder.",
            "markdown"
        ],
        [
            "We might also be curious about which words are maximally separated. Which\ntwo words take the most links to connect? We can determine this by computing\nthe matrix of all shortest paths. Note that, by convention, the\ndistance between two non-connected points is reported to be infinity, so\nwe\u00e2\u0080\u0099ll need to remove these before finding the maximum:",
            "markdown"
        ],
        [
            "distances, predecessors = dijkstra(graph, return_predecessors=True)\n max_distance = np.max(distances[~np.isinf(distances)])\n print(max_distance)\n13.0    # may vary",
            "code"
        ],
        [
            "So, there is at least one pair of words which takes 13 steps to get from one\nto the other! Let\u00e2\u0080\u0099s determine which these are:",
            "markdown"
        ],
        [
            "i1, i2 = np.nonzero(distances == max_distance)\n list(zip(word_list[i1], word_list[i2]))\n[('imp', 'ohm'),    # may vary\n ('imp', 'ohs'),\n ('ohm', 'imp'),\n ('ohm', 'ump'),\n ('ohs', 'imp'),\n ('ohs', 'ump'),\n ('ump', 'ohm'),\n ('ump', 'ohs')]",
            "code"
        ],
        [
            "We see that there are two pairs of words which are maximally separated from\neach other: \u00e2\u0080\u0098imp\u00e2\u0080\u0099 and \u00e2\u0080\u0098ump\u00e2\u0080\u0099 on the one hand, and \u00e2\u0080\u0098ohm\u00e2\u0080\u0099 and \u00e2\u0080\u0098ohs\u00e2\u0080\u0099 on the other.\nWe can find the connecting list in the same way as above:",
            "markdown"
        ],
        [
            "path = []\n i = i2[0]\n while i != i1[0]:\n...     path.append(word_list[i])\n...     i = predecessors[i1[0], i]\n path.append(word_list[i1[0]])\n print(path[::-1])\n['imp', 'amp', 'asp', 'ass', 'ads', 'add', 'aid', 'mid', 'mod', 'moo', 'too', 'tho', 'oho', 'ohm']    # may vary",
            "code"
        ],
        [
            "This gives us the path we desired to see.",
            "markdown"
        ],
        [
            "Word ladders are just one potential application of scipy\u00e2\u0080\u0099s fast graph\nalgorithms for sparse matrices. Graph theory makes appearances in many\nareas of mathematics, data analysis, and machine learning. The sparse graph\ntools are flexible enough to handle many of these situations.",
            "markdown"
        ]
    ],
    "scipy->Spatial data structures and algorithms (scipy.spatial)": [
        [
            "scipy.spatial can compute triangulations, Voronoi diagrams, and\nconvex hulls of a set of points, by leveraging the Qhull library.",
            "markdown"
        ],
        [
            "Moreover, it contains KDTree implementations for nearest-neighbor point\nqueries, and utilities for distance computations in various metrics.",
            "markdown"
        ]
    ],
    "scipy->Spatial data structures and algorithms (scipy.spatial)->Delaunay triangulations": [
        [
            "The Delaunay triangulation is a subdivision of a set of points into a\nnon-overlapping set of triangles, such that no point is inside the\ncircumcircle of any triangle. In practice, such triangulations tend to\navoid triangles with small angles.",
            "markdown"
        ],
        [
            "Delaunay triangulation can be computed using scipy.spatial as follows:",
            "markdown"
        ],
        [
            "from scipy.spatial import Delaunay\n import numpy as np\n points = np.array([[0, 0], [0, 1.1], [1, 0], [1, 1]])\n tri = Delaunay(points)",
            "code"
        ],
        [
            "We can visualize it:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n plt.triplot(points[:,0], points[:,1], tri.simplices)\n plt.plot(points[:,0], points[:,1], 'o')",
            "code"
        ],
        [
            "And add some further decorations:",
            "markdown"
        ],
        [
            "for j, p in enumerate(points):\n...     plt.text(p[0]-0.03, p[1]+0.03, j, ha='right') # label the points\n for j, s in enumerate(tri.simplices):\n...     p = points[s].mean(axis=0)\n...     plt.text(p[0], p[1], '#%d' % j, ha='center') # label triangles\n plt.xlim(-0.5, 1.5); plt.ylim(-0.5, 1.5)\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with four green points annotated 0 through 3 roughly in the shape of a box. The box is outlined with a diagonal line between points 0 and 3 forming two adjacent triangles. The top triangle is annotated as #1 and the bottom triangle is annotated as #0.\"' class=\"plot-directive\" src=\"../_images/spatial-1.png\"/>\n</figure>",
            "code"
        ],
        [
            "The structure of the triangulation is encoded in the following way:\nthe simplices attribute contains the indices of the points in the\npoints array that make up the triangle. For instance:",
            "markdown"
        ],
        [
            "i = 1\n tri.simplices[i,:]\narray([3, 1, 0], dtype=int32)\n points[tri.simplices[i,:]]\narray([[ 1. ,  1. ],\n       [ 0. ,  1.1],\n       [ 0. ,  0. ]])",
            "code"
        ],
        [
            "Moreover, neighboring triangles can also be found:",
            "markdown"
        ],
        [
            "tri.neighbors[i]\narray([-1,  0, -1], dtype=int32)",
            "code"
        ],
        [
            "What this tells us is that this triangle has triangle #0 as a neighbor,\nbut no other neighbors. Moreover, it tells us that neighbor 0 is\nopposite the vertex 1 of the triangle:",
            "markdown"
        ],
        [
            "points[tri.simplices[i, 1]]\narray([ 0. ,  1.1])",
            "code"
        ],
        [
            "Indeed, from the figure, we see that this is the case.",
            "markdown"
        ],
        [
            "Qhull can also perform tessellations to simplices for\nhigher-dimensional point sets (for instance, subdivision into\ntetrahedra in 3-D).",
            "markdown"
        ]
    ],
    "scipy->Spatial data structures and algorithms (scipy.spatial)->Delaunay triangulations->Coplanar points": [
        [
            "It is important to note that not all points necessarily appear as\nvertices of the triangulation, due to numerical precision issues in\nforming the triangulation. Consider the above with a duplicated\npoint:",
            "markdown"
        ],
        [
            "points = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n tri = Delaunay(points)\n np.unique(tri.simplices.ravel())\narray([0, 1, 2, 3], dtype=int32)",
            "code"
        ],
        [
            "Observe that point #4, which is a duplicate, does not occur as a\nvertex of the triangulation. That this happened is recorded:",
            "markdown"
        ],
        [
            "tri.coplanar\narray([[4, 0, 3]], dtype=int32)",
            "code"
        ],
        [
            "This means that point 4 resides near triangle 0 and vertex 3, but is\nnot included in the triangulation.",
            "markdown"
        ],
        [
            "Note that such degeneracies can occur not only because of duplicated\npoints, but also for more complicated geometrical reasons, even in\npoint sets that at first sight seem well-behaved.",
            "markdown"
        ],
        [
            "However, Qhull has the \u00e2\u0080\u009cQJ\u00e2\u0080\u009d option, which instructs it to perturb the\ninput data randomly until degeneracies are resolved:",
            "markdown"
        ],
        [
            "tri = Delaunay(points, qhull_options=\"QJ Pp\")\n points[tri.simplices]\narray([[[1, 0],\n        [1, 1],\n        [0, 0]],\n       [[1, 1],\n        [1, 1],\n        [1, 0]],\n       [[1, 1],\n        [0, 1],\n        [0, 0]],\n       [[0, 1],\n        [1, 1],\n        [1, 1]]])",
            "code"
        ],
        [
            "Two new triangles appeared. However, we see that they are degenerate\nand have zero area.",
            "markdown"
        ]
    ],
    "scipy->Spatial data structures and algorithms (scipy.spatial)->Convex hulls": [
        [
            "A convex hull is the smallest convex object containing all points in a\ngiven point set.",
            "markdown"
        ],
        [
            "These can be computed via the Qhull wrappers in scipy.spatial as\nfollows:",
            "markdown"
        ],
        [
            "from scipy.spatial import ConvexHull\n rng = np.random.default_rng()\n points = rng.random((30, 2))   # 30 random points in 2-D\n hull = ConvexHull(points)",
            "code"
        ],
        [
            "The convex hull is represented as a set of N 1-D simplices,\nwhich in 2-D means line segments. The storage scheme is exactly the\nsame as for the simplices in the Delaunay triangulation discussed\nabove.",
            "markdown"
        ],
        [
            "We can illustrate the above result:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n plt.plot(points[:,0], points[:,1], 'o')\n for simplex in hull.simplices:\n...     plt.plot(points[simplex,0], points[simplex,1], 'k-')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with a few dozen random blue markers randomly distributed throughout. A single black line forms a convex hull around the boundary of the markers.\"' class=\"plot-directive\" src=\"../_images/spatial-2.png\"/>\n</figure>",
            "code"
        ],
        [
            "The same can be achieved with scipy.spatial.convex_hull_plot_2d.",
            "markdown"
        ]
    ],
    "scipy->Spatial data structures and algorithms (scipy.spatial)->Voronoi diagrams": [
        [
            "A Voronoi diagram is a subdivision of the space into the nearest\nneighborhoods of a given set of points.",
            "markdown"
        ],
        [
            "There are two ways to approach this object using scipy.spatial.\nFirst, one can use the KDTree to answer the question \u00e2\u0080\u009cwhich of the\npoints is closest to this one\u00e2\u0080\u009d, and define the regions that way:",
            "markdown"
        ],
        [
            "from scipy.spatial import KDTree\n points = np.array([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2],\n...                    [2, 0], [2, 1], [2, 2]])\n tree = KDTree(points)\n tree.query([0.1, 0.1])\n(0.14142135623730953, 0)",
            "code"
        ],
        [
            "So the point (0.1, 0.1) belongs to region 0. In color:",
            "markdown"
        ],
        [
            "x = np.linspace(-0.5, 2.5, 31)\n y = np.linspace(-0.5, 2.5, 33)\n xx, yy = np.meshgrid(x, y)\n xy = np.c_[xx.ravel(), yy.ravel()]\n import matplotlib.pyplot as plt\n dx_half, dy_half = np.diff(x[:2])[0] / 2., np.diff(y[:2])[0] / 2.\n x_edges = np.concatenate((x - dx_half, [x[-1] + dx_half]))\n y_edges = np.concatenate((y - dy_half, [y[-1] + dy_half]))\n plt.pcolormesh(x_edges, y_edges, tree.query(xy)[1].reshape(33, 31), shading='flat')\n plt.plot(points[:,0], points[:,1], 'ko')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/spatial-3_00_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "This does not, however, give the Voronoi diagram as a geometrical\nobject.",
            "markdown"
        ],
        [
            "The representation in terms of lines and points can be again\nobtained via the Qhull wrappers in scipy.spatial:",
            "markdown"
        ],
        [
            "from scipy.spatial import Voronoi\n vor = Voronoi(points)\n vor.vertices\narray([[0.5, 0.5],\n       [0.5, 1.5],\n       [1.5, 0.5],\n       [1.5, 1.5]])",
            "code"
        ],
        [
            "The Voronoi vertices denote the set of points forming the polygonal\nedges of the Voronoi regions. In this case, there are 9 different\nregions:",
            "markdown"
        ],
        [
            "vor.regions\n[[], [-1, 0], [-1, 1], [1, -1, 0], [3, -1, 2], [-1, 3], [-1, 2], [0, 1, 3, 2], [2, -1, 0], [3, -1, 1]]",
            "code"
        ],
        [
            "Negative value -1 again indicates a point at infinity. Indeed,\nonly one of the regions, [0, 1, 3, 2], is bounded. Note here that\ndue to similar numerical precision issues as in Delaunay triangulation\nabove, there may be fewer Voronoi regions than input points.",
            "markdown"
        ],
        [
            "The ridges (lines in 2-D) separating the regions are described as a\nsimilar collection of simplices as the convex hull pieces:",
            "markdown"
        ],
        [
            "vor.ridge_vertices\n[[-1, 0], [-1, 0], [-1, 1], [-1, 1], [0, 1], [-1, 3], [-1, 2], [2, 3], [-1, 3], [-1, 2], [1, 3], [0, 2]]",
            "code"
        ],
        [
            "These numbers present the indices of the Voronoi vertices making up the\nline segments. -1 is again a point at infinity \u00e2\u0080\u0094 only 4 of\nthe 12 lines are a bounded line segment, while others extend to\ninfinity.",
            "markdown"
        ],
        [
            "The Voronoi ridges are perpendicular to the lines drawn between the\ninput points. To which two points each ridge corresponds is also\nrecorded:",
            "markdown"
        ],
        [
            "vor.ridge_points\narray([[0, 3],\n       [0, 1],\n       [2, 5],\n       [2, 1],\n       [1, 4],\n       [7, 8],\n       [7, 6],\n       [7, 4],\n       [8, 5],\n       [6, 3],\n       [4, 5],\n       [4, 3]], dtype=int32)",
            "code"
        ],
        [
            "This information, taken together, is enough to construct the full\ndiagram.",
            "markdown"
        ],
        [
            "We can plot it as follows. First, the points and the Voronoi vertices:",
            "markdown"
        ],
        [
            "plt.plot(points[:, 0], points[:, 1], 'o')\n plt.plot(vor.vertices[:, 0], vor.vertices[:, 1], '*')\n plt.xlim(-1, 3); plt.ylim(-1, 3)",
            "code"
        ],
        [
            "Plotting the finite line segments goes as for the convex hull,\nbut now we have to guard for the infinite edges:",
            "markdown"
        ],
        [
            "for simplex in vor.ridge_vertices:\n...     simplex = np.asarray(simplex)\n...     if np.all(simplex = 0):\n...         plt.plot(vor.vertices[simplex, 0], vor.vertices[simplex, 1], 'k-')",
            "code"
        ],
        [
            "The ridges extending to infinity require a bit more care:",
            "markdown"
        ],
        [
            "center = points.mean(axis=0)\n for pointidx, simplex in zip(vor.ridge_points, vor.ridge_vertices):\n...     simplex = np.asarray(simplex)\n...     if np.any(simplex &lt; 0):\n...         i = simplex[simplex = 0][0] # finite end Voronoi vertex\n...         t = points[pointidx[1]] - points[pointidx[0]]  # tangent\n...         t = t / np.linalg.norm(t)\n...         n = np.array([-t[1], t[0]]) # normal\n...         midpoint = points[pointidx].mean(axis=0)\n...         far_point = vor.vertices[i] + np.sign(np.dot(midpoint - center, n)) * n * 100\n...         plt.plot([vor.vertices[i,0], far_point[0]],\n...                  [vor.vertices[i,1], far_point[1]], 'k--')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/spatial-3_01_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "This plot can also be created using scipy.spatial.voronoi_plot_2d.",
            "markdown"
        ],
        [
            "Voronoi diagrams can be used to create interesting generative art.  Try playing\nwith the settings of this mandala function to create your own!",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import spatial\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "def mandala(n_iter, n_points, radius):\n...     \"\"\"Creates a mandala figure using Voronoi tesselations.\n...\n...     Parameters\n...     ----------\n...     n_iter : int\n...         Number of iterations, i.e. how many times the equidistant points will\n...         be generated.\n...     n_points : int\n...         Number of points to draw per iteration.\n...     radius : scalar\n...         The radial expansion factor.\n...\n...     Returns\n...     -------\n...     fig : matplotlib.Figure instance\n...\n...     Notes\n...     -----\n...     This code is adapted from the work of Audrey Roy Greenfeld [1]_ and Carlos\n...     Focil-Espinosa [2]_, who created beautiful mandalas with Python code.  That\n...     code in turn was based on Antonio S\u00c3\u00a1nchez Chinch\u00c3\u00b3n's R code [3]_.\n...\n...     References\n...     ----------\n...     .. [1] https://www.codemakesmehappy.com/2019/09/voronoi-mandalas.html\n...\n...     .. [2] https://github.com/CarlosFocil/mandalapy\n...\n...     .. [3] https://github.com/aschinchon/mandalas\n...\n...     \"\"\"\n...     fig = plt.figure(figsize=(10, 10))\n...     ax = fig.add_subplot(111)\n...     ax.set_axis_off()\n...     ax.set_aspect('equal', adjustable='box')\n...\n...     angles = np.linspace(0, 2*np.pi * (1 - 1/n_points), num=n_points) + np.pi/2\n...     # Starting from a single center point, add points iteratively\n...     xy = np.array([[0, 0]])\n...     for k in range(n_iter):\n...         t1 = np.array([])\n...         t2 = np.array([])\n...         # Add `n_points` new points around each existing point in this iteration\n...         for i in range(xy.shape[0]):\n...             t1 = np.append(t1, xy[i, 0] + radius**k * np.cos(angles))\n...             t2 = np.append(t2, xy[i, 1] + radius**k * np.sin(angles))\n...\n...         xy = np.column_stack((t1, t2))\n...\n...     # Create the Mandala figure via a Voronoi plot\n...     spatial.voronoi_plot_2d(spatial.Voronoi(xy), ax=ax)\n...\n...     return fig",
            "code"
        ],
        [
            "# Modify the following parameters in order to get different figures\n n_iter = 3\n n_points = 6\n radius = 4",
            "code"
        ],
        [
            "fig = mandala(n_iter, n_points, radius)\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/spatial-4.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Introduction": [
        [
            "In this tutorial, we discuss many, but certainly not all, features of\nscipy.stats. The intention here is to provide a user with a\nworking knowledge of this package. We refer to the\nreference manual for further details.",
            "markdown"
        ],
        [
            "Note: This documentation is work in progress.\n\n\nDiscrete Statistical Distributions\nContinuous Statistical Distributions\nUniversal Non-Uniform Random Number Sampling in SciPy\nResampling and Monte Carlo Methods",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Random variables": [
        [
            "There are two general distribution classes that have been implemented\nfor encapsulating continuous random variables and discrete random variables. Over 80 continuous random variables\n(RVs) and 10 discrete random variables have been implemented using\nthese classes. Besides this, new routines and distributions can be\neasily added by the end user. (If you create one, please contribute it.)",
            "markdown"
        ],
        [
            "All of the statistics functions are located in the sub-package\nscipy.stats and a fairly complete listing of these functions\ncan be obtained using info(stats). The list of the random\nvariables available can also be obtained from the docstring for the\nstats sub-package.",
            "markdown"
        ],
        [
            "In the discussion below, we mostly focus on continuous RVs. Nearly everything\nalso applies to discrete variables, but we point out some differences\nhere: Specific points for discrete distributions.",
            "markdown"
        ],
        [
            "In the code samples below, we assume that the scipy.stats package\nis imported as",
            "markdown"
        ],
        [
            "from scipy import stats",
            "code"
        ],
        [
            "and in some cases we assume that individual objects are imported as",
            "markdown"
        ],
        [
            "from scipy.stats import norm",
            "code"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Random variables->Getting help": [
        [
            "First of all, all distributions are accompanied with help\nfunctions. To obtain just some basic information, we print the relevant\ndocstring: print(stats.norm.__doc__).",
            "markdown"
        ],
        [
            "To find the support, i.e., upper and lower bounds of the distribution,\ncall:",
            "markdown"
        ],
        [
            "print('bounds of distribution lower: %s, upper: %s' % norm.support())\nbounds of distribution lower: -inf, upper: inf",
            "code"
        ],
        [
            "We can list all methods and properties of the distribution with\ndir(norm). As it turns out, some of the methods are private,\nalthough they are not named as such (their names do not start\nwith a leading underscore), for example veccdf, are only available\nfor internal calculation (those methods will give warnings when one tries to\nuse them, and will be removed at some point).",
            "markdown"
        ],
        [
            "To obtain the real main methods, we list the methods of the frozen\ndistribution. (We explain the meaning of a <em class=\"xref py py-obj\">frozen distribution\nbelow).",
            "markdown"
        ],
        [
            "rv = norm()\n dir(rv)  # reformatted\n['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__',\n '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__',\n '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__',\n '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__',\n '__str__', '__subclasshook__', '__weakref__', 'a', 'args', 'b', 'cdf',\n 'dist', 'entropy', 'expect', 'interval', 'isf', 'kwds', 'logcdf',\n 'logpdf', 'logpmf', 'logsf', 'mean', 'median', 'moment', 'pdf', 'pmf',\n 'ppf', 'random_state', 'rvs', 'sf', 'stats', 'std', 'var']",
            "code"
        ],
        [
            "Finally, we can obtain the list of available distribution through\nintrospection:",
            "markdown"
        ],
        [
            "dist_continu = [d for d in dir(stats) if\n...                 isinstance(getattr(stats, d), stats.rv_continuous)]\n dist_discrete = [d for d in dir(stats) if\n...                  isinstance(getattr(stats, d), stats.rv_discrete)]\n print('number of continuous distributions: %d' % len(dist_continu))\nnumber of continuous distributions: 107\n print('number of discrete distributions:   %d' % len(dist_discrete))\nnumber of discrete distributions:   19",
            "code"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Random variables->Common methods": [
        [
            "The main public methods for continuous  RVs are:",
            "markdown"
        ],
        [
            "rvs:   Random Variates",
            "markdown"
        ],
        [
            "pdf:   Probability Density Function",
            "markdown"
        ],
        [
            "cdf:   Cumulative Distribution Function",
            "markdown"
        ],
        [
            "sf:    Survival Function (1-CDF)",
            "markdown"
        ],
        [
            "ppf:   Percent Point Function (Inverse of CDF)",
            "markdown"
        ],
        [
            "isf:   Inverse Survival Function (Inverse of SF)",
            "markdown"
        ],
        [
            "stats: Return mean, variance, (Fisher\u00e2\u0080\u0099s) skew, or (Fisher\u00e2\u0080\u0099s) kurtosis",
            "markdown"
        ],
        [
            "moment: non-central moments of the distribution",
            "markdown"
        ],
        [
            "Let\u00e2\u0080\u0099s take a normal RV as an example.",
            "markdown"
        ],
        [
            "norm.cdf(0)\n0.5",
            "code"
        ],
        [
            "To compute the cdf at a number of points, we can pass a list or a numpy array.",
            "markdown"
        ],
        [
            "norm.cdf([-1., 0, 1])\narray([ 0.15865525,  0.5,  0.84134475])\n import numpy as np\n norm.cdf(np.array([-1., 0, 1]))\narray([ 0.15865525,  0.5,  0.84134475])",
            "code"
        ],
        [
            "Thus, the basic methods, such as <em class=\"xref py py-obj\">pdf, <em class=\"xref py py-obj\">cdf, and so on, are vectorized.",
            "markdown"
        ],
        [
            "Other generally useful methods are supported too:",
            "markdown"
        ],
        [
            "norm.mean(), norm.std(), norm.var()\n(0.0, 1.0, 1.0)\n norm.stats(moments=\"mv\")\n(array(0.0), array(1.0))",
            "code"
        ],
        [
            "To find the median of a distribution, we can use the percent point\nfunction ppf, which is the inverse of the cdf:",
            "markdown"
        ],
        [
            "norm.ppf(0.5)\n0.0",
            "code"
        ],
        [
            "To generate a sequence of random variates, use the size keyword\nargument:",
            "markdown"
        ],
        [
            "norm.rvs(size=3)\narray([-0.35687759,  1.34347647, -0.11710531])   # random",
            "code"
        ],
        [
            "Don\u00e2\u0080\u0099t think that norm.rvs(5) generates 5 variates:",
            "markdown"
        ],
        [
            "norm.rvs(5)\n5.471435163732493  # random",
            "code"
        ],
        [
            "Here, 5 with no keyword is being interpreted as the first possible\nkeyword argument, loc, which is the first of a pair of keyword arguments\ntaken by all continuous distributions.\nThis brings us to the topic of the next subsection.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Random variables->Random number generation": [
        [
            "Drawing random numbers relies on generators from numpy.random package.\nIn the examples above, the specific stream of\nrandom numbers is not reproducible across runs. To achieve reproducibility,\nyou can explicitly seed a random number generator. In NumPy, a generator\nis an instance of numpy.random.Generator. Here is the canonical way to create\na generator:",
            "markdown"
        ],
        [
            "from numpy.random import default_rng\n rng = default_rng()",
            "code"
        ],
        [
            "And fixing the seed can be done like this:",
            "markdown"
        ],
        [
            "# do NOT copy this value\n rng = default_rng(301439351238479871608357552876690613766)",
            "code"
        ],
        [
            "Warning",
            "markdown"
        ],
        [
            "Do not use this number or common values such as 0. Using just a\nsmall set of seeds to instantiate larger state spaces means that\nthere are some initial states that are impossible to reach. This\ncreates some biases if everyone uses such values. A good way to\nget a seed is to use a numpy.random.SeedSequence:",
            "markdown"
        ],
        [
            "from numpy.random import SeedSequence\n print(SeedSequence().entropy)\n301439351238479871608357552876690613766  # random",
            "code"
        ],
        [
            "The <em class=\"xref py py-obj\">random_state parameter in distributions accepts an instance of\nnumpy.random.Generator class, or an integer, which is then used to\nseed an internal Generator object:",
            "markdown"
        ],
        [
            "norm.rvs(size=5, random_state=rng)\narray([ 0.47143516, -1.19097569,  1.43270697, -0.3126519 , -0.72058873])  # random",
            "code"
        ],
        [
            "For further info, see NumPy\u00e2\u0080\u0099s documentation.",
            "markdown"
        ],
        [
            "To learn more about the random number samplers implemented in SciPy, see\nnon-uniform random number sampling tutorial and quasi monte carlo tutorial",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Random variables->Shifting and scaling": [
        [
            "All continuous distributions take loc and scale as keyword\nparameters to adjust the location and scale of the distribution,\ne.g., for the standard normal distribution, the location is the mean and\nthe scale is the standard deviation.",
            "markdown"
        ],
        [
            "norm.stats(loc=3, scale=4, moments=\"mv\")\n(array(3.0), array(16.0))",
            "code"
        ],
        [
            "In many cases, the standardized distribution for a random variable X\nis obtained through the transformation (X - loc) / scale. The\ndefault values are loc = 0 and scale = 1.",
            "markdown"
        ],
        [
            "Smart use of loc and scale can help modify the standard\ndistributions in many ways. To illustrate the scaling further, the\ncdf of an exponentially distributed RV with mean \\(1/\\lambda\\)\nis given by\n\n\\[F(x) = 1 - \\exp(-\\lambda x)\\]",
            "markdown"
        ],
        [
            "By applying the scaling rule above, it can be seen that by\ntaking scale\u00a0 = 1./lambda we get the proper scale.",
            "markdown"
        ],
        [
            "from scipy.stats import expon\n expon.mean(scale=3.)\n3.0",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Distributions that take shape parameters may\nrequire more than simple application of loc and/or\nscale to achieve the desired form. For example, the\ndistribution of 2-D vector lengths given a constant vector\nof length \\(R\\) perturbed by independent N(0, \\(\\sigma^2\\))\ndeviations in each component is\nrice(\\(R/\\sigma\\), scale= \\(\\sigma\\)). The first argument\nis a shape parameter that needs to be scaled along with \\(x\\).",
            "markdown"
        ],
        [
            "The uniform distribution is also interesting:",
            "markdown"
        ],
        [
            "from scipy.stats import uniform\n uniform.cdf([0, 1, 2, 3, 4, 5], loc=1, scale=4)\narray([ 0.  ,  0.  ,  0.25,  0.5 ,  0.75,  1.  ])",
            "code"
        ],
        [
            "Finally, recall from the previous paragraph that we are left with the\nproblem of the meaning of norm.rvs(5). As it turns out, calling a\ndistribution like this, the first argument, i.e., the 5, gets passed\nto set the loc parameter. Let\u00e2\u0080\u0099s see:",
            "markdown"
        ],
        [
            "np.mean(norm.rvs(5, size=500))\n5.0098355106969992  # random",
            "code"
        ],
        [
            "Thus, to explain the output of the example of the last section:\nnorm.rvs(5) generates a single normally distributed random variate with\nmean loc=5, because of the default size=1.",
            "markdown"
        ],
        [
            "We recommend that you set loc and scale parameters explicitly, by\npassing the values as keywords rather than as arguments. Repetition\ncan be minimized when calling more than one method of a given RV by\nusing the technique of Freezing a Distribution, as explained below.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Random variables->Shape parameters": [
        [
            "While a general continuous random variable can be shifted and scaled\nwith the loc and scale parameters, some distributions require\nadditional shape parameters. For instance, the gamma distribution with density\n\n\\[\\gamma(x, a) = \\frac{\\lambda (\\lambda x)^{a-1}}{\\Gamma(a)} e^{-\\lambda x}\\;,\\]",
            "markdown"
        ],
        [
            "requires the shape parameter \\(a\\). Observe that setting\n\\(\\lambda\\) can be obtained by setting the scale keyword to\n\\(1/\\lambda\\).",
            "markdown"
        ],
        [
            "Let\u00e2\u0080\u0099s check the number and name of the shape parameters of the gamma\ndistribution. (We know from the above that this should be 1.)",
            "markdown"
        ],
        [
            "from scipy.stats import gamma\n gamma.numargs\n1\n gamma.shapes\n'a'",
            "code"
        ],
        [
            "Now, we set the value of the shape variable to 1 to obtain the\nexponential distribution, so that we compare easily whether we get the\nresults we expect.",
            "markdown"
        ],
        [
            "gamma(1, scale=2.).stats(moments=\"mv\")\n(array(2.0), array(4.0))",
            "code"
        ],
        [
            "Notice that we can also specify shape parameters as keywords:",
            "markdown"
        ],
        [
            "gamma(a=1, scale=2.).stats(moments=\"mv\")\n(array(2.0), array(4.0))",
            "code"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Random variables->Freezing a distribution": [
        [
            "Passing the loc and scale keywords time and again can become\nquite bothersome. The concept of <em class=\"xref py py-obj\">freezing a RV is used to\nsolve such problems.",
            "markdown"
        ],
        [
            "rv = gamma(1, scale=2.)",
            "code"
        ],
        [
            "By using rv we no longer have to include the scale or the shape\nparameters anymore. Thus, distributions can be used in one of two\nways, either by passing all distribution parameters to each method\ncall (such as we did earlier) or by freezing the parameters for the\ninstance of the distribution. Let us check this:",
            "markdown"
        ],
        [
            "rv.mean(), rv.std()\n(2.0, 2.0)",
            "code"
        ],
        [
            "This is, indeed, what we should get.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Random variables->Broadcasting": [
        [
            "The basic methods pdf, and so on, satisfy the usual numpy broadcasting rules. For\nexample, we can calculate the critical values for the upper tail of\nthe t distribution for different probabilities and degrees of freedom.",
            "markdown"
        ],
        [
            "stats.t.isf([0.1, 0.05, 0.01], [[10], [11]])\narray([[ 1.37218364,  1.81246112,  2.76376946],\n       [ 1.36343032,  1.79588482,  2.71807918]])",
            "code"
        ],
        [
            "Here, the first row contains the critical values for 10 degrees of freedom\nand the second row for 11 degrees of freedom (d.o.f.). Thus, the\nbroadcasting rules give the same result of calling isf twice:",
            "markdown"
        ],
        [
            "stats.t.isf([0.1, 0.05, 0.01], 10)\narray([ 1.37218364,  1.81246112,  2.76376946])\n stats.t.isf([0.1, 0.05, 0.01], 11)\narray([ 1.36343032,  1.79588482,  2.71807918])",
            "code"
        ],
        [
            "If the array with probabilities, i.e., [0.1, 0.05, 0.01] and the\narray of degrees of freedom i.e., [10, 11, 12], have the same\narray shape, then element-wise matching is used. As an example, we can\nobtain the 10% tail for 10 d.o.f., the 5% tail for 11 d.o.f. and the\n1% tail for 12 d.o.f. by calling",
            "markdown"
        ],
        [
            "stats.t.isf([0.1, 0.05, 0.01], [10, 11, 12])\narray([ 1.37218364,  1.79588482,  2.68099799])",
            "code"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Random variables->Specific points for discrete distributions": [
        [
            "Discrete distributions have mostly the same basic methods as the\ncontinuous distributions. However pdf is replaced by the probability\nmass function pmf, no estimation methods, such as fit, are\navailable, and scale is not a valid keyword parameter. The\nlocation parameter, keyword loc, can still be used to shift the\ndistribution.",
            "markdown"
        ],
        [
            "The computation of the cdf requires some extra attention. In the case\nof continuous distribution, the cumulative distribution function is, in\nmost standard cases, strictly monotonic increasing in the bounds (a,b)\nand has, therefore, a unique inverse. The cdf of a discrete\ndistribution, however, is a step function, hence the inverse cdf,\ni.e., the percent point function, requires a different definition:",
            "markdown"
        ],
        [
            "ppf(q) = min{x : cdf(x) = q, x integer}",
            "code"
        ],
        [
            "For further info, see the docs here.",
            "markdown"
        ],
        [
            "We can look at the hypergeometric distribution as an example",
            "markdown"
        ],
        [
            "from scipy.stats import hypergeom\n [M, n, N] = [20, 7, 12]",
            "code"
        ],
        [
            "If we use the cdf at some integer points and then evaluate the ppf at those\ncdf values, we get the initial integers back, for example",
            "markdown"
        ],
        [
            "x = np.arange(4) * 2\n x\narray([0, 2, 4, 6])\n prb = hypergeom.cdf(x, M, n, N)\n prb\narray([  1.03199174e-04,   5.21155831e-02,   6.08359133e-01,\n         9.89783282e-01])\n hypergeom.ppf(prb, M, n, N)\narray([ 0.,  2.,  4.,  6.])",
            "code"
        ],
        [
            "If we use values that are not at the kinks of the cdf step function, we get\nthe next higher integer back:",
            "markdown"
        ],
        [
            "hypergeom.ppf(prb + 1e-8, M, n, N)\narray([ 1.,  3.,  5.,  7.])\n hypergeom.ppf(prb - 1e-8, M, n, N)\narray([ 0.,  2.,  4.,  6.])",
            "code"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Random variables->Fitting distributions": [
        [
            "The main additional methods of the not frozen distribution are related\nto the estimation of distribution parameters:\n\n\nfit:   maximum likelihood estimation of distribution parameters, including location",
            "markdown"
        ],
        [
            "and scale\n\n</dl>",
            "markdown"
        ],
        [
            "fit_loc_scale: estimation of location and scale when shape parameters are given",
            "markdown"
        ],
        [
            "nnlf:  negative log likelihood function",
            "markdown"
        ],
        [
            "expect: calculate the expectation of a function against the pdf or pmf",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Random variables->Performance issues and cautionary remarks": [
        [
            "The performance of the individual methods, in terms of speed, varies\nwidely by distribution and method. The results of a method are\nobtained in one of two ways: either by explicit calculation, or by a\ngeneric algorithm that is independent of the specific distribution.",
            "markdown"
        ],
        [
            "Explicit calculation, on the one hand, requires that the method is\ndirectly specified for the given distribution, either through analytic\nformulas or through special functions in scipy.special or\nnumpy.random for rvs. These are usually relatively fast\ncalculations.",
            "markdown"
        ],
        [
            "The generic methods, on the other hand, are used if the distribution\ndoes not specify any explicit calculation. To define a distribution,\nonly one of pdf or cdf is necessary; all other methods can be derived\nusing numeric integration and root finding. However, these indirect\nmethods can be <em class=\"xref py py-obj\">very slow. As an example, rgh =\nstats.gausshyper.rvs(0.5, 2, 2, 2, size=100) creates random\nvariables in a very indirect way and takes about 19 seconds for 100\nrandom variables on my computer, while one million random variables\nfrom the standard normal or from the t distribution take just above\none second.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Random variables->Remaining issues": [
        [
            "The distributions in scipy.stats have recently been corrected and improved\nand gained a considerable test suite; however, a few issues remain:",
            "markdown"
        ],
        [
            "The distributions have been tested over some range of parameters;\nhowever, in some corner ranges, a few incorrect results may remain.",
            "markdown"
        ],
        [
            "The maximum likelihood estimation in <em class=\"xref py py-obj\">fit does not work with\ndefault starting parameters for all distributions and the user\nneeds to supply good starting parameters. Also, for some\ndistribution using a maximum likelihood estimator might\ninherently not be the best choice.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Building specific distributions": [
        [
            "The next examples shows how to build your own distributions. Further\nexamples show the usage of the distributions and some statistical\ntests.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Building specific distributions->Making a continuous distribution, i.e., subclassing rv_continuous": [
        [
            "Making continuous distributions is fairly simple.",
            "markdown"
        ],
        [
            "from scipy import stats\n class deterministic_gen(stats.rv_continuous):\n...     def _cdf(self, x):\n...         return np.where(x &lt; 0, 0., 1.)\n...     def _stats(self):\n...         return 0., 0., 0., 0.",
            "code"
        ],
        [
            "deterministic = deterministic_gen(name=\"deterministic\")\n deterministic.cdf(np.arange(-3, 3, 0.5))\narray([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.])",
            "code"
        ],
        [
            "Interestingly,  the pdf is now computed automatically:",
            "markdown"
        ],
        [
            "deterministic.pdf(np.arange(-3, 3, 0.5))\narray([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         5.83333333e+04,   4.16333634e-12,   4.16333634e-12,\n         4.16333634e-12,   4.16333634e-12,   4.16333634e-12])",
            "code"
        ],
        [
            "Be aware of the performance issues mentioned in\nPerformance issues and cautionary remarks. The computation of unspecified\ncommon methods can become very slow, since only general methods are\ncalled, which, by their very nature, cannot use any specific\ninformation about the distribution. Thus, as a cautionary example:",
            "markdown"
        ],
        [
            "from scipy.integrate import quad\n quad(deterministic.pdf, -1e-1, 1e-1)\n(4.163336342344337e-13, 0.0)",
            "code"
        ],
        [
            "But this is not correct: the integral over this pdf should be 1. Let\u00e2\u0080\u0099s make the\nintegration interval smaller:",
            "markdown"
        ],
        [
            "quad(deterministic.pdf, -1e-3, 1e-3)  # warning removed\n(1.000076872229173, 0.0010625571718182458)",
            "code"
        ],
        [
            "This looks better. However, the problem originated from the fact that\nthe pdf is not specified in the class definition of the deterministic\ndistribution.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Building specific distributions->Subclassing rv_discrete": [
        [
            "In the following, we use stats.rv_discrete to generate a discrete\ndistribution that has the probabilities of the truncated normal for the\nintervals centered around the integers.",
            "markdown"
        ],
        [
            "<strong>General info</strong>",
            "markdown"
        ],
        [
            "From the docstring of rv_discrete, help(stats.rv_discrete),",
            "markdown"
        ],
        [
            "\u00e2\u0080\u009cYou can construct an arbitrary discrete rv where P{X=xk} = pk by\npassing to the rv_discrete initialization method (through the values=\nkeyword) a tuple of sequences (xk, pk) which describes only those\nvalues of X (xk) that occur with nonzero probability (pk).\u00e2\u0080\u009d\n</blockquote>",
            "markdown"
        ],
        [
            "Next to this, there are some further requirements for this approach to\nwork:",
            "markdown"
        ],
        [
            "The keyword <em class=\"xref py py-obj\">name is required.",
            "markdown"
        ],
        [
            "The support points of the distribution xk have to be integers.",
            "markdown"
        ],
        [
            "The number of significant digits (decimals) needs to be specified.",
            "markdown"
        ],
        [
            "In fact, if the last two requirements are not satisfied, an exception\nmay be raised or the resulting numbers may be incorrect.",
            "markdown"
        ],
        [
            "<strong>An example</strong>",
            "markdown"
        ],
        [
            "Let\u00e2\u0080\u0099s do the work. First:",
            "markdown"
        ],
        [
            "npoints = 20   # number of integer support points of the distribution minus 1\n npointsh = npoints // 2\n npointsf = float(npoints)\n nbound = 4   # bounds for the truncated normal\n normbound = (1+1/npointsf) * nbound   # actual bounds of truncated normal\n grid = np.arange(-npointsh, npointsh+2, 1)   # integer grid\n gridlimitsnorm = (grid-0.5) / npointsh * nbound   # bin limits for the truncnorm\n gridlimits = grid - 0.5   # used later in the analysis\n grid = grid[:-1]\n probs = np.diff(stats.truncnorm.cdf(gridlimitsnorm, -normbound, normbound))\n gridint = grid",
            "code"
        ],
        [
            "And, finally, we can subclass rv_discrete:",
            "markdown"
        ],
        [
            "normdiscrete = stats.rv_discrete(values=(gridint,\n...              np.round(probs, decimals=7)), name='normdiscrete')",
            "code"
        ],
        [
            "Now that we have defined the distribution, we have access to all\ncommon methods of discrete distributions.",
            "markdown"
        ],
        [
            "print('mean = %6.4f, variance = %6.4f, skew = %6.4f, kurtosis = %6.4f' %\n...       normdiscrete.stats(moments='mvsk'))\nmean = -0.0000, variance = 6.3302, skew = 0.0000, kurtosis = -0.0076",
            "code"
        ],
        [
            "nd_std = np.sqrt(normdiscrete.stats(moments='v'))",
            "code"
        ],
        [
            "<strong>Testing the implementation</strong>",
            "markdown"
        ],
        [
            "Let\u00e2\u0080\u0099s generate a random sample and compare observed frequencies with\nthe probabilities.",
            "markdown"
        ],
        [
            "n_sample = 500\n rvs = normdiscrete.rvs(size=n_sample)\n f, l = np.histogram(rvs, bins=gridlimits)\n sfreq = np.vstack([gridint, f, probs*n_sample]).T\n print(sfreq)\n[[-1.00000000e+01  0.00000000e+00  2.95019349e-02]  # random\n [-9.00000000e+00  0.00000000e+00  1.32294142e-01]\n [-8.00000000e+00  0.00000000e+00  5.06497902e-01]\n [-7.00000000e+00  2.00000000e+00  1.65568919e+00]\n [-6.00000000e+00  1.00000000e+00  4.62125309e+00]\n [-5.00000000e+00  9.00000000e+00  1.10137298e+01]\n [-4.00000000e+00  2.60000000e+01  2.24137683e+01]\n [-3.00000000e+00  3.70000000e+01  3.89503370e+01]\n [-2.00000000e+00  5.10000000e+01  5.78004747e+01]\n [-1.00000000e+00  7.10000000e+01  7.32455414e+01]\n [ 0.00000000e+00  7.40000000e+01  7.92618251e+01]\n [ 1.00000000e+00  8.90000000e+01  7.32455414e+01]\n [ 2.00000000e+00  5.50000000e+01  5.78004747e+01]\n [ 3.00000000e+00  5.00000000e+01  3.89503370e+01]\n [ 4.00000000e+00  1.70000000e+01  2.24137683e+01]\n [ 5.00000000e+00  1.10000000e+01  1.10137298e+01]\n [ 6.00000000e+00  4.00000000e+00  4.62125309e+00]\n [ 7.00000000e+00  3.00000000e+00  1.65568919e+00]\n [ 8.00000000e+00  0.00000000e+00  5.06497902e-01]\n [ 9.00000000e+00  0.00000000e+00  1.32294142e-01]\n [ 1.00000000e+01  0.00000000e+00  2.95019349e-02]]\n\n\n<figure class=\"align-center\">\n<img alt='\"An X-Y histogram plot showing the distribution of random variates. A blue trace shows a normal bell curve. A blue bar chart perfectly approximates the curve showing the true distribution. A red bar chart representing the sample is well described by the blue trace but not exact.\"' class=\"plot-directive\" src=\"../_images/normdiscr_plot1.png\"/>\n</figure>\n<figure class=\"align-center\">\n<img alt='\"An X-Y histogram plot showing the cumulative distribution of random variates. A blue trace shows a CDF for a typical normal distribution. A blue bar chart perfectly approximates the curve showing the true distribution. A red bar chart representing the sample is well described by the blue trace but not exact.\"' class=\"plot-directive\" src=\"../_images/normdiscr_plot2.png\"/>\n</figure>",
            "code"
        ],
        [
            "Next, we can test whether our sample was generated by our norm-discrete\ndistribution. This also verifies whether the random numbers were generated\ncorrectly.",
            "markdown"
        ],
        [
            "The chisquare test requires that there are a minimum number of observations\nin each bin. We combine the tail bins into larger bins so that they contain\nenough observations.",
            "markdown"
        ],
        [
            "f2 = np.hstack([f[:5].sum(), f[5:-5], f[-5:].sum()])\n p2 = np.hstack([probs[:5].sum(), probs[5:-5], probs[-5:].sum()])\n ch2, pval = stats.chisquare(f2, p2*n_sample)",
            "code"
        ],
        [
            "print('chisquare for normdiscrete: chi2 = %6.3f pvalue = %6.4f' % (ch2, pval))\nchisquare for normdiscrete: chi2 = 12.466 pvalue = 0.4090  # random",
            "code"
        ],
        [
            "The pvalue in this case is high, so we can be quite confident that\nour random sample was actually generated by the distribution.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Analysing one sample": [
        [
            "First, we create some random variables. We set a seed so that in each run\nwe get identical results to look at. As an example we take a sample from\nthe Student t distribution:",
            "markdown"
        ],
        [
            "x = stats.t.rvs(10, size=1000)",
            "code"
        ],
        [
            "Here, we set the required shape parameter of the t distribution, which\nin statistics corresponds to the degrees of freedom, to 10. Using size=1000 means\nthat our sample consists of 1000 independently drawn (pseudo) random numbers.\nSince we did not specify the keyword arguments <em class=\"xref py py-obj\">loc and <em class=\"xref py py-obj\">scale, those are\nset to their default values zero and one.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Analysing one sample->Descriptive statistics": [
        [
            "<em class=\"xref py py-obj\">x is a numpy array, and we have direct access to all array methods, e.g.,",
            "markdown"
        ],
        [
            "print(x.min())   # equivalent to np.min(x)\n-3.78975572422  # random\n print(x.max())   # equivalent to np.max(x)\n5.26327732981  # random\n print(x.mean())  # equivalent to np.mean(x)\n0.0140610663985  # random\n print(x.var())   # equivalent to np.var(x))\n1.28899386208  # random",
            "code"
        ],
        [
            "How do the sample properties compare to their theoretical counterparts?",
            "markdown"
        ],
        [
            "m, v, s, k = stats.t.stats(10, moments='mvsk')\n n, (smin, smax), sm, sv, ss, sk = stats.describe(x)",
            "code"
        ],
        [
            "sstr = '%-14s mean = %6.4f, variance = %6.4f, skew = %6.4f, kurtosis = %6.4f'\n print(sstr % ('distribution:', m, v, s ,k))\ndistribution:  mean = 0.0000, variance = 1.2500, skew = 0.0000, kurtosis = 1.0000  # random\n print(sstr % ('sample:', sm, sv, ss, sk))\nsample:        mean = 0.0141, variance = 1.2903, skew = 0.2165, kurtosis = 1.0556  # random",
            "code"
        ],
        [
            "Note: stats.describe uses the unbiased estimator for the variance, while\nnp.var is the biased estimator.",
            "markdown"
        ],
        [
            "For our sample the sample statistics differ a by a small amount from\ntheir theoretical counterparts.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Analysing one sample->T-test and KS-test": [
        [
            "We can use the t-test to test whether the mean of our sample differs\nin a statistically significant way from the theoretical expectation.",
            "markdown"
        ],
        [
            "print('t-statistic = %6.3f pvalue = %6.4f' %  stats.ttest_1samp(x, m))\nt-statistic =  0.391 pvalue = 0.6955  # random",
            "code"
        ],
        [
            "The pvalue is 0.7, this means that with an alpha error of, for\nexample, 10%, we cannot reject the hypothesis that the sample mean\nis equal to zero, the expectation of the standard t-distribution.",
            "markdown"
        ],
        [
            "As an exercise, we can calculate our ttest also directly without\nusing the provided function, which should give us the same answer,\nand so it does:",
            "markdown"
        ],
        [
            "tt = (sm-m)/np.sqrt(sv/float(n))  # t-statistic for mean\n pval = stats.t.sf(np.abs(tt), n-1)*2  # two-sided pvalue = Prob(abs(t)tt)\n print('t-statistic = %6.3f pvalue = %6.4f' % (tt, pval))\nt-statistic =  0.391 pvalue = 0.6955  # random",
            "code"
        ],
        [
            "The Kolmogorov-Smirnov test can be used to test the hypothesis that\nthe sample comes from the standard t-distribution",
            "markdown"
        ],
        [
            "print('KS-statistic D = %6.3f pvalue = %6.4f' % stats.kstest(x, 't', (10,)))\nKS-statistic D =  0.016 pvalue = 0.9571  # random",
            "code"
        ],
        [
            "Again, the p-value is high enough that we cannot reject the\nhypothesis that the random sample really is distributed according to the\nt-distribution. In real applications, we don\u00e2\u0080\u0099t know what the\nunderlying distribution is. If we perform the Kolmogorov-Smirnov\ntest of our sample against the standard normal distribution, then we\nalso cannot reject the hypothesis that our sample was generated by the\nnormal distribution given that, in this example, the p-value is almost 40%.",
            "markdown"
        ],
        [
            "print('KS-statistic D = %6.3f pvalue = %6.4f' % stats.kstest(x, 'norm'))\nKS-statistic D =  0.028 pvalue = 0.3918  # random",
            "code"
        ],
        [
            "However, the standard normal distribution has a variance of 1, while our\nsample has a variance of 1.29. If we standardize our sample and test it\nagainst the normal distribution, then the p-value is again large enough\nthat we cannot reject the hypothesis that the sample came form the\nnormal distribution.",
            "markdown"
        ],
        [
            "d, pval = stats.kstest((x-x.mean())/x.std(), 'norm')\n print('KS-statistic D = %6.3f pvalue = %6.4f' % (d, pval))\nKS-statistic D =  0.032 pvalue = 0.2397  # random",
            "code"
        ],
        [
            "Note: The Kolmogorov-Smirnov test assumes that we test against a\ndistribution with given parameters, since, in the last case, we\nestimated mean and variance, this assumption is violated and the\ndistribution of the test statistic, on which the p-value is based, is\nnot correct.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Analysing one sample->Tails of the distribution": [
        [
            "Finally, we can check the upper tail of the distribution. We can use\nthe percent point function ppf, which is the inverse of the cdf\nfunction, to obtain the critical values, or, more directly, we can use\nthe inverse of the survival function",
            "markdown"
        ],
        [
            "crit01, crit05, crit10 = stats.t.ppf([1-0.01, 1-0.05, 1-0.10], 10)\n print('critical values from ppf at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' % (crit01, crit05, crit10))\ncritical values from ppf at 1%, 5% and 10%   2.7638   1.8125   1.3722\n print('critical values from isf at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' % tuple(stats.t.isf([0.01,0.05,0.10],10)))\ncritical values from isf at 1%, 5% and 10%   2.7638   1.8125   1.3722",
            "code"
        ],
        [
            "freq01 = np.sum(xcrit01) / float(n) * 100\n freq05 = np.sum(xcrit05) / float(n) * 100\n freq10 = np.sum(xcrit10) / float(n) * 100\n print('sample %%-frequency at 1%%, 5%% and 10%% tail %8.4f %8.4f %8.4f' % (freq01, freq05, freq10))\nsample %-frequency at 1%, 5% and 10% tail   1.4000   5.8000  10.5000  # random",
            "code"
        ],
        [
            "In all three cases, our sample has more weight in the top tail than the\nunderlying distribution.\nWe can briefly check a larger sample to see if we get a closer match. In this\ncase, the empirical frequency is quite close to the theoretical probability,\nbut if we repeat this several times, the fluctuations are still pretty large.",
            "markdown"
        ],
        [
            "freq05l = np.sum(stats.t.rvs(10, size=10000)  crit05) / 10000.0 * 100\n print('larger sample %%-frequency at 5%% tail %8.4f' % freq05l)\nlarger sample %-frequency at 5% tail   4.8000  # random",
            "code"
        ],
        [
            "We can also compare it with the tail of the normal distribution, which\nhas less weight in the tails:",
            "markdown"
        ],
        [
            "print('tail prob. of normal at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' %\n...       tuple(stats.norm.sf([crit01, crit05, crit10])*100))\ntail prob. of normal at 1%, 5% and 10%   0.2857   3.4957   8.5003",
            "code"
        ],
        [
            "The chisquare test can be used to test whether for a finite number of bins,\nthe observed frequencies differ significantly from the probabilities of the\nhypothesized distribution.",
            "markdown"
        ],
        [
            "quantiles = [0.0, 0.01, 0.05, 0.1, 1-0.10, 1-0.05, 1-0.01, 1.0]\n crit = stats.t.ppf(quantiles, 10)\n crit\narray([       -inf, -2.76376946, -1.81246112, -1.37218364,  1.37218364,\n        1.81246112,  2.76376946,         inf])\n n_sample = x.size\n freqcount = np.histogram(x, bins=crit)[0]\n tprob = np.diff(quantiles)\n nprob = np.diff(stats.norm.cdf(crit))\n tch, tpval = stats.chisquare(freqcount, tprob*n_sample)\n nch, npval = stats.chisquare(freqcount, nprob*n_sample)\n print('chisquare for t:      chi2 = %6.2f pvalue = %6.4f' % (tch, tpval))\nchisquare for t:      chi2 =  2.30 pvalue = 0.8901  # random\n print('chisquare for normal: chi2 = %6.2f pvalue = %6.4f' % (nch, npval))\nchisquare for normal: chi2 = 64.60 pvalue = 0.0000  # random",
            "code"
        ],
        [
            "We see that the standard normal distribution is clearly rejected, while the\nstandard t-distribution cannot be rejected. Since the variance of our sample\ndiffers from both standard distributions, we can again redo the test taking\nthe estimate for scale and location into account.",
            "markdown"
        ],
        [
            "The fit method of the distributions can be used to estimate the parameters\nof the distribution, and the test is repeated using probabilities of the\nestimated distribution.",
            "markdown"
        ],
        [
            "tdof, tloc, tscale = stats.t.fit(x)\n nloc, nscale = stats.norm.fit(x)\n tprob = np.diff(stats.t.cdf(crit, tdof, loc=tloc, scale=tscale))\n nprob = np.diff(stats.norm.cdf(crit, loc=nloc, scale=nscale))\n tch, tpval = stats.chisquare(freqcount, tprob*n_sample)\n nch, npval = stats.chisquare(freqcount, nprob*n_sample)\n print('chisquare for t:      chi2 = %6.2f pvalue = %6.4f' % (tch, tpval))\nchisquare for t:      chi2 =  1.58 pvalue = 0.9542  # random\n print('chisquare for normal: chi2 = %6.2f pvalue = %6.4f' % (nch, npval))\nchisquare for normal: chi2 = 11.08 pvalue = 0.0858  # random",
            "code"
        ],
        [
            "Taking account of the estimated parameters, we can still reject the\nhypothesis that our sample came from a normal distribution (at the 5% level),\nbut again, with a p-value of 0.95, we cannot reject the t-distribution.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Analysing one sample->Special tests for normal distributions": [
        [
            "Since the normal distribution is the most common distribution in statistics,\nthere are several additional functions available to test whether a sample\ncould have been drawn from a normal distribution.",
            "markdown"
        ],
        [
            "First, we can test if skew and kurtosis of our sample differ significantly from\nthose of a normal distribution:",
            "markdown"
        ],
        [
            "print('normal skewtest teststat = %6.3f pvalue = %6.4f' % stats.skewtest(x))\nnormal skewtest teststat =  2.785 pvalue = 0.0054  # random\n print('normal kurtosistest teststat = %6.3f pvalue = %6.4f' % stats.kurtosistest(x))\nnormal kurtosistest teststat =  4.757 pvalue = 0.0000  # random",
            "code"
        ],
        [
            "These two tests are combined in the normality test",
            "markdown"
        ],
        [
            "print('normaltest teststat = %6.3f pvalue = %6.4f' % stats.normaltest(x))\nnormaltest teststat = 30.379 pvalue = 0.0000  # random",
            "code"
        ],
        [
            "In all three tests, the p-values are very low and we can reject the hypothesis\nthat the our sample has skew and kurtosis of the normal distribution.",
            "markdown"
        ],
        [
            "Since skew and kurtosis of our sample are based on central moments, we get\nexactly the same results if we test the standardized sample:",
            "markdown"
        ],
        [
            "print('normaltest teststat = %6.3f pvalue = %6.4f' %\n...       stats.normaltest((x-x.mean())/x.std()))\nnormaltest teststat = 30.379 pvalue = 0.0000  # random",
            "code"
        ],
        [
            "Because normality is rejected so strongly, we can check whether the\nnormaltest gives reasonable results for other cases:",
            "markdown"
        ],
        [
            "print('normaltest teststat = %6.3f pvalue = %6.4f' %\n...       stats.normaltest(stats.t.rvs(10, size=100)))\nnormaltest teststat =  4.698 pvalue = 0.0955  # random\n print('normaltest teststat = %6.3f pvalue = %6.4f' %\n...              stats.normaltest(stats.norm.rvs(size=1000)))\nnormaltest teststat =  0.613 pvalue = 0.7361  # random",
            "code"
        ],
        [
            "When testing for normality of a small sample of t-distributed observations\nand a large sample of normal-distributed observations, then in neither case\ncan we reject the null hypothesis that the sample comes from a normal\ndistribution. In the first case, this is because the test is not powerful\nenough to distinguish a t and a normally distributed random variable in a\nsmall sample.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Comparing two samples": [
        [
            "In the following, we are given two samples, which can come either from the\nsame or from different distribution, and we want to test whether these\nsamples have the same statistical properties.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Comparing two samples->Comparing means": [
        [
            "Test with sample with identical means:",
            "markdown"
        ],
        [
            "rvs1 = stats.norm.rvs(loc=5, scale=10, size=500)\n rvs2 = stats.norm.rvs(loc=5, scale=10, size=500)\n stats.ttest_ind(rvs1, rvs2)\nTtest_indResult(statistic=-0.5489036175088705, pvalue=0.5831943748663959)  # random",
            "code"
        ],
        [
            "Test with sample with different means:",
            "markdown"
        ],
        [
            "rvs3 = stats.norm.rvs(loc=8, scale=10, size=500)\n stats.ttest_ind(rvs1, rvs3)\nTtest_indResult(statistic=-4.533414290175026, pvalue=6.507128186389019e-06)  # random",
            "code"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Comparing two samples->Kolmogorov-Smirnov test for two samples ks_2samp": [
        [
            "For the example, where both samples are drawn from the same distribution,\nwe cannot reject the null hypothesis, since the pvalue is high",
            "markdown"
        ],
        [
            "stats.ks_2samp(rvs1, rvs2)\nKstestResult(statistic=0.026, pvalue=0.9959527565364388)  # random",
            "code"
        ],
        [
            "In the second example, with different location, i.e., means, we can\nreject the null hypothesis, since the pvalue is below 1%",
            "markdown"
        ],
        [
            "stats.ks_2samp(rvs1, rvs3)\nKstestResult(statistic=0.114, pvalue=0.00299005061044668)  # random",
            "code"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Kernel density estimation": [
        [
            "A common task in statistics is to estimate the probability density function\n(PDF) of a random variable from a set of data samples. This task is called\ndensity estimation. The most well-known tool to do this is the histogram.\nA histogram is a useful tool for visualization (mainly because everyone\nunderstands it), but doesn\u00e2\u0080\u0099t use the available data very efficiently. Kernel\ndensity estimation (KDE) is a more efficient tool for the same task. The\ngaussian_kde estimator can be used to estimate the PDF of univariate as\nwell as multivariate data. It works best if the data is unimodal.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Kernel density estimation->Univariate estimation": [
        [
            "We start with a minimal amount of data in order to see how gaussian_kde\nworks and what the different options for bandwidth selection do. The data\nsampled from the PDF are shown as blue dashes at the bottom of the figure (this\nis called a rug plot):",
            "markdown"
        ],
        [
            "from scipy import stats\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "x1 = np.array([-7, -5, 1, 4, 5], dtype=np.float64)\n kde1 = stats.gaussian_kde(x1)\n kde2 = stats.gaussian_kde(x1, bw_method='silverman')",
            "code"
        ],
        [
            "fig = plt.figure()\n ax = fig.add_subplot(111)",
            "code"
        ],
        [
            "ax.plot(x1, np.zeros(x1.shape), 'b+', ms=20)  # rug plot\n x_eval = np.linspace(-10, 10, num=200)\n ax.plot(x_eval, kde1(x_eval), 'k-', label=\"Scott's Rule\")\n ax.plot(x_eval, kde2(x_eval), 'r-', label=\"Silverman's Rule\")",
            "code"
        ],
        [
            "plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/stats-1.png\"/>\n</figure>",
            "code"
        ],
        [
            "We see that there is very little difference between Scott\u00e2\u0080\u0099s Rule and\nSilverman\u00e2\u0080\u0099s Rule, and that the bandwidth selection with a limited amount of\ndata is probably a bit too wide. We can define our own bandwidth function to\nget a less smoothed-out result.",
            "markdown"
        ],
        [
            "def my_kde_bandwidth(obj, fac=1./5):\n...     \"\"\"We use Scott's Rule, multiplied by a constant factor.\"\"\"\n...     return np.power(obj.n, -1./(obj.d+4)) * fac",
            "code"
        ],
        [
            "fig = plt.figure()\n ax = fig.add_subplot(111)",
            "code"
        ],
        [
            "ax.plot(x1, np.zeros(x1.shape), 'b+', ms=20)  # rug plot\n kde3 = stats.gaussian_kde(x1, bw_method=my_kde_bandwidth)\n ax.plot(x_eval, kde3(x_eval), 'g-', label=\"With smaller BW\")",
            "code"
        ],
        [
            "plt.show()\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/kde_plot2.png\"/>\n</figure>",
            "code"
        ],
        [
            "We see that if we set bandwidth to be very narrow, the obtained estimate for\nthe probability density function (PDF) is simply the sum of Gaussians around\neach data point.",
            "markdown"
        ],
        [
            "We now take a more realistic example and look at the difference between the\ntwo available bandwidth selection rules. Those rules are known to work well\nfor (close to) normal distributions, but even for unimodal distributions that\nare quite strongly non-normal they work reasonably well. As a non-normal\ndistribution we take a Student\u00e2\u0080\u0099s T distribution with 5 degrees of freedom.",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\nrng = np.random.default_rng()\nx1 = rng.normal(size=200)  # random data, normal distribution\nxs = np.linspace(x1.min()-1, x1.max()+1, 200)\n\nkde1 = stats.gaussian_kde(x1)\nkde2 = stats.gaussian_kde(x1, bw_method='silverman')\n\nfig = plt.figure(figsize=(8, 6))\n\nax1 = fig.add_subplot(211)\nax1.plot(x1, np.zeros(x1.shape), 'b+', ms=12)  # rug plot\nax1.plot(xs, kde1(xs), 'k-', label=\"Scott's Rule\")\nax1.plot(xs, kde2(xs), 'b-', label=\"Silverman's Rule\")\nax1.plot(xs, stats.norm.pdf(xs), 'r--', label=\"True PDF\")\n\nax1.set_xlabel('x')\nax1.set_ylabel('Density')\nax1.set_title(\"Normal (top) and Student's T$_{df=5}$ (bottom) distributions\")\nax1.legend(loc=1)\n\nx2 = stats.t.rvs(5, size=200, random_state=rng)  # random data, T distribution\nxs = np.linspace(x2.min() - 1, x2.max() + 1, 200)\n\nkde3 = stats.gaussian_kde(x2)\nkde4 = stats.gaussian_kde(x2, bw_method='silverman')\n\nax2 = fig.add_subplot(212)\nax2.plot(x2, np.zeros(x2.shape), 'b+', ms=12)  # rug plot\nax2.plot(xs, kde3(xs), 'k-', label=\"Scott's Rule\")\nax2.plot(xs, kde4(xs), 'b-', label=\"Silverman's Rule\")\nax2.plot(xs, stats.t.pdf(xs, 5), 'r--', label=\"True PDF\")\n\nax2.set_xlabel('x')\nax2.set_ylabel('Density')\n\nplt.show()\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/kde_plot3.png\"/>\n</figure>",
            "code"
        ],
        [
            "We now take a look at a bimodal distribution with one wider and one narrower\nGaussian feature. We expect that this will be a more difficult density to\napproximate, due to the different bandwidths required to accurately resolve\neach feature.",
            "markdown"
        ],
        [
            "from functools import partial",
            "code"
        ],
        [
            "loc1, scale1, size1 = (-2, 1, 175)\n loc2, scale2, size2 = (2, 0.2, 50)\n x2 = np.concatenate([np.random.normal(loc=loc1, scale=scale1, size=size1),\n...                      np.random.normal(loc=loc2, scale=scale2, size=size2)])",
            "code"
        ],
        [
            "x_eval = np.linspace(x2.min() - 1, x2.max() + 1, 500)",
            "code"
        ],
        [
            "kde = stats.gaussian_kde(x2)\n kde2 = stats.gaussian_kde(x2, bw_method='silverman')\n kde3 = stats.gaussian_kde(x2, bw_method=partial(my_kde_bandwidth, fac=0.2))\n kde4 = stats.gaussian_kde(x2, bw_method=partial(my_kde_bandwidth, fac=0.5))",
            "code"
        ],
        [
            "pdf = stats.norm.pdf\n bimodal_pdf = pdf(x_eval, loc=loc1, scale=scale1) * float(size1) / x2.size + \\\n...               pdf(x_eval, loc=loc2, scale=scale2) * float(size2) / x2.size",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(8, 6))\n ax = fig.add_subplot(111)",
            "code"
        ],
        [
            "ax.plot(x2, np.zeros(x2.shape), 'b+', ms=12)\n ax.plot(x_eval, kde(x_eval), 'k-', label=\"Scott's Rule\")\n ax.plot(x_eval, kde2(x_eval), 'b-', label=\"Silverman's Rule\")\n ax.plot(x_eval, kde3(x_eval), 'g-', label=\"Scott * 0.2\")\n ax.plot(x_eval, kde4(x_eval), 'c-', label=\"Scott * 0.5\")\n ax.plot(x_eval, bimodal_pdf, 'r--', label=\"Actual PDF\")",
            "code"
        ],
        [
            "ax.set_xlim([x_eval.min(), x_eval.max()])\n ax.legend(loc=2)\n ax.set_xlabel('x')\n ax.set_ylabel('Density')\n plt.show()\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/kde_plot4.png\"/>\n</figure>",
            "code"
        ],
        [
            "As expected, the KDE is not as close to the true PDF as we would like due to\nthe different characteristic size of the two features of the bimodal\ndistribution. By halving the default bandwidth (Scott * 0.5), we can do\nsomewhat better, while using a factor 5 smaller bandwidth than the default\ndoesn\u00e2\u0080\u0099t smooth enough. What we really need, though, in this case, is a\nnon-uniform (adaptive) bandwidth.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Kernel density estimation->Multivariate estimation": [
        [
            "With gaussian_kde we can perform multivariate, as well as univariate\nestimation. We demonstrate the bivariate case. First, we generate some random\ndata with a model in which the two variates are correlated.",
            "markdown"
        ],
        [
            "def measure(n):\n...     \"\"\"Measurement model, return two coupled measurements.\"\"\"\n...     m1 = np.random.normal(size=n)\n...     m2 = np.random.normal(scale=0.5, size=n)\n...     return m1+m2, m1-m2",
            "code"
        ],
        [
            "m1, m2 = measure(2000)\n xmin = m1.min()\n xmax = m1.max()\n ymin = m2.min()\n ymax = m2.max()",
            "code"
        ],
        [
            "Then we apply the KDE to the data:",
            "markdown"
        ],
        [
            "X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n positions = np.vstack([X.ravel(), Y.ravel()])\n values = np.vstack([m1, m2])\n kernel = stats.gaussian_kde(values)\n Z = np.reshape(kernel.evaluate(positions).T, X.shape)",
            "code"
        ],
        [
            "Finally, we plot the estimated bivariate distribution as a colormap and plot\nthe individual data points on top.",
            "markdown"
        ],
        [
            "fig = plt.figure(figsize=(8, 6))\n ax = fig.add_subplot(111)",
            "code"
        ],
        [
            "ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n...           extent=[xmin, xmax, ymin, ymax])\n ax.plot(m1, m2, 'k.', markersize=2)",
            "code"
        ],
        [
            "ax.set_xlim([xmin, xmax])\n ax.set_ylim([ymin, ymax])",
            "code"
        ],
        [
            "plt.show()\n\n\n<figure class=\"align-center\">\n<img alt='\"An X-Y plot showing a random scattering of points around a 2-D gaussian. The distribution has a semi-major axis at 45 degrees with a semi-minor axis about half as large. Each point in the plot is highlighted with the outer region in red, then yellow, then green, with the center in blue. \"' class=\"plot-directive\" src=\"../_images/kde_plot5.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Kernel density estimation->Multiscale Graph Correlation (MGC)": [
        [
            "With multiscale_graphcorr, we can test for independence on high\ndimensional and nonlinear data. Before we start, let\u00e2\u0080\u0099s import some useful\npackages:",
            "markdown"
        ],
        [
            "import numpy as np\n import matplotlib.pyplot as plt; plt.style.use('classic')\n from scipy.stats import multiscale_graphcorr",
            "code"
        ],
        [
            "Let\u00e2\u0080\u0099s use a custom plotting function to plot the data relationship:",
            "markdown"
        ],
        [
            "def mgc_plot(x, y, sim_name, mgc_dict=None, only_viz=False,\n...              only_mgc=False):\n...     \"\"\"Plot sim and MGC-plot\"\"\"\n...     if not only_mgc:\n...         # simulation\n...         plt.figure(figsize=(8, 8))\n...         ax = plt.gca()\n...         ax.set_title(sim_name + \" Simulation\", fontsize=20)\n...         ax.scatter(x, y)\n...         ax.set_xlabel('X', fontsize=15)\n...         ax.set_ylabel('Y', fontsize=15)\n...         ax.axis('equal')\n...         ax.tick_params(axis=\"x\", labelsize=15)\n...         ax.tick_params(axis=\"y\", labelsize=15)\n...         plt.show()\n...     if not only_viz:\n...         # local correlation map\n...         plt.figure(figsize=(8,8))\n...         ax = plt.gca()\n...         mgc_map = mgc_dict[\"mgc_map\"]\n...         # draw heatmap\n...         ax.set_title(\"Local Correlation Map\", fontsize=20)\n...         im = ax.imshow(mgc_map, cmap='YlGnBu')\n...         # colorbar\n...         cbar = ax.figure.colorbar(im, ax=ax)\n...         cbar.ax.set_ylabel(\"\", rotation=-90, va=\"bottom\")\n...         ax.invert_yaxis()\n...         # Turn spines off and create white grid.\n...         for edge, spine in ax.spines.items():\n...             spine.set_visible(False)\n...         # optimal scale\n...         opt_scale = mgc_dict[\"opt_scale\"]\n...         ax.scatter(opt_scale[0], opt_scale[1],\n...                    marker='X', s=200, color='red')\n...         # other formatting\n...         ax.tick_params(bottom=\"off\", left=\"off\")\n...         ax.set_xlabel('#Neighbors for X', fontsize=15)\n...         ax.set_ylabel('#Neighbors for Y', fontsize=15)\n...         ax.tick_params(axis=\"x\", labelsize=15)\n...         ax.tick_params(axis=\"y\", labelsize=15)\n...         ax.set_xlim(0, 100)\n...         ax.set_ylim(0, 100)\n...         plt.show()",
            "code"
        ],
        [
            "Let\u00e2\u0080\u0099s look at some linear data first:",
            "markdown"
        ],
        [
            "rng = np.random.default_rng()\n x = np.linspace(-1, 1, num=100)\n y = x + 0.3 * rng.random(x.size)",
            "code"
        ],
        [
            "The simulation relationship can be plotted below:",
            "markdown"
        ],
        [
            "mgc_plot(x, y, \"Linear\", only_viz=True)\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/mgc_plot1.png\"/>\n</figure>",
            "code"
        ],
        [
            "Now, we can see the test statistic, p-value, and MGC map visualized below. The\noptimal scale is shown on the map as a red \u00e2\u0080\u009cx\u00e2\u0080\u009d:",
            "markdown"
        ],
        [
            "stat, pvalue, mgc_dict = multiscale_graphcorr(x, y)\n print(\"MGC test statistic: \", round(stat, 1))\nMGC test statistic:  1.0\n print(\"P-value: \", round(pvalue, 1))\nP-value:  0.0\n mgc_plot(x, y, \"Linear\", mgc_dict, only_mgc=True)\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/mgc_plot2.png\"/>\n</figure>",
            "code"
        ],
        [
            "It is clear from here, that MGC is able to determine a relationship between the\ninput data matrices because the p-value is very low and the MGC test statistic\nis relatively high. The MGC-map indicates a <strong>strongly linear relationship</strong>.\nIntuitively, this is because having more neighbors will help in identifying a\nlinear relationship between \\(x\\) and \\(y\\). The optimal scale in this\ncase is <strong>equivalent to the global scale</strong>, marked by a red spot on the map.",
            "markdown"
        ],
        [
            "The same can be done for nonlinear data sets. The following \\(x\\) and\n\\(y\\) arrays are derived from a nonlinear simulation:",
            "markdown"
        ],
        [
            "unif = np.array(rng.uniform(0, 5, size=100))\n x = unif * np.cos(np.pi * unif)\n y = unif * np.sin(np.pi * unif) + 0.4 * rng.random(x.size)",
            "code"
        ],
        [
            "The simulation relationship can be plotted below:",
            "markdown"
        ],
        [
            "mgc_plot(x, y, \"Spiral\", only_viz=True)\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/mgc_plot3.png\"/>\n</figure>",
            "code"
        ],
        [
            "Now, we can see the test statistic, p-value, and MGC map visualized below. The\noptimal scale is shown on the map as a red \u00e2\u0080\u009cx\u00e2\u0080\u009d:",
            "markdown"
        ],
        [
            "stat, pvalue, mgc_dict = multiscale_graphcorr(x, y)\n print(\"MGC test statistic: \", round(stat, 1))\nMGC test statistic:  0.2  # random\n print(\"P-value: \", round(pvalue, 1))\nP-value:  0.0\n mgc_plot(x, y, \"Spiral\", mgc_dict, only_mgc=True)\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/mgc_plot4.png\"/>\n</figure>",
            "code"
        ],
        [
            "It is clear from here, that MGC is able to determine a relationship again\nbecause the p-value is very low and the MGC test statistic is relatively high.\nThe MGC-map indicates a <strong>strongly nonlinear relationship</strong>. The optimal scale\nin this case is <strong>equivalent to the local scale</strong>, marked by a red spot on the\nmap.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Quasi-Monte Carlo": [
        [
            "Before talking about Quasi-Monte Carlo (QMC), a quick introduction about Monte\nCarlo (MC). MC methods, or MC experiments, are a broad class of\ncomputational algorithms that rely on repeated random sampling to obtain\nnumerical results. The underlying concept is to use randomness to solve\nproblems that might be deterministic in principle. They are often used in\nphysical and mathematical problems and are most useful when it is difficult or\nimpossible to use other approaches. MC methods are mainly used in\nthree problem classes: optimization, numerical integration, and generating\ndraws from a probability distribution.",
            "markdown"
        ],
        [
            "Generating random numbers with specific properties is a more complex problem\nthan it sounds. Simple MC methods are designed to sample points to be\nindependent and identically distributed (IID). But generating multiple sets\nof random points can produce radically different results.\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_mc.png\"/>\n</figure>",
            "markdown"
        ],
        [
            "In both cases in the plot above, points are generated randomly without any\nknowledge about previously drawn points. It is clear that some regions of\nthe space are left unexplored - which can cause problems in simulations as a\nparticular set of points might trigger a totally different behaviour.",
            "markdown"
        ],
        [
            "A great benefit of MC is that it has known convergence properties.\nLet\u00e2\u0080\u0099s look at the mean of the squared sum in 5 dimensions:\n\n\\[f(\\mathbf{x}) = \\left( \\sum_{j=1}^{5}x_j \\right)^2,\\]",
            "markdown"
        ],
        [
            "with \\(x_j \\sim \\mathcal{U}(0,1)\\). It has a known mean value,\n\\(\\mu = 5/3+5(5-1)/4\\). Using MC sampling, we\ncan compute that mean numerically, and the approximation error follows a\ntheoretical rate of \\(O(n^{-1/2})\\).\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_conv_mc.png\"/>\n</figure>",
            "markdown"
        ],
        [
            "Although the convergence is ensured, practitioners tend to want to have an\nexploration process which is more deterministic. With normal MC, a seed can be\nused to have a repeatable process. But fixing the seed would break the\nconvergence property: a given seed could work for a given class of problem\nand break for another one.",
            "markdown"
        ],
        [
            "What is commonly done to walk through the space in a deterministic manner, is\nto use a regular grid spanning all parameter dimensions, also called a\nsaturated design. Let\u00e2\u0080\u0099s consider the unit-hypercube, with all bounds ranging\nfrom 0 to 1. Now, having a distance of 0.1 between points, the number of points\nrequired to fill the unit interval would be 10. In a 2-dimensional hypercube\nthe same spacing would require 100, and in 3 dimensions 1,000 points. As the\nnumber of dimensions grows, the number of experiments which is required to fill\nthe space rises exponentially as the dimensionality of the space increases.\nThis exponential growth is called \u00e2\u0080\u009cthe curse of dimensionality\u00e2\u0080\u009d.",
            "markdown"
        ],
        [
            "import numpy as np\n disc = 10\n x1 = np.linspace(0, 1, disc)\n x2 = np.linspace(0, 1, disc)\n x3 = np.linspace(0, 1, disc)\n x1, x2, x3 = np.meshgrid(x1, x2, x3)\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_curse.png\"/>\n</figure>",
            "code"
        ],
        [
            "To mitigate this issue, QMC methods have been designed. They are\ndeterministic, have a good coverage of the space and some of them can be\ncontinued and retain good properties.\nThe main difference with MC methods is that the points are not IID but they\nknow about previous points. Hence, some methods are also referred to as\nsequences.\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_mc_qmc.png\"/>\n</figure>",
            "markdown"
        ],
        [
            "This figure presents 2 sets of 256 points. The design of the left is a plain\nMC whereas the design of the right is a QMC design using the Sobol\u00e2\u0080\u0099 method.\nWe clearly see that the QMC version is more uniform. The points sample better\nnear the boundaries and there are less clusters or gaps.",
            "markdown"
        ],
        [
            "One way to assess the uniformity is to use a measure called the discrepancy.\nHere the discrepancy of Sobol\u00e2\u0080\u0099 points is better than crude MC.",
            "markdown"
        ],
        [
            "Coming back to the computation of the mean, QMC methods also have better rates\nof convergence for the error. They can achieve \\(O(n^{-1})\\) for this\nfunction, and even better rates on very smooth functions. This figure shows\nthat the Sobol\u00e2\u0080\u0099 method has a rate of \\(O(n^{-1})\\):\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_conv_mc_sobol.png\"/>\n</figure>",
            "markdown"
        ],
        [
            "We refer to the documentation of scipy.stats.qmc for\nmore mathematical details.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Quasi-Monte Carlo->Calculate the discrepancy": [
        [
            "Let\u00e2\u0080\u0099s consider two sets of points. From the figure below, it is clear that\nthe design on the left covers more of the space than the design on the right.\nThis can be quantified using a discrepancy measure.\nThe lower the discrepancy, the more uniform a sample is.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy.stats import qmc\n space_1 = np.array([[1, 3], [2, 6], [3, 2], [4, 5], [5, 1], [6, 4]])\n space_2 = np.array([[1, 5], [2, 4], [3, 3], [4, 2], [5, 1], [6, 6]])\n l_bounds = [0.5, 0.5]\n u_bounds = [6.5, 6.5]\n space_1 = qmc.scale(space_1, l_bounds, u_bounds, reverse=True)\n space_2 = qmc.scale(space_2, l_bounds, u_bounds, reverse=True)\n qmc.discrepancy(space_1)\n0.008142039609053464\n qmc.discrepancy(space_2)\n0.010456854423869011\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_discrepancy.png\"/>\n</figure>",
            "code"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Quasi-Monte Carlo->Using a QMC engine": [
        [
            "Several QMC samplers/engines are implemented. Here we look at two of the most\nused QMC methods: Sobol and Halton\nsequences.",
            "markdown"
        ],
        [
            "\"\"\"Sobol' and Halton sequences.\"\"\"\nfrom scipy.stats import qmc\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n\nrng = np.random.default_rng()\n\nn_sample = 256\ndim = 2\n\nsample = {}\n\n# Sobol'\nengine = qmc.Sobol(d=dim, seed=rng)\nsample[\"Sobol'\"] = engine.random(n_sample)\n\n# Halton\nengine = qmc.Halton(d=dim, seed=rng)\nsample[\"Halton\"] = engine.random(n_sample)\n\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i, kind in enumerate(sample):\n    axs[i].scatter(sample[kind][:, 0], sample[kind][:, 1])\n\n    axs[i].set_aspect('equal')\n    axs[i].set_xlabel(r'$x_1$')\n    axs[i].set_ylabel(r'$x_2$')\n    axs[i].set_title(f'{kind}\u00e2\u0080\u0094$C^2 = ${qmc.discrepancy(sample[kind]):.2}')\n\nplt.tight_layout()\nplt.show()\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_sobol_halton.png\"/>\n</figure>",
            "code"
        ],
        [
            "Warning",
            "markdown"
        ],
        [
            "QMC methods require particular care and the user must read the\ndocumentation to avoid common pitfalls. Sobol\u00e2\u0080\u0099 for instance requires a\nnumber of points following a power of 2. Also, thinning, burning or other\npoint selection can break the properties of the sequence and result in a\nset of points which would not be better than MC.",
            "markdown"
        ],
        [
            "QMC engines are state-aware. Meaning that you can continue the sequence,\nskip some points, or reset it. Let\u00e2\u0080\u0099s take 5 points from\nHalton. And then ask for a second set of 5 points:",
            "markdown"
        ],
        [
            "from scipy.stats import qmc\n engine = qmc.Halton(d=2)\n engine.random(5)\narray([[0.22166437, 0.07980522],  # random\n       [0.72166437, 0.93165708],\n       [0.47166437, 0.41313856],\n       [0.97166437, 0.19091633],\n       [0.01853937, 0.74647189]])\n engine.random(5)\narray([[0.51853937, 0.52424967],  # random\n       [0.26853937, 0.30202745],\n       [0.76853937, 0.857583  ],\n       [0.14353937, 0.63536078],\n       [0.64353937, 0.01807683]])",
            "code"
        ],
        [
            "Now we reset the sequence. Asking for 5 points leads to the same first 5\npoints:",
            "markdown"
        ],
        [
            "engine.reset()\n engine.random(5)\narray([[0.22166437, 0.07980522],  # random\n       [0.72166437, 0.93165708],\n       [0.47166437, 0.41313856],\n       [0.97166437, 0.19091633],\n       [0.01853937, 0.74647189]])",
            "code"
        ],
        [
            "And here we advance the sequence to get the same second set of 5 points:",
            "markdown"
        ],
        [
            "engine.reset()\n engine.fast_forward(5)\n engine.random(5)\narray([[0.51853937, 0.52424967],  # random\n       [0.26853937, 0.30202745],\n       [0.76853937, 0.857583  ],\n       [0.14353937, 0.63536078],\n       [0.64353937, 0.01807683]])",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "By default, both Sobol and\nHalton are scrambled. The convergence properties are\nbetter, and it prevents the appearance of fringes or noticeable patterns\nof points in high dimensions. There should be no practical reason not to\nuse the scrambled version.",
            "markdown"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Quasi-Monte Carlo->Making a QMC engine, i.e., subclassing QMCEngine": [
        [
            "To make your own QMCEngine, a few methods have to be\ndefined. Following is an example wrapping numpy.random.Generator.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy.stats import qmc\n class RandomEngine(qmc.QMCEngine):\n...     def __init__(self, d, seed=None):\n...         super().__init__(d=d, seed=seed)\n...         self.rng = np.random.default_rng(self.rng_seed)\n...\n...\n...     def _random(self, n=1, *, workers=1):\n...         return self.rng.random((n, self.d))\n...\n...\n...     def reset(self):\n...         self.rng = np.random.default_rng(self.rng_seed)\n...         self.num_generated = 0\n...         return self\n...\n...\n...     def fast_forward(self, n):\n...         self.random(n)\n...         return self",
            "code"
        ],
        [
            "Then we use it as any other QMC engine:",
            "markdown"
        ],
        [
            "engine = RandomEngine(2)\n engine.random(5)\narray([[0.22733602, 0.31675834],  # random\n       [0.79736546, 0.67625467],\n       [0.39110955, 0.33281393],\n       [0.59830875, 0.18673419],\n       [0.67275604, 0.94180287]])\n engine.reset()\n engine.random(5)\narray([[0.22733602, 0.31675834],  # random\n       [0.79736546, 0.67625467],\n       [0.39110955, 0.33281393],\n       [0.59830875, 0.18673419],\n       [0.67275604, 0.94180287]])",
            "code"
        ]
    ],
    "scipy->Statistics (scipy.stats)->Quasi-Monte Carlo->Guidelines on using QMC": [
        [
            "QMC has rules! Be sure to read the documentation or you might have no\nbenefit over MC.",
            "markdown"
        ],
        [
            "Use Sobol if you need <strong>exactly</strong> \\(2^m\\) points.",
            "markdown"
        ],
        [
            "Halton allows to sample, or skip, an arbitrary number of\npoints. This is at the cost of a slower rate of convergence than Sobol\u00e2\u0080\u0099.",
            "markdown"
        ],
        [
            "Never remove the first points of the sequence. It will destroy the\nproperties.",
            "markdown"
        ],
        [
            "Scrambling is always better.",
            "markdown"
        ],
        [
            "If you use LHS based methods, you cannot add points without losing the LHS\nproperties. (There are some methods to do so, but this is not implemented.)",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Introduction": [
        [
            "Image processing and analysis are generally seen as operations on\n2-D arrays of values. There are, however, a number of\nfields where images of higher dimensionality must be analyzed. Good\nexamples of these are medical imaging and biological imaging.\nnumpy is suited very well for this type of applications due to\nits inherent multidimensional nature. The scipy.ndimage\npackages provides a number of general image processing and analysis\nfunctions that are designed to operate with arrays of arbitrary\ndimensionality. The packages currently includes: functions for\nlinear and non-linear filtering, binary morphology, B-spline\ninterpolation, and object measurements.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Properties shared by all functions": [
        [
            "All functions share some common properties. Notably, all functions\nallow the specification of an output array with the output\nargument. With this argument, you can specify an array that will be\nchanged in-place with the result with the operation. In this case,\nthe result is not returned. Usually, using the output argument is\nmore efficient, since an existing array is used to store the\nresult.",
            "markdown"
        ],
        [
            "The type of arrays returned is dependent on the type of operation,\nbut it is, in most cases, equal to the type of the input. If,\nhowever, the output argument is used, the type of the result is\nequal to the type of the specified output argument. If no output\nargument is given, it is still possible to specify what the result\nof the output should be. This is done by simply assigning the\ndesired numpy type object to the output argument. For example:",
            "markdown"
        ],
        [
            "from scipy.ndimage import correlate\n import numpy as np\n correlate(np.arange(10), [1, 2.5])\narray([ 0,  2,  6,  9, 13, 16, 20, 23, 27, 30])\n correlate(np.arange(10), [1, 2.5], output=np.float64)\narray([  0. ,   2.5,   6. ,   9.5,  13. ,  16.5,  20. ,  23.5,  27. ,  30.5])",
            "code"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Filter functions": [
        [
            "The functions described in this section all perform some type of spatial\nfiltering of the input array: the elements in the output are some function\nof the values in the neighborhood of the corresponding input element. We refer\nto this neighborhood of elements as the filter kernel, which is often\nrectangular in shape but may also have an arbitrary footprint. Many\nof the functions described below allow you to define the footprint\nof the kernel by passing a mask through the footprint parameter.\nFor example, a cross-shaped kernel can be defined as follows:",
            "markdown"
        ],
        [
            "footprint = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n footprint\narray([[0, 1, 0],\n       [1, 1, 1],\n       [0, 1, 0]])",
            "code"
        ],
        [
            "Usually, the origin of the kernel is at the center calculated by\ndividing the dimensions of the kernel shape by two. For instance,\nthe origin of a 1-D kernel of length three is at the\nsecond element. Take, for example, the correlation of a\n1-D array with a filter of length 3 consisting of\nones:",
            "markdown"
        ],
        [
            "from scipy.ndimage import correlate1d\n a = [0, 0, 0, 1, 0, 0, 0]\n correlate1d(a, [1, 1, 1])\narray([0, 0, 1, 1, 1, 0, 0])",
            "code"
        ],
        [
            "Sometimes, it is convenient to choose a different origin for the\nkernel. For this reason, most functions support the origin\nparameter, which gives the origin of the filter relative to its\ncenter. For example:",
            "markdown"
        ],
        [
            "a = [0, 0, 0, 1, 0, 0, 0]\n correlate1d(a, [1, 1, 1], origin = -1)\narray([0, 1, 1, 1, 0, 0, 0])",
            "code"
        ],
        [
            "The effect is a shift of the result towards the left. This feature\nwill not be needed very often, but it may be useful, especially for\nfilters that have an even size. A good example is the calculation\nof backward and forward differences:",
            "markdown"
        ],
        [
            "a = [0, 0, 1, 1, 1, 0, 0]\n correlate1d(a, [-1, 1])               # backward difference\narray([ 0,  0,  1,  0,  0, -1,  0])\n correlate1d(a, [-1, 1], origin = -1)  # forward difference\narray([ 0,  1,  0,  0, -1,  0,  0])",
            "code"
        ],
        [
            "We could also have calculated the forward difference as follows:",
            "markdown"
        ],
        [
            "correlate1d(a, [0, -1, 1])\narray([ 0,  1,  0,  0, -1,  0,  0])",
            "code"
        ],
        [
            "However, using the origin parameter instead of a larger kernel is\nmore efficient. For multidimensional kernels, origin can be a\nnumber, in which case the origin is assumed to be equal along all\naxes, or a sequence giving the origin along each axis.",
            "markdown"
        ],
        [
            "Since the output elements are a function of elements in the\nneighborhood of the input elements, the borders of the array need to\nbe dealt with appropriately by providing the values outside the\nborders. This is done by assuming that the arrays are extended beyond\ntheir boundaries according to certain boundary conditions. In the\nfunctions described below, the boundary conditions can be selected\nusing the mode parameter, which must be a string with the name of the\nboundary condition. The following boundary conditions are currently\nsupported:\n\n\n</blockquote>",
            "markdown"
        ],
        [
            "The following synonyms are also supported for consistency with the\ninterpolation routines:\n\n\n</blockquote>",
            "markdown"
        ],
        [
            "* \u00e2\u0080\u009cgrid-constant\u00e2\u0080\u009d and \u00e2\u0080\u009cconstant\u00e2\u0080\u009d are equivalent for filtering operations, but\nhave different behavior in interpolation functions. For API consistency, the\nfiltering functions accept either name.",
            "markdown"
        ],
        [
            "The \u00e2\u0080\u009cconstant\u00e2\u0080\u009d mode is special since it needs an additional parameter to\nspecify the constant value that should be used.",
            "markdown"
        ],
        [
            "Note that modes mirror and reflect differ only in whether the sample at the\nboundary is repeated upon reflection. For mode mirror, the point of symmetry is\nexactly at the final sample, so that value is not repeated. This mode is also\nknown as whole-sample symmetric since the point of symmetry falls on the final\nsample. Similarly, reflect is often referred to as half-sample symmetric as the\npoint of symmetry is half a sample beyond the array boundary.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The easiest way to implement such boundary conditions would be to\ncopy the data to a larger array and extend the data at the borders\naccording to the boundary conditions. For large arrays and large\nfilter kernels, this would be very memory consuming, and the\nfunctions described below, therefore, use a different approach that\ndoes not require allocating large temporary buffers.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Filter functions->Correlation and convolution": [
        [
            "The correlate1d function calculates a 1-D\ncorrelation along the given axis. The lines of the array along the\ngiven axis are correlated with the given weights. The weights\nparameter must be a 1-D sequence of numbers.",
            "markdown"
        ],
        [
            "The function correlate implements multidimensional\ncorrelation of the input array with a given kernel.",
            "markdown"
        ],
        [
            "The convolve1d function calculates a 1-D\nconvolution along the given axis. The lines of the array along the\ngiven axis are convoluted with the given weights. The weights\nparameter must be a 1-D sequence of numbers.",
            "markdown"
        ],
        [
            "The function convolve implements multidimensional\nconvolution of the input array with a given kernel.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "A convolution is essentially a correlation after mirroring the\nkernel. As a result, the origin parameter behaves differently\nthan in the case of a correlation: the results is shifted in the\nopposite direction.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Filter functions->Smoothing filters": [
        [
            "The gaussian_filter1d function implements a 1-D\nGaussian filter. The standard deviation of the Gaussian filter is\npassed through the parameter sigma. Setting order = 0\ncorresponds to convolution with a Gaussian kernel. An order of 1, 2,\nor 3 corresponds to convolution with the first, second, or third\nderivatives of a Gaussian. Higher-order derivatives are not\nimplemented.",
            "markdown"
        ],
        [
            "The gaussian_filter function implements a multidimensional\nGaussian filter. The standard deviations of the Gaussian filter\nalong each axis are passed through the parameter sigma as a\nsequence or numbers. If sigma is not a sequence but a single\nnumber, the standard deviation of the filter is equal along all\ndirections. The order of the filter can be specified separately for\neach axis. An order of 0 corresponds to convolution with a Gaussian\nkernel. An order of 1, 2, or 3 corresponds to convolution with the\nfirst, second, or third derivatives of a Gaussian. Higher-order\nderivatives are not implemented. The order parameter must be a\nnumber, to specify the same order for all axes, or a sequence of\nnumbers to specify a different order for each axis. The example below\nshows the filter applied on test data with different values of sigma.\nThe order parameter is kept at 0.\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/gaussian_filter_plot1.png\"/>\n</figure>",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The multidimensional filter is implemented as a sequence of\n1-D Gaussian filters. The intermediate arrays are\nstored in the same data type as the output. Therefore, for\noutput types with a lower precision, the results may be imprecise\nbecause intermediate results may be stored with insufficient\nprecision. This can be prevented by specifying a more precise\noutput type.",
            "markdown"
        ],
        [
            "The uniform_filter1d function calculates a 1-D\nuniform filter of the given size along the given axis.",
            "markdown"
        ],
        [
            "The uniform_filter implements a multidimensional uniform\nfilter. The sizes of the uniform filter are given for each axis as a\nsequence of integers by the size parameter. If size is not a\nsequence, but a single number, the sizes along all axes are assumed\nto be equal.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The multidimensional filter is implemented as a sequence of\n1-D uniform filters. The intermediate arrays are\nstored in the same data type as the output. Therefore, for output\ntypes with a lower precision, the results may be imprecise\nbecause intermediate results may be stored with insufficient\nprecision. This can be prevented by specifying a more precise\noutput type.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Filter functions->Filters based on order statistics": [
        [
            "The minimum_filter1d function calculates a 1-D\nminimum filter of the given size along the given axis.",
            "markdown"
        ],
        [
            "The maximum_filter1d function calculates a 1-D\nmaximum filter of the given size along the given axis.",
            "markdown"
        ],
        [
            "The minimum_filter function calculates a multidimensional\nminimum filter. Either the sizes of a rectangular kernel or the\nfootprint of the kernel must be provided. The size parameter, if\nprovided, must be a sequence of sizes or a single number, in which\ncase the size of the filter is assumed to be equal along each axis.\nThe footprint, if provided, must be an array that defines the\nshape of the kernel by its non-zero elements.",
            "markdown"
        ],
        [
            "The maximum_filter function calculates a multidimensional\nmaximum filter. Either the sizes of a rectangular kernel or the\nfootprint of the kernel must be provided. The size parameter, if\nprovided, must be a sequence of sizes or a single number, in which\ncase the size of the filter is assumed to be equal along each axis.\nThe footprint, if provided, must be an array that defines the\nshape of the kernel by its non-zero elements.",
            "markdown"
        ],
        [
            "The rank_filter function calculates a multidimensional rank\nfilter. The rank may be less than zero, i.e., rank = -1\nindicates the largest element. Either the sizes of a rectangular\nkernel or the footprint of the kernel must be provided. The size\nparameter, if provided, must be a sequence of sizes or a single\nnumber, in which case the size of the filter is assumed to be equal\nalong each axis. The footprint, if provided, must be an array that\ndefines the shape of the kernel by its non-zero elements.",
            "markdown"
        ],
        [
            "The percentile_filter function calculates a multidimensional\npercentile filter. The percentile may be less than zero, i.e.,\npercentile = -20 equals percentile = 80. Either the sizes of a\nrectangular kernel or the footprint of the kernel must be provided.\nThe size parameter, if provided, must be a sequence of sizes or a\nsingle number, in which case the size of the filter is assumed to be\nequal along each axis. The footprint, if provided, must be an\narray that defines the shape of the kernel by its non-zero elements.",
            "markdown"
        ],
        [
            "The median_filter function calculates a multidimensional\nmedian filter. Either the sizes of a rectangular kernel or the\nfootprint of the kernel must be provided. The size parameter, if\nprovided, must be a sequence of sizes or a single number, in which\ncase the size of the filter is assumed to be equal along each\naxis. The footprint if provided, must be an array that defines the\nshape of the kernel by its non-zero elements.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Filter functions->Derivatives": [
        [
            "Derivative filters can be constructed in several ways. The function\ngaussian_filter1d, described in\nSmoothing filters, can be used to calculate\nderivatives along a given axis using the order parameter. Other\nderivative filters are the Prewitt and Sobel filters:",
            "markdown"
        ],
        [
            "The prewitt function calculates a derivative along the given\naxis.",
            "markdown"
        ],
        [
            "The sobel function calculates a derivative along the given\naxis.",
            "markdown"
        ],
        [
            "The Laplace filter is calculated by the sum of the second derivatives\nalong all axes. Thus, different Laplace filters can be constructed\nusing different second-derivative functions. Therefore, we provide a\ngeneral function that takes a function argument to calculate the\nsecond derivative along a given direction.",
            "markdown"
        ],
        [
            "The function generic_laplace calculates a Laplace filter\nusing the function passed through derivative2 to calculate\nsecond derivatives. The function derivative2 should have the\nfollowing signature",
            "markdown"
        ],
        [
            "derivative2(input, axis, output, mode, cval, *extra_arguments, **extra_keywords)",
            "code"
        ],
        [
            "It should calculate the second derivative along the dimension\naxis. If output is not None, it should use that for the\noutput and return None, otherwise it should return the\nresult. mode, cval have the usual meaning.",
            "markdown"
        ],
        [
            "The extra_arguments and extra_keywords arguments can be used\nto pass a tuple of extra arguments and a dictionary of named\narguments that are passed to derivative2 at each call.",
            "markdown"
        ],
        [
            "For example",
            "markdown"
        ],
        [
            "def d2(input, axis, output, mode, cval):\n...     return correlate1d(input, [1, -2, 1], axis, output, mode, cval, 0)\n...\n a = np.zeros((5, 5))\n a[2, 2] = 1\n from scipy.ndimage import generic_laplace\n generic_laplace(a, d2)\narray([[ 0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  1., -4.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.]])",
            "code"
        ],
        [
            "To demonstrate the use of the extra_arguments argument, we could do",
            "markdown"
        ],
        [
            "def d2(input, axis, output, mode, cval, weights):\n...     return correlate1d(input, weights, axis, output, mode, cval, 0,)\n...\n a = np.zeros((5, 5))\n a[2, 2] = 1\n generic_laplace(a, d2, extra_arguments = ([1, -2, 1],))\narray([[ 0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  1., -4.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.]])",
            "code"
        ],
        [
            "or",
            "markdown"
        ],
        [
            "generic_laplace(a, d2, extra_keywords = {'weights': [1, -2, 1]})\narray([[ 0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  1., -4.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.]])",
            "code"
        ],
        [
            "The following two functions are implemented using\ngeneric_laplace by providing appropriate functions for the\nsecond-derivative function:",
            "markdown"
        ],
        [
            "The function laplace calculates the Laplace using discrete\ndifferentiation for the second derivative (i.e., convolution with\n[1, -2, 1]).",
            "markdown"
        ],
        [
            "The function gaussian_laplace calculates the Laplace filter\nusing gaussian_filter to calculate the second\nderivatives. The standard deviations of the Gaussian filter along\neach axis are passed through the parameter sigma as a sequence or\nnumbers. If sigma is not a sequence but a single number, the\nstandard deviation of the filter is equal along all directions.",
            "markdown"
        ],
        [
            "The gradient magnitude is defined as the square root of the sum of the\nsquares of the gradients in all directions. Similar to the generic\nLaplace function, there is a generic_gradient_magnitude\nfunction that calculates the gradient magnitude of an array.",
            "markdown"
        ],
        [
            "The function generic_gradient_magnitude calculates a\ngradient magnitude using the function passed through\nderivative to calculate first derivatives. The function\nderivative should have the following signature",
            "markdown"
        ],
        [
            "derivative(input, axis, output, mode, cval, *extra_arguments, **extra_keywords)",
            "code"
        ],
        [
            "It should calculate the derivative along the dimension axis. If\noutput is not None, it should use that for the output and return\nNone, otherwise it should return the result. mode, cval have the\nusual meaning.",
            "markdown"
        ],
        [
            "The extra_arguments and extra_keywords arguments can be used to\npass a tuple of extra arguments and a dictionary of named arguments\nthat are passed to derivative at each call.",
            "markdown"
        ],
        [
            "For example, the sobel function fits the required signature",
            "markdown"
        ],
        [
            "a = np.zeros((5, 5))\n a[2, 2] = 1\n from scipy.ndimage import sobel, generic_gradient_magnitude\n generic_gradient_magnitude(a, sobel)\narray([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.41421356,  2.        ,  1.41421356,  0.        ],\n       [ 0.        ,  2.        ,  0.        ,  2.        ,  0.        ],\n       [ 0.        ,  1.41421356,  2.        ,  1.41421356,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])",
            "code"
        ],
        [
            "See the documentation of generic_laplace for examples of\nusing the extra_arguments and extra_keywords arguments.",
            "markdown"
        ],
        [
            "The sobel and prewitt functions fit the required\nsignature and can, therefore, be used directly with\ngeneric_gradient_magnitude.",
            "markdown"
        ],
        [
            "The function gaussian_gradient_magnitude calculates the\ngradient magnitude using gaussian_filter to calculate the\nfirst derivatives. The standard deviations of the Gaussian filter\nalong each axis are passed through the parameter sigma as a\nsequence or numbers. If sigma is not a sequence but a single\nnumber, the standard deviation of the filter is equal along all\ndirections.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Filter functions->Generic filter functions": [
        [
            "To implement filter functions, generic functions can be used that\naccept a callable object that implements the filtering operation. The\niteration over the input and output arrays is handled by these generic\nfunctions, along with such details as the implementation of the\nboundary conditions. Only a callable object implementing a callback\nfunction that does the actual filtering work must be provided. The\ncallback function can also be written in C and passed using a\nPyCapsule (see Extending scipy.ndimage in C for more\ninformation).",
            "markdown"
        ],
        [
            "The generic_filter1d function implements a generic\n1-D filter function, where the actual filtering\noperation must be supplied as a python function (or other callable\nobject). The generic_filter1d function iterates over the\nlines of an array and calls function at each line. The\narguments that are passed to function are 1-D\narrays of the numpy.float64 type. The first contains the values\nof the current line. It is extended at the beginning and the end,\naccording to the filter_size and origin arguments. The second\narray should be modified in-place to provide the output values of\nthe line. For example, consider a correlation along one dimension:",
            "markdown"
        ],
        [
            "a = np.arange(12).reshape(3,4)\n correlate1d(a, [1, 2, 3])\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "code"
        ],
        [
            "The same operation can be implemented using generic_filter1d,\nas follows:",
            "markdown"
        ],
        [
            "def fnc(iline, oline):\n...     oline[...] = iline[:-2] + 2 * iline[1:-1] + 3 * iline[2:]\n...\n from scipy.ndimage import generic_filter1d\n generic_filter1d(a, fnc, 3)\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "code"
        ],
        [
            "Here, the origin of the kernel was (by default) assumed to be in the\nmiddle of the filter of length 3. Therefore, each input line had been\nextended by one value at the beginning and at the end, before the\nfunction was called.",
            "markdown"
        ],
        [
            "Optionally, extra arguments can be defined and passed to the filter\nfunction. The extra_arguments and extra_keywords arguments can\nbe used to pass a tuple of extra arguments and/or a dictionary of\nnamed arguments that are passed to derivative at each call. For\nexample, we can pass the parameters of our filter as an argument",
            "markdown"
        ],
        [
            "def fnc(iline, oline, a, b):\n...     oline[...] = iline[:-2] + a * iline[1:-1] + b * iline[2:]\n...\n generic_filter1d(a, fnc, 3, extra_arguments = (2, 3))\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "code"
        ],
        [
            "or",
            "markdown"
        ],
        [
            "generic_filter1d(a, fnc, 3, extra_keywords = {'a':2, 'b':3})\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "code"
        ],
        [
            "The generic_filter function implements a generic filter\nfunction, where the actual filtering operation must be supplied as a\npython function (or other callable object). The\ngeneric_filter function iterates over the array and calls\nfunction at each element. The argument of function\nis a 1-D array of the numpy.float64 type that\ncontains the values around the current element that are within the\nfootprint of the filter. The function should return a single value\nthat can be converted to a double precision number. For example,\nconsider a correlation:",
            "markdown"
        ],
        [
            "a = np.arange(12).reshape(3,4)\n correlate(a, [[1, 0], [0, 3]])\narray([[ 0,  3,  7, 11],\n       [12, 15, 19, 23],\n       [28, 31, 35, 39]])",
            "code"
        ],
        [
            "The same operation can be implemented using generic_filter, as\nfollows:",
            "markdown"
        ],
        [
            "def fnc(buffer):\n...     return (buffer * np.array([1, 3])).sum()\n...\n from scipy.ndimage import generic_filter\n generic_filter(a, fnc, footprint = [[1, 0], [0, 1]])\narray([[ 0,  3,  7, 11],\n       [12, 15, 19, 23],\n       [28, 31, 35, 39]])",
            "code"
        ],
        [
            "Here, a kernel footprint was specified that contains only two\nelements. Therefore, the filter function receives a buffer of length\nequal to two, which was multiplied with the proper weights and the\nresult summed.",
            "markdown"
        ],
        [
            "When calling generic_filter, either the sizes of a\nrectangular kernel or the footprint of the kernel must be\nprovided. The size parameter, if provided, must be a sequence of\nsizes or a single number, in which case the size of the filter is\nassumed to be equal along each axis. The footprint, if provided,\nmust be an array that defines the shape of the kernel by its\nnon-zero elements.",
            "markdown"
        ],
        [
            "Optionally, extra arguments can be defined and passed to the filter\nfunction. The extra_arguments and extra_keywords arguments can\nbe used to pass a tuple of extra arguments and/or a dictionary of\nnamed arguments that are passed to derivative at each call. For\nexample, we can pass the parameters of our filter as an argument",
            "markdown"
        ],
        [
            "def fnc(buffer, weights):\n...     weights = np.asarray(weights)\n...     return (buffer * weights).sum()\n...\n generic_filter(a, fnc, footprint = [[1, 0], [0, 1]], extra_arguments = ([1, 3],))\narray([[ 0,  3,  7, 11],\n       [12, 15, 19, 23],\n       [28, 31, 35, 39]])",
            "code"
        ],
        [
            "or",
            "markdown"
        ],
        [
            "generic_filter(a, fnc, footprint = [[1, 0], [0, 1]], extra_keywords= {'weights': [1, 3]})\narray([[ 0,  3,  7, 11],\n       [12, 15, 19, 23],\n       [28, 31, 35, 39]])",
            "code"
        ],
        [
            "These functions iterate over the lines or elements starting at the\nlast axis, i.e., the last index changes the fastest. This order of\niteration is guaranteed for the case that it is important to adapt the\nfilter depending on spatial location. Here is an example of using a\nclass that implements the filter and keeps track of the current\ncoordinates while iterating. It performs the same filter operation as\ndescribed above for generic_filter, but additionally prints\nthe current coordinates:",
            "markdown"
        ],
        [
            "a = np.arange(12).reshape(3,4)\n\n class fnc_class:\n...     def __init__(self, shape):\n...         # store the shape:\n...         self.shape = shape\n...         # initialize the coordinates:\n...         self.coordinates = [0] * len(shape)\n...\n...     def filter(self, buffer):\n...         result = (buffer * np.array([1, 3])).sum()\n...         print(self.coordinates)\n...         # calculate the next coordinates:\n...         axes = list(range(len(self.shape)))\n...         axes.reverse()\n...         for jj in axes:\n...             if self.coordinates[jj] &lt; self.shape[jj] - 1:\n...                 self.coordinates[jj] += 1\n...                 break\n...             else:\n...                 self.coordinates[jj] = 0\n...         return result\n...\n fnc = fnc_class(shape = (3,4))\n generic_filter(a, fnc.filter, footprint = [[1, 0], [0, 1]])\n[0, 0]\n[0, 1]\n[0, 2]\n[0, 3]\n[1, 0]\n[1, 1]\n[1, 2]\n[1, 3]\n[2, 0]\n[2, 1]\n[2, 2]\n[2, 3]\narray([[ 0,  3,  7, 11],\n       [12, 15, 19, 23],\n       [28, 31, 35, 39]])",
            "code"
        ],
        [
            "For the generic_filter1d function, the same approach works,\nexcept that this function does not iterate over the axis that is being\nfiltered. The example for generic_filter1d then becomes this:",
            "markdown"
        ],
        [
            "a = np.arange(12).reshape(3,4)\n\n class fnc1d_class:\n...     def __init__(self, shape, axis = -1):\n...         # store the filter axis:\n...         self.axis = axis\n...         # store the shape:\n...         self.shape = shape\n...         # initialize the coordinates:\n...         self.coordinates = [0] * len(shape)\n...\n...     def filter(self, iline, oline):\n...         oline[...] = iline[:-2] + 2 * iline[1:-1] + 3 * iline[2:]\n...         print(self.coordinates)\n...         # calculate the next coordinates:\n...         axes = list(range(len(self.shape)))\n...         # skip the filter axis:\n...         del axes[self.axis]\n...         axes.reverse()\n...         for jj in axes:\n...             if self.coordinates[jj] &lt; self.shape[jj] - 1:\n...                 self.coordinates[jj] += 1\n...                 break\n...             else:\n...                 self.coordinates[jj] = 0\n...\n fnc = fnc1d_class(shape = (3,4))\n generic_filter1d(a, fnc.filter, 3)\n[0, 0]\n[1, 0]\n[2, 0]\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "code"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Filter functions->Fourier domain filters": [
        [
            "The functions described in this section perform filtering\noperations in the Fourier domain. Thus, the input array of such a\nfunction should be compatible with an inverse Fourier transform\nfunction, such as the functions from the numpy.fft module. We,\ntherefore, have to deal with arrays that may be the result of a real\nor a complex Fourier transform. In the case of a real Fourier\ntransform, only half of the of the symmetric complex transform is\nstored. Additionally, it needs to be known what the length of the\naxis was that was transformed by the real fft. The functions\ndescribed here provide a parameter n that, in the case of a real\ntransform, must be equal to the length of the real transform axis\nbefore transformation. If this parameter is less than zero, it is\nassumed that the input array was the result of a complex Fourier\ntransform. The parameter axis can be used to indicate along which\naxis the real transform was executed.",
            "markdown"
        ],
        [
            "The fourier_shift function multiplies the input array with\nthe multidimensional Fourier transform of a shift operation for the\ngiven shift. The shift parameter is a sequence of shifts for each\ndimension or a single value for all dimensions.",
            "markdown"
        ],
        [
            "The fourier_gaussian function multiplies the input array\nwith the multidimensional Fourier transform of a Gaussian filter\nwith given standard deviations sigma. The sigma parameter is a\nsequence of values for each dimension or a single value for all\ndimensions.",
            "markdown"
        ],
        [
            "The fourier_uniform function multiplies the input array with\nthe multidimensional Fourier transform of a uniform filter with\ngiven sizes size. The size parameter is a sequence of values\nfor each dimension or a single value for all dimensions.",
            "markdown"
        ],
        [
            "The fourier_ellipsoid function multiplies the input array\nwith the multidimensional Fourier transform of an elliptically-shaped\nfilter with given sizes size. The size parameter is a sequence\nof values for each dimension or a single value for all dimensions.\nThis function is only implemented for dimensions 1, 2, and 3.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Interpolation functions": [
        [
            "This section describes various interpolation functions that are based\non B-spline theory. A good introduction to B-splines can be found\nin [1] with detailed algorithms for image interpolation given in [5].",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Interpolation functions->Spline pre-filters": [
        [
            "Interpolation using splines of an order larger than 1 requires a\npre-filtering step. The interpolation functions described in section\nInterpolation functions apply pre-filtering by calling\nspline_filter, but they can be instructed not to do this by\nsetting the prefilter keyword equal to False. This is useful if more\nthan one interpolation operation is done on the same array. In this\ncase, it is more efficient to do the pre-filtering only once and use a\npre-filtered array as the input of the interpolation functions. The\nfollowing two functions implement the pre-filtering:",
            "markdown"
        ],
        [
            "The spline_filter1d function calculates a 1-D\nspline filter along the given axis. An output array can optionally\nbe provided. The order of the spline must be larger than 1 and less\nthan 6.",
            "markdown"
        ],
        [
            "The spline_filter function calculates a multidimensional\nspline filter.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The multidimensional filter is implemented as a sequence of\n1-D spline filters. The intermediate arrays are\nstored in the same data type as the output. Therefore, if an\noutput with a limited precision is requested, the results may be\nimprecise because intermediate results may be stored with\ninsufficient precision. This can be prevented by specifying a\noutput type of high precision.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Interpolation functions->Interpolation boundary handling": [
        [
            "The interpolation functions all employ spline interpolation to effect some\ntype of geometric transformation of the input array. This requires a\nmapping of the output coordinates to the input coordinates, and\ntherefore, the possibility arises that input values outside the\nboundaries may be needed. This problem is solved in the same way as\ndescribed in Filter functions for the multidimensional\nfilter functions. Therefore, these functions all support a mode\nparameter that determines how the boundaries are handled, and a cval\nparameter that gives a constant value in case that the \u00e2\u0080\u0098constant\u00e2\u0080\u0099 mode\nis used. The behavior of all modes, including at non-integer locations is\nillustrated below. Note the boundaries are not handled the same for all modes;\n<em class=\"xref py py-obj\">reflect (aka <em class=\"xref py py-obj\">grid-mirror) and <em class=\"xref py py-obj\">grid-wrap involve symmetry or repetition\nabout a point that is half way between image samples (dashed vertical lines)\nwhile modes <em class=\"xref py py-obj\">mirror and <em class=\"xref py py-obj\">wrap treat the image as if it\u00e2\u0080\u0099s extent ends exactly\nat the first and last sample point rather than 0.5 samples past it.\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/plot_boundary_modes.png\"/>\n</figure>",
            "markdown"
        ],
        [
            "The coordinates of image samples fall on integer sampling locations\nin the range from 0 to shape[i] - 1 along each axis, i. The figure\nbelow illustrates the interpolation of a point at location (3.7, 3.3)\nwithin an image of shape (7, 7). For an interpolation of order n,\nn + 1 samples are involved along each axis. The filled circles\nillustrate the sampling locations involved in the interpolation of the value at\nthe location of the red x.\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/plot_interp_grid.png\"/>\n</figure>",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Interpolation functions->Interpolation functions": [
        [
            "The geometric_transform function applies an arbitrary\ngeometric transform to the input. The given mapping function is\ncalled at each point in the output to find the corresponding\ncoordinates in the input. mapping must be a callable object that\naccepts a tuple of length equal to the output array rank and returns\nthe corresponding input coordinates as a tuple of length equal to\nthe input array rank. The output shape and output type can\noptionally be provided. If not given, they are equal to the input\nshape and type.",
            "markdown"
        ],
        [
            "For example:",
            "markdown"
        ],
        [
            "a = np.arange(12).reshape(4,3).astype(np.float64)\n def shift_func(output_coordinates):\n...     return (output_coordinates[0] - 0.5, output_coordinates[1] - 0.5)\n...\n from scipy.ndimage import geometric_transform\n geometric_transform(a, shift_func)\narray([[ 0.    ,  0.    ,  0.    ],\n       [ 0.    ,  1.3625,  2.7375],\n       [ 0.    ,  4.8125,  6.1875],\n       [ 0.    ,  8.2625,  9.6375]])",
            "code"
        ],
        [
            "Optionally, extra arguments can be defined and passed to the filter\nfunction. The extra_arguments and extra_keywords arguments can\nbe used to pass a tuple of extra arguments and/or a dictionary of\nnamed arguments that are passed to derivative at each call. For\nexample, we can pass the shifts in our example as arguments",
            "markdown"
        ],
        [
            "def shift_func(output_coordinates, s0, s1):\n...     return (output_coordinates[0] - s0, output_coordinates[1] - s1)\n...\n geometric_transform(a, shift_func, extra_arguments = (0.5, 0.5))\narray([[ 0.    ,  0.    ,  0.    ],\n       [ 0.    ,  1.3625,  2.7375],\n       [ 0.    ,  4.8125,  6.1875],\n       [ 0.    ,  8.2625,  9.6375]])",
            "code"
        ],
        [
            "or",
            "markdown"
        ],
        [
            "geometric_transform(a, shift_func, extra_keywords = {'s0': 0.5, 's1': 0.5})\narray([[ 0.    ,  0.    ,  0.    ],\n       [ 0.    ,  1.3625,  2.7375],\n       [ 0.    ,  4.8125,  6.1875],\n       [ 0.    ,  8.2625,  9.6375]])",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The mapping function can also be written in C and passed using a\nscipy.LowLevelCallable. See Extending scipy.ndimage in C for more\ninformation.",
            "markdown"
        ],
        [
            "The function map_coordinates applies an arbitrary coordinate\ntransformation using the given array of coordinates. The shape of\nthe output is derived from that of the coordinate array by dropping\nthe first axis. The parameter coordinates is used to find for each\npoint in the output the corresponding coordinates in the input. The\nvalues of coordinates along the first axis are the coordinates in\nthe input array at which the output value is found. (See also the\nnumarray <em class=\"xref py py-obj\">coordinates function.) Since the coordinates may be non-\ninteger coordinates, the value of the input at these coordinates is\ndetermined by spline interpolation of the requested order.",
            "markdown"
        ],
        [
            "Here is an example that interpolates a 2D array at (0.5, 0.5) and\n(1, 2):",
            "markdown"
        ],
        [
            "a = np.arange(12).reshape(4,3).astype(np.float64)\n a\narray([[  0.,   1.,   2.],\n       [  3.,   4.,   5.],\n       [  6.,   7.,   8.],\n       [  9.,  10.,  11.]])\n from scipy.ndimage import map_coordinates\n map_coordinates(a, [[0.5, 2], [0.5, 1]])\narray([ 1.3625,  7.])",
            "code"
        ],
        [
            "The affine_transform function applies an affine\ntransformation to the input array. The given transformation matrix\nand offset are used to find for each point in the output the\ncorresponding coordinates in the input. The value of the input at\nthe calculated coordinates is determined by spline interpolation of\nthe requested order. The transformation matrix must be\n2-D or can also be given as a 1-D sequence\nor array. In the latter case, it is assumed that the matrix is\ndiagonal. A more efficient interpolation algorithm is then applied\nthat exploits the separability of the problem. The output shape and\noutput type can optionally be provided. If not given, they are equal\nto the input shape and type.",
            "markdown"
        ],
        [
            "The shift function returns a shifted version of the input,\nusing spline interpolation of the requested order.",
            "markdown"
        ],
        [
            "The zoom function returns a rescaled version of the input,\nusing spline interpolation of the requested order.",
            "markdown"
        ],
        [
            "The rotate function returns the input array rotated in the\nplane defined by the two axes given by the parameter axes, using\nspline interpolation of the requested order. The angle must be\ngiven in degrees. If reshape is true, then the size of the output\narray is adapted to contain the rotated input.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Morphology->Binary morphology": [
        [
            "The generate_binary_structure functions generates a binary\nstructuring element for use in binary morphology operations. The\nrank of the structure must be provided. The size of the structure\nthat is returned is equal to three in each direction. The value of\neach element is equal to one if the square of the Euclidean distance\nfrom the element to the center is less than or equal to\nconnectivity. For instance, 2-D 4-connected and\n8-connected structures are generated as follows:",
            "markdown"
        ],
        [
            "from scipy.ndimage import generate_binary_structure\n generate_binary_structure(2, 1)\narray([[False,  True, False],\n       [ True,  True,  True],\n       [False,  True, False]], dtype=bool)\n generate_binary_structure(2, 2)\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]], dtype=bool)",
            "code"
        ],
        [
            "This is a viusal presentation of generate_binary_structure in 3D:\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/3D_binary_structure.png\"/>\n</figure>\n</blockquote>",
            "markdown"
        ],
        [
            "Most binary morphology functions can be expressed in terms of the\nbasic operations erosion and dilation, which can be seen here:\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/morphology_binary_dilation_erosion.png\"/>\n</figure>\n</blockquote>",
            "markdown"
        ],
        [
            "The binary_erosion function implements binary erosion of\narrays of arbitrary rank with the given structuring element. The\norigin parameter controls the placement of the structuring element,\nas described in Filter functions. If no structuring\nelement is provided, an element with connectivity equal to one is\ngenerated using generate_binary_structure. The\nborder_value parameter gives the value of the array outside\nboundaries. The erosion is repeated iterations times. If\niterations is less than one, the erosion is repeated until the\nresult does not change anymore. If a mask array is given, only\nthose elements with a true value at the corresponding mask element\nare modified at each iteration.",
            "markdown"
        ],
        [
            "The binary_dilation function implements binary dilation of\narrays of arbitrary rank with the given structuring element. The\norigin parameter controls the placement of the structuring element,\nas described in Filter functions. If no structuring\nelement is provided, an element with connectivity equal to one is\ngenerated using generate_binary_structure. The\nborder_value parameter gives the value of the array outside\nboundaries. The dilation is repeated iterations times. If\niterations is less than one, the dilation is repeated until the\nresult does not change anymore. If a mask array is given, only\nthose elements with a true value at the corresponding mask element\nare modified at each iteration.",
            "markdown"
        ],
        [
            "Here is an example of using binary_dilation to find all elements\nthat touch the border, by repeatedly dilating an empty array from\nthe border using the data array as the mask:",
            "markdown"
        ],
        [
            "struct = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n a = np.array([[1,0,0,0,0], [1,1,0,1,0], [0,0,1,1,0], [0,0,0,0,0]])\n a\narray([[1, 0, 0, 0, 0],\n       [1, 1, 0, 1, 0],\n       [0, 0, 1, 1, 0],\n       [0, 0, 0, 0, 0]])\n from scipy.ndimage import binary_dilation\n binary_dilation(np.zeros(a.shape), struct, -1, a, border_value=1)\narray([[ True, False, False, False, False],\n       [ True,  True, False, False, False],\n       [False, False, False, False, False],\n       [False, False, False, False, False]], dtype=bool)",
            "code"
        ],
        [
            "The binary_erosion and binary_dilation functions both\nhave an iterations parameter, which allows the erosion or dilation to\nbe repeated a number of times. Repeating an erosion or a dilation with\na given structure n times is equivalent to an erosion or a dilation\nwith a structure that is n-1 times dilated with itself. A function\nis provided that allows the calculation of a structure that is dilated\na number of times with itself:",
            "markdown"
        ],
        [
            "The iterate_structure function returns a structure by dilation\nof the input structure iteration - 1 times with itself.",
            "markdown"
        ],
        [
            "For instance:",
            "markdown"
        ],
        [
            "struct = generate_binary_structure(2, 1)\n struct\narray([[False,  True, False],\n       [ True,  True,  True],\n       [False,  True, False]], dtype=bool)\n from scipy.ndimage import iterate_structure\n iterate_structure(struct, 2)\narray([[False, False,  True, False, False],\n       [False,  True,  True,  True, False],\n       [ True,  True,  True,  True,  True],\n       [False,  True,  True,  True, False],\n       [False, False,  True, False, False]], dtype=bool)\n\nIf the origin of the original structure is equal to 0, then it is\nalso equal to 0 for the iterated structure. If not, the origin\nmust also be adapted if the equivalent of the *iterations*\nerosions or dilations must be achieved with the iterated\nstructure. The adapted origin is simply obtained by multiplying\nwith the number of iterations. For convenience, the\n:func:`iterate_structure` also returns the adapted origin if the\n*origin* parameter is not ``None``:\n\n.. code:: python\n\n    iterate_structure(struct, 2, -1)\n   (array([[False, False,  True, False, False],\n           [False,  True,  True,  True, False],\n           [ True,  True,  True,  True,  True],\n           [False,  True,  True,  True, False],\n           [False, False,  True, False, False]], dtype=bool), [-2, -2])",
            "code"
        ],
        [
            "Other morphology operations can be defined in terms of erosion and\ndilation. The following functions provide a few of these operations\nfor convenience:",
            "markdown"
        ],
        [
            "The binary_opening function implements binary opening of\narrays of arbitrary rank with the given structuring element. Binary\nopening is equivalent to a binary erosion followed by a binary\ndilation with the same structuring element. The origin parameter\ncontrols the placement of the structuring element, as described in\nFilter functions. If no structuring element is\nprovided, an element with connectivity equal to one is generated\nusing generate_binary_structure. The iterations parameter\ngives the number of erosions that is performed followed by the same\nnumber of dilations.",
            "markdown"
        ],
        [
            "The binary_closing function implements binary closing of\narrays of arbitrary rank with the given structuring element. Binary\nclosing is equivalent to a binary dilation followed by a binary\nerosion with the same structuring element. The origin parameter\ncontrols the placement of the structuring element, as described in\nFilter functions. If no structuring element is\nprovided, an element with connectivity equal to one is generated\nusing generate_binary_structure. The iterations parameter\ngives the number of dilations that is performed followed by the same\nnumber of erosions.",
            "markdown"
        ],
        [
            "The binary_fill_holes function is used to close holes in\nobjects in a binary image, where the structure defines the\nconnectivity of the holes. The origin parameter controls the\nplacement of the structuring element, as described in\nFilter functions. If no structuring element is\nprovided, an element with connectivity equal to one is generated\nusing generate_binary_structure.",
            "markdown"
        ],
        [
            "The binary_hit_or_miss function implements a binary\nhit-or-miss transform of arrays of arbitrary rank with the given\nstructuring elements. The hit-or-miss transform is calculated by\nerosion of the input with the first structure, erosion of the\nlogical not of the input with the second structure, followed by\nthe logical and of these two erosions. The origin parameters\ncontrol the placement of the structuring elements, as described in\nFilter functions. If origin2 equals None, it is set\nequal to the origin1 parameter. If the first structuring element\nis not provided, a structuring element with connectivity equal to\none is generated using generate_binary_structure. If\nstructure2 is not provided, it is set equal to the logical not\nof structure1.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Morphology->Grey-scale morphology": [
        [
            "Grey-scale morphology operations are the equivalents of binary\nmorphology operations that operate on arrays with arbitrary values.\nBelow, we describe the grey-scale equivalents of erosion, dilation,\nopening and closing. These operations are implemented in a similar\nfashion as the filters described in Filter functions,\nand we refer to this section for the description of filter kernels and\nfootprints, and the handling of array borders. The grey-scale\nmorphology operations optionally take a structure parameter that\ngives the values of the structuring element. If this parameter is not\ngiven, the structuring element is assumed to be flat with a value equal\nto zero. The shape of the structure can optionally be defined by the\nfootprint parameter. If this parameter is not given, the structure\nis assumed to be rectangular, with sizes equal to the dimensions of\nthe structure array, or by the size parameter if structure is\nnot given. The size parameter is only used if both structure and\nfootprint are not given, in which case the structuring element is\nassumed to be rectangular and flat with the dimensions given by\nsize. The size parameter, if provided, must be a sequence of sizes\nor a single number in which case the size of the filter is assumed to\nbe equal along each axis. The footprint parameter, if provided, must\nbe an array that defines the shape of the kernel by its non-zero\nelements.",
            "markdown"
        ],
        [
            "Similarly to binary erosion and dilation, there are operations for\ngrey-scale erosion and dilation:",
            "markdown"
        ],
        [
            "The grey_erosion function calculates a multidimensional\ngrey-scale erosion.",
            "markdown"
        ],
        [
            "The grey_dilation function calculates a multidimensional\ngrey-scale dilation.",
            "markdown"
        ],
        [
            "Grey-scale opening and closing operations can be defined similarly to\ntheir binary counterparts:",
            "markdown"
        ],
        [
            "The grey_opening function implements grey-scale opening of\narrays of arbitrary rank. Grey-scale opening is equivalent to a\ngrey-scale erosion followed by a grey-scale dilation.",
            "markdown"
        ],
        [
            "The grey_closing function implements grey-scale closing of\narrays of arbitrary rank. Grey-scale opening is equivalent to a\ngrey-scale dilation followed by a grey-scale erosion.",
            "markdown"
        ],
        [
            "The morphological_gradient function implements a grey-scale\nmorphological gradient of arrays of arbitrary rank. The grey-scale\nmorphological gradient is equal to the difference of a grey-scale\ndilation and a grey-scale erosion.",
            "markdown"
        ],
        [
            "The morphological_laplace function implements a grey-scale\nmorphological laplace of arrays of arbitrary rank. The grey-scale\nmorphological laplace is equal to the sum of a grey-scale dilation\nand a grey-scale erosion minus twice the input.",
            "markdown"
        ],
        [
            "The white_tophat function implements a white top-hat filter\nof arrays of arbitrary rank. The white top-hat is equal to the\ndifference of the input and a grey-scale opening.",
            "markdown"
        ],
        [
            "The black_tophat function implements a black top-hat filter\nof arrays of arbitrary rank. The black top-hat is equal to the\ndifference of a grey-scale closing and the input.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Distance transforms": [
        [
            "Distance transforms are used to calculate the minimum distance from\neach element of an object to the background. The following functions\nimplement distance transforms for three different distance metrics:\nEuclidean, city block, and chessboard distances.",
            "markdown"
        ],
        [
            "The function distance_transform_cdt uses a chamfer type\nalgorithm to calculate the distance transform of the input, by\nreplacing each object element (defined by values larger than zero)\nwith the shortest distance to the background (all non-object\nelements). The structure determines the type of chamfering that is\ndone. If the structure is equal to \u00e2\u0080\u0098cityblock\u00e2\u0080\u0099, a structure is\ngenerated using generate_binary_structure with a squared\ndistance equal to 1. If the structure is equal to \u00e2\u0080\u0098chessboard\u00e2\u0080\u0099, a\nstructure is generated using generate_binary_structure with\na squared distance equal to the rank of the array. These choices\ncorrespond to the common interpretations of the city block and the\nchessboard distance metrics in two dimensions.",
            "markdown"
        ],
        [
            "In addition to the distance transform, the feature transform can be\ncalculated. In this case, the index of the closest background element\nis returned along the first axis of the result. The\nreturn_distances, and return_indices flags can be used to\nindicate if the distance transform, the feature transform, or both\nmust be returned.",
            "markdown"
        ],
        [
            "The distances and indices arguments can be used to give optional\noutput arrays that must be of the correct size and type (both\nnumpy.int32). The basics of the algorithm used to implement this\nfunction are described in [2].",
            "markdown"
        ],
        [
            "The function distance_transform_edt calculates the exact\nEuclidean distance transform of the input, by replacing each object\nelement (defined by values larger than zero) with the shortest\nEuclidean distance to the background (all non-object elements).",
            "markdown"
        ],
        [
            "In addition to the distance transform, the feature transform can be\ncalculated. In this case, the index of the closest background element\nis returned along the first axis of the result. The\nreturn_distances and return_indices flags can be used to\nindicate if the distance transform, the feature transform, or both\nmust be returned.",
            "markdown"
        ],
        [
            "Optionally, the sampling along each axis can be given by the\nsampling parameter, which should be a sequence of length equal to\nthe input rank, or a single number in which the sampling is assumed\nto be equal along all axes.",
            "markdown"
        ],
        [
            "The distances and indices arguments can be used to give optional\noutput arrays that must be of the correct size and type\n(numpy.float64 and numpy.int32).The algorithm used to\nimplement this function is described in [3].",
            "markdown"
        ],
        [
            "The function distance_transform_bf uses a brute-force\nalgorithm to calculate the distance transform of the input, by\nreplacing each object element (defined by values larger than zero)\nwith the shortest distance to the background (all non-object\nelements). The metric must be one of \u00e2\u0080\u009ceuclidean\u00e2\u0080\u009d, \u00e2\u0080\u009ccityblock\u00e2\u0080\u009d, or\n\u00e2\u0080\u009cchessboard\u00e2\u0080\u009d.",
            "markdown"
        ],
        [
            "In addition to the distance transform, the feature transform can be\ncalculated. In this case, the index of the closest background element\nis returned along the first axis of the result. The\nreturn_distances and return_indices flags can be used to\nindicate if the distance transform, the feature transform, or both\nmust be returned.",
            "markdown"
        ],
        [
            "Optionally, the sampling along each axis can be given by the\nsampling parameter, which should be a sequence of length equal to\nthe input rank, or a single number in which the sampling is assumed\nto be equal along all axes. This parameter is only used in the case\nof the Euclidean distance transform.",
            "markdown"
        ],
        [
            "The distances and indices arguments can be used to give optional\noutput arrays that must be of the correct size and type\n(numpy.float64 and numpy.int32).",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This function uses a slow brute-force algorithm, the function\ndistance_transform_cdt can be used to more efficiently\ncalculate city block and chessboard distance transforms. The\nfunction distance_transform_edt can be used to more\nefficiently calculate the exact Euclidean distance transform.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Segmentation and labeling": [
        [
            "Segmentation is the process of separating objects of interest from\nthe background. The most simple approach is, probably, intensity\nthresholding, which is easily done with numpy functions:",
            "markdown"
        ],
        [
            "a = np.array([[1,2,2,1,1,0],\n...               [0,2,3,1,2,0],\n...               [1,1,1,3,3,2],\n...               [1,1,1,1,2,1]])\n np.where(a  1, 1, 0)\narray([[0, 1, 1, 0, 0, 0],\n       [0, 1, 1, 0, 1, 0],\n       [0, 0, 0, 1, 1, 1],\n       [0, 0, 0, 0, 1, 0]])",
            "code"
        ],
        [
            "The result is a binary image, in which the individual objects still\nneed to be identified and labeled. The function label\ngenerates an array where each object is assigned a unique number:",
            "markdown"
        ],
        [
            "The label function generates an array where the objects in\nthe input are labeled with an integer index. It returns a tuple\nconsisting of the array of object labels and the number of objects\nfound, unless the output parameter is given, in which case only\nthe number of objects is returned. The connectivity of the objects\nis defined by a structuring element. For instance, in 2D\nusing a 4-connected structuring element gives:",
            "markdown"
        ],
        [
            "a = np.array([[0,1,1,0,0,0],[0,1,1,0,1,0],[0,0,0,1,1,1],[0,0,0,0,1,0]])\n s = [[0, 1, 0], [1,1,1], [0,1,0]]\n from scipy.ndimage import label\n label(a, s)\n(array([[0, 1, 1, 0, 0, 0],\n        [0, 1, 1, 0, 2, 0],\n        [0, 0, 0, 2, 2, 2],\n        [0, 0, 0, 0, 2, 0]]), 2)",
            "code"
        ],
        [
            "These two objects are not connected because there is no way in which\nwe can place the structuring element, such that it overlaps with both\nobjects. However, an 8-connected structuring element results in only\na single object:",
            "markdown"
        ],
        [
            "a = np.array([[0,1,1,0,0,0],[0,1,1,0,1,0],[0,0,0,1,1,1],[0,0,0,0,1,0]])\n s = [[1,1,1], [1,1,1], [1,1,1]]\n label(a, s)[0]\narray([[0, 1, 1, 0, 0, 0],\n       [0, 1, 1, 0, 1, 0],\n       [0, 0, 0, 1, 1, 1],\n       [0, 0, 0, 0, 1, 0]])",
            "code"
        ],
        [
            "If no structuring element is provided, one is generated by calling\ngenerate_binary_structure (see\nBinary morphology) using a connectivity of one (which\nin 2D is the 4-connected structure of the first example). The input\ncan be of any type, any value not equal to zero is taken to be part\nof an object. This is useful if you need to \u00e2\u0080\u0098re-label\u00e2\u0080\u0099 an array of\nobject indices, for instance, after removing unwanted objects. Just\napply the label function again to the index array. For instance:",
            "markdown"
        ],
        [
            "l, n = label([1, 0, 1, 0, 1])\n l\narray([1, 0, 2, 0, 3])\n l = np.where(l != 2, l, 0)\n l\narray([1, 0, 0, 0, 3])\n label(l)[0]\narray([1, 0, 0, 0, 2])",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The structuring element used by label is assumed to be\nsymmetric.",
            "markdown"
        ],
        [
            "There is a large number of other approaches for segmentation, for\ninstance, from an estimation of the borders of the objects that can be\nobtained by derivative filters. One such approach is\nwatershed segmentation. The function watershed_ift generates\nan array where each object is assigned a unique label, from an array\nthat localizes the object borders, generated, for instance, by a\ngradient magnitude filter. It uses an array containing initial markers\nfor the objects:",
            "markdown"
        ],
        [
            "The watershed_ift function applies a watershed from markers\nalgorithm, using Image Foresting Transform, as described in\n[4].",
            "markdown"
        ],
        [
            "The inputs of this function are the array to which the transform is\napplied, and an array of markers that designate the objects by a\nunique label, where any non-zero value is a marker. For instance:",
            "markdown"
        ],
        [
            "input = np.array([[0, 0, 0, 0, 0, 0, 0],\n...                   [0, 1, 1, 1, 1, 1, 0],\n...                   [0, 1, 0, 0, 0, 1, 0],\n...                   [0, 1, 0, 0, 0, 1, 0],\n...                   [0, 1, 0, 0, 0, 1, 0],\n...                   [0, 1, 1, 1, 1, 1, 0],\n...                   [0, 0, 0, 0, 0, 0, 0]], np.uint8)\n markers = np.array([[1, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 2, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0]], np.int8)\n from scipy.ndimage import watershed_ift\n watershed_ift(input, markers)\narray([[1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 2, 2, 2, 1, 1],\n       [1, 2, 2, 2, 2, 2, 1],\n       [1, 2, 2, 2, 2, 2, 1],\n       [1, 2, 2, 2, 2, 2, 1],\n       [1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1]], dtype=int8)",
            "code"
        ],
        [
            "Here, two markers were used to designate an object (marker = 2) and\nthe background (marker = 1). The order in which these are\nprocessed is arbitrary: moving the marker for the background to the\nlower-right corner of the array yields a different result:",
            "markdown"
        ],
        [
            "markers = np.array([[0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 2, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 1]], np.int8)\n watershed_ift(input, markers)\narray([[1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1]], dtype=int8)",
            "code"
        ],
        [
            "The result is that the object (marker = 2) is smaller because the\nsecond marker was processed earlier. This may not be the desired\neffect if the first marker was supposed to designate a background\nobject. Therefore, watershed_ift treats markers with a\nnegative value explicitly as background markers and processes them\nafter the normal markers. For instance, replacing the first marker\nby a negative marker gives a result similar to the first example:",
            "markdown"
        ],
        [
            "markers = np.array([[0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 2, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, -1]], np.int8)\n watershed_ift(input, markers)\narray([[-1, -1, -1, -1, -1, -1, -1],\n       [-1, -1,  2,  2,  2, -1, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1, -1,  2,  2,  2, -1, -1],\n       [-1, -1, -1, -1, -1, -1, -1]], dtype=int8)",
            "code"
        ],
        [
            "The connectivity of the objects is defined by a structuring\nelement. If no structuring element is provided, one is generated by\ncalling generate_binary_structure (see\nBinary morphology) using a connectivity of one (which\nin 2D is a 4-connected structure.) For example, using an 8-connected\nstructure with the last example yields a different object:",
            "markdown"
        ],
        [
            "watershed_ift(input, markers,\n...               structure = [[1,1,1], [1,1,1], [1,1,1]])\narray([[-1, -1, -1, -1, -1, -1, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1, -1, -1, -1, -1, -1, -1]], dtype=int8)",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The implementation of watershed_ift limits the data types\nof the input to numpy.uint8 and numpy.uint16.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Object measurements": [
        [
            "Given an array of labeled objects, the properties of the individual\nobjects can be measured. The find_objects function can be used\nto generate a list of slices that for each object, give the\nsmallest sub-array that fully contains the object:",
            "markdown"
        ],
        [
            "The find_objects function finds all objects in a labeled\narray and returns a list of slices that correspond to the smallest\nregions in the array that contains the object.",
            "markdown"
        ],
        [
            "For instance:",
            "markdown"
        ],
        [
            "a = np.array([[0,1,1,0,0,0],[0,1,1,0,1,0],[0,0,0,1,1,1],[0,0,0,0,1,0]])\n l, n = label(a)\n from scipy.ndimage import find_objects\n f = find_objects(l)\n a[f[0]]\narray([[1, 1],\n       [1, 1]])\n a[f[1]]\narray([[0, 1, 0],\n       [1, 1, 1],\n       [0, 1, 0]])",
            "code"
        ],
        [
            "The function find_objects returns slices for all objects,\nunless the max_label parameter is larger then zero, in which case\nonly the first max_label objects are returned. If an index is\nmissing in the label array, None is return instead of a\nslice. For example:",
            "markdown"
        ],
        [
            "from scipy.ndimage import find_objects\n find_objects([1, 0, 3, 4], max_label = 3)\n[(slice(0, 1, None),), None, (slice(2, 3, None),)]",
            "code"
        ],
        [
            "The list of slices generated by find_objects is useful to find\nthe position and dimensions of the objects in the array, but can also\nbe used to perform measurements on the individual objects. Say, we want\nto find the sum of the intensities of an object in image:",
            "markdown"
        ],
        [
            "image = np.arange(4 * 6).reshape(4, 6)\n mask = np.array([[0,1,1,0,0,0],[0,1,1,0,1,0],[0,0,0,1,1,1],[0,0,0,0,1,0]])\n labels = label(mask)[0]\n slices = find_objects(labels)",
            "code"
        ],
        [
            "Then we can calculate the sum of the elements in the second object:",
            "markdown"
        ],
        [
            "np.where(labels[slices[1]] == 2, image[slices[1]], 0).sum()\n80",
            "code"
        ],
        [
            "That is, however, not particularly efficient and may also be more\ncomplicated for other types of measurements. Therefore, a few\nmeasurements functions are defined that accept the array of object\nlabels and the index of the object to be measured. For instance,\ncalculating the sum of the intensities can be done by:",
            "markdown"
        ],
        [
            "from scipy.ndimage import sum as ndi_sum\n ndi_sum(image, labels, 2)\n80",
            "code"
        ],
        [
            "For large arrays and small objects, it is more efficient to call the\nmeasurement functions after slicing the array:",
            "markdown"
        ],
        [
            "ndi_sum(image[slices[1]], labels[slices[1]], 2)\n80",
            "code"
        ],
        [
            "Alternatively, we can do the measurements for a number of labels with\na single function call, returning a list of results. For instance, to\nmeasure the sum of the values of the background and the second object\nin our example, we give a list of labels:",
            "markdown"
        ],
        [
            "ndi_sum(image, labels, [0, 2])\narray([178.0, 80.0])",
            "code"
        ],
        [
            "The measurement functions described below all support the index\nparameter to indicate which object(s) should be measured. The default\nvalue of index is None. This indicates that all elements where the\nlabel is larger than zero should be treated as a single object and\nmeasured. Thus, in this case the labels array is treated as a mask\ndefined by the elements that are larger than zero. If index is a\nnumber or a sequence of numbers it gives the labels of the objects\nthat are measured. If index is a sequence, a list of the results is\nreturned. Functions that return more than one result return their\nresult as a tuple if index is a single number, or as a tuple of\nlists if index is a sequence.",
            "markdown"
        ],
        [
            "The sum function calculates the sum of the elements of the\nobject with label(s) given by index, using the labels array for\nthe object labels. If index is None, all elements with a\nnon-zero label value are treated as a single object. If label is\nNone, all elements of input are used in the calculation.",
            "markdown"
        ],
        [
            "The mean function calculates the mean of the elements of the\nobject with label(s) given by index, using the labels array for\nthe object labels. If index is None, all elements with a\nnon-zero label value are treated as a single object. If label is\nNone, all elements of input are used in the calculation.",
            "markdown"
        ],
        [
            "The variance function calculates the variance of the\nelements of the object with label(s) given by index, using the\nlabels array for the object labels. If index is None, all\nelements with a non-zero label value are treated as a single\nobject. If label is None, all elements of input are used in\nthe calculation.",
            "markdown"
        ],
        [
            "The standard_deviation function calculates the standard\ndeviation of the elements of the object with label(s) given by\nindex, using the labels array for the object labels. If index\nis None, all elements with a non-zero label value are treated as\na single object. If label is None, all elements of input are\nused in the calculation.",
            "markdown"
        ],
        [
            "The minimum function calculates the minimum of the elements\nof the object with label(s) given by index, using the labels\narray for the object labels. If index is None, all elements\nwith a non-zero label value are treated as a single object. If\nlabel is None, all elements of input are used in the\ncalculation.",
            "markdown"
        ],
        [
            "The maximum function calculates the maximum of the elements\nof the object with label(s) given by index, using the labels\narray for the object labels. If index is None, all elements\nwith a non-zero label value are treated as a single object. If\nlabel is None, all elements of input are used in the\ncalculation.",
            "markdown"
        ],
        [
            "The minimum_position function calculates the position of the\nminimum of the elements of the object with label(s) given by\nindex, using the labels array for the object labels. If index\nis None, all elements with a non-zero label value are treated as\na single object. If label is None, all elements of input are\nused in the calculation.",
            "markdown"
        ],
        [
            "The maximum_position function calculates the position of the\nmaximum of the elements of the object with label(s) given by\nindex, using the labels array for the object labels. If index\nis None, all elements with a non-zero label value are treated as\na single object. If label is None, all elements of input are\nused in the calculation.",
            "markdown"
        ],
        [
            "The extrema function calculates the minimum, the maximum,\nand their positions, of the elements of the object with label(s)\ngiven by index, using the labels array for the object labels. If\nindex is None, all elements with a non-zero label value are\ntreated as a single object. If label is None, all elements of\ninput are used in the calculation. The result is a tuple giving\nthe minimum, the maximum, the position of the minimum, and the\nposition of the maximum. The result is the same as a tuple formed by\nthe results of the functions minimum, maximum,\nminimum_position, and maximum_position that are described above.",
            "markdown"
        ],
        [
            "The center_of_mass function calculates the center of mass of\nthe object with label(s) given by index, using the labels\narray for the object labels. If index is None, all elements\nwith a non-zero label value are treated as a single object. If\nlabel is None, all elements of input are used in the\ncalculation.",
            "markdown"
        ],
        [
            "The histogram function calculates a histogram of the\nobject with label(s) given by index, using the labels array for\nthe object labels. If index is None, all elements with a\nnon-zero label value are treated as a single object. If label is\nNone, all elements of input are used in the calculation.\nHistograms are defined by their minimum (min), maximum (max), and\nthe number of bins (bins). They are returned as 1-D\narrays of type numpy.int32.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->Extending scipy.ndimage in C": [
        [
            "A few functions in scipy.ndimage take a callback argument. This\ncan be either a python function or a scipy.LowLevelCallable containing a\npointer to a C function. Using a C function will generally be more\nefficient, since it avoids the overhead of calling a python function on\nmany elements of an array. To use a C function, you must write a C\nextension that contains the callback function and a Python function\nthat returns a scipy.LowLevelCallable containing a pointer to the\ncallback.",
            "markdown"
        ],
        [
            "An example of a function that supports callbacks is\ngeometric_transform, which accepts a callback function that\ndefines a mapping from all output coordinates to corresponding\ncoordinates in the input array. Consider the following python example,\nwhich uses geometric_transform to implement a shift function.",
            "markdown"
        ],
        [
            "from scipy import ndimage\n\ndef transform(output_coordinates, shift):\n    input_coordinates = output_coordinates[0] - shift, output_coordinates[1] - shift\n    return input_coordinates\n\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nshift = 0.5\nprint(ndimage.geometric_transform(im, transform, extra_arguments=(shift,)))",
            "code"
        ],
        [
            "We can also implement the callback function with the following C code:",
            "markdown"
        ],
        [
            "/* example.c */\n\n#include &lt;Python.h\n#include &lt;numpy/npy_common.h\n\nstatic int\n_transform(npy_intp *output_coordinates, double *input_coordinates,\n           int output_rank, int input_rank, void *user_data)\n{\n    npy_intp i;\n    double shift = *(double *)user_data;\n\n    for (i = 0; i &lt; input_rank; i++) {\n        input_coordinates[i] = output_coordinates[i] - shift;\n    }\n    return 1;\n}\n\nstatic char *transform_signature = \"int (npy_intp *, double *, int, int, void *)\";\n\nstatic PyObject *\npy_get_transform(PyObject *obj, PyObject *args)\n{\n    if (!PyArg_ParseTuple(args, \"\")) return NULL;\n    return PyCapsule_New(_transform, transform_signature, NULL);\n}\n\nstatic PyMethodDef ExampleMethods[] = {\n    {\"get_transform\", (PyCFunction)py_get_transform, METH_VARARGS, \"\"},\n    {NULL, NULL, 0, NULL}\n};\n\n/* Initialize the module */\nstatic struct PyModuleDef example = {\n    PyModuleDef_HEAD_INIT,\n    \"example\",\n    NULL,\n    -1,\n    ExampleMethods,\n    NULL,\n    NULL,\n    NULL,\n    NULL\n};\n\nPyMODINIT_FUNC\nPyInit_example(void)\n{\n    return PyModule_Create(&example);\n}",
            "code"
        ],
        [
            "More information on writing Python extension modules can be found\nhere. If the C code is in the file example.c, then it can be\ncompiled with the following setup.py,",
            "markdown"
        ],
        [
            "from distutils.core import setup, Extension\nimport numpy\n\nshift = Extension('example',\n                  ['example.c'],\n                  include_dirs=[numpy.get_include()]\n)\n\nsetup(name='example',\n      ext_modules=[shift]\n)",
            "code"
        ],
        [
            "and now running the script",
            "markdown"
        ],
        [
            "import ctypes\nimport numpy as np\nfrom scipy import ndimage, LowLevelCallable\n\nfrom example import get_transform\n\nshift = 0.5\n\nuser_data = ctypes.c_double(shift)\nptr = ctypes.cast(ctypes.pointer(user_data), ctypes.c_void_p)\ncallback = LowLevelCallable(get_transform(), ptr)\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nprint(ndimage.geometric_transform(im, callback))",
            "code"
        ],
        [
            "produces the same result as the original python script.",
            "markdown"
        ],
        [
            "In the C version, _transform is the callback function and the\nparameters output_coordinates and input_coordinates play the\nsame role as they do in the python version, while output_rank and\ninput_rank provide the equivalents of len(output_coordinates)\nand len(input_coordinates). The variable shift is passed\nthrough user_data instead of\nextra_arguments. Finally, the C callback function returns an integer\nstatus, which is one upon success and zero otherwise.",
            "markdown"
        ],
        [
            "The function py_transform wraps the callback function in a\nPyCapsule. The main steps are:",
            "markdown"
        ],
        [
            "Initialize a PyCapsule. The first argument is a pointer to\nthe callback function.",
            "markdown"
        ],
        [
            "The second argument is the function signature, which must match exactly\nthe one expected by ndimage.",
            "markdown"
        ],
        [
            "Above, we used  scipy.LowLevelCallable to specify user_data\nthat we generated with ctypes.",
            "markdown"
        ],
        [
            "A different approach would be to supply the data in the capsule context,\nthat can be set by <em class=\"xref py py-obj\">PyCapsule_SetContext and omit specifying\nuser_data in scipy.LowLevelCallable. However, in this approach we would\nneed to deal with allocation/freeing of the data \u00e2\u0080\u0094 freeing the data\nafter the capsule has been destroyed can be done by specifying a non-NULL\ncallback function in the third argument of <em class=\"xref py py-obj\">PyCapsule_New.",
            "markdown"
        ],
        [
            "C callback functions for ndimage all follow this scheme. The\nnext section lists the ndimage functions that accept a C\ncallback function and gives the prototype of the function.",
            "markdown"
        ],
        [
            "See also",
            "markdown"
        ],
        [
            "The functions that support low-level callback arguments are:",
            "markdown"
        ],
        [
            "generic_filter, generic_filter1d, geometric_transform",
            "markdown"
        ],
        [
            "Below, we show alternative ways to write the code, using Numba, Cython,\nctypes, or cffi instead of writing wrapper code in C.",
            "markdown"
        ],
        [
            "Numba",
            "markdown"
        ],
        [
            "Numba provides a way to write low-level functions easily in Python.\nWe can write the above using Numba as:",
            "markdown"
        ],
        [
            "# example.py\nimport numpy as np\nimport ctypes\nfrom scipy import ndimage, LowLevelCallable\nfrom numba import cfunc, types, carray\n\n@cfunc(types.intc(types.CPointer(types.intp),\n                  types.CPointer(types.double),\n                  types.intc,\n                  types.intc,\n                  types.voidptr))\ndef transform(output_coordinates_ptr, input_coordinates_ptr,\n              output_rank, input_rank, user_data):\n    input_coordinates = carray(input_coordinates_ptr, (input_rank,))\n    output_coordinates = carray(output_coordinates_ptr, (output_rank,))\n    shift = carray(user_data, (1,), types.double)[0]\n\n    for i in range(input_rank):\n        input_coordinates[i] = output_coordinates[i] - shift\n\n    return 1\n\nshift = 0.5\n\n# Then call the function\nuser_data = ctypes.c_double(shift)\nptr = ctypes.cast(ctypes.pointer(user_data), ctypes.c_void_p)\ncallback = LowLevelCallable(transform.ctypes, ptr)\n\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nprint(ndimage.geometric_transform(im, callback))",
            "code"
        ],
        [
            "Cython",
            "markdown"
        ],
        [
            "Functionally the same code as above can be written in Cython with\nsomewhat less boilerplate as follows:",
            "markdown"
        ],
        [
            "# example.pyx\n\nfrom numpy cimport npy_intp as intp\n\ncdef api int transform(intp *output_coordinates, double *input_coordinates,\n                       int output_rank, int input_rank, void *user_data):\n    cdef intp i\n    cdef double shift = (&lt;double *user_data)[0]\n\n    for i in range(input_rank):\n        input_coordinates[i] = output_coordinates[i] - shift\n    return 1",
            "code"
        ],
        [
            "# script.py\n\nimport ctypes\nimport numpy as np\nfrom scipy import ndimage, LowLevelCallable\n\nimport example\n\nshift = 0.5\n\nuser_data = ctypes.c_double(shift)\nptr = ctypes.cast(ctypes.pointer(user_data), ctypes.c_void_p)\ncallback = LowLevelCallable.from_cython(example, \"transform\", ptr)\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nprint(ndimage.geometric_transform(im, callback))",
            "code"
        ],
        [
            "cffi",
            "markdown"
        ],
        [
            "With cffi, you can interface with a C function residing in a shared\nlibrary (DLL). First, we need to write the shared library, which we do\nin C \u00e2\u0080\u0094 this example is for Linux/OSX:",
            "markdown"
        ],
        [
            "/*\n  example.c\n  Needs to be compiled with \"gcc -std=c99 -shared -fPIC -o example.so example.c\"\n  or similar\n */\n\n#include &lt;stdint.h\n\nint\n_transform(intptr_t *output_coordinates, double *input_coordinates,\n           int output_rank, int input_rank, void *user_data)\n{\n    int i;\n    double shift = *(double *)user_data;\n\n    for (i = 0; i &lt; input_rank; i++) {\n        input_coordinates[i] = output_coordinates[i] - shift;\n    }\n    return 1;\n}",
            "code"
        ],
        [
            "The Python code calling the library is:",
            "markdown"
        ],
        [
            "import os\nimport numpy as np\nfrom scipy import ndimage, LowLevelCallable\nimport cffi\n\n# Construct the FFI object, and copypaste the function declaration\nffi = cffi.FFI()\nffi.cdef(\"\"\"\nint _transform(intptr_t *output_coordinates, double *input_coordinates,\n               int output_rank, int input_rank, void *user_data);\n\"\"\")\n\n# Open library\nlib = ffi.dlopen(os.path.abspath(\"example.so\"))\n\n# Do the function call\nuser_data = ffi.new('double *', 0.5)\ncallback = LowLevelCallable(lib._transform, user_data)\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nprint(ndimage.geometric_transform(im, callback))",
            "code"
        ],
        [
            "You can find more information in the cffi documentation.",
            "markdown"
        ],
        [
            "ctypes",
            "markdown"
        ],
        [
            "With ctypes, the C code and the compilation of the so/DLL is as for\ncffi above.  The Python code is different:",
            "markdown"
        ],
        [
            "# script.py\n\nimport os\nimport ctypes\nimport numpy as np\nfrom scipy import ndimage, LowLevelCallable\n\nlib = ctypes.CDLL(os.path.abspath('example.so'))\n\nshift = 0.5\n\nuser_data = ctypes.c_double(shift)\nptr = ctypes.cast(ctypes.pointer(user_data), ctypes.c_void_p)\n\n# Ctypes has no built-in intptr type, so override the signature\n# instead of trying to get it via ctypes\ncallback = LowLevelCallable(lib._transform, ptr,\n    \"int _transform(intptr_t *, double *, int, int, void *)\")\n\n# Perform the call\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nprint(ndimage.geometric_transform(im, callback))",
            "code"
        ],
        [
            "You can find more information in the ctypes documentation.",
            "markdown"
        ]
    ],
    "scipy->Multidimensional image processing (scipy.ndimage)->References": [
        [
            "M. Unser, \u00e2\u0080\u009cSplines: A Perfect Fit for Signal and Image\nProcessing,\u00e2\u0080\u009d IEEE Signal Processing Magazine, vol. 16, no. 6, pp.\n22-38, November 1999.\n</aside>\n<aside class=\"footnote brackets\" id=\"id9\" role=\"note\">\n[2]",
            "markdown"
        ],
        [
            "G. Borgefors, \u00e2\u0080\u009cDistance transformations in arbitrary\ndimensions.\u00e2\u0080\u009d, Computer Vision, Graphics, and Image Processing,\n27:321-345, 1984.\n</aside>\n<aside class=\"footnote brackets\" id=\"id10\" role=\"note\">\n[3]",
            "markdown"
        ],
        [
            "C. R. Maurer, Jr., R. Qi, and V. Raghavan, \u00e2\u0080\u009cA linear time\nalgorithm for computing exact euclidean distance transforms of\nbinary images in arbitrary dimensions.\u00e2\u0080\u009d IEEE Trans. PAMI 25,\n265-270, 2003.\n</aside>\n<aside class=\"footnote brackets\" id=\"id11\" role=\"note\">\n[4]",
            "markdown"
        ],
        [
            "A. X. Falc\u00c3\u00a3o, J. Stolfi, and R. A. Lotufo. \u00e2\u0080\u009cThe image foresting\ntransform: Theory, algorithms, and applications.\u00e2\u0080\u009d IEEE Trans.\nPAMI 26, 19-29. 2004.\n</aside>\n<aside class=\"footnote brackets\" id=\"id12\" role=\"note\">\n[5]",
            "markdown"
        ],
        [
            "T. Briand and P. Monasse, \u00e2\u0080\u009cTheory and Practice of Image B-Spline\nInterpolation\u00e2\u0080\u009d, Image Processing On Line, 8, pp. 99\u00e2\u0080\u0093141, 2018.\nhttps://doi.org/10.5201/ipol.2018.221\n</aside>\n</aside>",
            "markdown"
        ]
    ],
    "scipy->File IO (scipy.io)": [
        [
            "See also",
            "markdown"
        ],
        [
            "NumPy IO routines",
            "markdown"
        ],
        [
            "IDL files#",
            "markdown"
        ],
        [
            "Matrix Market files#",
            "markdown"
        ],
        [
            "Wav sound files (scipy.io.wavfile)#",
            "markdown"
        ],
        [
            "Arff files (scipy.io.arff)#",
            "markdown"
        ]
    ],
    "scipy->File IO (scipy.io)->MATLAB files->The basic functions": [
        [
            "We\u00e2\u0080\u0099ll start by importing scipy.io and calling it sio for\nconvenience:",
            "markdown"
        ],
        [
            "import scipy.io as sio",
            "code"
        ],
        [
            "If you are using IPython, try tab-completing on sio. Among the many\noptions, you will find:",
            "markdown"
        ],
        [
            "sio.loadmat\nsio.savemat\nsio.whosmat",
            "code"
        ],
        [
            "These are the high-level functions you will most likely use when working\nwith MATLAB files. You\u00e2\u0080\u0099ll also find:",
            "markdown"
        ],
        [
            "sio.matlab",
            "code"
        ],
        [
            "This is the package from which loadmat, savemat, and whosmat\nare imported. Within sio.matlab, you will find the mio module\nThis module contains the machinery that loadmat and savemat use.\nFrom time to time you may find yourself re-using this machinery.",
            "markdown"
        ]
    ],
    "scipy->File IO (scipy.io)->MATLAB files->How do I start?": [
        [
            "You may have a .mat file that you want to read into SciPy. Or, you\nwant to pass some variables from SciPy / NumPy into MATLAB.",
            "markdown"
        ],
        [
            "To save us using a MATLAB license, let\u00e2\u0080\u0099s start in Octave. Octave has\nMATLAB-compatible save and load functions. Start Octave (octave at\nthe command line for me):",
            "markdown"
        ],
        [
            "octave:1 a = 1:12\na =\n\n   1   2   3   4   5   6   7   8   9  10  11  12\n\noctave:2 a = reshape(a, [1 3 4])\na =\n\nans(:,:,1) =\n\n   1   2   3\n\nans(:,:,2) =\n\n   4   5   6\n\nans(:,:,3) =\n\n   7   8   9\n\nans(:,:,4) =\n\n   10   11   12\n\noctave:3 save -6 octave_a.mat a % MATLAB 6 compatible\noctave:4 ls octave_a.mat\noctave_a.mat",
            "code"
        ],
        [
            "Now, to Python:",
            "markdown"
        ],
        [
            "mat_contents = sio.loadmat('octave_a.mat')\n mat_contents\n{'a': array([[[  1.,   4.,   7.,  10.],\n        [  2.,   5.,   8.,  11.],\n        [  3.,   6.,   9.,  12.]]]),\n '__version__': '1.0',\n '__header__': 'MATLAB 5.0 MAT-file, written by\n Octave 3.6.3, 2013-02-17 21:02:11 UTC',\n '__globals__': []}\n oct_a = mat_contents['a']\n oct_a\narray([[[  1.,   4.,   7.,  10.],\n        [  2.,   5.,   8.,  11.],\n        [  3.,   6.,   9.,  12.]]])\n oct_a.shape\n(1, 3, 4)",
            "code"
        ],
        [
            "Now let\u00e2\u0080\u0099s try the other way round:",
            "markdown"
        ],
        [
            "import numpy as np\n vect = np.arange(10)\n vect.shape\n(10,)\n sio.savemat('np_vector.mat', {'vect':vect})",
            "code"
        ],
        [
            "Then back to Octave:",
            "markdown"
        ],
        [
            "octave:8 load np_vector.mat\noctave:9 vect\nvect =\n\n  0  1  2  3  4  5  6  7  8  9\n\noctave:10 size(vect)\nans =\n\n    1   10",
            "code"
        ],
        [
            "If you want to inspect the contents of a MATLAB file without reading the\ndata into memory, use the whosmat command:",
            "markdown"
        ],
        [
            "sio.whosmat('octave_a.mat')\n[('a', (1, 3, 4), 'double')]",
            "code"
        ],
        [
            "whosmat returns a list of tuples, one for each array (or other object)\nin the file. Each tuple contains the name, shape and data type of the\narray.",
            "markdown"
        ]
    ],
    "scipy->File IO (scipy.io)->MATLAB files->MATLAB structs": [
        [
            "MATLAB structs are a little bit like Python dicts, except the field\nnames must be strings. Any MATLAB object can be a value of a field. As\nfor all objects in MATLAB, structs are, in fact, arrays of structs, where\na single struct is an array of shape (1, 1).",
            "markdown"
        ],
        [
            "octave:11 my_struct = struct('field1', 1, 'field2', 2)\nmy_struct =\n{\n  field1 =  1\n  field2 =  2\n}\n\noctave:12 save -6 octave_struct.mat my_struct",
            "code"
        ],
        [
            "We can load this in Python:",
            "markdown"
        ],
        [
            "mat_contents = sio.loadmat('octave_struct.mat')\n mat_contents\n{'my_struct': array([[([[1.0]], [[2.0]])]],\n      dtype=[('field1', 'O'), ('field2', 'O')]), '__version__': '1.0', '__header__': 'MATLAB 5.0 MAT-file, written by Octave 3.6.3, 2013-02-17 21:23:14 UTC', '__globals__': []}\n oct_struct = mat_contents['my_struct']\n oct_struct.shape\n(1, 1)\n val = oct_struct[0,0]\n val\n([[1.0]], [[2.0]])\n val['field1']\narray([[ 1.]])\n val['field2']\narray([[ 2.]])\n val.dtype\ndtype([('field1', 'O'), ('field2', 'O')])",
            "code"
        ],
        [
            "In the SciPy versions from 0.12.0, MATLAB structs come back as NumPy\nstructured arrays, with fields named for the struct fields. You can see\nthe field names in the dtype output above. Note also:",
            "markdown"
        ],
        [
            "val = oct_struct[0,0]",
            "code"
        ],
        [
            "and:",
            "markdown"
        ],
        [
            "octave:13 size(my_struct)\nans =\n\n   1   1",
            "code"
        ],
        [
            "So, in MATLAB, the struct array must be at least 2-D, and we replicate\nthat when we read into SciPy. If you want all length 1 dimensions\nsqueezed out, try this:",
            "markdown"
        ],
        [
            "mat_contents = sio.loadmat('octave_struct.mat', squeeze_me=True)\n oct_struct = mat_contents['my_struct']\n oct_struct.shape\n()",
            "code"
        ],
        [
            "Sometimes, it\u00e2\u0080\u0099s more convenient to load the MATLAB structs as Python\nobjects rather than NumPy structured arrays - it can make the access\nsyntax in Python a bit more similar to that in MATLAB.  In order to do\nthis, use the struct_as_record=False parameter setting to loadmat.",
            "markdown"
        ],
        [
            "mat_contents = sio.loadmat('octave_struct.mat', struct_as_record=False)\n oct_struct = mat_contents['my_struct']\n oct_struct[0,0].field1\narray([[ 1.]])",
            "code"
        ],
        [
            "struct_as_record=False works nicely with squeeze_me:",
            "markdown"
        ],
        [
            "mat_contents = sio.loadmat('octave_struct.mat', struct_as_record=False, squeeze_me=True)\n oct_struct = mat_contents['my_struct']\n oct_struct.shape # but no - it's a scalar\nTraceback (most recent call last):\n  File \"&lt;stdin\", line 1, in &lt;module\nAttributeError: 'mat_struct' object has no attribute 'shape'\n type(oct_struct)\n&lt;class 'scipy.io.matlab.mio5_params.mat_struct'\n oct_struct.field1\n1.0",
            "code"
        ],
        [
            "Saving struct arrays can be done in various ways. One simple method is\nto use dicts:",
            "markdown"
        ],
        [
            "a_dict = {'field1': 0.5, 'field2': 'a string'}\n sio.savemat('saved_struct.mat', {'a_dict': a_dict})",
            "code"
        ],
        [
            "loaded as:",
            "markdown"
        ],
        [
            "octave:21 load saved_struct\noctave:22 a_dict\na_dict =\n\n  scalar structure containing the fields:\n\n    field2 = a string\n    field1 =  0.50000",
            "code"
        ],
        [
            "You can also save structs back again to MATLAB (or Octave in our case)\nlike this:",
            "markdown"
        ],
        [
            "dt = [('f1', 'f8'), ('f2', 'S10')]\n arr = np.zeros((2,), dtype=dt)\n arr\narray([(0.0, ''), (0.0, '')],\n      dtype=[('f1', '&lt;f8'), ('f2', 'S10')])\n arr[0]['f1'] = 0.5\n arr[0]['f2'] = 'python'\n arr[1]['f1'] = 99\n arr[1]['f2'] = 'not perl'\n sio.savemat('np_struct_arr.mat', {'arr': arr})",
            "code"
        ]
    ],
    "scipy->File IO (scipy.io)->MATLAB files->MATLAB cell arrays": [
        [
            "Cell arrays in MATLAB are rather like Python lists, in the sense that\nthe elements in the arrays can contain any type of MATLAB object. In\nfact, they are most similar to NumPy object arrays, and that is how we\nload them into NumPy.",
            "markdown"
        ],
        [
            "octave:14 my_cells = {1, [2, 3]}\nmy_cells =\n{\n  [1,1] =  1\n  [1,2] =\n\n     2   3\n\n}\n\noctave:15 save -6 octave_cells.mat my_cells",
            "code"
        ],
        [
            "Back to Python:",
            "markdown"
        ],
        [
            "mat_contents = sio.loadmat('octave_cells.mat')\n oct_cells = mat_contents['my_cells']\n print(oct_cells.dtype)\nobject\n val = oct_cells[0,0]\n val\narray([[ 1.]])\n print(val.dtype)\nfloat64",
            "code"
        ],
        [
            "Saving to a MATLAB cell array just involves making a NumPy object array:",
            "markdown"
        ],
        [
            "obj_arr = np.zeros((2,), dtype=np.object)\n obj_arr[0] = 1\n obj_arr[1] = 'a string'\n obj_arr\narray([1, 'a string'], dtype=object)\n sio.savemat('np_cells.mat', {'obj_arr':obj_arr})",
            "code"
        ],
        [
            "octave:16 load np_cells.mat\noctave:17 obj_arr\nobj_arr =\n{\n  [1,1] = 1\n  [2,1] = a string\n}",
            "code"
        ]
    ],
    "scipy->File IO (scipy.io)->Netcdf": [
        [
            "Allows reading of  NetCDF files (version of pupynere package)",
            "markdown"
        ]
    ]
}