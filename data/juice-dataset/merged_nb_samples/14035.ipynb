{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55bd9d56",
   "metadata": {},
   "source": [
    "# Statefarm Data - Phase 4B - All-Convolutional Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f70543b",
   "metadata": {},
   "source": [
    "Vgg16 performed better than InceptionV3 and Resnet50 in Phase4 experiments.  It was obvious that Vgg became over-fitted quite quickly.  It's no wonder when trying to train over 3 million parameters in the dense layers based on only 50 subjects (in turn providing approx. 22000 training images). Using dropout to control over-fitting is not an efficient way of creating a stable model either. Overfitting results when there is not enough data for the quantity of parameters requiring training.  (Though with infinitely flexible non-linear models, over-fitting will eventually happen with too much training unless is done to disrupt that process). Most of the parameters from the Vgg16 neural network are contributed by the dense fully connected layers, and comparatively few from the convolutional layers.  All convolutional model architectures are a way to reduce the number of parameters in a model and help eliminate an overfitting problem when not much training data is available.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3666fb66",
   "metadata": {},
   "source": [
    "#### In this notebook, my objective is comparing the performance of various all convolutional models based on Vgg19. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff701e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d82f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "IMPORT_DIR = '/home/ubuntu/nbs'\n",
    "%cd $IMPORT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "import daveutils\n",
    "from daveutils import *\n",
    "import davenet\n",
    "from davenet import *\n",
    "import my_cv_modeler\n",
    "from my_cv_modeler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33583de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DATA_DIR = '/home/ubuntu/'\n",
    "DATA_HOME_DIR = ALL_DATA_DIR+'statefarm1/'\n",
    "TRAIN_DIR = DATA_HOME_DIR+'train/'\n",
    "VALID_DIR = DATA_HOME_DIR+'valid/'\n",
    "SAMPLE_DIR = DATA_HOME_DIR+'sample/'\n",
    "MODELS_DIR = DATA_HOME_DIR+'models/'\n",
    "RESULTS_DIR = DATA_HOME_DIR+'results/'\n",
    "TEST_DIR = DATA_HOME_DIR+'test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991acb4",
   "metadata": {},
   "source": [
    "# 1. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2dedb8",
   "metadata": {},
   "source": [
    "#### Identify and remove poor quality training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9dce38",
   "metadata": {},
   "source": [
    "Previously Identified Data that is badly classified or multi-class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_img_nums=np.array(['16927','101091','31121','27454','49471','47068','18737','14223','68147','68040','54867',\n",
    "                  '38427', '8131', '62871', '99733', '92769','75819', '79819'])\n",
    "#n.b. some of these image numbers at in the validation folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133a7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27408d3d",
   "metadata": {},
   "source": [
    "Move bad images from /train to /bad folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import move\n",
    "from shutil import copytree #(src, dst, symlinks=False, ignore=None)\n",
    "%cd $DATA_HOME_DIR\n",
    "def move_bad_to_bad_folder(from_dir, bad_filenames, bad_dir = 'bad_train'):  #bad_dir must not already exist\n",
    "    count = 0\n",
    "    copytree(from_dir, bad_dir)\n",
    "    g = glob(from_dir+'/c?/*.jpg')\n",
    "    for filename in g:\n",
    "        if filename[len(from_dir)+8:][:-4] in bad_filenames:\n",
    "            print(filename[len(from_dir)+1:])\n",
    "            move(filename, bad_dir+'/'+filename[len(from_dir)+1:])\n",
    "            count+=1\n",
    "    print(count,\"items successfully moved from /\",from_dir,\"folder to: ../\",bad_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad53dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_bad_to_bad_folder('train', bad_img_nums, 'bad_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a10e305",
   "metadata": {},
   "source": [
    "# 2. Create a Sequential Vgg Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b735e40",
   "metadata": {},
   "source": [
    "### 1. Add fc_bn layers, and train only the final layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf5455",
   "metadata": {},
   "source": [
    "Import the fully trained Vgg16bn network from Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f31e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "vgg19layers = VGG19(include_top=True, weights='imagenet')\n",
    "#base_model = VGG19(weights='imagenet')\n",
    "#model = Model(input=base_model.input, output=base_model.get_layer('block4_pool').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(vgg19layers,1,'vgg19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(vgg19layers.layers):\n",
    "    print(i, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00877ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19layers.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ed935",
   "metadata": {},
   "source": [
    "Make it so that the convoluted layers are not trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb8bf0",
   "metadata": {},
   "source": [
    "# Freeze Conv Layers to FC1 and Add new Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_frozen = 0\n",
    "for layer in vgg19layers.layers:\n",
    "    layer.trainable = False\n",
    "    if layer.trainable == False: count_frozen+=1\n",
    "print(count_frozen,\"layers are frozen\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded2b52",
   "metadata": {},
   "source": [
    "Create a functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Model(input=vgg19layers.input, output=vgg19layers.output)\n",
    "model = Model(input=vgg19layers.input, output=vgg19layers.get_layer('fc1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b1f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.get_layer('fc1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9d5ee",
   "metadata": {},
   "source": [
    "# Baseline Model: Finetune a truncated Vgg19 model (1x4096 fc hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da73fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Dense(10, activation='softmax')(x.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19short = Model(input=vgg19layers.input,output=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429eed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19short.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b46213",
   "metadata": {},
   "source": [
    "### Train the Baseline Model (1 hidden dense layer w 4096 filters)- including use of 14k pseudo label test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f12e34",
   "metadata": {},
   "source": [
    "Use ImageGenerator because there are too many training images to store (resized) in an array.\n",
    "1. Not using data augmentation at this stage.\n",
    "2. Not using validation data for training at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc910731",
   "metadata": {},
   "source": [
    "n.b. Mixiterator was not used.  Only test data having a prediction probability >0.995 has been used.\n",
    "This data is considered to be of such good quality that it can be mixed with real data. The pseudo training data will make up 43% of the training data at this stage (39% after validation data is added). Yes, it's a little high, but lets see how it goes.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1979387",
   "metadata": {},
   "source": [
    "Create the image generator (no augmentation)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
