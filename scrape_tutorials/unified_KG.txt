root
├── matplotlib
│   └── Tutorials
│       ├── Advanced
│       │   ├── Faster rendering by using blitting
│       │   │   ├── Class-based example
│       │   │   └── Minimal example
│       │   ├── Path Tutorial
│       │   │   ├── Bézier example
│       │   │   └── Compound paths
│       │   ├── Path effects guide
│       │   │   ├── Adding a shadow
│       │   │   ├── Greater control of the path effect artist
│       │   │   └── Making an artist stand out
│       │   └── Transformations Tutorial
│       │       ├── Axes coordinates
│       │       ├── Blended transformations
│       │       ├── Data coordinates
│       │       ├── Plotting in physical coordinates
│       │       ├── The transformation pipeline
│       │       └── Using offset transforms to create a shadow effect
│       ├── Colors
│       │   ├── Choosing Colormaps in Matplotlib
│       │   │   ├── Classes of colormaps
│       │   │   │   ├── Cyclic
│       │   │   │   ├── Diverging
│       │   │   │   ├── Miscellaneous
│       │   │   │   ├── Qualitative
│       │   │   │   ├── Sequential
│       │   │   │   └── Sequential2
│       │   │   ├── Color vision deficiencies
│       │   │   ├── Grayscale conversion
│       │   │   ├── Lightness of Matplotlib colormaps
│       │   │   ├── Overview
│       │   │   └── References
│       │   ├── Colormap Normalization
│       │   │   ├── Centered
│       │   │   ├── Custom normalization: Manually implement two linear ranges
│       │   │   ├── Discrete bounds
│       │   │   ├── FuncNorm: Arbitrary function normalization
│       │   │   ├── Logarithmic
│       │   │   ├── Power-law
│       │   │   ├── Symmetric logarithmic
│       │   │   └── TwoSlopeNorm: Different mapping on either side of a center
│       │   ├── Creating Colormaps in Matplotlib
│       │   │   ├── Creating linear segmented colormaps
│       │   │   │   └── Directly creating a segmented colormap from a list
│       │   │   ├── Creating listed colormaps
│       │   │   ├── Getting colormaps and accessing their values
│       │   │   │   ├── LinearSegmentedColormap
│       │   │   │   └── ListedColormap
│       │   │   ├── Registering a colormap
│       │   │   └── Reversing a colormap
│       │   ├── Customized Colorbars Tutorial
│       │   │   └── Customized Colorbars
│       │   │       ├── Basic continuous colorbar
│       │   │       ├── Colorbar with custom extension lengths
│       │   │       ├── Discrete intervals colorbar
│       │   │       └── Extended colorbar with continuous colorscale
│       │   └── Specifying colors
│       │       ├── "CN" color selection
│       │       ├── Color formats
│       │       ├── Comparison between X11/CSS4 and xkcd colors
│       │       └── Transparency
│       ├── Intermediate
│       │   ├── *origin* and *extent* in `~.Axes.imshow`
│       │   │   ├── Default extent
│       │   │   ├── Explicit extent
│       │   │   └── Explicit extent and axes limits
│       │   ├── Arranging multiple Axes in a Figure
│       │   │   ├── High-level methods for making grids
│       │   │   │   ├── Axes spanning rows or columns in a grid
│       │   │   │   ├── Basic 2x2 grid
│       │   │   │   ├── Grids of fixed-aspect ratio Axes
│       │   │   │   ├── Nested Axes layouts
│       │   │   │   └── Variable widths or heights in a grid
│       │   │   ├── Low-level and advanced grid methods
│       │   │   │   ├── Axes spanning rows or grids in a grid
│       │   │   │   ├── Basic 2x2 grid
│       │   │   │   ├── Manual adjustments to a GridSpec layout
│       │   │   │   └── Nested layouts with SubplotSpec
│       │   │   ├── More reading
│       │   │   └── Overview
│       │   │       ├── Adding single Axes at a time
│       │   │       ├── Create grid-shaped combinations of Axes
│       │   │       └── Underlying tools
│       │   ├── Artist tutorial
│       │   │   ├── Customizing your objects
│       │   │   └── Object containers
│       │   │       ├── Axes container
│       │   │       ├── Axis containers
│       │   │       ├── Figure container
│       │   │       └── Tick containers
│       │   ├── Autoscaling
│       │   │   ├── Controlling autoscale
│       │   │   ├── Margins
│       │   │   ├── Sticky edges
│       │   │   └── Working with collections
│       │   ├── Constrained Layout Guide
│       │   │   ├── Colorbars
│       │   │   ├── Debugging
│       │   │   ├── Grids of fixed aspect-ratio Axes: "compressed" layout
│       │   │   ├── Legends
│       │   │   ├── Limitations
│       │   │   │   ├── Incompatible functions
│       │   │   │   └── Other Caveats
│       │   │   ├── Manually setting axes positions
│       │   │   ├── Manually turning off constrained_layout
│       │   │   ├── Notes on the algorithm
│       │   │   │   ├── Colorbar associated with a Gridspec
│       │   │   │   ├── Simple case: one Axes
│       │   │   │   ├── Simple case: two Axes
│       │   │   │   ├── Two Axes and colorbar
│       │   │   │   └── Uneven sized Axes
│       │   │   ├── Padding and Spacing
│       │   │   │   └── Spacing with colorbars
│       │   │   ├── Simple Example
│       │   │   ├── Suptitle
│       │   │   ├── Use with GridSpec
│       │   │   └── rcParams
│       │   ├── Legend guide
│       │   │   ├── Controlling the legend entries
│       │   │   ├── Creating artists specifically for adding to the legend (aka. Proxy artists)
│       │   │   ├── Legend Handlers
│       │   │   │   └── Implementing a custom legend handler
│       │   │   ├── Legend location
│       │   │   │   └── Figure legends
│       │   │   └── Multiple legends on the same Axes
│       │   ├── Styling with cycler
│       │   │   ├── Cycling through multiple properties
│       │   │   └── Setting prop_cycle in the matplotlibrc file or style files
│       │   └── Tight Layout guide
│       │       ├── Caveats
│       │       ├── Colorbar
│       │       ├── Legends and Annotations
│       │       ├── Simple Example
│       │       ├── Use with AxesGrid1
│       │       └── Use with GridSpec
│       ├── Introductory
│       │   ├── Animations using Matplotlib
│       │   │   ├── Animation Classes
│       │   │   │   ├── ArtistAnimation
│       │   │   │   └── FuncAnimation
│       │   │   └── Animation Writers
│       │   │       └── Saving Animations
│       │   ├── Customizing Matplotlib with style sheets and rcParams
│       │   │   ├── Runtime rc settings
│       │   │   │   └── Temporary rc settings
│       │   │   ├── The matplotlibrc file
│       │   │   │   └── The default matplotlibrc file
│       │   │   └── Using style sheets
│       │   │       ├── Composing styles
│       │   │       ├── Defining your own style
│       │   │       ├── Distributing styles
│       │   │       └── Temporary styling
│       │   ├── Image tutorial
│       │   │   ├── Importing image data into Numpy arrays
│       │   │   ├── Plotting numpy arrays as images
│       │   │   │   ├── Applying pseudocolor schemes to image plots
│       │   │   │   ├── Array Interpolation schemes
│       │   │   │   ├── Color scale reference
│       │   │   │   └── Examining a specific data range
│       │   │   └── Startup commands
│       │   ├── Pyplot tutorial
│       │   │   ├── Controlling line properties
│       │   │   ├── Introduction to pyplot
│       │   │   │   └── Formatting the style of your plot
│       │   │   ├── Logarithmic and other nonlinear axes
│       │   │   ├── Plotting with categorical variables
│       │   │   ├── Plotting with keyword strings
│       │   │   ├── Working with multiple figures and axes
│       │   │   └── Working with text
│       │   │       ├── Annotating text
│       │   │       └── Using mathematical expressions in text
│       │   ├── Quick start guide
│       │   │   ├── A simple example
│       │   │   ├── Axis scales and ticks
│       │   │   │   ├── Additional Axis objects
│       │   │   │   ├── Plotting dates and strings
│       │   │   │   ├── Scales
│       │   │   │   └── Tick locators and formatters
│       │   │   ├── Coding styles
│       │   │   │   ├── Making a helper functions
│       │   │   │   └── The explicit and the implicit interfaces
│       │   │   ├── Color mapped data
│       │   │   │   ├── Colorbars
│       │   │   │   ├── Colormaps
│       │   │   │   └── Normalizations
│       │   │   ├── Labelling plots
│       │   │   │   ├── Annotations
│       │   │   │   ├── Axes labels and text
│       │   │   │   ├── Legends
│       │   │   │   └── Using mathematical expressions in text
│       │   │   ├── More reading
│       │   │   ├── Parts of a Figure
│       │   │   │   ├── Artist
│       │   │   │   ├── Axes
│       │   │   │   ├── Axis
│       │   │   │   └── Figure
│       │   │   ├── Styling Artists
│       │   │   │   ├── Colors
│       │   │   │   └── Linewidths, linestyles, and markersizes
│       │   │   ├── Types of inputs to plotting functions
│       │   │   └── Working with multiple Figures and Axes
│       │   └── The Lifecycle of a Plot
│       │       ├── A note on the explicit vs. implicit interfaces
│       │       ├── Combining multiple visualizations
│       │       ├── Controlling the style
│       │       ├── Customizing the plot
│       │       ├── Getting started
│       │       ├── Our data
│       │       └── Saving our plot
│       ├── Provisional
│       ├── Text
│       │   ├── Annotations
│       │   │   ├── Advanced annotation
│       │   │   │   ├── Annotating with boxed text
│       │   │   │   ├── Customizing annotation arrows
│       │   │   │   ├── Defining custom box styles
│       │   │   │   └── Placing Artist at anchored Axes locations
│       │   │   ├── Basic annotation
│       │   │   │   ├── Annotating data
│       │   │   │   ├── Annotating with arrows
│       │   │   │   └── Placing text annotations relative to data
│       │   │   └── Coordinate systems for annotations
│       │   │       ├── Using ConnectionPatch
│       │   │       └── Zoom effect between Axes
│       │   ├── Text in Matplotlib Plots
│       │   │   ├── Basic text commands
│       │   │   ├── Labels for x- and y-axis
│       │   │   ├── Legends and Annotations
│       │   │   ├── Ticks and ticklabels
│       │   │   │   ├── Dateticks
│       │   │   │   ├── Simple ticks
│       │   │   │   ├── Terminology
│       │   │   │   └── Tick Locators and Formatters
│       │   │   └── Titles
│       │   ├── Text properties and layout
│       │   ├── Text rendering with LaTeX
│       │   │   ├── Possible hangups
│       │   │   ├── PostScript options
│       │   │   └── Troubleshooting
│       │   ├── Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend
│       │   │   ├── Choosing the TeX system
│       │   │   ├── Custom preamble
│       │   │   ├── Font specification
│       │   │   ├── Multi-Page PDF Files
│       │   │   └── Troubleshooting
│       │   └── Writing mathematical expressions
│       │       ├── Accents
│       │       ├── Example
│       │       ├── Fonts
│       │       │   └── Custom fonts
│       │       ├── Fractions, binomials, and stacked numbers
│       │       ├── Radicals
│       │       ├── Subscripts and superscripts
│       │       └── Symbols
│       └── Toolkits
│           ├── The axes_grid1 toolkit
│           │   ├── AxesDivider
│           │   └── axes_grid1
│           │       ├── AnchoredArtists
│           │       ├── AxesDivider Class
│           │       ├── ImageGrid
│           │       ├── InsetLocator
│           │       ├── ParasiteAxes
│           │       │   └── Example 2: twin
│           │       ├── RGBAxes
│           │       └── colorbar whose height (or width) is in sync with the main axes
│           │           └── scatter_hist.py with AxesDivider
│           ├── The axisartist toolkit
│           │   ├── AxisArtist
│           │   │   ├── axislabel
│           │   │   ├── line
│           │   │   ├── major_ticklabels, minor_ticklabels
│           │   │   └── major_ticks, minor_ticks
│           │   ├── Current Limitations and TODO's
│           │   ├── Default AxisArtists
│           │   ├── FloatingAxis
│           │   ├── GridHelper
│           │   ├── HowTo
│           │   ├── Rotation and Alignment of TickLabels
│           │   │   ├── Adjusting pad
│           │   │   └── Adjusting ticklabels alignment
│           │   ├── axisartist
│           │   │   ├── Curvilinear Grid
│           │   │   ├── Floating Axes
│           │   │   └── axisartist with ParasiteAxes
│           │   └── axisartist namespace
│           └── The mplot3d toolkit
│               ├── Bar plots
│               ├── Contour plots
│               ├── Filled contour plots
│               ├── Line plots
│               ├── Polygon plots
│               ├── Quiver
│               ├── Scatter plots
│               ├── Surface plots
│               ├── Text
│               ├── Tri-Surface plots
│               └── Wireframe plots
├── numpy
│   ├── Articles
│   │   ├── Deep reinforcement learning with Pong from pixels
│   │   │   ├── Appendix
│   │   │   │   ├── How to set up video playback in your Jupyter notebook
│   │   │   │   └── Notes on RL and deep RL
│   │   │   ├── Create the policy (the neural network) and the forward pass
│   │   │   ├── Define the discounted rewards (expected return) function
│   │   │   ├── Next steps
│   │   │   ├── Preprocess frames (the observation)
│   │   │   ├── Prerequisites
│   │   │   ├── Set up Pong
│   │   │   ├── Set up the update step (backpropagation)
│   │   │   ├── Table of contents
│   │   │   │   ├── A note on RL and deep RL
│   │   │   │   └── Deep RL glossary
│   │   │   └── Train the agent for a number of episodes
│   │   └── Sentiment Analysis on notable speeches of the last decade
│   │       ├── 1. Data Collection
│   │       │   ├── Collecting and loading the speech transcripts
│   │       │   └── Collecting the IMDb reviews dataset
│   │       ├── 2. Preprocess the datasets
│   │       ├── 3. Build the Deep Learning Model¶
│   │       │   ├── Backpropagation
│   │       │   ├── But how do you obtain sentiment from the LSTM’s output?
│   │       │   ├── Forward Propagation
│   │       │   ├── Introduction to a Long Short Term Memory Network
│   │       │   ├── Overview of the Model Architecture
│   │       │   ├── Sentiment Analysis on the Speech Data
│   │       │   ├── Training the Network
│   │       │   └── Updating the Parameters
│   │       ├── Looking at our Neural Network from an ethical perspective
│   │       ├── Next Steps
│   │       ├── Prerequisites
│   │       └── Table of contents
│   ├── NumPy Applications
│   │   ├── Analyzing the impact of the lockdown on air quality in Delhi, India
│   │   │   ├── Building the dataset
│   │   │   ├── Calculating the Air Quality Index
│   │   │   │   ├── Air quality indices
│   │   │   │   ├── Moving averages
│   │   │   │   └── Sub-indices
│   │   │   ├── Further reading
│   │   │   ├── In practice…
│   │   │   ├── Paired Student’s t-test on the AQIs
│   │   │   │   ├── Calculating the test statistics
│   │   │   │   ├── Defining the hypothesis
│   │   │   │   └── Sampling
│   │   │   ├── The problem of air pollution
│   │   │   ├── What do the t and p values mean?
│   │   │   ├── What you’ll do
│   │   │   ├── What you’ll learn
│   │   │   └── What you’ll need
│   │   ├── Deep learning on MNIST
│   │   │   ├── 1. Load the MNIST dataset
│   │   │   ├── 2. Preprocess the data
│   │   │   │   ├── Convert the image data to the floating-point format
│   │   │   │   └── Convert the labels to floating point through categorical/one-hot encoding
│   │   │   ├── 3. Build and train a small neural network from scratch
│   │   │   │   ├── Compose the model and begin training and testing it
│   │   │   │   ├── Model architecture and training summary
│   │   │   │   └── Neural network building blocks with NumPy
│   │   │   ├── Next steps
│   │   │   ├── Prerequisites
│   │   │   └── Table of contents
│   │   ├── Determining Moore’s Law with real data in NumPy
│   │   │   ├── Building Moore’s law as an exponential function
│   │   │   ├── Calculating the historical growth curve for transistors
│   │   │   ├── Loading historical manufacturing data to your workspace
│   │   │   ├── References
│   │   │   ├── Sharing your results as zipped arrays and a csv
│   │   │   │   ├── Creating your own comma separated value file
│   │   │   │   └── Zipping the arrays into a file
│   │   │   ├── Skills you’ll learn
│   │   │   ├── What you’ll do
│   │   │   ├── What you’ll need
│   │   │   └── Wrapping up
│   │   ├── Determining Static Equilibrium in NumPy
│   │   │   ├── Finding values with physical properties
│   │   │   │   └── Another Example
│   │   │   ├── Solving Equilibrium as a sum of moments
│   │   │   ├── Solving equilibrium with Newton’s second law
│   │   │   ├── What you’ll do:
│   │   │   ├── What you’ll learn:
│   │   │   ├── What you’ll need:
│   │   │   └── Wrapping up
│   │   │       ├── Additional Applications
│   │   │       └── References
│   │   ├── Plotting Fractals
│   │   │   ├── Creating your own fractals
│   │   │   ├── Further reading
│   │   │   ├── Generalizing the Julia set
│   │   │   │   └── Newton Fractals
│   │   │   ├── In conclusion
│   │   │   ├── Julia set
│   │   │   ├── Mandelbrot set
│   │   │   ├── On your own
│   │   │   ├── Warmup
│   │   │   ├── What you’ll do
│   │   │   ├── What you’ll learn
│   │   │   └── What you’ll need
│   │   └── X-ray image processing
│   │       ├── Apply masks to X-rays with np.where()
│   │       ├── Combine images into a multidimensional array to demonstrate progression
│   │       ├── Compare the results
│   │       ├── Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters
│   │       │   ├── The Canny filter
│   │       │   ├── The Gaussian gradient magnitude method
│   │       │   ├── The Laplace filter with Gaussian second derivatives
│   │       │   └── The Sobel-Feldman operator (the Sobel filter)
│   │       ├── Examine an X-ray with imageio
│   │       ├── Next steps
│   │       ├── Prerequisites
│   │       └── Table of contents
│   └── NumPy Features
│       ├── Linear algebra on n-dimensional arrays
│       │   ├── Approximation
│       │   │   ├── Applying to all colors
│       │   │   ├── Final words
│       │   │   └── Products with n-dimensional arrays
│       │   ├── Content
│       │   │   ├── Operations on an axis
│       │   │   └── Shape, axis and array properties
│       │   ├── Further reading
│       │   ├── Learner profile
│       │   ├── Learning Objectives
│       │   └── Prerequisites
│       ├── Masked Arrays
│       │   ├── Exploring the data
│       │   ├── Fitting Data
│       │   ├── Further reading
│       │   │   └── Reference
│       │   ├── In practice
│       │   ├── Missing data
│       │   ├── Using masked arrays to see COVID-19 data
│       │   ├── What are masked arrays?
│       │   ├── What you’ll do
│       │   ├── What you’ll learn
│       │   ├── What you’ll need
│       │   └── When can they be useful?
│       └── Saving and sharing your NumPy arrays
│           ├── Another option: saving to human-readable csv
│           ├── Create your arrays
│           ├── Our arrays as a csv file
│           ├── Rearrange the data into a single 2D array
│           ├── Reassign the NpzFile arrays to x and y
│           ├── Remove the saved arrays and load them back with NumPy’s
│           ├── Save the data to csv file using
│           ├── Save your arrays with NumPy’s
│           ├── Success
│           ├── Success, but remember your types
│           ├── What you’ll do
│           ├── What you’ll learn
│           ├── What you’ll need
│           └── Wrapping up
├── pandas_toms_blog
│   ├── Fast Pandas
│   │   ├── Categoricals
│   │   ├── Constructors
│   │   ├── Datatypes
│   │   ├── Going Further
│   │   ├── Iteration, Apply, And Vectorization
│   │   └── Summary
│   ├── Indexes
│   │   ├── Flavors
│   │   │   ├── Indexes for Alignment
│   │   │   ├── Indexes for Easier Arithmetic, Analysis
│   │   │   └── Row Slicing
│   │   ├── Merging
│   │   │   ├── Concat Version
│   │   │   ├── Merge Version
│   │   │   └── The merge version
│   │   └── Set Operations
│   ├── Method Chaining
│   │   └── Method Chaining
│   │       ├── Application
│   │       ├── Costs
│   │       └── Inplace?
│   ├── Modern Pandas
│   │   └── Effective Pandas
│   │       ├── Get the Data
│   │       ├── Indexing
│   │       ├── Introduction
│   │       ├── Multidimensional Indexing
│   │       ├── Prior Art
│   │       ├── SettingWithCopy
│   │       ├── Slicing
│   │       └── WrapUp
│   ├── Scaling
│   │   ├── Dask
│   │   ├── Joining
│   │   ├── Try It Out!
│   │   ├── Using Dask
│   │   └── Using Iteration
│   ├── Tidy Data
│   │   └── Reshaping & Tidy Data
│   │       ├── Mini Project: Home Court Advantage?
│   │       │   ├── Step 1: Create an outcome variable
│   │       │   └── Step 2: Find the win percent for each team
│   │       ├── NBA Data
│   │       └── Stack / Unstack
│   ├── Time Series
│   │   └── Timeseries
│   │       ├── ARIMA
│   │       │   ├── 
│   │       │   ├── Combining
│   │       │   └── Integrated
│   │       ├── Conclusion
│   │       ├── Forecasting
│   │       ├── Grab Bag
│   │       │   ├── Holiday Calendars
│   │       │   ├── Offsets
│   │       │   └── Timezones
│   │       ├── Modeling Time Series
│   │       │   └── Autocorrelation
│   │       ├── Resources
│   │       ├── Seasonality
│   │       ├── Special Methods
│   │       │   ├── Resampling
│   │       │   └── Rolling / Expanding / EW
│   │       └── Special Slicing
│   └── Visualization
│       └── Visualization and Exploratory Analysis
│           ├── 
│           ├── Examples
│           ├── Matplotlib
│           ├── Other Libraries
│           ├── Overview
│           ├── Pandas Built-in Plotting
│           └── Seaborn
├── scipy
│   ├── Compressed Sparse Graph Routines (scipy.sparse.csgraph)
│   │   └── Example: Word Ladders
│   ├── File IO (scipy.io)
│   │   ├── MATLAB files
│   │   │   ├── How do I start?
│   │   │   ├── MATLAB cell arrays
│   │   │   ├── MATLAB structs
│   │   │   └── The basic functions
│   │   └── Netcdf
│   ├── Fourier Transforms (scipy.fft)
│   │   ├── Discrete Cosine Transforms
│   │   │   ├── DCT and IDCT
│   │   │   ├── Example
│   │   │   ├── Type I DCT
│   │   │   ├── Type II DCT
│   │   │   ├── Type III DCT
│   │   │   └── Type IV DCT
│   │   ├── Discrete Sine Transforms
│   │   │   ├── DST and IDST
│   │   │   ├── Type I DST
│   │   │   ├── Type II DST
│   │   │   ├── Type III DST
│   │   │   └── Type IV DST
│   │   ├── Fast Fourier transforms
│   │   │   ├── 1-D discrete Fourier transforms
│   │   │   └── 2- and N-D discrete Fourier transforms
│   │   ├── Fast Hankel Transform
│   │   └── References
│   ├── Integration (scipy.integrate)
│   │   ├── Faster integration using low-level callback functions
│   │   ├── Gaussian quadrature
│   │   ├── General integration (quad)
│   │   ├── General multiple integration (dblquad, tplquad, nquad)
│   │   ├── Integrating using Samples
│   │   ├── Ordinary differential equations (solve_ivp)
│   │   │   ├── References
│   │   │   └── Solving a system with a banded Jacobian matrix
│   │   └── Romberg Integration
│   ├── Interpolation (scipy.interpolate)
│   ├── Introduction
│   │   ├── Finding Documentation
│   │   └── SciPy Organization
│   ├── Linear Algebra (scipy.linalg)
│   │   ├── Basic routines
│   │   │   ├── Computing norms
│   │   │   ├── Finding the determinant
│   │   │   ├── Finding the inverse
│   │   │   ├── Generalized inverse
│   │   │   ├── Solving a linear system
│   │   │   └── Solving linear least-squares problems and pseudo-inverses
│   │   ├── Decompositions
│   │   │   ├── Cholesky decomposition
│   │   │   ├── Eigenvalues and eigenvectors
│   │   │   ├── Interpolative decomposition
│   │   │   ├── LU decomposition
│   │   │   ├── QR decomposition
│   │   │   ├── Schur decomposition
│   │   │   └── Singular value decomposition
│   │   ├── Matrix functions
│   │   │   ├── Arbitrary function
│   │   │   ├── Exponential and logarithm functions
│   │   │   ├── Hyperbolic trigonometric functions
│   │   │   └── Trigonometric functions
│   │   ├── Special matrices
│   │   ├── numpy.matrix vs 2-D numpy.ndarray
│   │   └── scipy.linalg vs numpy.linalg
│   ├── Multidimensional image processing (scipy.ndimage)
│   │   ├── Distance transforms
│   │   ├── Extending scipy.ndimage in C
│   │   ├── Filter functions
│   │   │   ├── Correlation and convolution
│   │   │   ├── Derivatives
│   │   │   ├── Filters based on order statistics
│   │   │   ├── Fourier domain filters
│   │   │   ├── Generic filter functions
│   │   │   └── Smoothing filters
│   │   ├── Interpolation functions
│   │   │   ├── Interpolation boundary handling
│   │   │   ├── Interpolation functions
│   │   │   └── Spline pre-filters
│   │   ├── Introduction
│   │   ├── Morphology
│   │   │   ├── Binary morphology
│   │   │   └── Grey-scale morphology
│   │   ├── Object measurements
│   │   ├── Properties shared by all functions
│   │   ├── References
│   │   └── Segmentation and labeling
│   ├── Optimization (scipy.optimize)
│   │   ├── Assignment problems
│   │   │   └── Linear sum assignment problem example
│   │   ├── Constrained minimization of multivariate scalar functions (minimize)
│   │   │   ├── Sequential Least SQuares Programming (SLSQP) Algorithm (method='SLSQP')
│   │   │   └── Trust-Region Constrained Algorithm (method='trust-constr')
│   │   │       ├── Defining Bounds Constraints:
│   │   │       ├── Defining Linear Constraints:
│   │   │       ├── Defining Nonlinear Constraints:
│   │   │       └── Solving the Optimization Problem:
│   │   ├── Custom minimizers
│   │   ├── Global optimization
│   │   ├── Least-squares minimization (least_squares)
│   │   │   ├── Example of solving a fitting problem
│   │   │   └── Further examples
│   │   ├── Linear programming (linprog)
│   │   │   └── Linear programming example
│   │   ├── Mixed integer linear programming
│   │   │   └── Knapsack problem example
│   │   ├── Root finding
│   │   │   ├── Fixed-point solving
│   │   │   ├── Root finding for large problems
│   │   │   ├── Scalar functions
│   │   │   ├── Sets of equations
│   │   │   └── Still too slow? Preconditioning.
│   │   ├── Unconstrained minimization of multivariate scalar functions (minimize)
│   │   │   ├── Broyden-Fletcher-Goldfarb-Shanno algorithm (method='BFGS')
│   │   │   ├── Nelder-Mead Simplex algorithm (method='Nelder-Mead')
│   │   │   ├── Newton-Conjugate-Gradient algorithm (method='Newton-CG')
│   │   │   │   ├── Full Hessian example:
│   │   │   │   └── Hessian product example:
│   │   │   ├── Trust-Region Nearly Exact Algorithm (method='trust-exact')
│   │   │   ├── Trust-Region Newton-Conjugate-Gradient Algorithm (method='trust-ncg')
│   │   │   │   ├── Full Hessian example:
│   │   │   │   └── Hessian product example:
│   │   │   └── Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm (method='trust-krylov')
│   │   │       ├── Full Hessian example:
│   │   │       └── Hessian product example:
│   │   └── Univariate function minimizers (minimize_scalar)
│   │       ├── Bounded minimization (method='bounded')
│   │       └── Unconstrained minimization (method='brent')
│   ├── Signal Processing (scipy.signal)
│   │   ├── B-splines
│   │   ├── Detrend
│   │   ├── Filtering
│   │   │   ├── Analog Filter Design
│   │   │   ├── Convolution/Correlation
│   │   │   ├── Difference-equation filtering
│   │   │   │   └── Analysis of Linear Systems
│   │   │   ├── Filter Design
│   │   │   │   ├── FIR Filter
│   │   │   │   ├── Filter Coefficients
│   │   │   │   │   ├── Second-order sections representation
│   │   │   │   │   ├── State-space system representation
│   │   │   │   │   ├── Transfer function representation
│   │   │   │   │   └── Zeros and poles representation
│   │   │   │   ├── Filter transformations
│   │   │   │   └── IIR Filter
│   │   │   └── Other filters
│   │   │       ├── Hilbert filter
│   │   │       ├── Median Filter
│   │   │       ├── Order Filter
│   │   │       └── Wiener filter
│   │   └── Spectral Analysis
│   │       ├── Lomb-Scargle Periodograms (lombscargle)
│   │       ├── Periodogram Measurements
│   │       └── Spectral Analysis using Welchâs Method
│   ├── Sparse eigenvalue problems with ARPACK
│   │   ├── Basic functionality
│   │   ├── Examples
│   │   ├── Introduction
│   │   ├── References
│   │   ├── Shift-invert mode
│   │   └── Use of LinearOperator
│   ├── Spatial data structures and algorithms (scipy.spatial)
│   │   ├── Convex hulls
│   │   ├── Delaunay triangulations
│   │   │   └── Coplanar points
│   │   └── Voronoi diagrams
│   ├── Special functions (scipy.special)
│   │   ├── Bessel functions of real order(jv, jn_zeros)
│   │   ├── Cython Bindings for Special Functions (scipy.special.cython_special)
│   │   │   ├── Avoiding Python Function Overhead
│   │   │   └── Releasing the GIL
│   │   └── Functions not in scipy.special
│   └── Statistics (scipy.stats)
│       ├── Analysing one sample
│       │   ├── Descriptive statistics
│       │   ├── Special tests for normal distributions
│       │   ├── T-test and KS-test
│       │   └── Tails of the distribution
│       ├── Building specific distributions
│       │   ├── Making a continuous distribution, i.e., subclassing rv_continuous
│       │   └── Subclassing rv_discrete
│       ├── Comparing two samples
│       │   ├── Comparing means
│       │   └── Kolmogorov-Smirnov test for two samples ks_2samp
│       ├── Introduction
│       ├── Kernel density estimation
│       │   ├── Multiscale Graph Correlation (MGC)
│       │   ├── Multivariate estimation
│       │   └── Univariate estimation
│       ├── Quasi-Monte Carlo
│       │   ├── Calculate the discrepancy
│       │   ├── Guidelines on using QMC
│       │   ├── Making a QMC engine, i.e., subclassing QMCEngine
│       │   └── Using a QMC engine
│       └── Random variables
│           ├── Broadcasting
│           ├── Common methods
│           ├── Fitting distributions
│           ├── Freezing a distribution
│           ├── Getting help
│           ├── Performance issues and cautionary remarks
│           ├── Random number generation
│           ├── Remaining issues
│           ├── Shape parameters
│           ├── Shifting and scaling
│           └── Specific points for discrete distributions
├── seaborn
│   ├── API Overview
│   │   ├── Data structures accepted by seaborn
│   │   │   ├── Long-form vs. wide-form data
│   │   │   │   ├── Further reading and take-home points
│   │   │   │   ├── Long-form data
│   │   │   │   ├── Messy data
│   │   │   │   └── Wide-form data
│   │   │   ├── Options for visualizing long-form data
│   │   │   └── Options for visualizing wide-form data
│   │   └── Overview of seaborn plotting functions
│   │       ├── Combining multiple views on the data
│   │       ├── Figure-level vs. axes-level functions
│   │       │   ├── Axes-level functions make self-contained plots
│   │       │   ├── Customizing plots from a figure-level function
│   │       │   ├── Figure-level functions own their figure
│   │       │   ├── Relative merits of figure-level functions
│   │       │   └── Specifying figure sizes
│   │       └── Similar functions for similar tasks
│   ├── Figure aesthetics
│   │   ├── Choosing color palettes
│   │   │   ├── Diverging color palettes
│   │   │   │   ├── Custom diverging palettes
│   │   │   │   ├── Other diverging palettes
│   │   │   │   └── Perceptually uniform diverging palettes
│   │   │   ├── General principles for using color in plots
│   │   │   │   ├── Components of color
│   │   │   │   ├── Vary hue to distinguish categories
│   │   │   │   └── Vary luminance to represent numbers
│   │   │   ├── Qualitative color palettes
│   │   │   │   ├── Using categorical Color Brewer palettes
│   │   │   │   └── Using circular color systems
│   │   │   ├── Sequential color palettes
│   │   │   │   ├── Custom sequential palettes
│   │   │   │   ├── Discrete vs. continuous mapping
│   │   │   │   ├── Perceptually uniform palettes
│   │   │   │   ├── Sequential Color Brewer palettes
│   │   │   │   └── Sequential “cubehelix” palettes
│   │   │   └── Tools for choosing color palettes
│   │   └── Controlling figure aesthetics
│   │       ├── Overriding elements of the seaborn styles
│   │       ├── Removing axes spines
│   │       ├── Scaling plot elements
│   │       ├── Seaborn figure styles
│   │       └── Temporarily setting figure style
│   ├── Multi-plot grids
│   │   └── Building structured multi-plot grids
│   │       ├── Conditional small multiples
│   │       ├── Plotting pairwise data relationships
│   │       └── Using custom functions
│   ├── Objects interface
│   │   ├── Properties of Mark objects
│   │   │   ├── Color properties
│   │   │   │   ├── alpha, fillalpha, edgealpha
│   │   │   │   └── color, fillcolor, edgecolor
│   │   │   ├── Coordinate properties
│   │   │   │   └── x, y, xmin, xmax, ymin, ymax
│   │   │   ├── Other properties
│   │   │   │   ├── group
│   │   │   │   └── text
│   │   │   ├── Size properties
│   │   │   │   ├── edgewidth
│   │   │   │   ├── linewidth
│   │   │   │   ├── pointsize
│   │   │   │   └── stroke
│   │   │   ├── Style properties
│   │   │   │   ├── fill
│   │   │   │   ├── linestyle, edgestyle
│   │   │   │   └── marker
│   │   │   └── Text properties
│   │   │       ├── fontsize
│   │   │       ├── halign, valign
│   │   │       └── offset
│   │   └── The seaborn.objects interface
│   │       ├── Building and displaying the plot
│   │       │   ├── Adding multiple layers
│   │       │   ├── Building and displaying the plot
│   │       │   ├── Faceting and pairing subplots
│   │       │   ├── Integrating with matplotlib
│   │       │   └── Layer-specific mappings
│   │       ├── Customizing the appearance
│   │       │   ├── Customizing legends and ticks
│   │       │   ├── Customizing limits, labels, and titles
│   │       │   ├── Parameterizing scales
│   │       │   └── Theme customization
│   │       ├── Specifying a plot and mapping data
│   │       │   ├── Defining groups
│   │       │   ├── Mapping properties
│   │       │   └── Setting properties
│   │       └── Transforming data before plotting
│   │           ├── Creating variables through transformation
│   │           ├── Orienting marks and transforms
│   │           ├── Resolving overplotting
│   │           └── Statistical transformation
│   ├── Plotting functions
│   │   ├── Visualizing categorical data
│   │   │   ├── Categorical scatterplots
│   │   │   ├── Comparing distributions
│   │   │   │   ├── Boxplots
│   │   │   │   └── Violinplots
│   │   │   ├── Estimating central tendency
│   │   │   │   ├── Bar plots
│   │   │   │   └── Point plots
│   │   │   └── Showing additional dimensions
│   │   ├── Visualizing distributions of data
│   │   │   ├── Distribution visualization in other settings
│   │   │   │   ├── Plotting joint and marginal distributions
│   │   │   │   └── Plotting many distributions
│   │   │   ├── Empirical cumulative distributions
│   │   │   ├── Kernel density estimation
│   │   │   │   ├── Choosing the smoothing bandwidth
│   │   │   │   ├── Conditioning on other variables
│   │   │   │   └── Kernel density estimation pitfalls
│   │   │   ├── Plotting univariate histograms
│   │   │   │   ├── Choosing the bin size
│   │   │   │   ├── Conditioning on other variables
│   │   │   │   └── Normalized histogram statistics
│   │   │   └── Visualizing bivariate distributions
│   │   └── Visualizing statistical relationships
│   │       ├── Emphasizing continuity with line plots
│   │       │   ├── Aggregation and representing uncertainty
│   │       │   ├── Controlling sorting and orientation
│   │       │   └── Plotting subsets of data with semantic mappings
│   │       ├── Relating variables with scatter plots
│   │       └── Showing multiple relationships with facets
│   ├── Statistical operations
│   │   ├── Estimating regression fits
│   │   │   ├── Conditioning on other variables
│   │   │   ├── Fitting different kinds of models
│   │   │   ├── Functions for drawing linear regression models
│   │   │   └── Plotting a regression in other contexts
│   │   └── Statistical estimation and error bars
│   │       ├── Are error bars enough?
│   │       ├── Error bars on regression fits
│   │       ├── Measures of data spread
│   │       │   ├── Percentile interval error bars
│   │       │   └── Standard deviation error bars
│   │       └── Measures of estimate uncertainty
│   │           ├── Confidence interval error bars
│   │           ├── Custom error bars
│   │           └── Standard error bars
│   └── User guide and tutorial
│       ├── An introduction to seaborn
│       │   ├── A high-level API for statistical graphics
│       │   │   ├── Distributional representations
│       │   │   ├── Plots for categorical data
│       │   │   └── Statistical estimation
│       │   ├── Multivariate views on complex datasets
│       │   │   └── Lower-level tools for building figures
│       │   └── Opinionated defaults and flexible customization
│       │       ├── Next steps
│       │       └── Relationship to matplotlib
│       ├── Building structured multi-plot grids
│       │   ├── Conditional small multiples
│       │   ├── Plotting pairwise data relationships
│       │   └── Using custom functions
│       ├── Choosing color palettes
│       │   ├── Diverging color palettes
│       │   │   ├── Custom diverging palettes
│       │   │   ├── Other diverging palettes
│       │   │   └── Perceptually uniform diverging palettes
│       │   ├── General principles for using color in plots
│       │   │   ├── Components of color
│       │   │   ├── Vary hue to distinguish categories
│       │   │   └── Vary luminance to represent numbers
│       │   ├── Qualitative color palettes
│       │   │   ├── Using categorical Color Brewer palettes
│       │   │   └── Using circular color systems
│       │   ├── Sequential color palettes
│       │   │   ├── Custom sequential palettes
│       │   │   ├── Discrete vs. continuous mapping
│       │   │   ├── Perceptually uniform palettes
│       │   │   ├── Sequential Color Brewer palettes
│       │   │   └── Sequential “cubehelix” palettes
│       │   └── Tools for choosing color palettes
│       ├── Controlling figure aesthetics
│       │   ├── Overriding elements of the seaborn styles
│       │   ├── Removing axes spines
│       │   ├── Scaling plot elements
│       │   ├── Seaborn figure styles
│       │   └── Temporarily setting figure style
│       ├── Data structures accepted by seaborn
│       │   ├── Long-form vs. wide-form data
│       │   │   ├── Further reading and take-home points
│       │   │   ├── Long-form data
│       │   │   ├── Messy data
│       │   │   └── Wide-form data
│       │   ├── Options for visualizing long-form data
│       │   └── Options for visualizing wide-form data
│       ├── Estimating regression fits
│       │   ├── Conditioning on other variables
│       │   ├── Fitting different kinds of models
│       │   ├── Functions for drawing linear regression models
│       │   └── Plotting a regression in other contexts
│       ├── Overview of seaborn plotting functions
│       │   ├── Combining multiple views on the data
│       │   ├── Figure-level vs. axes-level functions
│       │   │   ├── Axes-level functions make self-contained plots
│       │   │   ├── Customizing plots from a figure-level function
│       │   │   ├── Figure-level functions own their figure
│       │   │   ├── Relative merits of figure-level functions
│       │   │   └── Specifying figure sizes
│       │   └── Similar functions for similar tasks
│       ├── Properties of Mark objects
│       │   ├── Color properties
│       │   │   ├── alpha, fillalpha, edgealpha
│       │   │   └── color, fillcolor, edgecolor
│       │   ├── Coordinate properties
│       │   │   └── x, y, xmin, xmax, ymin, ymax
│       │   ├── Other properties
│       │   │   ├── group
│       │   │   └── text
│       │   ├── Size properties
│       │   │   ├── edgewidth
│       │   │   ├── linewidth
│       │   │   ├── pointsize
│       │   │   └── stroke
│       │   ├── Style properties
│       │   │   ├── fill
│       │   │   ├── linestyle, edgestyle
│       │   │   └── marker
│       │   └── Text properties
│       │       ├── fontsize
│       │       ├── halign, valign
│       │       └── offset
│       ├── Statistical estimation and error bars
│       │   ├── Are error bars enough?
│       │   ├── Error bars on regression fits
│       │   ├── Measures of data spread
│       │   │   ├── Percentile interval error bars
│       │   │   └── Standard deviation error bars
│       │   └── Measures of estimate uncertainty
│       │       ├── Confidence interval error bars
│       │       ├── Custom error bars
│       │       └── Standard error bars
│       ├── The seaborn.objects interface
│       │   ├── Building and displaying the plot
│       │   │   ├── Adding multiple layers
│       │   │   ├── Building and displaying the plot
│       │   │   ├── Faceting and pairing subplots
│       │   │   ├── Integrating with matplotlib
│       │   │   └── Layer-specific mappings
│       │   ├── Customizing the appearance
│       │   │   ├── Customizing legends and ticks
│       │   │   ├── Customizing limits, labels, and titles
│       │   │   ├── Parameterizing scales
│       │   │   └── Theme customization
│       │   ├── Specifying a plot and mapping data
│       │   │   ├── Defining groups
│       │   │   ├── Mapping properties
│       │   │   └── Setting properties
│       │   └── Transforming data before plotting
│       │       ├── Creating variables through transformation
│       │       ├── Orienting marks and transforms
│       │       ├── Resolving overplotting
│       │       └── Statistical transformation
│       ├── Visualizing categorical data
│       │   ├── Categorical scatterplots
│       │   ├── Comparing distributions
│       │   │   ├── Boxplots
│       │   │   └── Violinplots
│       │   ├── Estimating central tendency
│       │   │   ├── Bar plots
│       │   │   └── Point plots
│       │   └── Showing additional dimensions
│       ├── Visualizing distributions of data
│       │   ├── Distribution visualization in other settings
│       │   │   ├── Plotting joint and marginal distributions
│       │   │   └── Plotting many distributions
│       │   ├── Empirical cumulative distributions
│       │   ├── Kernel density estimation
│       │   │   ├── Choosing the smoothing bandwidth
│       │   │   ├── Conditioning on other variables
│       │   │   └── Kernel density estimation pitfalls
│       │   ├── Plotting univariate histograms
│       │   │   ├── Choosing the bin size
│       │   │   ├── Conditioning on other variables
│       │   │   └── Normalized histogram statistics
│       │   └── Visualizing bivariate distributions
│       └── Visualizing statistical relationships
│           ├── Emphasizing continuity with line plots
│           │   ├── Aggregation and representing uncertainty
│           │   ├── Controlling sorting and orientation
│           │   └── Plotting subsets of data with semantic mappings
│           ├── Relating variables with scatter plots
│           └── Showing multiple relationships with facets
├── sklearn
│   ├── Examples
│   │   ├── Biclustering
│   │   │   ├── A demo of the Spectral Biclustering algorithm
│   │   │   ├── A demo of the Spectral Co-Clustering algorithm
│   │   │   └── Biclustering documents with the Spectral Co-clustering algorithm
│   │   ├── Calibration
│   │   │   ├── Comparison of Calibration of Classifiers
│   │   │   │   ├── Calibration curves
│   │   │   │   ├── Dataset
│   │   │   │   └── References
│   │   │   ├── Probability Calibration curves
│   │   │   │   ├── Calibration curves
│   │   │   │   │   ├── Gaussian Naive Bayes
│   │   │   │   │   └── Linear support vector classifier
│   │   │   │   ├── Dataset
│   │   │   │   ├── References
│   │   │   │   └── Summary
│   │   │   ├── Probability Calibration for 3-class classification
│   │   │   │   ├── Compare probabilities
│   │   │   │   ├── Data
│   │   │   │   └── Fitting and calibration
│   │   │   └── Probability calibration of classifiers
│   │   │       ├── Gaussian Naive-Bayes
│   │   │       ├── Generate synthetic dataset
│   │   │       └── Plot data and the predicted probabilities
│   │   ├── Classification
│   │   │   ├── Classifier comparison
│   │   │   ├── Linear and Quadratic Discriminant Analysis with covariance ellipsoid
│   │   │   │   ├── Colormap
│   │   │   │   ├── Datasets generation functions
│   │   │   │   ├── Plot
│   │   │   │   └── Plot functions
│   │   │   ├── Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification
│   │   │   ├── Plot classification probability
│   │   │   └── Recognizing hand-written digits
│   │   │       ├── Classification
│   │   │       └── Digits dataset
│   │   ├── Clustering
│   │   │   ├── A demo of K-Means clustering on the handwritten digits data
│   │   │   │   ├── Define our evaluation benchmark
│   │   │   │   ├── Load the dataset
│   │   │   │   ├── Run the benchmark
│   │   │   │   └── Visualize the results on PCA-reduced data
│   │   │   ├── A demo of structured Ward hierarchical clustering on an image of coins
│   │   │   │   ├── Compute clustering
│   │   │   │   ├── Define structure of the data
│   │   │   │   ├── Generate data
│   │   │   │   └── Plot the results on an image
│   │   │   ├── A demo of the mean-shift clustering algorithm
│   │   │   │   ├── Compute clustering with MeanShift
│   │   │   │   ├── Generate sample data
│   │   │   │   └── Plot result
│   │   │   ├── Adjustment for chance in clustering performance evaluation
│   │   │   │   ├── Defining the list of metrics to evaluate
│   │   │   │   ├── First experiment: fixed ground truth labels and growing number of clusters
│   │   │   │   └── Second experiment: varying number of classes and clusters
│   │   │   ├── Agglomerative clustering with and without structure
│   │   │   ├── Agglomerative clustering with different metrics
│   │   │   ├── An example of K-Means++ initialization
│   │   │   ├── Bisecting K-Means and Regular K-Means Performance Comparison
│   │   │   ├── Color Quantization using K-Means
│   │   │   ├── Compare BIRCH and MiniBatchKMeans
│   │   │   ├── Comparing different clustering algorithms on toy datasets
│   │   │   ├── Comparing different hierarchical linkage methods on toy datasets
│   │   │   ├── Comparison of the K-Means and MiniBatchKMeans clustering algorithms
│   │   │   │   ├── Compute clustering with KMeans
│   │   │   │   ├── Compute clustering with MiniBatchKMeans
│   │   │   │   ├── Establishing parity between clusters
│   │   │   │   ├── Generate the data
│   │   │   │   └── Plotting the results
│   │   │   ├── Demo of DBSCAN clustering algorithm
│   │   │   │   ├── Compute DBSCAN
│   │   │   │   ├── Data generation
│   │   │   │   └── Plot results
│   │   │   ├── Demo of OPTICS clustering algorithm
│   │   │   ├── Demo of affinity propagation clustering algorithm
│   │   │   │   ├── Compute Affinity Propagation
│   │   │   │   ├── Generate sample data
│   │   │   │   └── Plot result
│   │   │   ├── Demonstration of k-means assumptions
│   │   │   │   ├── Data generation
│   │   │   │   ├── Final remarks
│   │   │   │   ├── Fit models and plot results
│   │   │   │   └── Possible solutions
│   │   │   ├── Empirical evaluation of the impact of k-means initialization
│   │   │   ├── Feature agglomeration
│   │   │   ├── Feature agglomeration vs. univariate selection
│   │   │   ├── Hierarchical clustering: structured vs unstructured ward
│   │   │   │   ├── Compute clustering
│   │   │   │   ├── Generate data
│   │   │   │   ├── Plot result
│   │   │   │   └── We are defining k-Nearest Neighbors with 10 neighbors
│   │   │   ├── Inductive Clustering
│   │   │   ├── K-means Clustering
│   │   │   ├── Online learning of a dictionary of parts of faces
│   │   │   │   ├── Learn the dictionary of images
│   │   │   │   ├── Load the data
│   │   │   │   └── Plot the results
│   │   │   ├── Plot Hierarchical Clustering Dendrogram
│   │   │   ├── Segmenting the picture of greek coins in regions
│   │   │   ├── Selecting the number of clusters with silhouette analysis on KMeans clustering
│   │   │   ├── Spectral clustering for image segmentation
│   │   │   │   ├── Generate the data
│   │   │   │   ├── Plotting four circles
│   │   │   │   └── Plotting two circles
│   │   │   ├── Various Agglomerative Clustering on a 2D embedding of digits
│   │   │   └── Vector Quantization Example
│   │   │       ├── Compression via vector quantization
│   │   │       │   ├── Encoding strategy
│   │   │       │   └── Memory footprint
│   │   │       └── Original image
│   │   ├── Covariance estimation
│   │   │   ├── Ledoit-Wolf vs OAS estimation
│   │   │   ├── Robust covariance estimation and Mahalanobis distances relevance
│   │   │   │   ├── Comparison of results
│   │   │   │   └── Generate data
│   │   │   ├── Robust vs Empirical covariance estimate
│   │   │   │   ├── Evaluation
│   │   │   │   ├── Minimum Covariance Determinant Estimator
│   │   │   │   └── References
│   │   │   ├── Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood
│   │   │   │   ├── Compare different approaches to setting the regularization parameter
│   │   │   │   ├── Compute the likelihood on test data
│   │   │   │   ├── Generate sample data
│   │   │   │   └── Plot results
│   │   │   └── Sparse inverse covariance estimation
│   │   │       ├── Estimate the covariance
│   │   │       ├── Generate the data
│   │   │       └── Plot the results
│   │   ├── Cross decomposition
│   │   │   ├── Compare cross decomposition methods
│   │   │   │   ├── CCA (PLS mode B with symmetric deflation)
│   │   │   │   ├── Canonical (symmetric) PLS
│   │   │   │   │   ├── Scatter plot of scores
│   │   │   │   │   └── Transform data
│   │   │   │   ├── Dataset based latent variables model
│   │   │   │   ├── PLS regression, with multivariate response, a.k.a. PLS2
│   │   │   │   └── PLS regression, with univariate response, a.k.a. PLS1
│   │   │   └── Principal Component Regression vs Partial Least Squares Regression
│   │   │       ├── Projection on one component and predictive power
│   │   │       └── The data
│   │   ├── Dataset examples
│   │   │   ├── Plot randomly generated classification dataset
│   │   │   ├── Plot randomly generated multilabel dataset
│   │   │   ├── The Digit Dataset
│   │   │   └── The Iris Dataset
│   │   ├── Decision Trees
│   │   │   ├── Decision Tree Regression
│   │   │   ├── Multi-output Decision Tree Regression
│   │   │   ├── Plot the decision surface of decision trees trained on the iris dataset
│   │   │   ├── Post pruning decision trees with cost complexity pruning
│   │   │   │   ├── Accuracy vs alpha for training and testing sets
│   │   │   │   └── Total impurity of leaves vs effective alphas of pruned tree
│   │   │   └── Understanding the decision tree structure
│   │   │       ├── Decision path
│   │   │       ├── Train tree classifier
│   │   │       └── Tree structure
│   │   ├── Decomposition
│   │   │   ├── Beta-divergence loss functions
│   │   │   ├── Blind source separation using FastICA
│   │   │   │   ├── Fit ICA and PCA models
│   │   │   │   ├── Generate sample data
│   │   │   │   └── Plot results
│   │   │   ├── Comparison of LDA and PCA 2D projection of Iris dataset
│   │   │   ├── Faces dataset decompositions
│   │   │   │   ├── Dataset preparation
│   │   │   │   ├── Decomposition
│   │   │   │   │   ├── Cluster centers - MiniBatchKMeans
│   │   │   │   │   ├── Dictionary learning
│   │   │   │   │   ├── Eigenfaces - PCA using randomized SVD
│   │   │   │   │   ├── Factor Analysis components - FA
│   │   │   │   │   ├── Independent components - FastICA
│   │   │   │   │   ├── Non-negative components - NMF
│   │   │   │   │   └── Sparse components - MiniBatchSparsePCA
│   │   │   │   └── Decomposition: Dictionary learning
│   │   │   │       ├── Dictionary learning - positive code
│   │   │   │       ├── Dictionary learning - positive dictionary
│   │   │   │       └── Dictionary learning - positive dictionary & code
│   │   │   ├── Factor Analysis (with rotation) to visualize patterns
│   │   │   ├── FastICA on 2D point clouds
│   │   │   │   ├── Generate sample data
│   │   │   │   └── Plot results
│   │   │   ├── Image denoising using dictionary learning
│   │   │   │   ├── Display the distorted image
│   │   │   │   ├── Extract noisy patches and reconstruct them using the dictionary
│   │   │   │   ├── Extract reference patches
│   │   │   │   ├── Generate distorted image
│   │   │   │   └── Learn the dictionary from reference patches
│   │   │   ├── Incremental PCA
│   │   │   ├── Kernel PCA
│   │   │   │   ├── Projecting data: PCA vs. KernelPCA
│   │   │   │   └── Projecting into the original feature space
│   │   │   ├── Model selection with Probabilistic PCA and Factor Analysis (FA)
│   │   │   │   ├── Create the data
│   │   │   │   └── Fit the models
│   │   │   ├── PCA example with Iris Data-set
│   │   │   ├── Principal components analysis (PCA)
│   │   │   │   ├── Create the data
│   │   │   │   └── Plot the figures
│   │   │   └── Sparse coding with a precomputed dictionary
│   │   ├── Ensemble methods
│   │   │   ├── Categorical Feature Support in Gradient Boosting
│   │   │   │   ├── Gradient boosting estimator with dropped categorical features
│   │   │   │   ├── Gradient boosting estimator with native categorical support
│   │   │   │   ├── Gradient boosting estimator with one-hot encoding
│   │   │   │   ├── Gradient boosting estimator with ordinal encoding
│   │   │   │   ├── Limiting the number of splits
│   │   │   │   ├── Load Ames Housing dataset
│   │   │   │   └── Model comparison
│   │   │   ├── Combine predictors using stacking
│   │   │   │   ├── Download the dataset
│   │   │   │   ├── Make pipeline to preprocess the data
│   │   │   │   ├── Measure and plot the results
│   │   │   │   └── Stack of predictors on a single data set
│   │   │   ├── Comparing random forests and the multi-output meta estimator
│   │   │   ├── Decision Tree Regression with AdaBoost
│   │   │   │   ├── Plotting the results
│   │   │   │   ├── Preparing the data
│   │   │   │   └── Training and prediction with DecisionTree and AdaBoost Regressors
│   │   │   ├── Discrete versus Real AdaBoost
│   │   │   │   ├── Adaboost with discrete SAMME and real SAMME.R
│   │   │   │   ├── Concluding remarks
│   │   │   │   ├── Plotting the results
│   │   │   │   └── Preparing the data and baseline models
│   │   │   ├── Early stopping of Gradient Boosting
│   │   │   │   ├── Compare fit times with and without early stopping
│   │   │   │   └── Compare scores with and without early stopping
│   │   │   ├── Feature importances with a forest of trees
│   │   │   │   ├── Data generation and model fitting
│   │   │   │   ├── Feature importance based on feature permutation
│   │   │   │   └── Feature importance based on mean decrease in impurity
│   │   │   ├── Feature transformations with ensembles of trees
│   │   │   ├── Gradient Boosting Out-of-Bag estimates
│   │   │   ├── Gradient Boosting regression
│   │   │   │   ├── Data preprocessing
│   │   │   │   ├── Fit regression model
│   │   │   │   ├── Load the data
│   │   │   │   ├── Plot feature importance
│   │   │   │   └── Plot training deviance
│   │   │   ├── Gradient Boosting regularization
│   │   │   ├── Hashing feature transformation using Totally Random Trees
│   │   │   ├── IsolationForest example
│   │   │   │   ├── Data generation
│   │   │   │   ├── Plot discrete decision boundary
│   │   │   │   ├── Plot path length decision boundary
│   │   │   │   └── Training of the model
│   │   │   ├── Monotonic Constraints
│   │   │   │   └── Using feature names to specify monotonic constraints
│   │   │   ├── Multi-class AdaBoosted Decision Trees
│   │   │   ├── OOB Errors for Random Forests
│   │   │   ├── Pixel importances with a parallel forest of trees
│   │   │   │   ├── Feature importance based on mean decrease in impurity (MDI)
│   │   │   │   └── Loading the data and model fitting
│   │   │   ├── Plot class probabilities calculated by the VotingClassifier
│   │   │   ├── Plot individual and voting regression predictions
│   │   │   │   ├── Making predictions
│   │   │   │   ├── Plot the results
│   │   │   │   └── Training classifiers
│   │   │   ├── Plot the decision boundaries of a VotingClassifier
│   │   │   ├── Plot the decision surfaces of ensembles of trees on the iris dataset
│   │   │   ├── Prediction Intervals for Gradient Boosting Regression
│   │   │   │   ├── Analysis of the error metrics
│   │   │   │   ├── Calibration of the confidence interval
│   │   │   │   ├── Fitting non-linear quantile and least squares regressors
│   │   │   │   └── Tuning the hyper-parameters of the quantile regressors
│   │   │   ├── Single estimator versus bagging: bias-variance decomposition
│   │   │   │   └── References
│   │   │   └── Two-class AdaBoost
│   │   ├── Examples based on real world datasets
│   │   │   ├── Compressive sensing: tomography reconstruction with L1 prior (Lasso)
│   │   │   ├── Faces recognition example using eigenfaces and SVMs
│   │   │   ├── Image denoising using kernel PCA
│   │   │   │   ├── Learn the PCA basis
│   │   │   │   ├── Load the dataset via OpenML
│   │   │   │   └── Reconstruct and denoise test images
│   │   │   ├── Libsvm GUI
│   │   │   ├── Model Complexity Influence
│   │   │   │   ├── Benchmark influence
│   │   │   │   ├── Choose parameters
│   │   │   │   ├── Conclusion
│   │   │   │   ├── Load the data
│   │   │   │   └── Run the code and plot the results
│   │   │   ├── Out-of-core classification of text documents
│   │   │   │   ├── Main
│   │   │   │   ├── Plot results
│   │   │   │   └── Reuters Dataset related routines
│   │   │   ├── Outlier detection on a real data set
│   │   │   │   ├── First example
│   │   │   │   └── Second example
│   │   │   ├── Prediction Latency
│   │   │   │   ├── Benchmark and plot helper functions
│   │   │   │   ├── Benchmark bulk/atomic prediction speed for various regressors
│   │   │   │   ├── Benchmark n_features influence on prediction speed
│   │   │   │   └── Benchmark throughput
│   │   │   ├── Species distribution modeling
│   │   │   │   └── References
│   │   │   ├── Time-related feature engineering
│   │   │   │   ├── Concluding remarks
│   │   │   │   ├── Data exploration on the Bike Sharing Demand dataset
│   │   │   │   ├── Gradient Boosting
│   │   │   │   ├── Modeling non-linear feature interactions with kernels
│   │   │   │   ├── Modeling pairwise interactions with splines and polynomial features
│   │   │   │   ├── Naive linear regression
│   │   │   │   ├── Periodic spline features
│   │   │   │   ├── Qualitative analysis of the impact of features on linear model predictions
│   │   │   │   ├── Time-based cross-validation
│   │   │   │   ├── Time-steps as categories
│   │   │   │   └── Trigonometric features
│   │   │   ├── Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
│   │   │   ├── Visualizing the stock market structure
│   │   │   │   ├── Clustering using affinity propagation
│   │   │   │   ├── Embedding in 2D space
│   │   │   │   ├── Learning a graph structure
│   │   │   │   ├── Retrieve the data from Internet
│   │   │   │   └── Visualization
│   │   │   └── Wikipedia principal eigenvector
│   │   │       ├── Computing Centrality scores
│   │   │       ├── Computing Principal Singular Vector using Randomized SVD
│   │   │       ├── Computing the Adjacency matrix
│   │   │       ├── Download data, if not already on disk
│   │   │       └── Loading the redirect files
│   │   ├── Feature Selection
│   │   │   ├── Comparison of F-test and mutual information
│   │   │   ├── Model-based and sequential feature selection
│   │   │   │   ├── Discussion
│   │   │   │   ├── Feature importance from coefficients
│   │   │   │   ├── Loading the data
│   │   │   │   ├── Selecting features based on importance
│   │   │   │   └── Selecting features with Sequential Feature Selection
│   │   │   ├── Pipeline ANOVA SVM
│   │   │   ├── Recursive feature elimination
│   │   │   ├── Recursive feature elimination with cross-validation
│   │   │   │   ├── Data generation
│   │   │   │   ├── Model training and selection
│   │   │   │   └── Plot number of features VS. cross-validation scores
│   │   │   └── Univariate Feature Selection
│   │   │       ├── Compare with SVMs
│   │   │       ├── Generate sample data
│   │   │       └── Univariate feature selection
│   │   ├── Gaussian Mixture Models
│   │   │   ├── Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture
│   │   │   ├── Density Estimation for a Gaussian mixture
│   │   │   ├── GMM Initialization Methods
│   │   │   ├── GMM covariances
│   │   │   ├── Gaussian Mixture Model Ellipsoids
│   │   │   ├── Gaussian Mixture Model Selection
│   │   │   │   ├── Data generation
│   │   │   │   ├── Model training and selection
│   │   │   │   ├── Plot the BIC scores
│   │   │   │   └── Plot the best model
│   │   │   └── Gaussian Mixture Model Sine Curve
│   │   ├── Gaussian Process for Machine Learning
│   │   │   ├── Comparison of kernel ridge and Gaussian process regression
│   │   │   │   ├── Final conclusion
│   │   │   │   ├── Generating a dataset
│   │   │   │   ├── Kernel methods: kernel ridge and Gaussian process
│   │   │   │   │   ├── Gaussian process regression
│   │   │   │   │   └── Kernel ridge
│   │   │   │   └── Limitations of a simple linear model
│   │   │   ├── Gaussian Processes regression: basic introductory example
│   │   │   │   ├── Dataset generation
│   │   │   │   ├── Example with noise-free target
│   │   │   │   └── Example with noisy targets
│   │   │   ├── Gaussian process classification (GPC) on iris dataset
│   │   │   ├── Gaussian process regression (GPR) on Mauna Loa CO2 data
│   │   │   │   ├── Build the dataset
│   │   │   │   ├── Design the proper kernel
│   │   │   │   ├── Interpretation of kernel hyperparameters
│   │   │   │   └── Model fitting and extrapolation
│   │   │   ├── Gaussian process regression (GPR) with noise-level estimation
│   │   │   │   ├── Data generation
│   │   │   │   └── Optimisation of kernel hyperparameters in GPR
│   │   │   ├── Gaussian processes on discrete data structures
│   │   │   │   ├── Classification
│   │   │   │   ├── Regression
│   │   │   │   └── Sequence similarity matrix under the kernel
│   │   │   ├── Illustration of Gaussian process classification (GPC) on the XOR dataset
│   │   │   ├── Illustration of prior and posterior Gaussian process for different kernels
│   │   │   │   ├── Dataset and Gaussian process generation
│   │   │   │   ├── Helper function
│   │   │   │   └── Kernel cookbook
│   │   │   │       ├── Dot-product kernel
│   │   │   │       ├── Exp-Sine-Squared kernel
│   │   │   │       ├── Matérn kernel
│   │   │   │       ├── Radial Basis Function kernel
│   │   │   │       └── Rational Quadradtic kernel
│   │   │   ├── Iso-probability lines for Gaussian Processes classification (GPC)
│   │   │   └── Probabilistic predictions with Gaussian process classification (GPC)
│   │   ├── Generalized Linear Models
│   │   │   ├── Comparing Linear Bayesian Regressors
│   │   │   │   ├── Bayesian regressions with polynomial feature expansion
│   │   │   │   │   ├── Fit the regressors
│   │   │   │   │   ├── Generate synthetic dataset
│   │   │   │   │   └── Plotting polynomial regressions with std errors of the scores
│   │   │   │   └── Models robustness to recover the ground truth weights
│   │   │   │       ├── Fit the regressors
│   │   │   │       ├── Generate synthetic dataset
│   │   │   │       ├── Plot the marginal log-likelihood
│   │   │   │       └── Plot the true and estimated coefficients
│   │   │   ├── Comparing various online solvers
│   │   │   ├── Curve Fitting with Bayesian Ridge Regression
│   │   │   │   ├── Fit by cubic polynomial
│   │   │   │   ├── Generate sinusoidal data with noise
│   │   │   │   └── Plot the true and predicted curves with log marginal likelihood (L)
│   │   │   ├── Early stopping of Stochastic Gradient Descent
│   │   │   ├── Fitting an Elastic Net with a precomputed Gram Matrix and Weighted Samples
│   │   │   ├── HuberRegressor vs Ridge on dataset with strong outliers
│   │   │   ├── Joint feature selection with multi-task Lasso
│   │   │   │   ├── Fit models
│   │   │   │   ├── Generate data
│   │   │   │   └── Plot support and time series
│   │   │   ├── L1 Penalty and Sparsity in Logistic Regression
│   │   │   ├── Lasso and Elastic Net
│   │   │   ├── Lasso and Elastic Net for Sparse Signals
│   │   │   │   ├── Data Generation
│   │   │   │   ├── ElasticNet
│   │   │   │   ├── Lasso
│   │   │   │   └── Plot
│   │   │   ├── Lasso model selection via information criteria
│   │   │   ├── Lasso model selection: AIC-BIC / cross-validation
│   │   │   │   ├── Conclusion
│   │   │   │   ├── Dataset
│   │   │   │   ├── Selecting Lasso via an information criterion
│   │   │   │   └── Selecting Lasso via cross-validation
│   │   │   │       ├── Lasso via coordinate descent
│   │   │   │       ├── Lasso via least angle regression
│   │   │   │       └── Summary of cross-validation approach
│   │   │   ├── Lasso on dense and sparse data
│   │   │   │   ├── Comparing the two Lasso implementations on Dense data
│   │   │   │   └── Comparing the two Lasso implementations on Sparse data
│   │   │   ├── Lasso path using LARS
│   │   │   ├── Linear Regression Example
│   │   │   ├── Logistic Regression 3-class Classifier
│   │   │   ├── Logistic function
│   │   │   ├── MNIST classification using multinomial logistic + L1
│   │   │   ├── Multiclass sparse logistic regression on 20newgroups
│   │   │   ├── Non-negative least squares
│   │   │   ├── One-Class SVM versus One-Class SVM using Stochastic Gradient Descent
│   │   │   ├── Ordinary Least Squares and Ridge Regression Variance
│   │   │   ├── Orthogonal Matching Pursuit
│   │   │   ├── Plot Ridge coefficients as a function of the L2 regularization
│   │   │   ├── Plot Ridge coefficients as a function of the regularization
│   │   │   │   ├── Compute paths
│   │   │   │   └── Display results
│   │   │   ├── Plot multi-class SGD on the iris dataset
│   │   │   ├── Plot multinomial and One-vs-Rest Logistic Regression
│   │   │   ├── Poisson regression and non-normal loss
│   │   │   │   ├── (Generalized) linear models
│   │   │   │   ├── A constant prediction baseline
│   │   │   │   ├── Evaluation of the calibration of predictions
│   │   │   │   ├── Evaluation of the ranking power
│   │   │   │   ├── Gradient Boosting Regression Trees for Poisson regression
│   │   │   │   ├── Main takeaways
│   │   │   │   └── The French Motor Third-Party Liability Claims dataset
│   │   │   ├── Polynomial and Spline interpolation
│   │   │   │   └── Periodic Splines
│   │   │   ├── Quantile regression
│   │   │   │   ├── Comparing QuantileRegressor and LinearRegression
│   │   │   │   ├── Dataset generation
│   │   │   │   └── Fitting a QuantileRegressor
│   │   │   ├── Regularization path of L1- Logistic Regression
│   │   │   │   ├── Compute regularization path
│   │   │   │   ├── Load data
│   │   │   │   └── Plot regularization path
│   │   │   ├── Robust linear estimator fitting
│   │   │   ├── Robust linear model estimation using RANSAC
│   │   │   ├── SGD: Maximum margin separating hyperplane
│   │   │   ├── SGD: Penalties
│   │   │   ├── SGD: Weighted samples
│   │   │   ├── SGD: convex loss functions
│   │   │   ├── Sparsity Example: Fitting only features 1  and 2
│   │   │   ├── Theil-Sen Regression
│   │   │   │   ├── Outliers in the X direction
│   │   │   │   └── Outliers only in the y direction
│   │   │   └── Tweedie regression on insurance claims
│   │   │       ├── Frequency model – Poisson distribution
│   │   │       ├── Loading datasets, basic feature extraction and target definitions
│   │   │       ├── Pure Premium Modeling via a Product Model vs single TweedieRegressor
│   │   │       └── Severity Model -  Gamma distribution
│   │   ├── Inspection
│   │   │   ├── Common pitfalls in the interpretation of coefficients of linear models
│   │   │   │   └── 
│   │   │   ├── Failure of Machine Learning to infer causal effects
│   │   │   │   ├── Description of the simulated data
│   │   │   │   ├── Income prediction with fully observed variables
│   │   │   │   ├── Income prediction with partial observations
│   │   │   │   ├── Lessons learned
│   │   │   │   └── The dataset: simulated hourly wages
│   │   │   ├── Partial Dependence and Individual Conditional Expectation Plots
│   │   │   │   ├── 1-way partial dependence with different models
│   │   │   │   ├── 2D interaction plots
│   │   │   │   ├── Bike sharing dataset preprocessing
│   │   │   │   └── Preprocessor for machine-learning models
│   │   │   │       ├── Preprocessor for the gradient boosting model
│   │   │   │       └── Preprocessor for the neural network model
│   │   │   ├── Permutation Importance vs Random Forest Feature Importance (MDI)
│   │   │   │   ├── Accuracy of the Model
│   │   │   │   ├── Data Loading and Feature Engineering
│   │   │   │   └── Tree’s Feature Importance from Mean Decrease in Impurity (MDI)
│   │   │   └── Permutation Importance with Multicollinear or Correlated Features
│   │   │       ├── Handling Multicollinear Features
│   │   │       └── Random Forest Feature Importance on Breast Cancer Data
│   │   ├── Kernel Approximation
│   │   │   └── Scalable learning with polynomial kernel approximation
│   │   │       ├── Comparing the results
│   │   │       │   └── References
│   │   │       ├── Establishing a baseline model
│   │   │       ├── Establishing the kernel approximation model
│   │   │       ├── Establishing the kernelized SVM model
│   │   │       ├── Feature normalization
│   │   │       ├── Partitioning the data
│   │   │       └── Preparing the data
│   │   ├── Manifold learning
│   │   │   ├── Comparison of Manifold Learning methods
│   │   │   │   ├── Dataset preparation
│   │   │   │   └── Define algorithms for the manifold learning
│   │   │   │       ├── Isomap Embedding
│   │   │   │       ├── Locally Linear Embeddings
│   │   │   │       ├── Multidimensional scaling
│   │   │   │       ├── Spectral embedding for non-linear dimensionality reduction
│   │   │   │       └── T-distributed Stochastic Neighbor Embedding
│   │   │   ├── Manifold Learning methods on a severed sphere
│   │   │   ├── Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
│   │   │   ├── Multi-dimensional scaling
│   │   │   ├── Swiss Roll And Swiss-Hole Reduction
│   │   │   │   ├── Concluding remarks
│   │   │   │   ├── Swiss Roll
│   │   │   │   └── Swiss-Hole
│   │   │   └── t-SNE: The effect of various perplexity values on the shape
│   │   ├── Miscellaneous
│   │   │   ├── Advanced Plotting With Partial Dependence
│   │   │   │   ├── Plotting partial dependence for one feature
│   │   │   │   ├── Plotting partial dependence for two features
│   │   │   │   ├── Plotting partial dependence of the two models together
│   │   │   │   └── Train models on the diabetes dataset
│   │   │   ├── Comparing anomaly detection algorithms for outlier detection on toy datasets
│   │   │   ├── Comparison of kernel ridge regression and SVR
│   │   │   │   ├── Compare times of SVR and Kernel Ridge Regression
│   │   │   │   ├── Construct the kernel-based regression models
│   │   │   │   ├── Generate sample data
│   │   │   │   ├── Look at the results
│   │   │   │   ├── Visualize the learning curves
│   │   │   │   └── Visualize training and prediction times
│   │   │   ├── Displaying Pipelines
│   │   │   │   ├── Displaying a Complex Pipeline Chaining a Column Transformer
│   │   │   │   ├── Displaying a Grid Search over a Pipeline with a Classifier
│   │   │   │   ├── Displaying a Pipeline Chaining Multiple Preprocessing Steps & Classifier
│   │   │   │   ├── Displaying a Pipeline and Dimensionality Reduction and Classifier
│   │   │   │   └── Displaying a Pipeline with a Preprocessing Step and Classifier
│   │   │   ├── Displaying estimators and complex pipelines
│   │   │   │   ├── Compact text representation
│   │   │   │   └── Rich HTML representation
│   │   │   ├── Evaluation of outlier detection estimators
│   │   │   │   ├── Define a data preprocessing function
│   │   │   │   ├── Define an outlier prediction function
│   │   │   │   └── Plot and interpret results
│   │   │   ├── Explicit feature map approximation for RBF kernels
│   │   │   │   ├── Decision Surfaces of RBF Kernel SVM and Linear SVM
│   │   │   │   ├── Python package and dataset imports, load dataset
│   │   │   │   └── Timing and accuracy plots
│   │   │   ├── Face completion with a multi-output estimators
│   │   │   ├── Introducing the `set_output` API
│   │   │   ├── Isotonic Regression
│   │   │   ├── Multilabel classification
│   │   │   ├── ROC Curve with Visualization API
│   │   │   │   ├── Load Data and Train a SVC
│   │   │   │   ├── Plotting the ROC Curve
│   │   │   │   └── Training a Random Forest and Plotting the ROC Curve
│   │   │   ├── The Johnson-Lindenstrauss bound for embedding with random projections
│   │   │   │   ├── Empirical validation
│   │   │   │   ├── Remarks
│   │   │   │   └── Theoretical bounds
│   │   │   └── Visualizations with Display Objects
│   │   │       └── Load Data and train model
│   │   │           ├── Combining the display objects into a single plot
│   │   │           └── Create
│   │   ├── Missing Value Imputation
│   │   │   ├── Imputing missing values before building an estimator
│   │   │   │   ├── Download the data and make missing values sets
│   │   │   │   ├── Impute the missing data and score
│   │   │   │   │   ├── Estimate the score
│   │   │   │   │   ├── Impute missing values with mean
│   │   │   │   │   ├── Iterative imputation of the missing values
│   │   │   │   │   ├── Missing information
│   │   │   │   │   ├── Replace missing values by 0
│   │   │   │   │   └── kNN-imputation of the missing values
│   │   │   │   └── Plot the results
│   │   │   └── Imputing missing values with variants of IterativeImputer
│   │   ├── Model Selection
│   │   │   ├── Balance model complexity and cross-validated score
│   │   │   ├── Class Likelihood Ratios to measure classification performance
│   │   │   │   ├── Cross-validation of likelihood ratios
│   │   │   │   ├── Invariance with respect to prevalence
│   │   │   │   └── Pre-test vs. post-test analysis
│   │   │   ├── Comparing randomized search and grid search for hyperparameter estimation
│   │   │   ├── Comparison between grid search and successive halving
│   │   │   ├── Confusion matrix
│   │   │   ├── Custom refit strategy of a grid search with cross-validation
│   │   │   │   ├── Define our grid-search strategy
│   │   │   │   ├── The dataset
│   │   │   │   └── Tuning hyper-parameters
│   │   │   ├── Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV
│   │   │   │   ├── Plotting the result
│   │   │   │   └── Running GridSearchCV using multiple evaluation metrics
│   │   │   ├── Detection error tradeoff (DET) curve
│   │   │   │   ├── Define the classifiers
│   │   │   │   ├── Generate synthetic data
│   │   │   │   └── Plot ROC and DET curves
│   │   │   ├── Multiclass Receiver Operating Characteristic (ROC)
│   │   │   │   ├── Load and prepare data
│   │   │   │   ├── One-vs-One multiclass ROC
│   │   │   │   │   ├── Plot all OvO ROC curves together
│   │   │   │   │   └── ROC curve using the OvO macro-average
│   │   │   │   └── One-vs-Rest multiclass ROC
│   │   │   │       ├── Plot all OvR ROC curves together
│   │   │   │       ├── ROC curve showing a specific class
│   │   │   │       ├── ROC curve using micro-averaged OvR
│   │   │   │       └── ROC curve using the OvR macro-average
│   │   │   ├── Nested versus non-nested cross-validation
│   │   │   ├── Plotting Cross-Validated Predictions
│   │   │   ├── Plotting Learning Curves and Checking Models' Scalability
│   │   │   ├── Plotting Validation Curves
│   │   │   ├── Precision-Recall
│   │   │   │   ├── In binary classification settings
│   │   │   │   │   ├── Dataset and model
│   │   │   │   │   └── Plot the Precision-Recall curve
│   │   │   │   └── In multi-label settings
│   │   │   │       ├── Create multi-label data, fit, and predict
│   │   │   │       ├── Plot Precision-Recall curve for each class and iso-f1 curves
│   │   │   │       ├── Plot the micro-averaged Precision-Recall curve
│   │   │   │       └── The average precision score in multi-label settings
│   │   │   ├── Receiver Operating Characteristic (ROC) with cross validation
│   │   │   │   └── Load and prepare data
│   │   │   │       └── Classification and ROC analysis
│   │   │   ├── Sample pipeline for text feature extraction and evaluation
│   │   │   │   ├── Data loading
│   │   │   │   └── Pipeline with hyperparameter tuning
│   │   │   ├── Statistical comparison of models using grid search
│   │   │   │   ├── Comparing two models: Bayesian approach
│   │   │   │   │   └── Region of Practical Equivalence
│   │   │   │   ├── Comparing two models: frequentist approach
│   │   │   │   ├── Pairwise comparison of all models: Bayesian approach
│   │   │   │   ├── Pairwise comparison of all models: frequentist approach
│   │   │   │   └── Take-home messages
│   │   │   ├── Successive Halving Iterations
│   │   │   │   └── Number of candidates and amount of resource at each iteration
│   │   │   ├── Test with permutations the significance of a classification score
│   │   │   │   ├── Dataset
│   │   │   │   └── Permutation test score
│   │   │   │       ├── Original data
│   │   │   │       └── Random data
│   │   │   ├── Train error vs Test error
│   │   │   │   ├── Compute train and test errors
│   │   │   │   ├── Generate sample data
│   │   │   │   └── Plot results functions
│   │   │   ├── Underfitting vs. Overfitting
│   │   │   └── Visualizing cross-validation behavior in scikit-learn
│   │   │       ├── Define a function to visualize cross-validation behavior
│   │   │       ├── Visualize cross-validation indices for many CV objects
│   │   │       └── Visualize our data
│   │   ├── Multioutput methods
│   │   │   └── Classifier Chain
│   │   ├── Nearest Neighbors
│   │   │   ├── Approximate nearest neighbors in TSNE
│   │   │   ├── Caching nearest neighbors
│   │   │   ├── Comparing Nearest Neighbors with and without Neighborhood Components Analysis
│   │   │   ├── Dimensionality Reduction with Neighborhood Components Analysis
│   │   │   ├── Kernel Density Estimate of Species Distributions
│   │   │   │   └── References
│   │   │   ├── Kernel Density Estimation
│   │   │   ├── Nearest Centroid Classification
│   │   │   ├── Nearest Neighbors Classification
│   │   │   ├── Nearest Neighbors regression
│   │   │   │   ├── Fit regression model
│   │   │   │   └── Generate sample data
│   │   │   ├── Neighborhood Components Analysis Illustration
│   │   │   │   ├── Learning an embedding
│   │   │   │   └── Original points
│   │   │   ├── Novelty detection with Local Outlier Factor (LOF)
│   │   │   ├── Outlier detection with Local Outlier Factor (LOF)
│   │   │   └── Simple 1D Kernel Density Estimation
│   │   ├── Neural Networks
│   │   │   ├── Compare Stochastic learning strategies for MLPClassifier
│   │   │   ├── Restricted Boltzmann Machine features for digit classification
│   │   │   │   ├── Evaluation
│   │   │   │   ├── Generate data
│   │   │   │   ├── Models definition
│   │   │   │   ├── Plotting
│   │   │   │   └── Training
│   │   │   ├── Varying regularization in Multi-layer Perceptron
│   │   │   └── Visualization of MLP weights on MNIST
│   │   ├── Pipelines and composite estimators
│   │   │   ├── Column Transformer with Heterogeneous Data Sources
│   │   │   │   ├── 20 newsgroups dataset
│   │   │   │   ├── Classification pipeline
│   │   │   │   └── Creating transformers
│   │   │   ├── Column Transformer with Mixed Types
│   │   │   ├── Concatenating multiple feature extraction methods
│   │   │   ├── Effect of transforming the targets in regression model
│   │   │   │   ├── Real-world data set
│   │   │   │   └── Synthetic example
│   │   │   ├── Pipelining: chaining a PCA and a logistic regression
│   │   │   └── Selecting dimensionality reduction with Pipeline and GridSearchCV
│   │   │       ├── Caching transformers within a Pipeline
│   │   │       └── Illustration of Pipeline and GridSearchCV
│   │   ├── Preprocessing
│   │   │   ├── Compare the effect of different scalers on data with outliers
│   │   │   │   ├── MaxAbsScaler
│   │   │   │   ├── MinMaxScaler
│   │   │   │   ├── Normalizer
│   │   │   │   ├── Original data
│   │   │   │   ├── PowerTransformer
│   │   │   │   ├── QuantileTransformer (Gaussian output)
│   │   │   │   ├── QuantileTransformer (uniform output)
│   │   │   │   ├── RobustScaler
│   │   │   │   └── StandardScaler
│   │   │   ├── Demonstrating the different strategies of KBinsDiscretizer
│   │   │   ├── Feature discretization
│   │   │   ├── Importance of Feature Scaling
│   │   │   │   ├── Effect of rescaling on a PCA dimensional reduction
│   │   │   │   ├── Effect of rescaling on a k-neighbors models
│   │   │   │   ├── Effect of rescaling on model’s performance
│   │   │   │   └── Load and prepare data
│   │   │   ├── Map data to a normal distribution
│   │   │   └── Using KBinsDiscretizer to discretize continuous features
│   │   ├── Release Highlights
│   │   │   ├── Release Highlights for scikit-learn 0.22
│   │   │   │   ├── Checking scikit-learn compatibility of an estimator
│   │   │   │   ├── KNN Based Imputation
│   │   │   │   ├── Native support for missing values for gradient boosting
│   │   │   │   ├── New plotting API
│   │   │   │   ├── Permutation-based feature importance
│   │   │   │   ├── Precomputed sparse nearest neighbors graph
│   │   │   │   ├── ROC AUC now supports multiclass classification
│   │   │   │   ├── Retrieve dataframes from OpenML
│   │   │   │   ├── Stacking Classifier and Regressor
│   │   │   │   └── Tree pruning
│   │   │   ├── Release Highlights for scikit-learn 0.23
│   │   │   │   ├── Generalized Linear Models, and Poisson loss for gradient boosting
│   │   │   │   ├── Improvements to the histogram-based Gradient Boosting estimators
│   │   │   │   ├── Rich visual representation of estimators
│   │   │   │   ├── Sample-weight support for Lasso and ElasticNet
│   │   │   │   └── Scalability and stability improvements to KMeans
│   │   │   ├── Release Highlights for scikit-learn 0.24
│   │   │   │   ├── Improved performances of HistGradientBoosting estimators
│   │   │   │   ├── Individual Conditional Expectation plots
│   │   │   │   ├── Native support for categorical features in HistGradientBoosting estimators
│   │   │   │   ├── New Poisson splitting criterion for DecisionTreeRegressor
│   │   │   │   ├── New PolynomialCountSketch kernel approximation function
│   │   │   │   ├── New SequentialFeatureSelector transformer
│   │   │   │   ├── New documentation improvements
│   │   │   │   ├── New self-training meta-estimator
│   │   │   │   └── Successive Halving estimators for tuning hyper-parameters
│   │   │   ├── Release Highlights for scikit-learn 1.0
│   │   │   │   ├── A more flexible plotting API
│   │   │   │   ├── Feature Names Support
│   │   │   │   ├── Histogram-based Gradient Boosting Models are now stable
│   │   │   │   ├── Keyword and positional arguments
│   │   │   │   ├── New documentation improvements
│   │   │   │   ├── Online One-Class SVM
│   │   │   │   ├── Quantile Regressor
│   │   │   │   └── Spline Transformers
│   │   │   ├── Release Highlights for scikit-learn 1.1
│   │   │   │   ├── BisectingKMeans: divide and cluster
│   │   │   │   ├── Grouping infrequent categories in OneHotEncoder
│   │   │   │   ├── MiniBatchNMF: an online version of NMF
│   │   │   │   ├── Performance improvements
│   │   │   │   ├── Quantile loss in
│   │   │   │   └── get_feature_names_out Available in all Transformers
│   │   │   └── Release Highlights for scikit-learn 1.2
│   │   │       ├── Experimental Array API support in
│   │   │       ├── Faster parser in
│   │   │       ├── Improved efficiency of many estimators
│   │   │       ├── Interaction constraints in Histogram-based Gradient Boosting Trees
│   │   │       ├── New and enhanced displays
│   │   │       └── Pandas output with set_output API
│   │   ├── Semi Supervised Classification
│   │   │   ├── Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset
│   │   │   ├── Effect of varying threshold for self-training
│   │   │   ├── Label Propagation digits active learning
│   │   │   ├── Label Propagation digits: Demonstrating performance
│   │   │   │   ├── Data generation
│   │   │   │   ├── Plot the most uncertain predictions
│   │   │   │   └── Semi-supervised learning
│   │   │   ├── Label Propagation learning a complex structure
│   │   │   └── Semi-supervised Classification on a Text Dataset
│   │   ├── Support Vector Machines
│   │   │   ├── Non-linear SVM
│   │   │   ├── One-class SVM with non-linear kernel (RBF)
│   │   │   ├── Plot different SVM classifiers in the iris dataset
│   │   │   ├── Plot the support vectors in LinearSVC
│   │   │   ├── RBF SVM parameters
│   │   │   │   ├── Load and prepare data set
│   │   │   │   ├── Train classifiers
│   │   │   │   └── Visualization
│   │   │   ├── SVM Margins Example
│   │   │   ├── SVM Tie Breaking Example
│   │   │   ├── SVM with custom kernel
│   │   │   ├── SVM-Anova: SVM with univariate feature selection
│   │   │   │   ├── Create the pipeline
│   │   │   │   ├── Load some data to play with
│   │   │   │   └── Plot the cross-validation score as a function of percentile of features
│   │   │   ├── SVM-Kernels
│   │   │   ├── SVM: Maximum margin separating hyperplane
│   │   │   ├── SVM: Separating hyperplane for unbalanced classes
│   │   │   ├── SVM: Weighted samples
│   │   │   ├── Scaling the regularization parameter for SVCs
│   │   │   │   ├── L1-penalty case
│   │   │   │   └── L2-penalty case
│   │   │   └── Support Vector Regression (SVR) using linear and non-linear kernels
│   │   │       ├── Fit regression model
│   │   │       ├── Generate sample data
│   │   │       └── Look at the results
│   │   ├── Tutorial exercises
│   │   │   ├── Cross-validation on Digits Dataset Exercise
│   │   │   ├── Cross-validation on diabetes Dataset Exercise
│   │   │   │   ├── Bonus: how much can you trust the selection of alpha?
│   │   │   │   ├── Load dataset and apply GridSearchCV
│   │   │   │   └── Plot error lines showing +/- std. errors of the scores
│   │   │   ├── Digits Classification Exercise
│   │   │   └── SVM Exercise
│   │   └── Working with text documents
│   │       ├── Classification of text documents using sparse features
│   │       │   ├── Analysis of a bag-of-words document classifier
│   │       │   │   ├── Model with metadata stripping
│   │       │   │   └── Model without metadata stripping
│   │       │   ├── Benchmarking classifiers
│   │       │   ├── Loading and vectorizing the 20 newsgroups text dataset
│   │       │   └── Plot accuracy, training and test time of each classifier
│   │       ├── Clustering text documents using k-means
│   │       │   ├── Clustering evaluation summary
│   │       │   ├── K-means clustering on text features
│   │       │   │   ├── Clustering sparse data with k-means
│   │       │   │   ├── Feature Extraction using TfidfVectorizer
│   │       │   │   ├── HashingVectorizer
│   │       │   │   ├── Performing dimensionality reduction using LSA
│   │       │   │   └── Top terms per cluster
│   │       │   ├── Loading text data
│   │       │   └── Quantifying the quality of clustering results
│   │       └── FeatureHasher and DictVectorizer Comparison
│   │           ├── Comparison with special purpose text vectorizers
│   │           ├── Define preprocessing functions
│   │           ├── DictVectorizer
│   │           ├── FeatureHasher
│   │           ├── Load Data
│   │           ├── Summary
│   │           └── TfidfVectorizer
│   └── Tutorials
│       ├── A tutorial on statistical-learning for scientific data processing
│       ├── An introduction to machine learning with scikit-learn
│       │   ├── Conventions
│       │   │   ├── Multiclass vs. multilabel fitting
│       │   │   ├── Refitting and updating parameters
│       │   │   └── Type casting
│       │   ├── Learning and predicting
│       │   ├── Loading an example dataset
│       │   └── Machine learning: the problem setting
│       ├── Choosing the right estimator
│       ├── External Resources, Videos and Talks
│       │   ├── External Tutorials
│       │   ├── New to Scientific Python?
│       │   └── Videos
│       └── Working With Text Data
│           ├── Building a pipeline
│           ├── Evaluation of the performance on the test set
│           ├── Exercise 1: Language identification
│           ├── Exercise 2: Sentiment Analysis on movie reviews
│           ├── Exercise 3: CLI text classification utility
│           ├── Extracting features from text files
│           │   ├── Bags of words
│           │   ├── From occurrences to frequencies
│           │   └── Tokenizing text with scikit-learn
│           ├── Loading the 20 newsgroups dataset
│           ├── Parameter tuning using grid search
│           │   └── Exercises
│           ├── Training a classifier
│           ├── Tutorial setup
│           └── Where to from here
└── torch
    ├── Audio
    │   ├── Audio Data Augmentation
    │   ├── Audio Datasets
    │   ├── Audio Feature Augmentation
    │   ├── Audio Feature Extractions
    │   ├── Audio I/O
    │   ├── Audio Resampling
    │   ├── Forced Alignment with Wav2Vec2
    │   ├── Speech Recognition with Wav2Vec2
    │   └── Text-to-speech with Tacotron2
    ├── Code Transforms with FX
    │   ├── (beta) Building a Convolution/Batch Norm fuser in FX
    │   │   ├── Benchmarking our Fusion on ResNet18
    │   │   ├── FX Fusion Pass
    │   │   ├── Fusing Convolution with Batch Norm
    │   │   └── Testing out our Fusion Pass
    │   └── (beta) Building a Simple CPU Performance Profiler with FX
    │       ├── Capturing the Model with Symbolic Tracing
    │       ├── Conclusion
    │       ├── Creating a Profiling Interpreter
    │       └── Investigating the Performance of ResNet18
    ├── Deploying PyTorch Models in Production
    │   ├── (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime
    │   │   └── Running the model on an image using ONNX Runtime
    │   ├── Deploying PyTorch in Python via a REST API with Flask
    │   │   ├── API Definition
    │   │   ├── Dependencies
    │   │   ├── Inference
    │   │   │   ├── Prediction
    │   │   │   └── Preparing the image
    │   │   ├── Integrating the model in our API Server
    │   │   ├── Next steps
    │   │   └── Simple Web Server
    │   ├── Introduction to TorchScript
    │   │   ├── Basics of PyTorch Model Authoring
    │   │   ├── Basics of TorchScript
    │   │   │   └── Tracing Modules
    │   │   ├── Saving and Loading models
    │   │   │   └── Further Reading
    │   │   └── Using Scripting to Convert Modules
    │   │       └── Mixing Scripting and Tracing
    │   ├── Loading a TorchScript Model in C++
    │   │   ├── Step 1: Converting Your PyTorch Model to Torch Script
    │   │   │   ├── Converting to Torch Script via Annotation
    │   │   │   └── Converting to Torch Script via Tracing
    │   │   ├── Step 2: Serializing Your Script Module to a File
    │   │   ├── Step 3: Loading Your Script Module in C++
    │   │   │   ├── A Minimal C++ Application
    │   │   │   └── Depending on LibTorch and Building the Application
    │   │   ├── Step 4: Executing the Script Module in C++
    │   │   └── Step 5: Getting Help and Exploring the API
    │   └── Real Time Inference on Raspberry Pi 4 (30 fps!)
    │       ├── Image Preprocessing
    │       ├── Installing PyTorch and OpenCV
    │       ├── MobileNetV2: Quantization and JIT
    │       ├── Model Choices
    │       ├── Next Steps
    │       ├── Prerequisites
    │       ├── Putting It Together
    │       ├── Raspberry Pi 4 Setup
    │       ├── Troubleshooting: Performance
    │       └── Video Capture
    ├── Extending PyTorch
    │   ├── Custom C++ and CUDA Extensions
    │   │   ├── Conclusion
    │   │   ├── Motivation and Example
    │   │   ├── Writing a C++ Extension
    │   │   │   ├── Binding to Python
    │   │   │   ├── Building with setuptools
    │   │   │   ├── JIT Compiling Extensions
    │   │   │   ├── Using Your Extension
    │   │   │   │   ├── Performance Comparison
    │   │   │   │   └── Performance on GPU Devices
    │   │   │   └── Writing the C++ Op
    │   │   │       ├── Backward Pass
    │   │   │       └── Forward Pass
    │   │   └── Writing a Mixed C++/CUDA extension
    │   │       ├── Integrating a C++/CUDA Operation with PyTorch
    │   │       │   └── Performance Comparison
    │   │       └── Using accessors
    │   ├── Double Backward with Custom Functions
    │   │   ├── Saving Intermediate Results
    │   │   ├── Saving Intermediate Results: What not to do
    │   │   ├── Saving the Inputs
    │   │   ├── Saving the Outputs
    │   │   └── When Backward is not Tracked
    │   ├── Extending TorchScript with Custom C++ Operators
    │   │   ├── Appendix A: More Ways of Building Custom Operators
    │   │   │   ├── Building with JIT compilation
    │   │   │   └── Building with Setuptools
    │   │   ├── Building the Custom Operator
    │   │   │   ├── Building with CMake
    │   │   │   └── Environment setup
    │   │   ├── Conclusion
    │   │   ├── Implementing the Custom Operator in C++
    │   │   ├── Registering the Custom Operator with TorchScript
    │   │   ├── Using the TorchScript Custom Operator in C++
    │   │   └── Using the TorchScript Custom Operator in Python
    │   │       ├── Using the Custom Operator with Script
    │   │       └── Using the Custom Operator with Tracing
    │   ├── Extending dispatcher for a new backend in C++
    │   │   ├── Autograd support for the new backend
    │   │   ├── Backward Compatibility
    │   │   ├── Build an extension
    │   │   ├── Custom operator support
    │   │   ├── Future Work
    │   │   ├── Get a dispatch key for your backend
    │   │   ├── Get the full list of PyTorch operators
    │   │   ├── JIT support
    │   │   ├── Known issues & additional notes
    │   │   ├── Register kernels for the new backend
    │   │   ├── Stay in touch
    │   │   ├── Testing your backend against native PyTorch backends
    │   │   └── What’s a new backend?
    │   ├── Fusing Convolution and Batch Norm using Custom Function
    │   │   ├── A Comparison of Memory Usage
    │   │   ├── Backward Formula Implementation for Batch Norm
    │   │   ├── Backward Formula Implementation for Convolution
    │   │   ├── Fusing Convolution and BatchNorm
    │   │   └── Testing out our new Layer
    │   └── Registering a Dispatched Operator in C++
    │       ├── Adding autograd support
    │       ├── Defining schema and backend implementations
    │       ├── For operators that do not need autograd
    │       │   └── In-place or view ops
    │       └── Going beyond autograd
    │           ├── Autocast
    │           ├── Batched
    │           └── Tracer
    ├── Frontend APIs
    │   ├── (beta) Channels Last Memory Format in PyTorch
    │   │   ├── Converting existing models
    │   │   ├── Memory Format API
    │   │   ├── Performance Gains
    │   │   ├── What is Channels Last
    │   │   └── Work to do
    │   ├── Autograd in C++ Frontend
    │   │   ├── Basic autograd operations
    │   │   ├── Computing higher-order gradients in C++
    │   │   ├── Conclusion
    │   │   ├── Translating autograd code from Python to C++
    │   │   └── Using custom autograd function in C++
    │   ├── Dynamic Parallelism in TorchScript
    │   │   ├── Applied Example: Ensemble of Bidirectional LSTMs
    │   │   ├── Aside: Visualizing Parallelism
    │   │   ├── Basic Syntax
    │   │   ├── Conclusion
    │   │   ├── Parallelizing Forward and Backward Layers
    │   │   └── Parallelizing Models in the Ensemble
    │   ├── Forward-mode Automatic Differentiation (Beta)
    │   │   ├── Basic Usage
    │   │   ├── Custom autograd Function
    │   │   ├── Functional API (beta)
    │   │   ├── Usage with Modules
    │   │   ├── Using the functional API with Modules
    │   │   └── Using the functional Module API (beta)
    │   ├── Jacobians, Hessians, hvp, vhp, and more: composing function transforms
    │   │   ├── Batch Jacobian and Batch Hessian
    │   │   ├── Computing Hessian-vector products
    │   │   ├── Computing the Jacobian
    │   │   ├── Hessian computation with functorch.hessian
    │   │   └── reverse-mode Jacobian (jacrev) vs forward-mode Jacobian (jacfwd)
    │   ├── Model ensembling
    │   │   ├── Performance
    │   │   ├── Using vmap to vectorize the ensemble
    │   │   └── What is model ensembling?
    │   ├── Per-sample-gradients
    │   │   ├── Per-sample-grads, the efficient way, using function transforms
    │   │   ├── Performance comparison
    │   │   └── What is it?
    │   └── Using the PyTorch C++ Frontend
    │       ├── Checkpointing and Recovering the Training State
    │       ├── Conclusion
    │       ├── Defining the Neural Network Models
    │       │   ├── Defining the DCGAN Modules
    │       │   │   ├── The Discriminator Module
    │       │   │   ├── The Generator Module
    │       │   │   └── What was a GAN aGAN?
    │       │   └── Module API Basics
    │       │       ├── Defining a Module and Registering Parameters
    │       │       ├── Module Ownership
    │       │       ├── Registering Submodules and Traversing the Module Hierarchy
    │       │       └── Running the Network in Forward Mode
    │       ├── Inspecting Generated Images
    │       ├── Loading Data
    │       ├── Motivation
    │       ├── Moving to the GPU
    │       ├── Writing a Basic Application
    │       └── Writing the Training Loop
    ├── Image and Video
    │   ├── Adversarial Example Generation
    │   │   ├── Fast Gradient Sign Attack
    │   │   ├── Implementation
    │   │   │   ├── FGSM Attack
    │   │   │   ├── Inputs
    │   │   │   ├── Model Under Attack
    │   │   │   ├── Run Attack
    │   │   │   └── Testing Function
    │   │   ├── Results
    │   │   │   ├── Accuracy vs Epsilon
    │   │   │   └── Sample Adversarial Examples
    │   │   ├── Threat Model
    │   │   └── Where to go next?
    │   ├── Optimizing Vision Transformer Model for Deployment
    │   │   ├── Classifying Images with DeiT
    │   │   ├── Comparing Inference Speed
    │   │   │   └── Learn More
    │   │   ├── Optimizing DeiT
    │   │   ├── Quantizing DeiT
    │   │   ├── Scripting DeiT
    │   │   ├── Using Lite Interpreter
    │   │   └── What is DeiT
    │   ├── Spatial Transformer Networks Tutorial
    │   │   ├── Depicting spatial transformer networks
    │   │   ├── Loading the data
    │   │   ├── Training the model
    │   │   └── Visualizing the STN results
    │   ├── TorchVision Object Detection Finetuning Tutorial
    │   │   ├── Defining the Dataset
    │   │   │   └── Writing a custom dataset for PennFudan
    │   │   ├── Defining your model
    │   │   │   ├── 1 - Finetuning from a pretrained model
    │   │   │   ├── 2 - Modifying the model to add a different backbone
    │   │   │   └── An Instance segmentation model for PennFudan Dataset
    │   │   ├── Putting everything together
    │   │   ├── Testing forward() method (Optional)
    │   │   └── Wrapping up
    │   └── Transfer Learning for Computer Vision Tutorial
    │       ├── ConvNet as fixed feature extractor
    │       │   └── Train and evaluate
    │       ├── Finetuning the convnet
    │       │   └── Train and evaluate
    │       ├── Further Learning
    │       ├── Load Data
    │       │   └── Visualize a few images
    │       └── Training the model
    │           └── Visualizing the model predictions
    ├── Introduction to PyTorch
    │   ├── Automatic Differentiation with torch.autograd
    │   │   ├── Computing Gradients
    │   │   ├── Disabling Gradient Tracking
    │   │   ├── More on Computational Graphs
    │   │   ├── Optional Reading: Tensor Gradients and Jacobian Products
    │   │   │   └── Further Reading
    │   │   └── Tensors, Functions and Computational graph
    │   ├── Build the Neural Network
    │   │   ├── Define the Class
    │   │   ├── Further Reading
    │   │   ├── Get Device for Training
    │   │   ├── Model Layers
    │   │   │   ├── nn.Flatten
    │   │   │   ├── nn.Linear
    │   │   │   ├── nn.ReLU
    │   │   │   ├── nn.Sequential
    │   │   │   └── nn.Softmax
    │   │   └── Model Parameters
    │   ├── Datasets & DataLoaders
    │   │   ├── Creating a Custom Dataset for your files
    │   │   │   ├── __getitem__
    │   │   │   ├── __init__
    │   │   │   └── __len__
    │   │   ├── Further Reading
    │   │   ├── Iterate through the DataLoader
    │   │   ├── Iterating and Visualizing the Dataset
    │   │   ├── Loading a Dataset
    │   │   └── Preparing your data for training with DataLoaders
    │   ├── Learn the Basics
    │   │   ├── How to Use this Guide
    │   │   └── Running the Tutorial Code
    │   ├── Optimizing Model Parameters
    │   │   ├── Full Implementation
    │   │   ├── Further Reading
    │   │   ├── Hyperparameters
    │   │   ├── Optimization Loop
    │   │   │   ├── Loss Function
    │   │   │   └── Optimizer
    │   │   └── Prerequisite Code
    │   ├── Quickstart
    │   │   ├── Creating Models
    │   │   ├── Loading Models
    │   │   ├── Optimizing the Model Parameters
    │   │   ├── Saving Models
    │   │   └── Working with data
    │   ├── Save and Load the Model
    │   │   ├── Related Tutorials
    │   │   ├── Saving and Loading Model Weights
    │   │   └── Saving and Loading Models with Shapes
    │   ├── Tensors
    │   │   ├── Attributes of a Tensor
    │   │   ├── Bridge with NumPy
    │   │   │   ├── NumPy array to Tensor
    │   │   │   └── Tensor to NumPy array
    │   │   ├── Initializing a Tensor
    │   │   └── Operations on Tensors
    │   └── Transforms
    │       ├── Lambda Transforms
    │       │   └── Further Reading
    │       └── ToTensor()
    ├── Learning PyTorch
    │   ├── Deep Learning with PyTorch: A 60 Minute Blitz
    │   │   ├── Goal of this tutorial:
    │   │   └── What is PyTorch?
    │   ├── Learning PyTorch with Examples
    │   │   └── 
    │   │       └── 
    │   ├── Visualizing Models, Data, and Training with TensorBoard
    │   │   ├── 1. TensorBoard setup
    │   │   ├── 2. Writing to TensorBoard
    │   │   ├── 3. Inspect the model using TensorBoard
    │   │   ├── 4. Adding a “Projector” to TensorBoard
    │   │   ├── 5. Tracking model training with TensorBoard
    │   │   └── 6. Assessing trained models with TensorBoard
    │   └── What is torch.nn really?
    │       ├── Add validation
    │       ├── Closing thoughts
    │       ├── Create fit() and get_data()
    │       ├── MNIST data setup
    │       ├── Neural net from scratch (no torch.nn)
    │       ├── Refactor using DataLoader
    │       ├── Refactor using Dataset
    │       ├── Refactor using nn.Linear
    │       ├── Refactor using nn.Module
    │       ├── Refactor using optim
    │       ├── Switch to CNN
    │       ├── Using torch.nn.functional
    │       ├── Using your GPU
    │       ├── Wrapping DataLoader
    │       └── nn.Sequential
    ├── Mobile
    │   ├── Image Segmentation DeepLabV3 on Android
    │   │   ├── Introduction
    │   │   ├── Learn More
    │   │   ├── Learning Objectives
    │   │   ├── Pre-requisites
    │   │   ├── Recap
    │   │   └── Steps
    │   │       ├── 1. Convert the DeepLabV3 model for Android deployment
    │   │       ├── 2. Get example input and output of the model in Python
    │   │       ├── 3. Build a new Android app or reuse an example app and load the model
    │   │       ├── 4. Process the model input and output for model inference
    │   │       └── 5. Complete the UI, refactor, build and run the app
    │   └── Image Segmentation DeepLabV3 on iOS
    │       ├── Introduction
    │       ├── Learn More
    │       ├── Learning Objectives
    │       ├── Pre-requisites
    │       ├── Recap
    │       └── Steps
    │           ├── 1. Convert the DeepLabV3 model for iOS deployment
    │           ├── 2. Get example input and output of the model in Python
    │           ├── 3. Build a new iOS app or reuse an example app and load the model
    │           ├── 4. Process the model input and output for model inference
    │           └── 5. Complete the UI, refactor, build and run the app
    ├── Model Optimization
    │   ├── (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
    │   │   ├── Causal Self Attention
    │   │   │   └── NestedTensor and Dense tensor support
    │   │   ├── Explicit Dispatcher Control
    │   │   ├── Fused implementations
    │   │   ├── Hardware dependence
    │   │   ├── Overview
    │   │   └── Summary
    │   ├── (beta) Dynamic Quantization on BERT
    │   │   ├── 1. Setup
    │   │   │   ├── 1.1 Install PyTorch and HuggingFace Transformers
    │   │   │   ├── 1.2 Import the necessary modules
    │   │   │   ├── 1.3 Learn about helper functions
    │   │   │   └── 1.4 Download the dataset
    │   │   ├── 2. Fine-tune the BERT model
    │   │   │   ├── 2.1 Set global configurations
    │   │   │   ├── 2.2 Load the fine-tuned BERT model
    │   │   │   └── 2.3 Define the tokenize and evaluation function
    │   │   ├── 3. Apply the dynamic quantization
    │   │   │   ├── 3.1 Check the model size
    │   │   │   ├── 3.2 Evaluate the inference accuracy and time
    │   │   │   └── 3.3 Serialize the quantized model
    │   │   ├── Conclusion
    │   │   ├── Introduction
    │   │   └── References
    │   ├── (beta) Dynamic Quantization on an LSTM Word Language Model
    │   │   ├── 1. Define the model
    │   │   ├── 2. Load in the text data
    │   │   ├── 3. Load the pre-trained model
    │   │   ├── 4. Test dynamic quantization
    │   │   ├── Conclusion
    │   │   └── Introduction
    │   ├── (beta) Quantized Transfer Learning for Computer Vision Tutorial
    │   │   ├── Part 0. Prerequisites
    │   │   │   ├── Installing the Nightly Build
    │   │   │   ├── Load Data
    │   │   │   ├── Support Function for Model Training
    │   │   │   ├── Support Function for Visualizing the Model Predictions
    │   │   │   └── Visualize a few images
    │   │   ├── Part 1. Training a Custom Classifier based on a Quantized Feature Extractor
    │   │   │   └── Train and evaluate
    │   │   └── Part 2. Finetuning the Quantizable Model
    │   │       └── Finetuning the model
    │   ├── (beta) Static Quantization with Eager Mode in PyTorch
    │   │   ├── 1. Model architecture
    │   │   ├── 2. Helper functions
    │   │   ├── 3. Define dataset and data loaders
    │   │   │   └── ImageNet Data
    │   │   ├── 4. Post-training static quantization
    │   │   ├── 5. Quantization-aware training
    │   │   │   └── Speedup from quantization
    │   │   └── Conclusion
    │   ├── Conclusion
    │   ├── Getting Started - Accelerate Your Scripts with nvFuser
    │   │   ├── Defining novel operations with nvFuser and FuncTorch
    │   │   ├── Importing Packages and Selecting a Device
    │   │   ├── Introduction
    │   │   ├── Setup and Performance Metrics
    │   │   ├── The Transformer Block
    │   │   ├── TorchScript & nvFuser
    │   │   ├── Transformer Block With a Novel Normalization
    │   │   └── nvFuser & Dynamic Shapes
    │   ├── Grokking PyTorch Intel CPU performance from first principles
    │   │   ├── 1. Default TorchServe setting (no core pinning)
    │   │   ├── 2. torch.set_num_threads = number of physical cores / number of workers (no core pinning)
    │   │   ├── 3. launcher core pinning
    │   │   ├── Acknowledgement
    │   │   ├── Avoid logical cores for deep learning
    │   │   ├── Conclusion
    │   │   ├── Efficient CPU usage with core pinning for multi-worker inference
    │   │   └── Local memory access is always faster than remote memory access
    │   ├── Grokking PyTorch Intel CPU performance from first principles (Part 2)
    │   │   ├── Acknowledgement
    │   │   ├── Conclusion
    │   │   ├── Prerequisites
    │   │   │   ├── Intel® VTune™ Profiler’s Instrumentation and Tracing Technology (ITT)
    │   │   │   └── Top-down Microarchitecture Analysis Method (TMA)
    │   │   │       └── Tune for the Back End Bound
    │   │   ├── Related Readings
    │   │   └── TorchServe with Intel® Extension for PyTorch*
    │   │       ├── Exercise with TorchServe
    │   │       ├── Intel® Extension for PyTorch*
    │   │       │   ├── Channels Last Memory Format
    │   │       │   │   └── Exercise
    │   │       │   ├── Graph Optimization
    │   │       │   │   └── Exercise
    │   │       │   └── Operator Optimization
    │   │       │       └── Exercise
    │   │       ├── Leveraging Advanced Launcher Configuration: Memory Allocator
    │   │       │   └── TCMalloc, JeMalloc, PTMalloc
    │   │       │       ├── Exercise
    │   │       │       └── Exercise with TorchServe
    │   │       └── Performance Boost with Intel® Extension for PyTorch*
    │   ├── Hyperparameter tuning with Ray Tune
    │   │   ├── Configurable neural network
    │   │   ├── Configuring the search space
    │   │   ├── Data loaders
    │   │   ├── Setup / Imports
    │   │   ├── Test set accuracy
    │   │   └── The train function
    │   │       ├── Adding (multi) GPU support with DataParallel
    │   │       ├── Communicating with Ray Tune
    │   │       └── Full training function
    │   ├── Multi-Objective NAS with Ax
    │   │   ├── Acknowledgements
    │   │   ├── Choosing the GenerationStrategy
    │   │   ├── Configuring the Scheduler
    │   │   ├── Creating the Ax Experiment
    │   │   ├── Defining the TorchX App
    │   │   ├── Evaluating the results
    │   │   ├── Running the optimization
    │   │   ├── Setting up Metrics
    │   │   ├── Setting up the OptimizationConfig
    │   │   ├── Setting up the Runner
    │   │   └── Setting up the SearchSpace
    │   ├── Optimizing Vision Transformer Model for Deployment
    │   │   ├── Classifying Images with DeiT
    │   │   ├── Comparing Inference Speed
    │   │   │   └── Learn More
    │   │   ├── Optimizing DeiT
    │   │   ├── Quantizing DeiT
    │   │   ├── Scripting DeiT
    │   │   ├── Using Lite Interpreter
    │   │   └── What is DeiT
    │   ├── Parametrizations Tutorial
    │   │   ├── Caching the value of a parametrization
    │   │   ├── Concatenating parametrizations
    │   │   ├── Implementing parametrizations by hand
    │   │   ├── Inspecting a parametrized module
    │   │   ├── Intializing parametrizations
    │   │   ├── Introduction to parametrizations
    │   │   ├── Parametrizations are first-class citizens
    │   │   └── Removing parametrizations
    │   ├── Profiling your PyTorch Module
    │   │   ├── Further Reading
    │   │   ├── Improve memory performance
    │   │   ├── Improve time performance
    │   │   ├── Performance debugging using Profiler
    │   │   ├── Print profiler results
    │   │   └── Profile the forward pass
    │   ├── Pruning Tutorial
    │   │   ├── Create a model
    │   │   ├── Extending torch.nn.utils.prune with custom pruning functions
    │   │   ├── Global pruning
    │   │   ├── Inspect a Module
    │   │   ├── Iterative Pruning
    │   │   ├── Pruning a Module
    │   │   ├── Pruning multiple parameters in a model
    │   │   ├── Remove pruning re-parametrization
    │   │   ├── Requirements
    │   │   └── Serializing a pruned model
    │   ├── PyTorch Profiler With TensorBoard
    │   │   ├── Introduction
    │   │   ├── Learn More
    │   │   ├── Setup
    │   │   └── Steps
    │   │       ├── 1. Prepare the data and model
    │   │       ├── 2. Use profiler to record execution events
    │   │       ├── 3. Run the profiler
    │   │       ├── 4. Use TensorBoard to view results and analyze model performance
    │   │       ├── 5. Improve performance with the help of profiler
    │   │       └── 6. Analyze performance with other advanced features
    │   ├── Using SDPA with torch.compile
    │   └── torch.compile Tutorial
    │       ├── Basic Usage
    │       ├── Comparison to TorchScript and FX Tracing
    │       ├── Conclusion
    │       ├── Demonstrating Speedups
    │       └── TorchDynamo and FX Graphs
    ├── Multimodality
    │   └── TorchMultimodal Tutorial: Finetuning FLAVA
    │       ├── Conclusion
    │       ├── Installation
    │       └── Steps
    ├── Parallel and Distributed Training
    │   ├── Advanced Model Training with Fully Sharded Data Parallel (FSDP)
    │   │   ├── Backward Prefetch
    │   │   ├── FSDP Features in This Tutorial
    │   │   ├── Fine-tuning HF T5
    │   │   ├── Intializing FSDP Model on Device
    │   │   ├── Mixed Precision
    │   │   ├── Model Checkpoint Saving, by streaming to the Rank0 CPU
    │   │   ├── Recap on How FSDP Works
    │   │   ├── Sharding Strategy
    │   │   ├── Summary
    │   │   └── Transformer Wrapping Policy
    │   ├── Combining Distributed DataParallel with Distributed RPC Framework
    │   ├── Customize Process Group Backends Using Cpp Extensions
    │   │   ├── Basics
    │   │   ├── Step 1: Implement a Subclass of ProcessGroup
    │   │   ├── Step 2: Expose The Extension Python APIs
    │   │   ├── Step 3: Build The Custom Extension
    │   │   └── Step 4: Use The Extension in Application
    │   ├── Distributed Pipeline Parallelism Using RPC
    │   │   ├── Basics
    │   │   ├── Step 1: Partition ResNet50 Model
    │   │   ├── Step 2: Stitch ResNet50 Model Shards Into One Module
    │   │   ├── Step 3: Define The Training Loop
    │   │   └── Step 4: Launch RPC Processes
    │   ├── Distributed Training with Uneven Inputs Using the Join Context Manager
    │   │   ├── How Does Join Work?
    │   │   │   ├── Join
    │   │   │   ├── JoinHook
    │   │   │   └── Joinable
    │   │   ├── Making a Toy Class Work with Join
    │   │   ├── Passing Keyword Arguments
    │   │   ├── Requirements
    │   │   ├── Using Join with DistributedDataParallel
    │   │   ├── Using Join with DistributedDataParallel and ZeroRedundancyOptimizer
    │   │   └── What is Join?
    │   ├── Distributed and Parallel Training Tutorials
    │   │   ├── Custom Extensions
    │   │   ├── Learn DDP
    │   │   ├── Learn FSDP
    │   │   └── Learn RPC
    │   ├── Getting Started with Distributed Data Parallel
    │   │   ├── Basic Use Case
    │   │   ├── Combining DDP with Model Parallelism
    │   │   ├── Comparison between DataParallel and DistributedDataParallel
    │   │   ├── Initialize DDP with torch.distributed.run/torchrun
    │   │   ├── Save and Load Checkpoints
    │   │   └── Skewed Processing Speeds
    │   ├── Getting Started with Distributed RPC Framework
    │   │   ├── Distributed RNN using Distributed Autograd and Distributed Optimizer
    │   │   └── Distributed Reinforcement Learning using RPC and RRef
    │   ├── Getting Started with Fully Sharded Data Parallel(FSDP)
    │   │   ├── How FSDP works
    │   │   └── How to use FSDP
    │   ├── Implementing Batch RPC Processing Using Asynchronous Executions
    │   │   ├── Basics
    │   │   ├── Batch-Processing CartPole Solver
    │   │   ├── Batch-Updating Parameter Server
    │   │   └── Learn More
    │   ├── Implementing a Parameter Server Using Distributed RPC Framework
    │   ├── PyTorch Distributed Overview
    │   │   ├── Data Parallel Training
    │   │   │   ├── torch.distributed.elastic
    │   │   │   ├── torch.nn.DataParallel
    │   │   │   └── torch.nn.parallel.DistributedDataParallel
    │   │   ├── Introduction
    │   │   ├── PyTorch Distributed Developers
    │   │   └── RPC-Based Distributed Training
    │   ├── Single-Machine Model Parallel Best Practices
    │   │   ├── Apply Model Parallel to Existing Modules
    │   │   ├── Basic Usage
    │   │   └── Speed Up by Pipelining Inputs
    │   ├── Training Transformer models using Distributed Data Parallel and Pipeline Parallelism
    │   │   ├── Define the model
    │   │   ├── Evaluate the model with the test dataset
    │   │   ├── Load and batch data
    │   │   │   └── Functions to generate input and target sequence
    │   │   ├── Model scale and Pipe initialization
    │   │   ├── Output
    │   │   ├── Run the model
    │   │   └── Start multiple processes for training
    │   ├── Training Transformer models using Pipeline Parallelism
    │   │   ├── Define the model
    │   │   ├── Evaluate the model with the test dataset
    │   │   ├── Load and batch data
    │   │   │   └── Functions to generate input and target sequence
    │   │   ├── Model scale and Pipe initialization
    │   │   └── Run the model
    │   └── Writing Distributed Applications with PyTorch
    │       ├── Advanced Topics
    │       │   ├── Communication Backends
    │       │   └── Initialization Methods
    │       ├── Collective Communication
    │       ├── Distributed Training
    │       │   └── Our Own Ring-Allreduce
    │       ├── Point-to-Point Communication
    │       └── Setup
    ├── PyTorch Recipes
    │   ├── Automatic Mixed Precision
    │   │   ├── A simple network
    │   │   ├── Adding GradScaler
    │   │   ├── Adding autocast
    │   │   ├── Advanced topics
    │   │   ├── All together: “Automatic Mixed Precision”
    │   │   ├── Default Precision
    │   │   ├── Inference/Evaluation
    │   │   ├── Inspecting/modifying gradients (e.g., clipping)
    │   │   ├── Saving/Resuming
    │   │   └── Troubleshooting
    │   │       ├── Loss is inf/NaN
    │   │       ├── Speedup with Amp is minor
    │   │       └── Type mismatch error (may manifest as CUDNN_STATUS_BAD_PARAM)
    │   ├── Changing default device
    │   ├── Defining a Neural Network in PyTorch
    │   │   ├── Introduction
    │   │   ├── Learn More
    │   │   ├── Setup
    │   │   └── Steps
    │   │       ├── 1. Import necessary libraries for loading our data
    │   │       ├── 2. Define and intialize the neural network
    │   │       ├── 3. Specify how data will pass through your model
    │   │       └── 4. [Optional] Pass data through your model to test
    │   ├── Developing Custom PyTorch Dataloaders
    │   │   ├── Part 1: The Dataset
    │   │   │   ├── 1.1 Write a simple helper function to show an image
    │   │   │   ├── 1.2 Create a dataset class
    │   │   │   └── 1.3 Iterate through data samples
    │   │   ├── Part 2: Data Tranformations
    │   │   │   ├── 2.1 Create callable classes
    │   │   │   ├── 2.2 Compose transforms and apply to a sample
    │   │   │   └── 2.3 Iterate through the dataset
    │   │   ├── Part 3: The Dataloader
    │   │   └── Setup
    │   ├── Dynamic Quantization
    │   │   ├── Introduction
    │   │   ├── Learn More
    │   │   ├── Steps
    │   │   │   ├── 1: Set Up
    │   │   │   ├── 2: Do the Quantization
    │   │   │   ├── 3. Look at Model Size
    │   │   │   ├── 4. Look at Latency
    │   │   │   └── 5: Look at Accuracy
    │   │   └── What is dynamic quantization?
    │   ├── How to use TensorBoard with PyTorch
    │   │   ├── Installation
    │   │   ├── Learn More
    │   │   ├── Log scalars
    │   │   ├── Run TensorBoard
    │   │   ├── Share TensorBoard dashboards
    │   │   └── Using TensorBoard in PyTorch
    │   ├── Loading data in PyTorch
    │   │   ├── 1. Import necessary libraries for loading our data
    │   │   ├── 2. Access the data in the dataset
    │   │   ├── 3. Loading the data
    │   │   ├── 4. Iterate over the data
    │   │   ├── 5. [Optional] Visualize the data
    │   │   ├── Introduction
    │   │   ├── Learn More
    │   │   ├── Setup
    │   │   └── Steps
    │   ├── Model Interpretability using Captum
    │   │   ├── Before you begin
    │   │   ├── Computing Attribution
    │   │   ├── Final Notes
    │   │   └── Visualizing the Results
    │   ├── Performance Tuning Guide
    │   │   ├── CPU specific optimizations
    │   │   │   ├── Intel OpenMP Runtime Library (libiomp)
    │   │   │   ├── Switch Memory allocator
    │   │   │   ├── Train a model on CPU with PyTorch DistributedDataParallel(DDP) functionality
    │   │   │   ├── Use oneDNN Graph with TorchScript for inference
    │   │   │   ├── Utilize Non-Uniform Memory Access (NUMA) Controls
    │   │   │   └── Utilize OpenMP
    │   │   ├── Distributed optimizations
    │   │   │   ├── Load-balance workload in a distributed setting
    │   │   │   ├── Match the order of layers in constructors and during the execution if using DistributedDataParallel(find_unused_parameters=True)
    │   │   │   ├── Skip unnecessary all-reduce if training with DistributedDataParallel and gradient accumulation
    │   │   │   └── Use efficient data-parallel backend
    │   │   ├── GPU specific optimizations
    │   │   │   ├── Avoid unnecessary CPU-GPU synchronization
    │   │   │   ├── Create tensors directly on the target device
    │   │   │   ├── Enable cuDNN auto-tuner
    │   │   │   ├── Pre-allocate memory in case of variable input length
    │   │   │   └── Use mixed precision and AMP
    │   │   └── General optimizations
    │   │       ├── Checkpoint intermediate buffers
    │   │       ├── Disable bias for convolutions directly followed by a batch norm
    │   │       ├── Disable debugging APIs
    │   │       ├── Disable gradient calculation for validation or inference
    │   │       ├── Enable async data loading and augmentation
    │   │       ├── Enable channels_last memory format for computer vision models
    │   │       ├── Fuse pointwise operations
    │   │       └── Use parameter.grad = None instead of model.zero_grad() or optimizer.zero_grad()
    │   ├── PyTorch Benchmark
    │   │   ├── Introduction
    │   │   ├── Learn More
    │   │   ├── Setup
    │   │   └── Steps
    │   │       ├── 1. Defining functions to benchmark
    │   │       ├── 2. Benchmarking with timeit.Timer
    │   │       ├── 3. Benchmarking with torch.utils.benchmark.Timer
    │   │       ├── 4. Benchmarking with Blocked Autorange
    │   │       ├── 5. Comparing benchmark results
    │   │       ├── 6. Saving/Loading benchmark results
    │   │       ├── 7. Generating inputs with Fuzzed Parameters
    │   │       └── 8. Collecting instruction counts with Callgrind
    │   ├── PyTorch Profiler
    │   │   ├── Introduction
    │   │   ├── Learn More
    │   │   ├── Setup
    │   │   └── Steps
    │   │       ├── 1. Import all necessary libraries
    │   │       ├── 2. Instantiate a simple Resnet model
    │   │       ├── 3. Using profiler to analyze execution time
    │   │       ├── 4. Using profiler to analyze memory consumption
    │   │       ├── 5. Using tracing functionality
    │   │       ├── 6. Examining stack traces
    │   │       ├── 7. Visualizing data as a flamegraph
    │   │       └── 8. Using profiler to analyze long-running jobs
    │   ├── Pytorch Mobile Performance Recipes
    │   │   ├── Benchmarking
    │   │   │   ├── Android - Benchmarking Setup
    │   │   │   └── iOS - Benchmarking Setup
    │   │   ├── Introduction
    │   │   └── Model preparation
    │   │       ├── 1. Fuse operators using torch.quantization.fuse_modules
    │   │       ├── 2. Quantize your model
    │   │       ├── 3. Use torch.utils.mobile_optimizer
    │   │       ├── 4. Prefer Using Channels Last Tensor memory format
    │   │       ├── 5. Android - Reusing tensors for forward
    │   │       └── Setup
    │   ├── Saving and loading a general checkpoint in PyTorch
    │   │   ├── Introduction
    │   │   ├── Learn More
    │   │   ├── Setup
    │   │   └── Steps
    │   │       ├── 1. Import necessary libraries for loading our data
    │   │       ├── 2. Define and initialize the neural network
    │   │       ├── 3. Initialize the optimizer
    │   │       ├── 4. Save the general checkpoint
    │   │       └── 5. Load the general checkpoint
    │   ├── Saving and loading models across devices in PyTorch
    │   │   ├── Introduction
    │   │   ├── Learn More
    │   │   ├── Setup
    │   │   └── Steps
    │   │       ├── 1. Import necessary libraries for loading our data
    │   │       ├── 2. Define and intialize the neural network
    │   │       ├── 3. Save on GPU, Load on CPU
    │   │       ├── 4. Save on GPU, Load on GPU
    │   │       ├── 5. Save on CPU, Load on GPU
    │   │       └── 6. Saving torch.nn.DataParallel Models
    │   ├── Saving and loading models for inference in PyTorch
    │   │   ├── Introduction
    │   │   ├── Learn More
    │   │   ├── Setup
    │   │   └── Steps
    │   │       ├── 1. Import necessary libraries for loading our data
    │   │       ├── 2. Define and intialize the neural network
    │   │       ├── 3. Initialize the optimizer
    │   │       ├── 4. Save and load the model via state_dict
    │   │       └── 5. Save and load entire model
    │   ├── Saving and loading multiple models in one file using PyTorch
    │   │   ├── Introduction
    │   │   ├── Learn More
    │   │   ├── Setup
    │   │   └── Steps
    │   │       ├── 1. Import necessary libraries for loading our data
    │   │       ├── 2. Define and initialize the neural network
    │   │       ├── 3. Initialize the optimizer
    │   │       ├── 4. Load multiple models
    │   │       └── 4. Save multiple models
    │   ├── Timer quick start
    │   │   ├── 1. Defining a Timer
    │   │   ├── 2. Wall time: Timer.blocked_autorange(…)
    │   │   ├── 3. C++ snippets
    │   │   ├── 4. Instruction counts: Timer.collect_callgrind(…)
    │   │   ├── 5. Instruction counts: Delving deeper
    │   │   ├── 6. A/B testing with Callgrind
    │   │   ├── 7. Wrapping up
    │   │   └── 8. Footnotes
    │   ├── Warmstarting model using parameters from a different model in PyTorch
    │   │   ├── Introduction
    │   │   ├── Learn More
    │   │   ├── Setup
    │   │   └── Steps
    │   │       ├── 1. Import necessary libraries for loading our data
    │   │       ├── 2. Define and intialize the neural network A and B
    │   │       ├── 3. Save model A
    │   │       └── 4. Load into model B
    │   ├── What is a state_dict in PyTorch
    │   │   ├── Introduction
    │   │   ├── Learn More
    │   │   ├── Setup
    │   │   └── Steps
    │   │       ├── 1. Import necessary libraries for loading our data
    │   │       ├── 2. Define and intialize the neural network
    │   │       ├── 3. Initialize the optimizer
    │   │       └── 4. Access the model and optimizer state_dict
    │   └── Zeroing out gradients in PyTorch
    │       ├── Introduction
    │       ├── Learn More
    │       ├── Setup
    │       └── Steps
    │           ├── 1. Import necessary libraries for loading our data
    │           ├── 2. Load and normalize the dataset
    │           ├── 3. Build the neural network
    │           ├── 4. Define a Loss function and optimizer
    │           └── 5. Zero the gradients while training the network
    ├── Recommendation Systems
    │   ├── Exploring TorchRec sharding
    │   │   ├── Constructing our embedding model
    │   │   ├── Distributed Setup
    │   │   ├── DistributedModelParallel in multiprocessing
    │   │   │   ├── Explore other sharding modes
    │   │   │   ├── Multiprocessing Execution
    │   │   │   └── Table Wise Sharding
    │   │   └── Installation
    │   └── Introduction to TorchRec
    │       ├── Installation
    │       ├── More resources
    │       └── Overview
    │           ├── Distributed Setup
    │           ├── DistributedModelParallel
    │           ├── From EmbeddingBag to EmbeddingBagCollection
    │           ├── Putting it all together, querying our distributed model with a KJT minibatch
    │           ├── Query vanilla nn.EmbeddingBag with input and offsets
    │           └── Representing minibatches with KeyedJaggedTensor
    ├── Reinforcement Learning
    │   ├── Reinforcement Learning (DQN) Tutorial
    │   │   ├── DQN algorithm
    │   │   │   └── Q-network
    │   │   ├── Replay Memory
    │   │   └── Training
    │   │       ├── Hyperparameters and utilities
    │   │       └── Training loop
    │   ├── Reinforcement Learning (PPO) with TorchRL Tutorial
    │   │   ├── Conclusion and next steps
    │   │   ├── Data collector
    │   │   ├── Define Hyperparameters
    │   │   │   ├── Data collection parameters
    │   │   │   └── PPO parameters
    │   │   ├── Define an environment
    │   │   │   ├── Normalization
    │   │   │   └── Transforms
    │   │   ├── Loss function
    │   │   ├── Policy
    │   │   ├── Replay buffer
    │   │   ├── Results
    │   │   ├── Training loop
    │   │   └── Value network
    │   └── Train a Mario-playing RL Agent
    │       ├── Agent
    │       │   ├── Act
    │       │   ├── Cache and Recall
    │       │   ├── Learn
    │       │   │   ├── Neural Network
    │       │   │   ├── Putting it all together
    │       │   │   ├── Save checkpoint
    │       │   │   ├── TD Estimate & TD Target
    │       │   │   └── Updating the model
    │       │   └── Logging
    │       ├── Conclusion
    │       ├── Environment
    │       │   ├── Initialize Environment
    │       │   └── Preprocess Environment
    │       ├── Let’s play!
    │       └── RL Definitions
    └── Text
        ├── Fast Transformer Inference with Better Transformer
        │   ├── Additional Information
        │   ├── Better Transformer Features in This Tutorial
        │   └── Summary
        ├── Language Modeling with nn.Transformer and TorchText
        │   ├── Define the model
        │   ├── Evaluate the best model on the test dataset
        │   ├── Initiate an instance
        │   ├── Load and batch data
        │   │   └── Functions to generate input and target sequence
        │   └── Run the model
        ├── Language Translation with nn.Transformer and torchtext
        │   ├── Collation
        │   ├── Data Sourcing and Processing
        │   ├── References
        │   └── Seq2Seq Network using Transformer
        ├── NLP From Scratch: Classifying Names with a Character-Level RNN
        │   ├── Creating the Network
        │   ├── Evaluating the Results
        │   │   └── Running on User Input
        │   ├── Exercises
        │   ├── Preparing the Data
        │   │   └── Turning Names into Tensors
        │   └── Training
        │       ├── Plotting the Results
        │       ├── Preparing for Training
        │       └── Training the Network
        ├── NLP From Scratch: Generating Names with a Character-Level RNN
        │   ├── Creating the Network
        │   ├── Exercises
        │   ├── Preparing the Data
        │   ├── Sampling the Network
        │   └── Training
        │       ├── Plotting the Losses
        │       ├── Preparing for Training
        │       └── Training the Network
        └── Text classification with the torchtext library
            ├── Access to the raw dataset iterators
            ├── Define functions to train the model and evaluate results.
            ├── Define the model
            ├── Evaluate the model with test dataset
            ├── Generate data batch and iterator
            ├── Initiate an instance
            ├── Prepare data processing pipelines
            ├── Split the dataset and run the model
            └── Test on a random news
