{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a61b241",
   "metadata": {},
   "source": [
    "<div style=\"    font-variant: small-caps;\n",
    "    font-weight: normal;\n",
    "    font-size: 30px;\n",
    "    text-align: center;\n",
    "    padding: 15px;\n",
    "margin: 10px;\">Reparametrization Trick for Batch Normalization</div>\n",
    "<div style=\"    font-variant: small-caps;\n",
    "    font-weight: normal;\n",
    "    font-size: 20px;\n",
    "    text-align: center;\n",
    "    padding: 15px;\">Proof of unaltered Accuracy</div>\n",
    "<div style=\"  float:right;\n",
    "    font-size: 12px;\n",
    "    line-height: 12px;\n",
    "padding: 10px 15px 8px;\">Alberto IBARRONDO | Melek ONEN</div>\n",
    "\n",
    "<div style=\" display: inline-block; font-family: 'Lato', sans-serif; font-size: 12px; font-weight: bold; line-height: 12px; letter-spacing: 1px; padding: 10px 15px 8px; \">14/04/2018</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21dc90",
   "metadata": {},
   "source": [
    "This notebook serves as proof of the Reparametrization Trick used to absorb a Batch Normalization (BN) layer by the immediately previous Fully Connected (FC) or Convolutional (Conv) layer. To prove it, we will:\n",
    "1. Setup and CNN implementation using Tensorflow.\n",
    "2. Train a Convolutional Neural Network (CNN) containing BN layers from scratch, using MNIST training dataset.\n",
    "3. Evaluate accuracy of such CNN using the MNIST test dataset.\n",
    "4. Extract all the parameters from the trained CNN.\n",
    "5. Define a CNN with the same architecture except for the BN layers, that will be deleted.\n",
    "6. Perform the reparametrization trick to alter the weights and biases in Conv and FC layers of the original CNN and load them in the CNN without BN.\n",
    "7. Evaluate accuracy of reparametrized CNN using the MNIST test dataset, comparing it to the original.\n",
    "For an easy reproducibility of our proof, we include all the functions and CNNet class in `CNNet.py`. They can be easily imported using:\n",
    "             \n",
    "             from CNNet import *\n",
    "             \n",
    "We have also saved the models `tinyCNN` (with BN) and `NoBN_tinyCNN` (without BN and reparametrized) inside the `Models/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bada413d",
   "metadata": {},
   "source": [
    "# Setup & Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d055c5a6",
   "metadata": {},
   "source": [
    "We will use Tensorflow to implement the CNNs, and NumPy to perform the reparametrization trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a372a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e149e",
   "metadata": {},
   "source": [
    "As stated before, we will use the MNIST dataset, normalized to [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d32a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21486e90",
   "metadata": {},
   "source": [
    "# CNN implementation in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca311e09",
   "metadata": {},
   "source": [
    "**CNNet** implements a simple CNN with only one Conv layer and two FC layers, with this architecture:\n",
    "        \n",
    "        28x28 => Conv1 -> BN -> ReLU -> Flat -> FC1 -> BN_1 -> ReLU -> FC2 => [0-9]\n",
    "        \n",
    "        \n",
    "The details of the network architecture are:\n",
    "- **Conv1**: 20 Kernels/filters of 5x5, 20 biases. {1x28x28 -> 20x28x28}\n",
    "- **BN**: 20 betas, gammas, moving averages and variances. {20x28x28 -> 20x28x28}\n",
    "- **ReLU**: no parameters. {20x28x28 -> 20x28x28}\n",
    "- **Flat**: {20x28x28 -> 15684}\n",
    "- **FC1**: 15684\\*1000 weights, 1000 biases.  {15684 -> 1000}\n",
    "- **BN_1**: 1000 betas, gammas, moving averages and variances. {1000 -> 1000}\n",
    "- **ReLU**:no parameters. {1000 -> 1000}\n",
    "- **FC1**: 1000*10 weights, 10 biases.  {1000 -> 10}\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "![CNNwBN.png](attachment:CNNwBN.png)\n",
    "\n",
    "With this architecture we can use the reparametrization trick in **Conv1 + BN** and in **FC1 + BN_1**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04317a0",
   "metadata": {},
   "source": [
    "Our implemented python class **CNNet** takes charge of everything. It includes the following methods:\n",
    "- *init*: setup the whole tf graph and session.\n",
    "- *del*: clean the tf session before deleting the object.\n",
    "- *CNN_var_init*: initializing weights and biases as tf variables.\n",
    "- *preproc*: simple preprocessing for the input data (mean substraction).\n",
    "- *CNN_feedforward*: define the feedforward operations of the network.\n",
    "- *batchNorm*: wrapper of Batch Normalization in Tensorflow.\n",
    "- *save & restore*: handle different models/instances.\n",
    "- *train*: use MNIST train dataset to train the model.\n",
    "- *benchmark*: use one of the MNIST dataset parts (train/validation/test) to evaluate accuracy.\n",
    "- *predict*: perform inference on a single image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888174d7",
   "metadata": {},
   "source": [
    "ReLU will be our chosen activation function. Additionally, we initialize weights randomly and biases to a small constant. For all of the definitions we use the standard Tensorflow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15004239",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save('Models/tinyCNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f047d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNNet('tinyCNN', lr=0.002, loadInstance='Models/tinyCNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5925ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.train(epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876d9b4",
   "metadata": {},
   "source": [
    "# Evaluating CNN with BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa0885",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.benchmark('TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b2276b",
   "metadata": {},
   "source": [
    "As a result of the brief training, our trained CNN with BN layers yields a **test accuracy of 99.02%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70685c28",
   "metadata": {},
   "source": [
    "# Extracting parameters from trained CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b614daa3",
   "metadata": {},
   "source": [
    "For the sake of completeness, we will be verbose and show all the parameters being extracted from the current model (obtained from the default tf graph):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770b910",
   "metadata": {},
   "source": [
    "- Trainable variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f032e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVariables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "trainVariables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d05c8",
   "metadata": {},
   "source": [
    "- Moving means and variances from BN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56eba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelVariables = tf.get_collection(tf.GraphKeys.MODEL_VARIABLES)[2:4] + \\\n",
    "tf.get_collection(tf.GraphKeys.MODEL_VARIABLES)[6:8]\n",
    "modelVariables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf203b5",
   "metadata": {},
   "source": [
    "We will save them all inside a dictionary due to its ease of access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf42467",
   "metadata": {},
   "outputs": [],
   "source": [
    "allVariables = {}\n",
    "for var in trainVariables:\n",
    "    allVariables[var.name]=var.eval(session=cnn.sess)\n",
    "for var in modelVariables:\n",
    "    allVariables[var.name]=var.eval(session=cnn.sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "allVariables.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687329eb",
   "metadata": {},
   "source": [
    "# Defining a CNN without BN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952606bd",
   "metadata": {},
   "source": [
    "This CNN has exactly the same architecture as the previous one, save for the BN layers, that have been erased:\n",
    "\n",
    "![CNNrep.png](attachment:CNNrep.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoBNcnn = CNNet('NoBN_tinyCNN', lr=0.01, flag_bNorm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a1d25",
   "metadata": {},
   "source": [
    "Since we haven't trained this network, its initial test accuracy (13.6%) is based on random initialization of the weights, and thus it is similar to random picking a class for each image (10% chance of success)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoBNcnn.benchmark('TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9385a6",
   "metadata": {},
   "source": [
    "If we assign the parameters of the trained tinyCNN (without assigning any BN parameters, since there are none in this model) we obtain some kind of improvement in accuracy, but still far from optimal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79963044",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoBNcnn.sess.run(NoBNcnn.W_c1.assign(allVariables['Model/W_c1:0']))  \n",
    "NoBNcnn.sess.run(NoBNcnn.b_c1.assign(allVariables['Model/b_c1:0'])) \n",
    "NoBNcnn.sess.run(NoBNcnn.W_fc1.assign(allVariables['Model/W_fc1:0'])) \n",
    "NoBNcnn.sess.run(NoBNcnn.b_fc1.assign(allVariables['Model/b_fc1:0'])) \n",
    "NoBNcnn.sess.run(NoBNcnn.W_fc2.assign(allVariables['Model/W_fc2:0'])) \n",
    "NoBNcnn.sess.run(NoBNcnn.b_fc2.assign(allVariables['Model/b_fc2:0'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d1a17",
   "metadata": {},
   "source": [
    "# Applying the trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5689319",
   "metadata": {},
   "source": [
    "Now we apply the reparametrization trick described in the paper, that is valid both for Conv and for FC layers, modifying weights and biases using the parameters in the BN layer so that the Conv/FC reparametrized absorbs the operations in BN layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0652bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReparamTrickBN(W, b, beta, gamma, mu, sigma2):\n",
    "    W_rep = W * gamma / np.sqrt(sigma2)\n",
    "    b_rep = (b - mu) * gamma / np.sqrt(sigma2) + beta\n",
    "    return W_rep, b_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e24af56",
   "metadata": {},
   "source": [
    "First we apply it to Conv1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a647b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_rep_c1, b_rep_c1 = ReparamTrickBN(\n",
    "    W=allVariables['Model/W_c1:0'],\n",
    "    b=allVariables['Model/b_c1:0'],\n",
    "    beta=allVariables['BatchNorm/beta:0'],\n",
    "    gamma=allVariables['BatchNorm/gamma:0'],\n",
    "    mu=allVariables['BatchNorm/moving_mean:0'],\n",
    "    sigma2=allVariables['BatchNorm/moving_variance:0'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8265aa06",
   "metadata": {},
   "source": [
    "And then to FC1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_rep_fc1, b_rep_fc1 = ReparamTrickBN(\n",
    "    W=allVariables['Model/W_fc1:0'],\n",
    "    b=allVariables['Model/b_fc1:0'],\n",
    "    beta=allVariables['BatchNorm_1/beta:0'],\n",
    "    gamma=allVariables['BatchNorm_1/gamma:0'],\n",
    "    mu=allVariables['BatchNorm_1/moving_mean:0'],\n",
    "    sigma2=allVariables['BatchNorm_1/moving_variance:0'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee67360",
   "metadata": {},
   "source": [
    "Since we now have the reparametrized Weights and Biases, we can finally load them into the CNN without BN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4dcb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoBNcnn.sess.run(NoBNcnn.W_c1.assign(W_rep_c1))  \n",
    "NoBNcnn.sess.run(NoBNcnn.b_c1.assign(b_rep_c1)) \n",
    "NoBNcnn.sess.run(NoBNcnn.W_fc1.assign(W_rep_fc1)) \n",
    "NoBNcnn.sess.run(NoBNcnn.b_fc1.assign(b_rep_fc1)) \n",
    "NoBNcnn.sess.run(NoBNcnn.W_fc2.assign(allVariables['Model/W_fc2:0'])) \n",
    "NoBNcnn.sess.run(NoBNcnn.b_fc2.assign(allVariables['Model/b_fc2:0'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c15fe",
   "metadata": {},
   "source": [
    "At this point we save the model to be able to reproduce our results if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoBNcnn.save('Models/NoBN_tinyCNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2724711c",
   "metadata": {},
   "source": [
    "# Proving same accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e3a8d",
   "metadata": {},
   "source": [
    "With our recently loaded reparametrized Weights and Biases in Conv1 and FC1, we evaluate test accuracy once again:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67956a50",
   "metadata": {},
   "source": [
    "The final test accuracy for the CNN without BN is **99.02%**, which is the exact **same value** that the CNN with BN yielded (see section 4). We successfully modified the Weights and Biases of Conv and FC layers to absorb the subsequent BN layers.\n",
    "\n",
    "**This serves as proof of the reparametrization trick for Batch Normalization. QED**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18de806",
   "metadata": {},
   "source": [
    "# EXTRA: Performance gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e96be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNNet import CNNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8414c5d1",
   "metadata": {},
   "source": [
    "# Training a CNN with BN from stratch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90116b67",
   "metadata": {},
   "source": [
    "To speed up training, we will perform two phases of 5 steps each, one with learning rate 0.01 and the second with learning rate 0.002 for finer tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNNet('tinyCNN', lr=0.01)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
