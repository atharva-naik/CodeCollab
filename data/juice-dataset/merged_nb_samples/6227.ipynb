{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ec4685",
   "metadata": {},
   "source": [
    "# Character-level Language Models\n",
    "As seen in previous example, RNNs works well for sequential dataset. In this notebook, we want to train RNNs character-level language models i.e we'll give the RNNs a huge trunk of text and ask it to model the probability distribution of the next character in the sequence given a sequence of previous characters. This will then allow us to generate new text one character at a time.\n",
    "\n",
    "We recall the vanilla-RNNs dynamics\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "h_t &= \\tanh\\left(x_t\\times W_{xh} + h_{t-1}\\times W_{hh} + b_{h}\\right)\\\\\n",
    "o_t &= \\mathrm{softmax}\\left(h_t\\times W_{ho} + b_{o}\\right)\n",
    "\\end{array}\n",
    "$$\n",
    "where \n",
    "* $x_t$ is one-hot encoding of an input character\n",
    "* $W_{xh}$ is the input-to-hidden weight matrix\n",
    "* $W_{hh}$ is the hidden-to-hidden weight matrix\n",
    "* $W_{ho}$ is the hidden-to-output weight matrix\n",
    "* $b_h$ and $b_o$ are the biases\n",
    "\n",
    "Here we use $o_t$ to model the  conditional distribution\n",
    "$$\n",
    "P(x_{t+1}=j| x_{\\leq t}) = o_t[j]\n",
    "$$\n",
    "\n",
    "First we import the libraries we need and define the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "if '../common' not in sys.path:\n",
    "    sys.path.insert(0, '../common')\n",
    "\n",
    "from rnn.mrnn import BasicMRNNCell                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654e7e0",
   "metadata": {},
   "source": [
    "# Training dataset\n",
    "We will train RNNs model on Anna Karenina (~2Mb).\n",
    "\n",
    "## Pre-processing\n",
    "First we need to do the following pre-processing\n",
    "* get the set of all characters\n",
    "* get the map character to ids and vice-versa\n",
    "* convert text to ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed32b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../common/data/anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "\n",
    "# get all unique characters\n",
    "vocabs = set(text)\n",
    "\n",
    "# get the map char-to-id and vice-versa\n",
    "vocab_to_id = {c: i for i, c in enumerate(vocabs)}\n",
    "id_to_vocab = dict(enumerate(vocabs))\n",
    "\n",
    "# convert text-input into ids\n",
    "char_ids = np.array([vocab_to_id[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a43d8",
   "metadata": {},
   "source": [
    "Let's check out the first 50 characters in text & ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf51c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:50])\n",
    "print(chars[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde4c06",
   "metadata": {},
   "source": [
    "## Mini-batches\n",
    "Now we want to split data into mini-batches and into training and validation sets. We implement it in following helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(char_ids, batch_size, seq_len, split_frac = 0.9):\n",
    "    slice_size = batch_size*seq_len\n",
    "    nb_batches = (len(char_ids) - 1) // slice_size\n",
    "    \n",
    "    # get input/target\n",
    "    x = char_ids[  : nb_batches*slice_size]\n",
    "    y = char_ids[1 : nb_batches*slice_size+1]\n",
    "    \n",
    "    # split them to batches\n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # split into train/validation set\n",
    "    split_idx = int(nb_batches*split_frac) * seq_len\n",
    "    \n",
    "    train_x, train_y = x[:, :split_idx], y[:, :split_idx]\n",
    "    val_x, val_y = x[:, split_idx:], y[:, split_idx:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y\n",
    "\n",
    "def get_batches(train_inputs, train_targets, seq_len):\n",
    "    nb_batches = train_inputs.shape[1]//seq_len\n",
    "    idx = 0\n",
    "    for i in range(nb_batches):\n",
    "        idx += seq_len\n",
    "        yield train_inputs[:, idx-seq_len : idx], train_targets[:, idx-seq_len : idx]\n",
    "        \n",
    "def pick_top_idx(top_prob, top_idx):\n",
    "    c = np.random.choice(len(top_prob), 1, p = top_prob/np.sum(top_prob))[0]\n",
    "    return top_idx[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605bb6d3",
   "metadata": {},
   "source": [
    "# Ensemble a RNNs model\n",
    "As in previous post, we will use Tensorflow to create a RNNs model using the following functions\n",
    "* [`tf.one_hot`](https://www.tensorflow.org/api_docs/python/tf/one_hot) to convert target into one-hot representation\n",
    "* [`tf.contrib.rnn.BasicRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicRNNCell) to model a basic RNN cell\n",
    "* [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) to perform fully dynamic unrolling of our rnn i.e we compute the final state of our RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b33baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rnn character-lever language model\n",
    "class CharRnn(object):\n",
    "    def __init__(self, vocabs, vocab_to_id, id_to_voca\n",
    "                     , cell_type, rnn_size, batch_size, seq_len\n",
    "                     , num_factors = 3, num_layers = 2, learning_rate = 0.001):\n",
    "        # set input\n",
    "        self._vocabs = vocabs\n",
    "        self._vocabs_size = len(vocabs)\n",
    "        \n",
    "        self._vocab_to_id = vocab_to_id\n",
    "        self._id_to_vocab = id_to_vocab\n",
    "        self._rnn_size = rnn_size\n",
    "        self._batch_size = batch_size\n",
    "        self._seq_len = seq_len\n",
    "        self._cell_type = cell_type\n",
    "        self._num_factors = num_factors\n",
    "        self._num_layers = num_layers\n",
    "        self._lr = learning_rate        \n",
    "        \n",
    "        # check input\n",
    "        assert (self._cell_type in ['rnn', 'mrnn', 'lstm', 'gru'])\n",
    "        assert (self._num_layers >= 1)\n",
    "        \n",
    "        # build graph\n",
    "        self.build_graph()\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        # create placeholder for input/target\n",
    "        self._create_placeholder()\n",
    "        \n",
    "        # create rnn layers\n",
    "        self._create_rnn()\n",
    "        \n",
    "        # create loss/cost layers\n",
    "        self._create_loss()\n",
    "        \n",
    "        # create train-op & saver\n",
    "        self._create_train_op_saver()\n",
    "        \n",
    "        # create sample\n",
    "        self._create_sample()\n",
    "    \n",
    "    def _create_placeholder(self):\n",
    "        with self._graph.as_default():\n",
    "            # input & target has shape [batch_size, seq_len] \n",
    "            self._inputs  = tf.placeholder(tf.int32, [self._batch_size, None], name = 'inputs')\n",
    "            self._targets = tf.placeholder(tf.int32, [self._batch_size, None], name = 'targets')\n",
    "            \n",
    "            # convert to one-hot encoding\n",
    "            self._inputs_one_hot  = tf.one_hot(self._inputs,  self._vocabs_size)\n",
    "            self._targets_one_hot = tf.one_hot(self._targets, self._vocabs_size)\n",
    "            \n",
    "            # Keep probability placeholder for drop out layers\n",
    "            self._keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "            \n",
    "    def _create_rnn(self):\n",
    "        with self._graph.as_default():\n",
    "            with tf.variable_scope('rnn_scopes') as vs:\n",
    "                # create rnn-cell\n",
    "                if self._cell_type == 'rnn':\n",
    "                    cell = tf.contrib.rnn.BasicRNNCell(self._rnn_size)\n",
    "                elif self._cell_type == 'mrnn':\n",
    "                    cell = BasicMRNNCell(self._rnn_size, self._num_factors)\n",
    "                elif self._cell_type == 'lstm':\n",
    "                    cell = tf.contrib.rnn.BasicLSTMCell(self._rnn_size)\n",
    "                elif self._cell_type == 'gru':\n",
    "                    cell = tf.contrib.rnn.GRUCell(self._rnn_size)\n",
    "                \n",
    "                if (self._num_layers == 1):\n",
    "                    self._cell = cell\n",
    "                else:\n",
    "                    drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self._keep_prob)\n",
    "                    self._cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "                \n",
    "                # get initial_state\n",
    "                self._initial_state = self._cell.zero_state(self._batch_size, dtype = tf.float32)\n",
    "                \n",
    "                # run rnn through inputs to create outputs & final-state                \n",
    "                self._outputs, self._final_state = tf.nn.dynamic_rnn(self._cell,\n",
    "                                                                     self._inputs_one_hot,\n",
    "                                                                     initial_state = self._initial_state)\n",
    "                \n",
    "                # Retrieve just the RNNs variables.\n",
    "                self._rnn_variables = [v for v in tf.global_variables() if v.name.startswith(vs.name)]\n",
    "    \n",
    "    def _create_loss(self):\n",
    "        with self._graph.as_default():\n",
    "            # create softmax-weight & biases\n",
    "            init_stddev = 1.0 / np.sqrt(self._vocabs_size)\n",
    "            self._softmax_weights = tf.Variable(tf.truncated_normal([self._rnn_size, self._vocabs_size],\n",
    "                                                                    stddev = init_stddev), name = 'softmax_w')\n",
    "            self._softmax_biases  = tf.Variable(tf.zeros(self._vocabs_size), name = 'softmax_b')\n",
    "            \n",
    "            # reshape outputs/targets so we can use tf.matmul/tf.nn.softmax_cross_entropy_with_logits\n",
    "            outputs_flat = tf.reshape(self._outputs, [-1, self._rnn_size])\n",
    "            targets_flat = tf.reshape(self._targets_one_hot, [-1, self._vocabs_size])\n",
    "            \n",
    "            # compute logits (input to softmax)        \n",
    "            self._logits = tf.matmul(outputs_flat, self._softmax_weights) + self._softmax_biases\n",
    "            \n",
    "            # compute the cross-entropy loss at each time-step\n",
    "            self._loss = tf.nn.softmax_cross_entropy_with_logits(logits=self._logits, \n",
    "                                                                 labels=targets_flat)\n",
    "            \n",
    "            # cost is the reduce_mean of loss at all time-step\n",
    "            self._cost = tf.reduce_mean(self._loss)\n",
    "    \n",
    "    def _create_train_op_saver(self):\n",
    "        with self._graph.as_default():\n",
    "            # apply gradient clipping to control exploiding gradient\n",
    "            tvars    = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), 5.0)\n",
    "            \n",
    "            # create train-op with gradient clipping\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self._lr)\n",
    "            self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "            \n",
    "            # create saver\n",
    "            self._saver = tf.train.Saver(max_to_keep=100)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, train_inputs, train_targets, \n",
    "              val_inputs, val_targets,\n",
    "              epochs, save_every=50, \n",
    "              save_dir = 'checkpoints', keep_prob = 0.5):\n",
    "        with tf.Session(graph=self._graph) as sess:\n",
    "            # initialize variable\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # compute nb of iterations\n",
    "            nb_batches = train_inputs.shape[1]//self._seq_len            \n",
    "            nb_iters   = epochs * nb_batches\n",
    "            \n",
    "            # number of batches in validation set\n",
    "            val_nb_batches = val_inputs.shape[1]//self._seq_len\n",
    "            \n",
    "            iteration  = 0\n",
    "            for e in range(epochs):\n",
    "                # reset initial-state to 0\n",
    "                new_state  = sess.run(self._initial_state)\n",
    "                train_loss = 0.\n",
    "                b = 0\n",
    "                for inputs, targets in get_batches(train_inputs, train_targets, self._seq_len):\n",
    "                    # run the training-op\n",
    "                    # note that the final state of one batch shoud be used as initial-state of next batch\n",
    "                    start = time.time()\n",
    "                    batch_loss, new_state, _ = sess.run([self._cost, self._final_state, self._train_op],\n",
    "                                                        feed_dict = {self._inputs  : inputs,\n",
    "                                                                     self._targets : targets,\n",
    "                                                                     self._keep_prob : keep_prob,\n",
    "                                                                     self._initial_state : new_state})\n",
    "                    \n",
    "                    end = time.time()\n",
    "                    train_loss += batch_loss\n",
    "                    b          += 1\n",
    "                    iteration  +=1\n",
    "                    \n",
    "                    sys.stdout.write('\\rEpoch {}/{}'.format(e+1, epochs) + \n",
    "                                     ' Iteration {}/{}'.format(iteration, nb_iters) +\n",
    "                                     ' Training loss: {:.4f}'.format(train_loss/b) +\n",
    "                                     ' Running {:.4f} sec/batch'.format((end-start)))\n",
    "                        \n",
    "                    if (   (iteration%save_every == 0) \n",
    "                        or (iteration == nb_iters)):\n",
    "                        \n",
    "                        # reset state for validation set\n",
    "                        val_state = sess.run(self._initial_state)\n",
    "                        val_loss  = 0.\n",
    "                        \n",
    "                        # run rnn and measure the loss on validation set\n",
    "                        for val_x, val_y in get_batches(val_inputs, val_targets, self._seq_len):\n",
    "                            batch_loss, val_state = sess.run([self._cost, self._final_state],\n",
    "                                                             feed_dict = {self._inputs  : val_x,\n",
    "                                                                          self._targets : val_y,\n",
    "                                                                          self._keep_prob : 1.0,\n",
    "                                                                          self._initial_state : val_state})\n",
    "                            val_loss += batch_loss\n",
    "                        \n",
    "                        val_loss /= val_nb_batches\n",
    "                        # report validation loss & save down checkpoints\n",
    "                        print('\\nValidation loss: {:.4f}'.format(val_loss), 'Saving checkpoint!\\n')\n",
    "                        save_path = '{}/cell_{}_i{}_l{}_v{:.4f}.ckpt'.format(save_dir,\n",
    "                                                                             self._cell_type,\n",
    "                                                                             iteration, \n",
    "                                                                             self._rnn_size, \n",
    "                                                                             val_loss)\n",
    "                        self._saver.save(sess, save_path)\n",
    "\n",
    "    def _create_sample(self):\n",
    "        with self._graph.as_default():\n",
    "            dist = tf.nn.softmax(self._logits)\n",
    "            top_probs, top_indices = tf.nn.top_k(dist, k = 3)\n",
    "            self._top_probs   = tf.reshape(top_probs, [-1])\n",
    "            self._top_indices = tf.reshape(top_indices, [-1])\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint):\n",
    "        sess = tf.Session(graph = self._graph)\n",
    "        self._saver.restore(sess, checkpoint)\n",
    "        return sess\n",
    "    \n",
    "    def sample_text(self, sess, sample_len, prime = 'The '):\n",
    "        '''\n",
    "        We generate new text that given current text (prime)\n",
    "        '''\n",
    "        new_state = sess.run(self._initial_state)\n",
    "        \n",
    "        for c in prime:\n",
    "            c_id = self._vocab_to_id[c]\n",
    "            inputs = np.array([c_id]).reshape([1,1])\n",
    "            \n",
    "            # forward a single time-step \n",
    "            new_state, top_prob, top_idx = sess.run([self._final_state, self._top_probs, self._top_indices], \n",
    "                                                    feed_dict = {self._inputs : inputs, \n",
    "                                                                 self._keep_prob : 1.0,\n",
    "                                                                 self._initial_state : new_state})\n",
    "        \n",
    "        samples = []\n",
    "        \n",
    "        # pick next most probable character\n",
    "        c_id  = pick_top_idx(top_prob, top_idx)\n",
    "        samples.append(self._id_to_vocab[c_id])\n",
    "        \n",
    "        for i in range(sample_len-1):\n",
    "            inputs = np.array([c_id]).reshape([1,1])\n",
    "            \n",
    "            # forward a single time-step \n",
    "            new_state, top_prob, top_idx = sess.run([self._final_state, self._top_probs, self._top_indices], \n",
    "                                                    feed_dict = {self._inputs : inputs, \n",
    "                                                                 self._keep_prob : 1.0,\n",
    "                                                                 self._initial_state : new_state})\n",
    "            c_id  = pick_top_idx(top_prob, top_idx)\n",
    "            samples.append(self._id_to_vocab[c_id])\n",
    "        \n",
    "        return ''.join(samples)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2865b95f",
   "metadata": {},
   "source": [
    "# Training RNNs\n",
    "In this section, we will train our RNNs with various cell-type\n",
    "* [`BasicRNNCell`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicRNNCell)\n",
    "* [`BasicLSTMCell`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicLSTMCell)\n",
    "* [`BasicGRUCell`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicGRUCell)\n",
    "* [`BasicMRNNCell`](http://www.icml-2011.org/papers/524_icmlpaper.pdf)\n",
    "\n",
    "First create checkpoint directory so we can store trained-model's checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir to store checkpoints\n",
    "!mkdir checkpoints/crnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256718a",
   "metadata": {},
   "source": [
    "## Train with BasicRNNCell\n",
    "Before train with BasicRNNCell, we inspect the variables' shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 256\n",
    "batch_size = 128\n",
    "seq_len = 64\n",
    "num_layers = 2\n",
    "cell_type = 'rnn'\n",
    "\n",
    "# rnn models\n",
    "crnn = CharRnn(vocabs, vocab_to_id, id_to_vocab, cell_type,\n",
    "               rnn_size = rnn_size, batch_size = batch_size, \n",
    "               seq_len = seq_len, num_layers=num_layers)\n",
    "\n",
    "# view shape\n",
    "if num_layers > 1:\n",
    "    print ('initial_state is a tuple of len {} each has shape \\n\\t{} i.e (batch_size, rnn_size)\\n'.format(\n",
    "                                                                        len(crnn._initial_state),\n",
    "                                                                        crnn._initial_state[0].get_shape()))\n",
    "    \n",
    "    print ('rnn weights and biases:')\n",
    "    for v in crnn._rnn_variables:\n",
    "        print ('\\t{:<65} rank {} shape {}'.format(v.name, v.get_shape().ndims, v.get_shape().as_list()))\n",
    "    \n",
    "    print ('at each layer:\\n\\tweights should has shape [input_dim + hidden_dim, hidden_dim]',\n",
    "                         '\\n\\tbiases should has shape  [hidden_dim]')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa1d22",
   "metadata": {},
   "source": [
    "## Train with MRNN cell\n",
    "We try out the MRNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c70d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "rnn_size = 256\n",
    "batch_size = 128\n",
    "seq_len = 64\n",
    "num_factors= 3\n",
    "num_layers = 1\n",
    "cell_type = 'mrnn'\n",
    "\n",
    "# rnn models\n",
    "crnn = CharRnn(vocabs, vocab_to_id, id_to_vocab, cell_type,\n",
    "               num_factors = num_factors, rnn_size = rnn_size, \n",
    "               batch_size = batch_size, seq_len = seq_len, num_layers=num_layers)\n",
    "\n",
    "# create train/validation dataset\n",
    "train_x, train_y, val_x, val_y = split_data(char_ids, batch_size, seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b496d2",
   "metadata": {},
   "source": [
    "It's time to train MRNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "keep_prob = 0.5\n",
    "crnn.train(train_x, train_y, val_x, val_y, \n",
    "           epochs=epochs,\n",
    "           save_every=500, \n",
    "           keep_prob=keep_prob,\n",
    "           save_dir= 'checkpoints/crnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93076e",
   "metadata": {},
   "source": [
    "Let's use this to generate some new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_crnn = CharRnn(vocabs, vocab_to_id, id_to_vocab, cell_type,\n",
    "                   rnn_size = rnn_size, batch_size = 1, \n",
    "                   seq_len = seq_len, num_layers=num_layers)\n",
    "\n",
    "sess = val_crnn.load_checkpoint('checkpoints/crnn/crnn_mrnn_i10850_l256_v1.7655.ckpt')\n",
    "\n",
    "prime = 'Happy families are '\n",
    "new_text = val_crnn.sample_text(sess, 200, prime=prime)\n",
    "print ('Prime:  {}\\nSample: {}'.format(prime, new_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082f68f",
   "metadata": {},
   "source": [
    "## Train with LSTM cell\n",
    "Let's create a RNNs model so that we can train it with given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d1d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "rnn_size = 256\n",
    "batch_size = 128\n",
    "seq_len = 64\n",
    "num_layers = 2\n",
    "cell_type = 'lstm'\n",
    "\n",
    "# rnn models\n",
    "crnn = CharRnn(vocabs, vocab_to_id, id_to_vocab, cell_type,\n",
    "               rnn_size = rnn_size, batch_size = batch_size, \n",
    "               seq_len = seq_len, num_layers=num_layers)\n",
    "\n",
    "# create train/validation dataset\n",
    "train_x, train_y, val_x, val_y = split_data(char_ids, batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a1c8d",
   "metadata": {},
   "source": [
    "Time for training, we pass train/validation dataset to the `train` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567eeeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "keep_prob = 0.5\n",
    "crnn.train(train_x, train_y, val_x, val_y, \n",
    "           epochs=epochs,\n",
    "           save_every=500, \n",
    "           keep_prob=keep_prob,\n",
    "           save_dir= 'checkpoints/crnn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41726d",
   "metadata": {},
   "source": [
    "# Valuation\n",
    "Now that the RNNs is trained, we want to use it to generate some new text"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
