{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25695691",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "\n",
    "## APMTH 207: Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "\n",
    "**Due Date: ** Friday, March 23rd, 2018 at 11:00am\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as a Jupyter notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "import pymc3 as pm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "sns.set_context('talk')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05873d9d",
   "metadata": {},
   "source": [
    "## Problem 1: Gibbs Sampling On A Bivariate Normal\n",
    "\n",
    "Let $\\mathbf{X}$ be a random variable taking values in $\\mathbb{R}^2$. That is, $\\mathbf{X}$ is a 2-dimensional vector. Suppose that $\\mathbf{X}$ is normally distributed as follows\n",
    "$$ \n",
    "\\mathbf{X} \\sim \\mathcal{N} \\left(  \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    1  \\\\ \n",
    "    2  \\\\ \n",
    "  \\end{array} \n",
    "\\right],\n",
    "\\left[\n",
    "  \\begin{array}{ccc}\n",
    "    4 & 1.2  \\\\ \n",
    "    1.2 & 4 \\\\ \n",
    "  \\end{array} \n",
    "  \\right] \\right).\n",
    "$$ \n",
    "That is, the pdf of the distribution of $\\mathbf{X}$ is\n",
    "$$\n",
    "f_{\\mathbf{X}}(\\mathbf{x}) = \\frac{1}{2\\pi\\sqrt{\\vert \\Sigma\\vert }}\\mathrm{exp}\\left\\{ - \\frac{1}{2} (\\mathbf{x} - \\mu)^\\top \\Sigma^{-1} (\\mathbf{x} - \\mu)\\right\\}\n",
    "$$\n",
    "where $\\mu = \\left[\n",
    "\\begin{array}{c}\n",
    "    1  \\\\ \n",
    "    2  \\\\ \n",
    "  \\end{array} \n",
    "\\right]$, $\\Sigma = \\left[\n",
    "  \\begin{array}{ccc}\n",
    "    4 & 1.2  \\\\ \n",
    "    1.2 & 4 \\\\ \n",
    "  \\end{array} \n",
    "  \\right]$, and $\\vert \\cdot\\vert $ is the matrix determinant operator.\n",
    "\n",
    "In the following, we will denote the random variable corresponding to the first component of $\\mathbf{X}$ by $X_1$ and the second component by $X_2$.\n",
    "\n",
    "* Write a Gibbs sampler for this distribution by sampling sequentially from the two conditional distributions $f_{X_1\\vert X_2}, f_{X_2\\vert X_1}$. \n",
    "* Choose a thinning parameter, burn-in factor and total number of iterations that allow you to take 10000 non-autocorrelated draws. \n",
    "* You must justify your choice of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9fc118",
   "metadata": {},
   "source": [
    "### Answer to Problem 1\n",
    "\n",
    "Here we have $\\sigma_{1} = 2$, $\\sigma_{2} = 2$, $\\rho = 0.3$.\n",
    "\n",
    "At each iteration, we can sample from the conditionals\n",
    "$$X_{1}\\mid X_{2}=x_{2}\\ \\sim \\ {\\mathcal {N}}\\left(\\mu _{1}+{\\frac {\\sigma _{1}}{\\sigma _{2}}}\\rho (x_{2}-\\mu _{2}),\\,(1-\\rho ^{2})\\sigma _{1}^{2}\\right)$$\n",
    "$$X_{2}\\mid X_{1}=x_{1}\\ \\sim \\ {\\mathcal {N}}\\left(\\mu _{2}+{\\frac {\\sigma _{2}}{\\sigma _{1}}}\\rho (x_{1}-\\mu _{1}),\\,(1-\\rho ^{2})\\sigma _{2}^{2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4950ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrplot(trace, maxlags=50):\n",
    "    plt.acorr(trace-np.mean(trace),  normed=True, maxlags=maxlags);\n",
    "    plt.xlim([0, maxlags])\n",
    "\n",
    "def effective_sample_size(data, step=1):\n",
    "    \n",
    "    # References:\n",
    "    # https://code.google.com/p/biopy/source/browse/trunk/biopy/bayesianStats.py?r=67\n",
    "    # https://am207.github.io/2018spring/wiki/tetchygibbs.html\n",
    "    \n",
    "    n = len(data)\n",
    "    assert n > 1\n",
    "    maxlags = min(n//3, 1000)\n",
    "    \n",
    "    gamma_stat = [0, ] * maxlags\n",
    "    \n",
    "    var_stat = 0.0\n",
    "    \n",
    "    if type(data) != np.ndarray:\n",
    "        data = np.array(data)\n",
    "        \n",
    "    data_normed = data - data.mean()\n",
    "    \n",
    "    for lag in range(maxlags):\n",
    "        v1 = data_normed[:n-lag]\n",
    "        v2 = data_normed[lag:]\n",
    "        v = v1 * v2\n",
    "        gamma_stat[lag] = sum(v) / len(v)\n",
    "        \n",
    "        if lag == 0:\n",
    "            var_stat = gamma_stat[0]\n",
    "        elif lag % 2 == 0:\n",
    "            s = gamma_stat[lag-1] + gamma_stat[lag]\n",
    "            if s > 0:\n",
    "                var_stat += 2 * s\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    act = step * var_stat / gamma_stat[0]\n",
    "    ess = step * n / act\n",
    "    \n",
    "    return ess\n",
    "    \n",
    "def print_ess(data):\n",
    "    ess1 = effective_sample_size(data[:, 0])\n",
    "    ess2 = effective_sample_size(data[:, 1])\n",
    "    print('Effective size for x1:', ess1, ' of', len(data), 'samples; effective rate:', ess1/len(data))\n",
    "    print('Effective size for x2:', ess2, ' of', len(data), 'samples; effective rate:', ess2/len(data))\n",
    "    \n",
    "class Gibbs:\n",
    "    def __init__(self, mu1=1, mu2=2, sigma1=2, sigma2=2, rho=0.3):\n",
    "        self.mu1 = mu1\n",
    "        self.mu2 = mu2\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "        self.rho = rho\n",
    "        \n",
    "    def run(self, n=10000, x_init=np.array([1, 2]), seed=0):\n",
    "        start = time.time()\n",
    "        mu1 = self.mu1\n",
    "        mu2 = self.mu2\n",
    "        sigma1 = self.sigma1\n",
    "        sigma2 = self.sigma2\n",
    "        rho = self.rho\n",
    "        \n",
    "        s12 = sigma1*np.sqrt(1-rho*rho)\n",
    "        s21 = sigma2*np.sqrt(1-rho*rho)\n",
    "        \n",
    "        samples = np.empty((n+1, 2))\n",
    "        samples[0] = x_init\n",
    "        \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        for i in range(1, n, 2):\n",
    "            m12 = mu1 + sigma1/sigma2*rho*(samples[i-1, 1]-mu2)\n",
    "            samples[i, 0] = np.random.normal(m12, s12)\n",
    "            samples[i, 1] = samples[i-1, 1]\n",
    "            \n",
    "            m21 = mu2 + sigma2/sigma1*rho*(samples[i, 0]-mu1)\n",
    "            samples[i+1, 1] = np.random.normal(m21, s21)\n",
    "            samples[i+1, 0] = samples[i, 0]\n",
    "            \n",
    "        self.n = n\n",
    "        self.x_init = x_init\n",
    "        self.samples = samples[:-1]\n",
    "        self.time_ = time.time() - start\n",
    "        return self\n",
    "    \n",
    "    def process(self, burnin=0, thin=1):\n",
    "        self.burnin = burnin\n",
    "        self.thin = thin\n",
    "        self.samples2 = self.samples[burnin::thin]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d36665",
   "metadata": {},
   "source": [
    "We first try sampling 10000 samples without burnin and thining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e367407",
   "metadata": {},
   "outputs": [],
   "source": [
    "g0 = Gibbs().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16009b5",
   "metadata": {},
   "source": [
    "We can plot autocorrelations and calculate effective sample sizes for $X_1$ and $X_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d485ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "corrplot(g0.samples[:, 0])\n",
    "plt.ylabel('autocorrelation($X_1$)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "corrplot(g0.samples[:, 1])\n",
    "plt.ylabel('autocorrelation($X_2$)')\n",
    "plt.tight_layout()\n",
    "\n",
    "print('No burnin and thining:')\n",
    "print_ess(g0.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb7ca70",
   "metadata": {},
   "source": [
    "The ratio of effective sample size is around 40%. We can try thining of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "corrplot(g0.samples[::3, 0])\n",
    "plt.ylabel('autocorrelation($X_1$)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "corrplot(g0.samples[::3, 1])\n",
    "plt.ylabel('autocorrelation($X_2$)')\n",
    "plt.tight_layout()\n",
    "\n",
    "print('thining: 3')\n",
    "print_ess(g0.samples[::3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e7a92",
   "metadata": {},
   "source": [
    "Effective sample sizes increase significantly. We can further increase thining to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15efc934",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "corrplot(g0.samples[::4, 0])\n",
    "plt.ylabel('autocorrelation($X_1$)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "corrplot(g0.samples[::4, 1])\n",
    "plt.ylabel('autocorrelation($X_2$)')\n",
    "plt.tight_layout()\n",
    "\n",
    "print('thining = 4')\n",
    "print_ess(g0.samples[::4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e933d",
   "metadata": {},
   "source": [
    "Effective sample size is equal to total sample size, indicating thining at 4 is a good choice.\n",
    "\n",
    "We can take 10000 non-autocorrelated draws as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e30b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Gibbs().run(n=41000).process(burnin=1000, thin=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ca9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "corrplot(g.samples[:, 0])\n",
    "plt.ylabel('autocorrelation($X_1$)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "corrplot(g.samples[:, 1])\n",
    "plt.ylabel('autocorrelation($X_2$)')\n",
    "plt.tight_layout()\n",
    "\n",
    "print('Before burnin and thining:')\n",
    "print_ess(g.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "corrplot(g.samples2[:, 0])\n",
    "plt.ylabel('autocorrelation($X_1$)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "corrplot(g.samples2[:, 1])\n",
    "plt.ylabel('autocorrelation($X_2$)')\n",
    "plt.tight_layout()\n",
    "\n",
    "print('burnin = {}, thining = {}:'.format(g.burnin, g.thin))\n",
    "print_ess(g.samples2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ccab76",
   "metadata": {},
   "source": [
    "As we can see, we have managed to get 10000 non-autocorrelated samples.\n",
    "\n",
    "We can check the distribution of samples as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.jointplot(x=0, y=1, data=pd.DataFrame(g.samples2), marker='.')\n",
    "fig.ax_joint.collections[0].set_alpha(0.1);\n",
    "fig.set_axis_labels('$X_1$', '$X_2$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa2fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5.5))\n",
    "sns.kdeplot(g.samples2[:, 0], g.samples2[:, 1], shade=True);\n",
    "plt.xlabel('$X_1$');\n",
    "plt.ylabel('$X_2$');\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c0f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample mean: {}'.format(np.mean(g.samples2, axis=0)))\n",
    "print('Sample covariance matrix:\\n{}'.format(np.cov(g.samples2.T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f057f",
   "metadata": {},
   "source": [
    "As we can see, samples are desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f89807",
   "metadata": {},
   "source": [
    "## Problem 2: Rubber Chickens Bawk Bawk!\n",
    "In the competitive rubber chicken retail market, the success of a company is built on satisfying the exacting standards of a consumer base with refined and discriminating taste. In particular, customer product reviews are all important. But how should we judge the quality of a product based on customer reviews?\n",
    "\n",
    "On Amazon, the first customer review statistic displayed for a product is the ***average rating***. The following are the main product pages for two competing rubber chicken products, manufactured by Lotus World and Toysmith respectively:\n",
    "\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- |  - \n",
    "![alt](https://am207.github.io/2018spring/homework/lotus1.png) |  ![alt](https://am207.github.io/2018spring/homework/toysmith1.png)\n",
    "\n",
    "Clicking on the 'customer review' link on the product pages takes us to a detailed break-down of the reviews. In particular, we can now see the number of times a product is rated a given rating (between 1 and 5 stars).\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- |  - \n",
    "![alt](https://am207.github.io/2018spring/homework/lotus2.png) |  ![alt](https://am207.github.io/2018spring/homework/toysmith2.png)\n",
    "\n",
    "(The images above are also included on canvas in case you are offline, see below)\n",
    "\n",
    "In the following, we will ask you to compare these two products using the various rating statistics. **Larger versions of the images are available in the data set accompanying this notebook**.\n",
    "\n",
    "Suppose that for each product, we can model the probability of the value each new rating as the following vector:\n",
    "$$\n",
    "\\theta = [\\theta_1, \\theta_2, \\theta_3, \\theta_4, \\theta_5]\n",
    "$$\n",
    "where $\\theta_i$ is the probability that a given customer will give the product $i$ number of stars.\n",
    "\n",
    "\n",
    "### Part A: Inference\n",
    "\n",
    "1. Suppose you are told that customer opinions are very polarized in the retail world of rubber chickens, that is, most reviews will be 5 stars or 1 stars (with little middle ground). Choose an appropriate Dirichlet prior for $\\theta$. Recall that the Dirichlet pdf is given by:\n",
    "$$\n",
    "f_{\\Theta}(\\theta) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}, \\quad B(\\alpha) = \\frac{\\prod_{i=1}^k\\Gamma(\\alpha_i)}{\\Gamma\\left(\\sum_{i=1}^k\\alpha_i\\right)},\n",
    "$$\n",
    "where $\\theta_i \\in (0, 1)$ and $\\sum_{i=1}^k \\theta_i = 1$, $\\alpha_i > 0 $ for $i = 1, \\ldots, k$.\n",
    "\n",
    "2. Write an expression for the posterior pdf, using a using a multinomial model for observed ratings. Recall that the multinomial pdf is given by:\n",
    "$$\n",
    "f_{\\mathbf{X}\\vert  \\Theta}(\\mathbf{x}) = \\frac{n!}{x_1! \\ldots x_k!} \\theta_1^{x_1} \\ldots \\theta_k^{x_k}\n",
    "$$\n",
    "where $n$ is the total number of trials, $\\theta_i$ is the probability of event $i$ and $\\sum_i \\theta_i = 1$, and $x_i$ is count of outcome $i$ and $\\sum_i x_i = n$. \n",
    "\n",
    "  **Note:** The data you will need in order to define the likelihood function should be read off the image files included in the dataset.\n",
    "  \n",
    "\n",
    "3. Sample 1,000 values of $\\theta$ from the *posterior distribution*.\n",
    " \n",
    "4. Sample 1,000 values of $x$ from the *posterior predictive distribution*.\n",
    "\n",
    "\n",
    "### Part B: Ranking\n",
    "\n",
    "1. Name at least two major potential problems with using only the average customer ratings to compare products.\n",
    "\n",
    "  (**Hint:** if product 1 has a higher average rating than product 2, can we conclude that product 1 is better liked? If product 1 and product 2 have the same average rating, can we conclude that they are equally good?)\n",
    "  \n",
    "\n",
    "2. Using the samples from your *posterior distribution*, determine which rubber chicken product is superior. Justify your conclusion with sample statistics.\n",
    "\n",
    "3. Using the samples from your *posterior predictive distribution*, determine which rubber chicken product is superior. Justify your conclusion with sample statistics.\n",
    "\n",
    "4. Finally, which rubber chicken product is superior?\n",
    "\n",
    "  (**Note:** we're not looking for \"the correct answer\" here, any sound decision based on a statistically correct interpretation of your model will be fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a274de",
   "metadata": {},
   "source": [
    "### Answer to Problem 2 Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f25f28",
   "metadata": {},
   "source": [
    "We can set high $\\alpha_1$, $\\alpha_5$ and low $\\alpha_2$, $\\alpha_3$, $\\alpha_4$ since most reviews will be 5 stars or 1 stars. \n",
    "\n",
    "For example, we can set the following Dirichlet prior for $\\theta$:\n",
    "$$\\Theta \\sim {\\rm Dirichlet\\ (\\alpha = [6, 1, 1, 1, 6])}$$\n",
    "\n",
    "And we can check the prior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a817445",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array([6, 1, 1, 1, 6])\n",
    "\n",
    "np.random.seed(0)\n",
    "prior_samples = np.random.dirichlet(alpha, size=1000)\n",
    "\n",
    "for i in range(prior_samples.shape[1]):\n",
    "    sns.distplot(prior_samples[:, i], bins=50, norm_hist=True, \\\n",
    "                 label='prior for {} stars'.format(i + 1));\n",
    "\n",
    "plt.xlabel(r'$\\theta$');\n",
    "plt.xlim([0, 1]);\n",
    "plt.ylim([0, 18]);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6b5ae",
   "metadata": {},
   "source": [
    "As we can see, the prior distributions are desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0557a7",
   "metadata": {},
   "source": [
    "The posteriror for $\\theta$ is \n",
    "$$p(\\theta|X) \\propto p(\\theta)\\ p(X|\\theta) \\propto {\\rm Dirichlet}\\ (\\alpha)\\ {\\rm Multinomial}\\ (\\theta, X)$$\n",
    "$$\\therefore\\ p(\\theta|X) \\propto \\prod_{i=1}^k \\theta_i^{\\alpha_i + x_i - 1}$$\n",
    "i.e.,\n",
    "$$p(\\theta|X) \\propto  {\\rm Dirichlet}\\ (\\alpha + X)$$\n",
    "\n",
    "To sample $\\theta$ from the posterior distribution, we can sample from the above distribution directly. To sample $x$ from the posterior predictive distribution, we can sample $\\theta$ from the posterior distribution and then sample from the likelihood using the $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b4fb20",
   "metadata": {},
   "source": [
    "### Answer to Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce4ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "from theano import shared\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tumortuples=[e.strip().split() for e in tumordata.split(\"\\n\")]\n",
    "tumory=np.array([np.int(e[0].strip()) for e in tumortuples if len(e) > 0])\n",
    "tumorn=np.array([np.int(e[1].strip()) for e in tumortuples if len(e) > 0])\n",
    "tumory_shared = shared(tumory)\n",
    "tumorn_shared = shared(tumorn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with pm.Model() as rt:\n",
    "    mu = pm.Uniform('mu', lower=0, upper=1)\n",
    "    nu = pm.Uniform('nu', lower=0, upper=1)\n",
    "    alpha = mu / (nu * nu)\n",
    "    beta = (1 - mu) / (nu * nu)\n",
    "    theta = pm.Beta('theta', alpha=alpha, beta=beta, shape=len(tumorn))\n",
    "    obs = pm.Binomial('obs', n=tumorn_shared, p=theta, observed=tumory_shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with rt:\n",
    "    step = pm.NUTS()\n",
    "    rt_traces = pm.sample(15000, tune=5000, step=step, njobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97966f96",
   "metadata": {},
   "source": [
    "### Answer to Problem 3 Part A\n",
    "\n",
    "#### Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pm.autocorrplot(rt_traces, max_lag=50);\n",
    "plt.tight_layout();\n",
    "0;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3ec40",
   "metadata": {},
   "source": [
    "Autocorrelations are dying very quickly (no more than lag 20) for all parameters in all chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eff044",
   "metadata": {},
   "source": [
    "#### Parameter trace correlation\n",
    "\n",
    "We have set `tune` to be 5000 in our sampler, and we don't manually burnin here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pm.traceplot(rt_traces);\n",
    "0;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be232015",
   "metadata": {},
   "source": [
    "Traces in different chains are consistent, and traces look like white noises. The result indicates the convergence of parameters, and suggests that manual burnin might not be necessary when we set `tune` to be 5000 in our sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78540b53",
   "metadata": {},
   "source": [
    "#### Gewecke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b06af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pm.geweke(rt_traces, intervals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3101648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xlim = [0, 7500]\n",
    "plt.figure(figsize=(12, 120))\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(72, 4, i+1)\n",
    "    plt.scatter(*z[i]['mu'].T, marker='.');\n",
    "    plt.hlines([-1,1], xlim[0], xlim[1], linestyles='dotted');\n",
    "    plt.title('mu (chain {})'.format(i))\n",
    "    \n",
    "    plt.subplot(72, 4, i+5)\n",
    "    plt.scatter(*z[i]['nu'].T, marker='.');\n",
    "    plt.hlines([-1,1], xlim[0], xlim[1], linestyles='dotted');\n",
    "    plt.title('nu (chain {})'.format(i))\n",
    "\n",
    "    for j in range(70):\n",
    "        plt.subplot(72, 4, i+1+(j+2)*4)\n",
    "        plt.scatter(*z[i]['theta'][j].T, marker='.');\n",
    "        plt.hlines([-1,1], xlim[0], xlim[1], linestyles='dotted');\n",
    "        plt.title('theta_{} (chain {})'.format(j, i))\n",
    "    \n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56645f8",
   "metadata": {},
   "source": [
    "As we can see, z-scores are very close to 0 for all parameters in all chains, which provides evdidence for convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13522f9f",
   "metadata": {},
   "source": [
    "#### Gelman-Rubin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gelman-Rubin')\n",
    "print(pm.gelman_rubin(rt_traces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6949547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "pm.forestplot(rt_traces);\n",
    "0;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8616a3",
   "metadata": {},
   "source": [
    "The values of $\\hat{R}$ are very close to 1 for all parameters, indicating the convergence of these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95134544",
   "metadata": {},
   "source": [
    "#### Number of Effective Samples ($n_{eff}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Effective sample sizes')\n",
    "print(pm.effective_n(rt_traces))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918e2f5",
   "metadata": {},
   "source": [
    "As we can see, the number of effective samples approaches the number of samples for most parameters. The worst case is $\\nu$, which still contains nearly 10k effective samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579441b",
   "metadata": {},
   "source": [
    "### Answer to Problem 3 Part B\n",
    "#### 1. Carry out posterior predictive checks by using sample_ppc to generate posterior-predictives for all 70 experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c773bb2a",
   "metadata": {},
   "source": [
    "### Part B:  Posterior predictive check\n",
    "\n",
    "Recall from lab notes that in a hierarchical model there are two kinds of posterior predictions that are useful. (1) The distribution of future observations $y_i^*$  given a $\\theta_i$, and (2) The distribution of observations $y_j^*$  drawn from a future $\\theta_j$ drawn from the super-population (i.e. using the Beta on the estimated hyper parameters).\n",
    "\n",
    "1. Carry out posterior predictive checks by using `sample_ppc` to generate posterior-predictives for all 70 experiments. This generates predictives of the first type above. \n",
    "\n",
    "2. Plot histograms for these predictives with the actual value shown as a red-dot against the histogram (as in the coal disasters model in lecture 14). Is the data consistent with the predictive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86655ca",
   "metadata": {},
   "source": [
    "### Part C:  Shrinkage\n",
    "\n",
    "1. Plot the posterior median of the death rate parameters $\\theta_1, \\theta_2, ...\\theta_{70}$ against the observed death rates ($y_i/n_i$)\n",
    "\n",
    "2. Explain the shrinkage by comparing against a 45 degree line as done in the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e6faf",
   "metadata": {},
   "source": [
    "### PART D:  Experiment 71\n",
    "\n",
    "Consider an additional experiment -- experiment 71 -- in which 4 out of 14 rats died. \n",
    "\n",
    "1. Calculate the marginal posterior of $\\theta_{71}$, the \"new\" experiment,\n",
    "\n",
    "2. Find the $y_{71}^*$ posterior predictive for that experiment.\n",
    "\n",
    "**HINT: ** The critical thing to notice is that the posterior including the 71st experiment factorizes:\n",
    "\n",
    "$$p(\\theta_{71}, \\theta_{1..70}, \\alpha, \\beta \\vert  D)  \\propto p(y_{71} \\vert n_{71}, \\theta_{71} ) p(\\theta_{71} \\vert \\alpha, \\beta) p(\\theta_{1..70}, \\alpha, \\beta \\vert  D)$$\n",
    "\n",
    "Then we simply marginalize over everything to get the $\\theta_{71}$ posterior:\n",
    "\n",
    "$$p(\\theta_{71} \\vert \\theta_{1..70}, \\alpha, \\beta, D) = \\int d\\alpha \\,d\\beta \\,d\\theta_{1..70} \\,p(\\theta_{71}, \\theta_{1..70}, \\alpha, \\beta \\vert  D)$$\n",
    "\n",
    "$$=  \\int d\\alpha \\,d\\beta  Beta(\\alpha+y_{71}, \\beta + n_{71} - y_{71})  \\int_{\\theta_{1..70}} \\,d\\theta_{1..70} \\,p(\\theta_{1..70}, \\alpha, \\beta \\vert  D)$$\n",
    "\n",
    "The $y_{71}^*$ posterior predictive can be found in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a00c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rt:\n",
    "    rt_sim = pm.sample_ppc(rt_traces, samples=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a0002",
   "metadata": {},
   "source": [
    "#### 2. Plot histograms for these predictives with the actual value shown as a red-dot against the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b90f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 20))\n",
    "\n",
    "for i in range(70):\n",
    "    plt.subplot(10, 7, i + 1)\n",
    "    plt.hist(rt_sim['obs'].T[i], bins=10);\n",
    "    plt.plot(tumory[i] + 0.5, 200, 'ro');\n",
    "    plt.title('{}'.format(i + 1));\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b5315d",
   "metadata": {},
   "source": [
    "The data is consistent with the predictive for all 70 experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035ecd9",
   "metadata": {},
   "source": [
    "### Answer to Problem 3 Part C\n",
    "\n",
    "#### 1. Plot the posterior median of the death rate parameters against the observed death rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd7a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = rt_traces.get_values('theta')\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(tumory/tumorn, np.median(t, axis=0), \\\n",
    "             yerr=[np.percentile(t, 2.5, axis=0), np.percentile(t, 97.5, axis=0)],fmt='o', alpha=0.5);\n",
    "plt.plot([0, 0.5], [0, 0.5], 'k-');\n",
    "plt.xlabel('observed rates');\n",
    "plt.ylabel('posterior median of rate parameters');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5ca98",
   "metadata": {},
   "source": [
    "#### 2. Explain the shrinkage by comparing against a 45 degree line as done in the lab.\n",
    "\n",
    "The posterior rates are shrunk towards flatness, which would correspond to complete pooling. Here we put weakly informative hyper-priors on the hyper-parameters, and do a full bayesian analysis. In this case, low sample size experiments (which are statistically weak units) are regularized towards the prior. As a result, we observe the shrinkage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e86b9",
   "metadata": {},
   "source": [
    "### Answer to Problem 3 Part D\n",
    "#### 1. Calculate the marginal posterior of $\\theta_{71}$, the \"new\" experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dba0ff",
   "metadata": {},
   "source": [
    "## Problem 3:  Implementing Rat Tumors in pymc3\n",
    "\n",
    "(it may help to see the bioassay lab to see how to structure pymc3 code, and also the examples from lecture).\n",
    "\n",
    "Let us try to do full Bayesian inference with PyMC3 for the rat tumor example that we have solved using explicit Gibbs sampling in lab7. Remember that the goal is to estimate $\\theta_i$, the probability of developing a tumor in a population of female rats that have not received treatement. \n",
    "\n",
    "The posterior for the 70 experiments may be written thus:\n",
    "\n",
    "$$p( \\{\\theta_i\\}, \\alpha, \\beta  \\vert  Y, \\{n_i\\}) \\propto p(\\alpha, \\beta) \\prod_{i=1}^{70} Beta(\\theta_i, \\alpha, \\beta) \\prod_{i=1}^{70} Binom(n_i, y_i, \\theta_i)$$\n",
    "\n",
    "Use uniform priors on $[0,1]$ on the alternative variables $\\mu$ (the mean of the beta distribution) and $\\nu$:\n",
    "\n",
    "$$\\mu = \\frac{\\alpha}{\\alpha+\\beta}, \\nu = (\\alpha+\\beta)^{-1/2}$$\n",
    "\n",
    "You may then write $\\alpha$ and $\\beta$ as deterministics which depend on $\\mu$ and $\\nu$.\n",
    "\n",
    "Here is the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dadc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tumordata=\"\"\"0 20 \n",
    "0 20 \n",
    "0 20 \n",
    "0 20 \n",
    "0 20 \n",
    "0 20 \n",
    "0 20 \n",
    "0 19 \n",
    "0 19 \n",
    "0 19 \n",
    "0 19 \n",
    "0 18 \n",
    "0 18 \n",
    "0 17 \n",
    "1 20 \n",
    "1 20 \n",
    "1 20 \n",
    "1 20 \n",
    "1 19 \n",
    "1 19 \n",
    "1 18 \n",
    "1 18 \n",
    "3 27 \n",
    "2 25 \n",
    "2 24 \n",
    "2 23 \n",
    "2 20 \n",
    "2 20 \n",
    "2 20 \n",
    "2 20 \n",
    "2 20 \n",
    "2 20 \n",
    "1 10 \n",
    "5 49 \n",
    "2 19 \n",
    "5 46 \n",
    "2 17 \n",
    "7 49 \n",
    "7 47 \n",
    "3 20 \n",
    "3 20 \n",
    "2 13 \n",
    "9 48 \n",
    "10 50 \n",
    "4 20 \n",
    "4 20 \n",
    "4 20 \n",
    "4 20 \n",
    "4 20 \n",
    "4 20 \n",
    "4 20 \n",
    "10 48 \n",
    "4 19 \n",
    "4 19 \n",
    "4 19 \n",
    "5 22 \n",
    "11 46 \n",
    "12 49 \n",
    "5 20 \n",
    "5 20 \n",
    "6 23 \n",
    "5 19 \n",
    "6 22 \n",
    "6 20 \n",
    "6 20 \n",
    "6 20 \n",
    "16 52 \n",
    "15 46 \n",
    "15 47 \n",
    "9 24 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5fd0d",
   "metadata": {},
   "source": [
    "### Part A:  Report at least the following diagostics on your samples\n",
    "\n",
    "1. Autocorrelation (correlation dying by lag 20 is fine)\n",
    "2. Parameter trace correlation after burnin\n",
    "3. Gewecke\n",
    "4. Gelman-Rubin\n",
    "5. $n_{eff}$ (Number of Effective Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = rt_traces.get_values('mu')\n",
    "n = rt_traces.get_values('nu')\n",
    "a = m / (n * n)\n",
    "b = (1 - m) / (n * n)\n",
    "\n",
    "post71 = stats.beta.rvs(a + 4, b + 10)\n",
    "\n",
    "sns.distplot(post71);\n",
    "plt.xlabel(r'$\\theta_{71}$');\n",
    "plt.ylabel(r'marginal posterior of $\\theta_{71}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566429f",
   "metadata": {},
   "source": [
    "#### 2. Find the $y_{71}^*$ posterior predictive for that experiment."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
