{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9fb0f9",
   "metadata": {},
   "source": [
    "---\n",
    "```\n",
    "   Copyright 2016 Erik Jan de Vries\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af57c0",
   "metadata": {},
   "source": [
    "# OpenAI Gym Catch environment\n",
    "\n",
    "In this notebook we will implement an [OpenAI Gym](https://gym.openai.com/) environment for the game Catch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9567d9",
   "metadata": {},
   "source": [
    "## Catch\n",
    "\n",
    "Catch is a game in which fruit is dropping from a tree and you have to catch the fruit with your basket. The setup is very simple: the game area consists of a 10x10 grid. The fruit is in one cell, the basket covers three cells at the bottom of the game area. The fruit drops one cell per timestep and you can move the basket left or right (or you can choose not to move it). When the fruit reaches the bottom of the game area, it must be in de basket. You score one point for catching a piece of fruit; you lose a point for dropping the fruit next to the basket.\n",
    "\n",
    "<img alt=\"A game of Catch\" src=\"images/catch.png\" style=\"height: 200px\">\n",
    "\n",
    "While an observation consists of a 10x10 matrix of 0's and 1s, the entire state of the game can be described using only three numbers:\n",
    "\n",
    "1. The row number of the fruit\n",
    "2. The column number of the fruit\n",
    "3. The column number of the (middle of) the basket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bc43f",
   "metadata": {},
   "source": [
    "Let's begin by loading some required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6906673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "gym.undo_logger_setup()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(  format = '%(asctime)s %(name)s:%(levelname)s: %(message)s'\n",
    "                    , datefmt='%Y-%m-%d %H:%M:%S'\n",
    "                    , level  = logging.DEBUG)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gym import spaces\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b99e282",
   "metadata": {},
   "source": [
    "## OpenAI Gym environments\n",
    "\n",
    "An OpenAI Gym environment is a class based on [gym.Env](https://github.com/openai/gym/blob/master/gym/core.py). There is an excellent description of how to implement an environment in the documentation in the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f64a45",
   "metadata": {},
   "source": [
    "The main OpenAI Gym class encapsulates an environment with arbitrary behind-the-scenes dynamics. An environment can be partially or fully observed.\n",
    "\n",
    "The main API methods that users of this class need to know are:\n",
    "\n",
    "    step\n",
    "    reset\n",
    "    render\n",
    "    seed\n",
    "    close\n",
    "    configure\n",
    "\n",
    "When implementing an environment, always override the following methods in your subclass:\n",
    "\n",
    "    _step\n",
    "    _reset\n",
    "    _render\n",
    "    _seed\n",
    "\n",
    "and in some environments:\n",
    "\n",
    "    _close\n",
    "    _configure\n",
    "\n",
    "Always set the following attributes:\n",
    "\n",
    "    action_space:         The Space object corresponding to valid actions\n",
    "    observation_space:    The Space object corresponding to valid observations\n",
    "\n",
    "and sometimes:\n",
    "\n",
    "    reward_range:         A tuple corresponding to the min and max possible rewards\n",
    "\n",
    "The methods are accessed publicly as \"step\", \"reset\", etc.. The non-underscored versions are wrapper methods to which we may add functionality over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40a96b",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "\n",
    "So let's get going and set up the class one method/function at a time. First we define the required attributes in the init function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    log.info(\"Creating an OpenAI Gym environment to play Catch\");\n",
    "    self.grid_size = 10\n",
    "\n",
    "    self.action_space = spaces.Discrete(3)\n",
    "    self.observation_space = spaces.Discrete((self.grid_size,self.grid_size))\n",
    "    self.reward_range = (-1, 1)\n",
    "\n",
    "    self.seed()\n",
    "    self.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8916d",
   "metadata": {},
   "source": [
    "### _seed\n",
    "\n",
    "We will implement a simple and straightforward seed function, copied from the provided CartPoleEnv environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35312da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _seed(self, seed=None):\n",
    "    self.np_random, seed = seeding.np_random(seed)\n",
    "    return [seed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f5f474",
   "metadata": {},
   "source": [
    "### _reset\n",
    "\n",
    "Next we will define the reset function, which puts the fruit at a random position in the top row, and the basket in a random position in the bottom row.\n",
    "\n",
    "    \"\"\"\n",
    "    Resets the state of the environment and returns an initial observation.\n",
    "\n",
    "    Returns:\n",
    "        observation (object): the initial observation of the space. (Initial reward is assumed to be 0.)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reset(self):\n",
    "    n = np.random.randint(0, self.grid_size-1, size=1)\n",
    "    m = np.random.randint(1, self.grid_size-2, size=1)\n",
    "    self.state = np.asarray([0, n, m])\n",
    "    return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b428dbf",
   "metadata": {},
   "source": [
    "### _step\n",
    "\n",
    "The step function is the main function that takes the environment one step forward in time.\n",
    "\n",
    "    \"\"\"Run one timestep of the environment's dynamics. When end of\n",
    "    episode is reached, you are responsible for calling `reset()`\n",
    "    to reset this environment's state.\n",
    "\n",
    "    Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "\n",
    "    Args:\n",
    "        action (object): an action provided by the environment\n",
    "\n",
    "    Returns:\n",
    "        observation (object): agent's observation of the current environment\n",
    "        reward (float) : amount of reward returned after previous action\n",
    "        done (boolean): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "        info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e5c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _step(self, action):\n",
    "    if action == 0:\n",
    "        move = -1      # left\n",
    "    elif action == 1:\n",
    "        move = 0       # stay\n",
    "    else:\n",
    "        move = 1       # right\n",
    "\n",
    "    # Get current state\n",
    "    fruit_row, fruit_col, basket = self.state\n",
    "    # Transform state\n",
    "    fruit_row += 1\n",
    "    basket = min(max(1, basket + move), self.grid_size-1)\n",
    "    # Save new state\n",
    "    self.state = np.asarray([fruit_row, fruit_col, basket])\n",
    "\n",
    "    # Determine the observed new state\n",
    "    observation = self._get_observation()\n",
    "    # Determine if we are done\n",
    "    done = (fruit_row == self.grid_size-1)\n",
    "    # Determine the reward\n",
    "    reward = self._get_reward()\n",
    "    # Set information dictionary\n",
    "    info = {}\n",
    "\n",
    "    return observation, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf51c3",
   "metadata": {},
   "source": [
    "where we make use of the helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d91fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_observation(self):\n",
    "    # Get current state\n",
    "    fruit_row, fruit_col, basket = self.state\n",
    "    # Get observation\n",
    "    observation = np.zeros((self.grid_size, self.grid_size))\n",
    "    observation[fruit_row, fruit_col] = 1       # draw the fruit\n",
    "    observation[-1, (basket-1):(basket+2)] = 1  # draw the basket\n",
    "    return observation\n",
    "\n",
    "def _get_reward(self):\n",
    "    # Get current state\n",
    "    fruit_row, fruit_col, basket = self.state\n",
    "    # Get reward\n",
    "    if fruit_row == self.grid_size-1:\n",
    "        if abs(fruit_col - basket) <= 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549f3b3",
   "metadata": {},
   "source": [
    "### _render\n",
    "\n",
    "We include a simple rendering function with three rendering modes:\n",
    "\n",
    "**human**<br/>\n",
    "renders the observation on screen using matplotlib\n",
    "\n",
    "**matplotlib**<br/>\n",
    "plots the observation using matplotlib, so we can save the image to disk\n",
    "\n",
    "**rgb_array**<br/>\n",
    "returns a numpy array with RGB values representing the observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e82a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _render(self, mode='human', close=False):\n",
    "    if close:\n",
    "        if mode == 'human':\n",
    "            # close all matplotlib screens\n",
    "            pass\n",
    "        return\n",
    "    if mode == 'human':\n",
    "        self._plot_observation()\n",
    "        plt.show()\n",
    "        return\n",
    "    if mode == 'matplotlib':\n",
    "        self._plot_observation()\n",
    "        return\n",
    "    if mode == 'rgb_array':\n",
    "        rgb_array = self._get_observation_rgb();\n",
    "        return rgb_array;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce78651",
   "metadata": {},
   "source": [
    "Here we make use of two helper functions for creating an RGB-array of the observation, and for consistent rendering of the matplotlib images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10bca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_observation(self):\n",
    "    plt.imshow(  self._get_observation_rgb()\n",
    "               , interpolation='none'\n",
    "               )\n",
    "    plt.tick_params(\n",
    "          axis='both'        # changes apply to both the x-axis and the y-axis\n",
    "        , which='both'       # both major and minor ticks are affected\n",
    "        , bottom='off'       # ticks  along the bottom edge are off\n",
    "        , top='off'          # ticks  along the top    edge are off\n",
    "        , left='off'         # ticks  along the left   edge are off\n",
    "        , right='off'        # ticks  along the right  edge are off\n",
    "        , labelbottom='off'  # labels along the bottom edge are off\n",
    "        , labeltop='off'     # labels along the top    edge are off\n",
    "        , labelleft='off'    # labels along the left   edge are off\n",
    "        , labelright='off'   # labels along the right  edge are off\n",
    "        );\n",
    "\n",
    "def _get_observation_rgb(self):\n",
    "    # Get current state\n",
    "    fruit_row, fruit_col, basket = self.state\n",
    "    # Get observation\n",
    "    observation_rgb = np.zeros((self.grid_size, self.grid_size, 3), dtype='uint8')\n",
    "    # draw the basket\n",
    "    observation_rgb[-1, (basket-1):(basket+2), 0] = 87\n",
    "    observation_rgb[-1, (basket-1):(basket+2), 1] = 45\n",
    "    observation_rgb[-1, (basket-1):(basket+2), 2] = 9\n",
    "    # draw the fruit\n",
    "    observation_rgb[fruit_row, fruit_col,0] += 102\n",
    "    observation_rgb[fruit_row, fruit_col,1] += 141\n",
    "    observation_rgb[fruit_row, fruit_col,2] += 60\n",
    "    return observation_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702cf300",
   "metadata": {},
   "source": [
    "## The Catch environment\n",
    "\n",
    "Putting everything together, we get the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catch(gym.Env):\n",
    "    \"\"\"Catch environment for the OpenAI Gym\"\"\"\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array', 'matplotlib']\n",
    "    }\n",
    "\n",
    "Catch.__init__ = __init__\n",
    "Catch._seed    = _seed\n",
    "Catch._reset   = _reset\n",
    "Catch._step    = _step\n",
    "Catch._render  = _render\n",
    "\n",
    "Catch._get_observation      = _get_observation\n",
    "Catch._get_observation_rgb  = _get_observation_rgb\n",
    "Catch._get_reward           = _get_reward\n",
    "Catch._plot_observation     = _plot_observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed97a4",
   "metadata": {},
   "source": [
    "## Let's play\n",
    "\n",
    "First we have to register our new environment at the gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b4fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='Catch-v0',\n",
    "    entry_point='catch:Catch',\n",
    "    timestep_limit=200,\n",
    "    reward_threshold=25.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e652e",
   "metadata": {},
   "source": [
    "Then we can create a Catch environment using the gym's make function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Catch-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d95674",
   "metadata": {},
   "source": [
    "Finally we can render output of the game, in this case using random actions by sampling from the action space."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
