{
    "User Guide->Background->endog, exog, what\u2019s that?": [
        [
            "statsmodels is using endog and exog as names for the data, the\nobserved variables that are used in an estimation problem. Other names that\nare often used in different statistical packages or text books are, for\nexample,",
            "markdown"
        ],
        [
            "The usage is quite often domain and model specific; however, we have chosen\nto use endog and exog almost exclusively. A mnemonic hint to keep the two\nterms apart is that exogenous has an \u201cx\u201d, as in x-variable, in its name.",
            "markdown"
        ],
        [
            "x and y are one letter names that are sometimes used for temporary\nvariables and are not informative in itself. To avoid one letter names we\ndecided to use descriptive names and settled on endog and exog.\nSince this has been criticized, this might change in future.",
            "markdown"
        ]
    ],
    "User Guide->Background->endog, exog, what\u2019s that?->Background": [
        [
            "Some informal definitions of the terms are",
            "markdown"
        ],
        [
            "endogenous: caused by factors within the system",
            "markdown"
        ],
        [
            "exogenous: caused by factors outside the system",
            "markdown"
        ],
        [
            "Endogenous variables designates variables in an economic/econometric model\nthat are explained, or predicted, by that model.",
            "markdown"
        ],
        [
            "Exogenous variables designates variables that appear in an\neconomic/econometric model, but are not explained by that model (i.e. they are\ntaken as given by the model).",
            "markdown"
        ],
        [
            "In econometrics and statistics the terms are defined more formally, and\ndifferent definitions of exogeneity (weak, strong, strict) are used depending\non the model. The usage in statsmodels as variable names cannot always be\ninterpreted in a formal sense, but tries to follow the same principle.",
            "markdown"
        ],
        [
            "In the simplest form, a model relates an observed variable, y, to another set\nof variables, x, in some linear or nonlinear form",
            "markdown"
        ],
        [
            "y = f(x, beta) + noise\ny = x * beta + noise",
            "code"
        ],
        [
            "However, to have a statistical model we need additional assumptions on the\nproperties of the explanatory variables, x, and the noise. One standard\nassumption for many basic models is that x is not correlated with the noise.\nIn a more general definition, x being exogenous means that we do not have to\nconsider how the explanatory variables in x were generated, whether by design\nor by random draws from some underlying distribution, when we want to estimate\nthe effect or impact that x has on y, or test a hypothesis about this effect.",
            "markdown"
        ],
        [
            "In other words, y is endogenous to our model, x is exogenous to our model\nfor the estimation.",
            "markdown"
        ],
        [
            "As an example, suppose you run an experiment and for the second session some\nsubjects are not available anymore.\nIs the drop-out relevant for the conclusions you draw for the experiment?\nIn other words, can we treat the drop-out decision as exogenous for our\nproblem.",
            "markdown"
        ],
        [
            "It is up to the user to know (or to consult a text book to find out) what the\nunderlying statistical assumptions for the models are. As an example, exog\nin OLS can have lagged dependent variables if the error or noise term is\nindependently distributed over time (or uncorrelated over time). However, if\nthe error terms are autocorrelated, then OLS does not have good statistical\nproperties (is inconsistent) and the correct model will be ARMAX.\nstatsmodels has functions for regression diagnostics to test whether some of\nthe assumptions are justified or not.",
            "markdown"
        ]
    ],
    "User Guide->Background->Import Paths and Structure": [
        [
            "We offer two ways of importing functions and classes from statsmodels:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "Allows tab completion",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "Avoids importing unnecessary modules and commands",
            "markdown"
        ]
    ],
    "User Guide->Background->Import Paths and Structure->API Import for interactive use": [
        [
            "For interactive use the recommended import is:",
            "markdown"
        ],
        [
            "import statsmodels.api as sm",
            "code"
        ],
        [
            "Importing statsmodels.api will load most of the public parts of statsmodels.\nThis makes most functions and classes conveniently available within one or two\nlevels, without making the \u201csm\u201d namespace too crowded.",
            "markdown"
        ],
        [
            "To see what functions and classes available, you can type the following (or use\nthe namespace exploration features of IPython, Spyder, IDLE, etc.):",
            "markdown"
        ],
        [
            "dir(sm)\n['GLM', 'GLS', 'GLSAR', 'Logit', 'MNLogit', 'OLS', 'Poisson', 'Probit', 'RLM',\n'WLS', '__builtins__', '__doc__', '__file__', '__name__', '__package__',\n'add_constant', 'categorical', 'datasets', 'distributions', 'families',\n'graphics', 'iolib', 'nonparametric', 'qqplot', 'regression', 'robust',\n'stats', 'test', 'tools', 'tsa', 'version']\n\n dir(sm.graphics)\n['__builtins__', '__doc__', '__file__', '__name__', '__package__',\n'abline_plot', 'beanplot', 'fboxplot', 'interaction_plot', 'qqplot',\n'rainbow', 'rainbowplot', 'violinplot']\n\n dir(sm.tsa)\n['AR', 'ARMA', 'SVAR', 'VAR', '__builtins__', '__doc__',\n'__file__', '__name__', '__package__', 'acf', 'acovf', 'add_lag',\n'add_trend', 'adfuller', 'ccf', 'ccovf', 'datetools', 'detrend',\n'filters', 'grangercausalitytests', 'interp', 'lagmat', 'lagmat2ds', 'kpss',\n'pacf', 'pacf_ols', 'pacf_yw', 'periodogram', 'q_stat', 'range_unit_root_test',\n'stattools', 'tsatools', 'var']",
            "code"
        ]
    ],
    "User Guide->Background->Import Paths and Structure->API Import for interactive use->Notes": [
        [
            "The api modules may not include all the public functionality of statsmodels. If\nyou find something that should be added to the api, please file an issue on\ngithub or report it to the mailing list.",
            "markdown"
        ],
        [
            "The subpackages of statsmodels include api.py modules that are mainly\nintended to collect the imports needed for those subpackages. The subpackage/api.py\nfiles are imported into statsmodels api, for example",
            "markdown"
        ],
        [
            "from .nonparametric import api as nonparametric",
            "code"
        ],
        [
            "Users do not need to load the subpackage/api.py modules directly.",
            "markdown"
        ]
    ],
    "User Guide->Background->Import Paths and Structure->Direct import for programs": [
        [
            "statsmodels submodules are arranged by topic (e.g. discrete for discrete\nchoice models, or tsa for time series analysis). Our directory tree (stripped\ndown) looks something like this:",
            "markdown"
        ],
        [
            "statsmodels/\n    __init__.py\n    api.py\n    discrete/\n        __init__.py\n        discrete_model.py\n        tests/\n            results/\n    tsa/\n        __init__.py\n        api.py\n        tsatools.py\n        stattools.py\n        arima_process.py\n        vector_ar/\n            __init__.py\n            var_model.py\n            tests/\n                results/\n        tests/\n            results/\n    stats/\n        __init__.py\n        api.py\n        stattools.py\n        tests/\n    tools/\n        __init__.py\n        tools.py\n        decorators.py\n        tests/",
            "code"
        ],
        [
            "The submodules that can be import heavy contain an empty __init__.py, except\nfor some testing code for running tests for the submodules. The intention is to\nchange all directories to have an api.py and empty __init__.py in the next\nrelease.",
            "markdown"
        ]
    ],
    "User Guide->Background->Import Paths and Structure->Direct import for programs->Import examples": [
        [
            "Functions and classes:",
            "markdown"
        ],
        [
            "from statsmodels.regression.linear_model import OLS, WLS\nfrom statsmodels.tools.tools import rank, add_constant",
            "code"
        ],
        [
            "Modules",
            "markdown"
        ],
        [
            "from statsmodels.datasets import macrodata\nimport statsmodels.stats import diagnostic",
            "code"
        ],
        [
            "Modules with aliases",
            "markdown"
        ],
        [
            "import statsmodels.regression.linear_model as lm\nimport statsmodels.stats.diagnostic as smsdia\nimport statsmodels.stats.outliers_influence as oi",
            "code"
        ],
        [
            "We do not have currently a convention for aliases of submodules.",
            "markdown"
        ]
    ],
    "User Guide->Background->Fitting models using R-style formulas": [
        [
            "Since version 0.5.0, statsmodels allows users to fit statistical\nmodels using R-style formulas. Internally, statsmodels uses the\n package to convert formulas and\ndata to the matrices that are used in model fitting. The formula\nframework is quite powerful; this tutorial only scratches the surface. A\nfull description of the formula language can be found in the patsy\ndocs:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "User Guide->Background->Fitting models using R-style formulas->Loading modules and functions": [
        [
            "In [1]: import statsmodels.api as sm\n\nIn [2]: import statsmodels.formula.api as smf\n\nIn [3]: import numpy as np\n\nIn [4]: import pandas",
            "code"
        ],
        [
            "Notice that we called statsmodels.formula.api in addition to the usual\nstatsmodels.api. In fact, statsmodels.api is used here only to load\nthe dataset. The formula.api hosts many of the same\nfunctions found in api (e.g. OLS, GLM), but it also holds lower case\ncounterparts for most of these models. In general, lower case models\naccept formula and df arguments, whereas upper case ones take\nendog and exog design matrices. formula accepts a string\nwhich describes the model in terms of a patsy formula. df takes\na  data frame.",
            "markdown"
        ],
        [
            "dir(smf) will print a list of available models.",
            "markdown"
        ],
        [
            "Formula-compatible models have the following generic call signature:\n(formula, data, subset=None, *args, **kwargs)",
            "markdown"
        ]
    ],
    "User Guide->Background->Fitting models using R-style formulas->OLS regression using formulas": [
        [
            "To begin, we fit the linear model described on the  page. Download the data, subset columns,\nand list-wise delete to remove missing observations:",
            "markdown"
        ],
        [
            "In [5]: df = sm.datasets.get_rdataset(\"Guerry\", \"HistData\").data\n\nIn [6]: df = df[['Lottery', 'Literacy', 'Wealth', 'Region']].dropna()\n\nIn [7]: df.head()\nOut[7]: \n   Lottery  Literacy  Wealth Region\n0       41        37      73      E\n1       38        51      22      N\n2       66        13      61      C\n3       80        46      76      E\n4       79        69      83      E",
            "code"
        ],
        [
            "Fit the model:",
            "markdown"
        ],
        [
            "In [8]: mod = smf.ols(formula='Lottery ~ Literacy + Wealth + Region', data=df)\n\nIn [9]: res = mod.fit()\n\nIn [10]: print(res.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                Lottery   R-squared:                       0.338\nModel:                            OLS   Adj. R-squared:                  0.287\nMethod:                 Least Squares   F-statistic:                     6.636\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           1.07e-05\nTime:                        17:12:34   Log-Likelihood:                -375.30\nNo. Observations:                  85   AIC:                             764.6\nDf Residuals:                      78   BIC:                             781.7\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      38.6517      9.456      4.087      0.000      19.826      57.478\nRegion[T.E]   -15.4278      9.727     -1.586      0.117     -34.793       3.938\nRegion[T.N]   -10.0170      9.260     -1.082      0.283     -28.453       8.419\nRegion[T.S]    -4.5483      7.279     -0.625      0.534     -19.039       9.943\nRegion[T.W]   -10.0913      7.196     -1.402      0.165     -24.418       4.235\nLiteracy       -0.1858      0.210     -0.886      0.378      -0.603       0.232\nWealth          0.4515      0.103      4.390      0.000       0.247       0.656\n==============================================================================\nOmnibus:                        3.049   Durbin-Watson:                   1.785\nProb(Omnibus):                  0.218   Jarque-Bera (JB):                2.694\nSkew:                          -0.340   Prob(JB):                        0.260\nKurtosis:                       2.454   Cond. No.                         371.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ]
    ],
    "User Guide->Background->Fitting models using R-style formulas->Categorical variables": [
        [
            "Looking at the summary printed above, notice that patsy determined\nthat elements of Region were text strings, so it treated Region as a\ncategorical variable. patsy\u2019s default is also to include an\nintercept, so we automatically dropped one of the Region categories.",
            "markdown"
        ],
        [
            "If Region had been an integer variable that we wanted to treat\nexplicitly as categorical, we could have done so by using the C()\noperator:",
            "markdown"
        ],
        [
            "In [11]: res = smf.ols(formula='Lottery ~ Literacy + Wealth + C(Region)', data=df).fit()\n\nIn [12]: print(res.params)\nIntercept         38.651655\nC(Region)[T.E]   -15.427785\nC(Region)[T.N]   -10.016961\nC(Region)[T.S]    -4.548257\nC(Region)[T.W]   -10.091276\nLiteracy          -0.185819\nWealth             0.451475\ndtype: float64",
            "code"
        ],
        [
            "Examples more advanced features patsy\u2019s categorical variables\nfunction can be found here:",
            "markdown"
        ]
    ],
    "User Guide->Background->Fitting models using R-style formulas->Operators": [
        [
            "We have already seen that \u201c~\u201d separates the left-hand side of the model\nfrom the right-hand side, and that \u201c+\u201d adds new columns to the design\nmatrix.",
            "markdown"
        ]
    ],
    "User Guide->Background->Fitting models using R-style formulas->Operators->Removing variables": [
        [
            "The \u201c-\u201d sign can be used to remove columns/variables. For instance, we\ncan remove the intercept from a model by:",
            "markdown"
        ],
        [
            "In [13]: res = smf.ols(formula='Lottery ~ Literacy + Wealth + C(Region) -1 ', data=df).fit()\n\nIn [14]: print(res.params)\nC(Region)[C]    38.651655\nC(Region)[E]    23.223870\nC(Region)[N]    28.634694\nC(Region)[S]    34.103399\nC(Region)[W]    28.560379\nLiteracy        -0.185819\nWealth           0.451475\ndtype: float64",
            "code"
        ]
    ],
    "User Guide->Background->Fitting models using R-style formulas->Operators->Multiplicative interactions": [
        [
            "\u201c:\u201d adds a new column to the design matrix with the product of the other\ntwo columns. \u201c*\u201d will also include the individual columns that were\nmultiplied together:",
            "markdown"
        ],
        [
            "In [15]: res1 = smf.ols(formula='Lottery ~ Literacy : Wealth - 1', data=df).fit()\n\nIn [16]: res2 = smf.ols(formula='Lottery ~ Literacy * Wealth - 1', data=df).fit()\n\nIn [17]: print(res1.params)\nLiteracy:Wealth    0.018176\ndtype: float64\n\nIn [18]: print(res2.params)\nLiteracy           0.427386\nWealth             1.080987\nLiteracy:Wealth   -0.013609\ndtype: float64",
            "code"
        ],
        [
            "Many other things are possible with operators. Please consult the  to learn\nmore.",
            "markdown"
        ]
    ],
    "User Guide->Background->Fitting models using R-style formulas->Functions": [
        [
            "You can apply vectorized functions to the variables in your model:",
            "markdown"
        ],
        [
            "In [19]: res = smf.ols(formula='Lottery ~ np.log(Literacy)', data=df).fit()\n\nIn [20]: print(res.params)\nIntercept           115.609119\nnp.log(Literacy)    -20.393959\ndtype: float64",
            "code"
        ],
        [
            "Define a custom function:",
            "markdown"
        ],
        [
            "In [21]: def log_plus_1(x):\n   ....:     return np.log(x) + 1.0\n   ....: \n\nIn [22]: res = smf.ols(formula='Lottery ~ log_plus_1(Literacy)', data=df).fit()\n\nIn [23]: print(res.params)\nIntercept               136.003079\nlog_plus_1(Literacy)    -20.393959\ndtype: float64",
            "code"
        ]
    ],
    "User Guide->Background->Fitting models using R-style formulas->Namespaces": [
        [
            "Notice that all of the above examples use the calling namespace to look for the functions to apply. The namespace used can be controlled via the eval_env keyword. For example, you may want to give a custom namespace using the patsy:patsy.EvalEnvironment or you may want to use a \u201cclean\u201d namespace, which we provide by passing eval_func=-1. The default is to use the caller\u2019s namespace. This can have (un)expected consequences, if, for example, someone has a variable names C in the user namespace or in their data structure passed to patsy, and C is used in the formula to handle a categorical variable. See the  for more information.",
            "markdown"
        ]
    ],
    "User Guide->Background->Fitting models using R-style formulas->Using formulas with models that do not (yet) support them": [
        [
            "Even if a given statsmodels function does not support formulas, you\ncan still use patsy\u2019s formula language to produce design matrices.\nThose matrices can then be fed to the fitting function as endog and\nexog arguments.",
            "markdown"
        ],
        [
            "To generate numpy arrays:",
            "markdown"
        ],
        [
            "In [24]: import patsy\n\nIn [25]: f = 'Lottery ~ Literacy * Wealth'\n\nIn [26]: y, X = patsy.dmatrices(f, df, return_type='matrix')\n\nIn [27]: print(y[:5])\n[[41.]\n [38.]\n [66.]\n [80.]\n [79.]]\n\nIn [28]: print(X[:5])\n[[1.000e+00 3.700e+01 7.300e+01 2.701e+03]\n [1.000e+00 5.100e+01 2.200e+01 1.122e+03]\n [1.000e+00 1.300e+01 6.100e+01 7.930e+02]\n [1.000e+00 4.600e+01 7.600e+01 3.496e+03]\n [1.000e+00 6.900e+01 8.300e+01 5.727e+03]]",
            "code"
        ],
        [
            "y and X would be instances of patsy.DesignMatrix which is a subclass of numpy.ndarray.",
            "markdown"
        ],
        [
            "To generate pandas data frames:",
            "markdown"
        ],
        [
            "In [29]: f = 'Lottery ~ Literacy * Wealth'\n\nIn [30]: y, X = patsy.dmatrices(f, df, return_type='dataframe')\n\nIn [31]: print(y[:5])\n   Lottery\n0     41.0\n1     38.0\n2     66.0\n3     80.0\n4     79.0\n\nIn [32]: print(X[:5])\n   Intercept  Literacy  Wealth  Literacy:Wealth\n0        1.0      37.0    73.0           2701.0\n1        1.0      51.0    22.0           1122.0\n2        1.0      13.0    61.0            793.0\n3        1.0      46.0    76.0           3496.0\n4        1.0      69.0    83.0           5727.0",
            "code"
        ],
        [
            "In [33]: print(sm.OLS(y, X).fit().summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                Lottery   R-squared:                       0.309\nModel:                            OLS   Adj. R-squared:                  0.283\nMethod:                 Least Squares   F-statistic:                     12.06\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           1.32e-06\nTime:                        17:12:34   Log-Likelihood:                -377.13\nNo. Observations:                  85   AIC:                             762.3\nDf Residuals:                      81   BIC:                             772.0\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nIntercept          38.6348     15.825      2.441      0.017       7.149      70.121\nLiteracy           -0.3522      0.334     -1.056      0.294      -1.016       0.312\nWealth              0.4364      0.283      1.544      0.126      -0.126       0.999\nLiteracy:Wealth    -0.0005      0.006     -0.085      0.933      -0.013       0.012\n==============================================================================\nOmnibus:                        4.447   Durbin-Watson:                   1.953\nProb(Omnibus):                  0.108   Jarque-Bera (JB):                3.228\nSkew:                          -0.332   Prob(JB):                        0.199\nKurtosis:                       2.314   Cond. No.                     1.40e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.4e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
            "code"
        ]
    ],
    "User Guide->Background->Pitfalls": [
        [
            "This page lists issues which may arise while using statsmodels. These\ncan be the result of data-related or statistical problems, software design,\n\u201cnon-standard\u201d use of models, or edge cases.",
            "markdown"
        ],
        [
            "statsmodels provides several warnings and helper functions for diagnostic\nchecking (see this \nfor an example of misspecification checks in linear regression). The coverage\nis of course not comprehensive, but more warnings and diagnostic functions will\nbe added over time.",
            "markdown"
        ],
        [
            "While the underlying statistical problems are the same for all statistical\npackages, software implementations differ in the way extreme or corner cases\nare handled. Please report corner cases for which the models might not work, so\nwe can treat them appropriately.",
            "markdown"
        ]
    ],
    "User Guide->Background->Pitfalls->Repeated calls to fit with different parameters": [
        [
            "Result instances often need to access attributes from the corresponding model\ninstance. Fitting a model multiple times with different arguments can change\nmodel attributes. This means that the result instance may no longer point to\nthe correct model attributes after the model has been re-fit.",
            "markdown"
        ],
        [
            "It is therefore best practice to create separate model instances when we want\nto fit a model using different fit function arguments.",
            "markdown"
        ],
        [
            "For example, this works without problem because we are not keeping the results\ninstance for further use",
            "markdown"
        ],
        [
            "mod = AR(endog)\naic = []\nfor lag in range(1,11):\n    res = mod.fit(maxlag=lag)\n    aic.append(res.aic)",
            "code"
        ],
        [
            "However, when we want to hold on to two different estimation results, then it\nis recommended to create two separate model instances.",
            "markdown"
        ],
        [
            "mod1 = RLM(endog, exog)\nres1 = mod1.fit(scale_est='mad')\nmod2 = RLM(endog, exog)\nres2 = mod2.fit(scale_est=sm.robust.scale.HuberScale())",
            "code"
        ]
    ],
    "User Guide->Background->Pitfalls->Unidentified Parameters->Rank deficient exog, perfect multicollinearity": [
        [
            "Models based on linear models, GLS, RLM, GLM and similar, use a generalized\ninverse. This means that:",
            "markdown"
        ],
        [
            "Rank deficient matrices will not raise an error",
            "markdown"
        ],
        [
            "Cases of almost perfect multicollinearity or ill-conditioned design matrices might produce numerically unstable results. Users need to manually check the rank or condition number of the matrix if this is not the desired behavior",
            "markdown"
        ],
        [
            "Note: statsmodels currently fails on the NIST benchmark case for Filip if the\ndata is not rescaled, see",
            "markdown"
        ]
    ],
    "User Guide->Background->Pitfalls->Unidentified Parameters->Incomplete convergence in maximum likelihood estimation": [
        [
            "In some cases, the maximum likelihood estimator might not exist, parameters\nmight be infinite or not unique (e.g. (quasi-)separation in models with binary\nendogenous variable). Under the default settings, statsmodels will print\na warning if the optimization algorithm stops without reaching convergence.\nHowever, it is important to know that the convergence criteria may sometimes\nfalsely indicate convergence (e.g. if the value of the objective function\nconverged but not the parameters). In general, a user needs to verify\nconvergence.",
            "markdown"
        ],
        [
            "For binary Logit and Probit models, statsmodels raises an exception if perfect\nprediction is detected. There is, however, no check for quasi-perfect\nprediction.",
            "markdown"
        ]
    ],
    "User Guide->Background->Pitfalls->Other Problems->Insufficient variation in the data": [
        [
            "It is possible that there is insufficient variation in the data for small\ndatasets or for data with small groups in categorical variables. In these\ncases, the results might not be identified or some hidden problems might occur.",
            "markdown"
        ],
        [
            "The only currently known case is a perfect fit in robust linear model estimation.\nFor RLM, if residuals are equal to zero, then it does not cause an exception,\nbut having this perfect fit can produce NaNs in some results (scale=0 and 0/0\ndivision) (issue #55).",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Linear Regression": [
        [
            "Linear models with independently and identically distributed errors, and for\nerrors with heteroscedasticity or autocorrelation. This module allows\nestimation by ordinary least squares (OLS), weighted least squares (WLS),\ngeneralized least squares (GLS), and feasible generalized least squares with\nautocorrelated AR(p) errors.",
            "markdown"
        ],
        [
            "See  for commands and arguments.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Linear Regression->Examples": [
        [
            "# Load modules and data\nIn [1]: import numpy as np\n\nIn [2]: import statsmodels.api as sm\n\nIn [3]: spector_data = sm.datasets.spector.load()\n\nIn [4]: spector_data.exog = sm.add_constant(spector_data.exog, prepend=False)\n\n# Fit and summarize OLS model\nIn [5]: mod = sm.OLS(spector_data.endog, spector_data.exog)\n\nIn [6]: res = mod.fit()\n\nIn [7]: print(res.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  GRADE   R-squared:                       0.416\nModel:                            OLS   Adj. R-squared:                  0.353\nMethod:                 Least Squares   F-statistic:                     6.646\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):            0.00157\nTime:                        17:12:47   Log-Likelihood:                -12.978\nNo. Observations:                  32   AIC:                             33.96\nDf Residuals:                      28   BIC:                             39.82\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGPA            0.4639      0.162      2.864      0.008       0.132       0.796\nTUCE           0.0105      0.019      0.539      0.594      -0.029       0.050\nPSI            0.3786      0.139      2.720      0.011       0.093       0.664\nconst         -1.4980      0.524     -2.859      0.008      -2.571      -0.425\n==============================================================================\nOmnibus:                        0.176   Durbin-Watson:                   2.346\nProb(Omnibus):                  0.916   Jarque-Bera (JB):                0.167\nSkew:                           0.141   Prob(JB):                        0.920\nKurtosis:                       2.786   Cond. No.                         176.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "Detailed examples can be found here:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Linear Regression->Technical Documentation": [
        [
            "The statistical model is assumed to be",
            "markdown"
        ],
        [
            "\\(Y = X\\beta + \\mu\\),  where \\(\\mu\\sim N\\left(0,\\Sigma\\right).\\)\n</blockquote>",
            "markdown"
        ],
        [
            "Depending on the properties of \\(\\Sigma\\), we have currently four classes available:",
            "markdown"
        ],
        [
            "GLS : generalized least squares for arbitrary covariance \\(\\Sigma\\)",
            "markdown"
        ],
        [
            "OLS : ordinary least squares for i.i.d. errors \\(\\Sigma=\\textbf{I}\\)",
            "markdown"
        ],
        [
            "WLS : weighted least squares for heteroskedastic errors \\(\\text{diag}\\left  (\\Sigma\\right)\\)",
            "markdown"
        ],
        [
            "GLSAR : feasible generalized least squares with autocorrelated AR(p) errors\n\\(\\Sigma=\\Sigma\\left(\\rho\\right)\\)",
            "markdown"
        ],
        [
            "All regression models define the same methods and follow the same structure,\nand can be used in a similar fashion. Some of them contain additional model\nspecific methods and attributes.",
            "markdown"
        ],
        [
            "GLS is the superclass of the other regression classes except for RecursiveLS,\nRollingWLS and RollingOLS.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Linear Regression->Technical Documentation->References": [
        [
            "General reference for regression models:",
            "markdown"
        ],
        [
            "D.C. Montgomery and E.A. Peck. \u201cIntroduction to Linear Regression Analysis.\u201d 2nd. Ed., Wiley, 1992.",
            "markdown"
        ],
        [
            "Econometrics references for regression models:",
            "markdown"
        ],
        [
            "R.Davidson and J.G. MacKinnon. \u201cEconometric Theory and Methods,\u201d Oxford, 2004.",
            "markdown"
        ],
        [
            "W.Green. \u201cEconometric Analysis,\u201d 5th ed., Pearson, 2003.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Linear Regression->Technical Documentation->Attributes": [
        [
            "The following is more verbose description of the attributes which is mostly\ncommon to all regression classes\n\npinv_wexogarray",
            "markdown"
        ],
        [
            "The p x n Moore-Penrose pseudoinverse of the whitened design matrix.\nIt is approximately equal to\n\\(\\left(X^{T}\\Sigma^{-1}X\\right)^{-1}X^{T}\\Psi\\), where\n\\(\\Psi\\) is defined such that \\(\\Psi\\Psi^{T}=\\Sigma^{-1}\\).\n\ncholsimgainvarray",
            "markdown"
        ],
        [
            "The n x n upper triangular matrix \\(\\Psi^{T}\\) that satisfies\n\\(\\Psi\\Psi^{T}=\\Sigma^{-1}\\).\n\ndf_modelfloat",
            "markdown"
        ],
        [
            "The model degrees of freedom. This is equal to p - 1, where p is the\nnumber of regressors. Note that the intercept is not counted as using a\ndegree of freedom here.\n\ndf_residfloat",
            "markdown"
        ],
        [
            "The residual degrees of freedom. This is equal n - p where n is the\nnumber of observations and p is the number of parameters. Note that the\nintercept is counted as using a degree of freedom here.\n\nllffloat",
            "markdown"
        ],
        [
            "The value of the likelihood function of the fitted model.\n\nnobsfloat",
            "markdown"
        ],
        [
            "The number of observations n\n\nnormalized_cov_paramsarray",
            "markdown"
        ],
        [
            "A p x p array equal to \\((X^{T}\\Sigma^{-1}X)^{-1}\\).\n\nsigmaarray",
            "markdown"
        ],
        [
            "The n x n covariance matrix of the error terms:\n\\(\\mu\\sim N\\left(0,\\Sigma\\right)\\).\n\nwexogarray",
            "markdown"
        ],
        [
            "The whitened design matrix \\(\\Psi^{T}X\\).\n\nwendogarray",
            "markdown"
        ],
        [
            "The whitened response variable \\(\\Psi^{T}Y\\).\n\n</dl>",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Linear Regression->Module Reference": [
        [
            "Model Classes",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Linear Regression->Module Reference->Results Classes": [
        [
            "Fitting a linear regression model returns a results class. OLS has a\nspecific results class with some additional methods compared to the\nresults class of the other linear models.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Linear Models": [
        [
            "Generalized linear models currently supports estimation using the one-parameter\nexponential families.",
            "markdown"
        ],
        [
            "See  for commands and arguments.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Linear Models->Examples": [
        [
            "# Load modules and data\nIn [1]: import statsmodels.api as sm\n\nIn [2]: data = sm.datasets.scotland.load()\n\nIn [3]: data.exog = sm.add_constant(data.exog)\n\n# Instantiate a gamma family model with the default link function.\nIn [4]: gamma_model = sm.GLM(data.endog, data.exog, family=sm.families.Gamma())\n\nIn [5]: gamma_results = gamma_model.fit()\n\nIn [6]: print(gamma_results.summary())\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                    YES   No. Observations:                   32\nModel:                            GLM   Df Residuals:                       24\nModel Family:                   Gamma   Df Model:                            7\nLink Function:          inverse_power   Scale:                       0.0035843\nMethod:                          IRLS   Log-Likelihood:                -83.017\nDate:                Wed, 02 Nov 2022   Deviance:                     0.087389\nTime:                        17:12:43   Pearson chi2:                   0.0860\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.9800\nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nconst                 -0.0178      0.011     -1.548      0.122      -0.040       0.005\nCOUTAX              4.962e-05   1.62e-05      3.060      0.002    1.78e-05    8.14e-05\nUNEMPF                 0.0020      0.001      3.824      0.000       0.001       0.003\nMOR                -7.181e-05   2.71e-05     -2.648      0.008      -0.000   -1.87e-05\nACT                    0.0001   4.06e-05      2.757      0.006    3.23e-05       0.000\nGDP                -1.468e-07   1.24e-07     -1.187      0.235   -3.89e-07    9.56e-08\nAGE                   -0.0005      0.000     -2.159      0.031      -0.001   -4.78e-05\nCOUTAX_FEMALEUNEMP -2.427e-06   7.46e-07     -3.253      0.001   -3.89e-06   -9.65e-07\n======================================================================================",
            "code"
        ],
        [
            "Detailed examples can be found here:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Linear Models->Technical Documentation": [
        [
            "The statistical model for each observation \\(i\\) is assumed to be",
            "markdown"
        ],
        [
            "\\(Y_i \\sim F_{EDM}(\\cdot|\\theta,\\phi,w_i)\\) and\n\\(\\mu_i = E[Y_i|x_i] = g^{-1}(x_i^\\prime\\beta)\\).\n</blockquote>",
            "markdown"
        ],
        [
            "where \\(g\\) is the link function and \\(F_{EDM}(\\cdot|\\theta,\\phi,w)\\)\nis a distribution of the family of exponential dispersion models (EDM) with\nnatural parameter \\(\\theta\\), scale parameter \\(\\phi\\) and weight\n\\(w\\).\nIts density is given by",
            "markdown"
        ],
        [
            "\\(f_{EDM}(y|\\theta,\\phi,w) = c(y,\\phi,w)\n\\exp\\left(\\frac{y\\theta-b(\\theta)}{\\phi}w\\right)\\,.\\)\n</blockquote>",
            "markdown"
        ],
        [
            "It follows that \\(\\mu = b'(\\theta)\\) and\n\\(Var[Y|x]=\\frac{\\phi}{w}b''(\\theta)\\). The inverse of the first equation\ngives the natural parameter as a function of the expected value\n\\(\\theta(\\mu)\\) such that",
            "markdown"
        ],
        [
            "\\(Var[Y_i|x_i] = \\frac{\\phi}{w_i} v(\\mu_i)\\)\n</blockquote>",
            "markdown"
        ],
        [
            "with \\(v(\\mu) = b''(\\theta(\\mu))\\). Therefore it is said that a GLM is\ndetermined by link function \\(g\\) and variance function \\(v(\\mu)\\)\nalone (and \\(x\\) of course).",
            "markdown"
        ],
        [
            "Note that while \\(\\phi\\) is the same for every observation \\(y_i\\)\nand therefore does not influence the estimation of \\(\\beta\\),\nthe weights \\(w_i\\) might be different for every \\(y_i\\) such that the\nestimation of \\(\\beta\\) depends on them.",
            "markdown"
        ],
        [
            "The Tweedie distribution has special cases for \\(p=0,1,2\\) not listed in the\ntable and uses \\(\\alpha=\\frac{p-2}{p-1}\\).",
            "markdown"
        ],
        [
            "Correspondence of mathematical variables to code:",
            "markdown"
        ],
        [
            "\\(Y\\) and \\(y\\) are coded as endog, the variable one wants to\nmodel",
            "markdown"
        ],
        [
            "\\(x\\) is coded as exog, the covariates alias explanatory variables",
            "markdown"
        ],
        [
            "\\(\\beta\\) is coded as params, the parameters one wants to estimate",
            "markdown"
        ],
        [
            "\\(\\mu\\) is coded as mu, the expectation (conditional on \\(x\\))\nof \\(Y\\)",
            "markdown"
        ],
        [
            "\\(g\\) is coded as link argument to the class Family",
            "markdown"
        ],
        [
            "\\(\\phi\\) is coded as scale, the dispersion parameter of the EDM",
            "markdown"
        ],
        [
            "\\(w\\) is not yet supported (i.e. \\(w=1\\)), in the future it might be\nvar_weights",
            "markdown"
        ],
        [
            "\\(p\\) is coded as var_power for the power of the variance function\n\\(v(\\mu)\\) of the Tweedie distribution, see table",
            "markdown"
        ],
        [
            "\\(\\alpha\\) is either",
            "markdown"
        ],
        [
            "Negative Binomial: the ancillary parameter alpha, see table",
            "markdown"
        ],
        [
            "Tweedie: an abbreviation for \\(\\frac{p-2}{p-1}\\) of the power \\(p\\)\nof the variance function, see table",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Linear Models->Technical Documentation->References": [
        [
            "Gill, Jeff. 2000. Generalized Linear Models: A Unified Approach. SAGE QASS Series.",
            "markdown"
        ],
        [
            "Green, PJ. 1984. \u201cIteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant alternatives.\u201d Journal of the Royal Statistical Society, Series B, 46, 149-192.",
            "markdown"
        ],
        [
            "Hardin, J.W. and Hilbe, J.M. 2007. \u201cGeneralized Linear Models and Extensions.\u201d 2nd ed. Stata Press, College Station, TX.",
            "markdown"
        ],
        [
            "McCullagh, P. and Nelder, J.A. 1989. \u201cGeneralized Linear Models.\u201d 2nd ed. Chapman & Hall, Boca Rotan.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Linear Models->Module Reference": [
        [
            "Model Class",
            "markdown"
        ],
        [
            "Results Class",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Linear Models->Module Reference->Families": [
        [
            "The distribution families currently implemented are",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Linear Models->Module Reference->Link Functions": [
        [
            "The link functions currently implemented are the following. Not all link\nfunctions are available for each distribution family. The list of\navailable link functions can be obtained by",
            "markdown"
        ],
        [
            "sm.families.family.&lt;familyname.links",
            "code"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Linear Models->Module Reference->Variance Functions": [
        [
            "Each of the families has an associated variance function. You can access\nthe variance functions here:",
            "markdown"
        ],
        [
            "sm.families.&lt;familyname.variance",
            "code"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Estimating Equations": [
        [
            "Generalized Estimating Equations estimate generalized linear models for\npanel, cluster or repeated measures data when the observations are possibly\ncorrelated withing a cluster but uncorrelated across clusters. It supports\nestimation of the same one-parameter exponential families as Generalized\nLinear models (GLM).",
            "markdown"
        ],
        [
            "See  for commands and arguments.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Estimating Equations->Examples": [
        [
            "The following illustrates a Poisson regression with exchangeable correlation\nwithin clusters using data on epilepsy seizures.",
            "markdown"
        ],
        [
            "In [1]: import statsmodels.api as sm\n\nIn [2]: import statsmodels.formula.api as smf\n\nIn [3]: data = sm.datasets.get_rdataset('epil', package='MASS').data\n\nIn [4]: fam = sm.families.Poisson()\n\nIn [5]: ind = sm.cov_struct.Exchangeable()\n\nIn [6]: mod = smf.gee(\"y ~ age + trt + base\", \"subject\", data,\n   ...:               cov_struct=ind, family=fam)\n   ...: \n\nIn [7]: res = mod.fit()\n\nIn [8]: print(res.summary())\n                               GEE Regression Results                              \n===================================================================================\nDep. Variable:                           y   No. Observations:                  236\nModel:                                 GEE   No. clusters:                       59\nMethod:                        Generalized   Min. cluster size:                   4\n                      Estimating Equations   Max. cluster size:                   4\nFamily:                            Poisson   Mean cluster size:                 4.0\nDependence structure:         Exchangeable   Num. iterations:                     2\nDate:                     Wed, 02 Nov 2022   Scale:                           1.000\nCovariance type:                    robust   Time:                         17:13:23\n====================================================================================\n                       coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept            0.5730      0.361      1.589      0.112      -0.134       1.280\ntrt[T.progabide]    -0.1519      0.171     -0.888      0.375      -0.487       0.183\nage                  0.0223      0.011      1.960      0.050    2.11e-06       0.045\nbase                 0.0226      0.001     18.451      0.000       0.020       0.025\n==============================================================================\nSkew:                          3.7823   Kurtosis:                      28.6672\nCentered skew:                 2.7597   Centered kurtosis:             21.9865\n==============================================================================",
            "code"
        ],
        [
            "Several notebook examples of the use of GEE can be found on the Wiki:",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Estimating Equations->Examples->References": [
        [
            "KY Liang and S Zeger. \u201cLongitudinal data analysis using generalized\nlinear models\u201d. Biometrika (1986) 73 (1): 13-22.",
            "markdown"
        ],
        [
            "S Zeger and KY Liang. \u201cLongitudinal Data Analysis for Discrete and\nContinuous Outcomes\u201d. Biometrics Vol. 42, No. 1 (Mar., 1986),\npp. 121-130",
            "markdown"
        ],
        [
            "A Rotnitzky and NP Jewell (1990). \u201cHypothesis testing of regression\nparameters in semiparametric generalized linear models for cluster\ncorrelated data\u201d, Biometrika, 77, 485-497.",
            "markdown"
        ],
        [
            "Xu Guo and Wei Pan (2002). \u201cSmall sample performance of the score test in\nGEE\u201d.",
            "markdown"
        ],
        [
            "LA Mancl LA, TA DeRouen (2001). A covariance estimator for GEE with improved\nsmall-sample properties.  Biometrics. 2001 Mar;57(1):126-34.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Estimating Equations->Module Reference": [
        [
            "Model Class",
            "markdown"
        ],
        [
            "Results Classes",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Estimating Equations->Module Reference->Dependence Structures": [
        [
            "The dependence structures currently implemented are",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Estimating Equations->Module Reference->Families": [
        [
            "The distribution families are the same as for GLM, currently implemented are",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Estimating Equations->Module Reference->Link Functions": [
        [
            "The link functions are the same as for GLM, currently implemented are the\nfollowing. Not all link functions are available for each distribution family.\nThe list of available link functions can be obtained by",
            "markdown"
        ],
        [
            "sm.families.family.&lt;familyname.links",
            "code"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Additive Models (GAM)": [
        [
            "Generalized Additive Models allow for penalized estimation of smooth terms\nin generalized linear models.",
            "markdown"
        ],
        [
            "See  for commands and arguments.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Additive Models (GAM)->Examples": [
        [
            "The following illustrates a Gaussian and a Poisson regression where\ncategorical variables are treated as linear terms and the effect of\ntwo explanatory variables is captured by penalized B-splines.\nThe data is from the automobile dataset\n\nWe can load a dataframe with selected columns from the unit test module.",
            "markdown"
        ],
        [
            "In [1]: import statsmodels.api as sm\n\nIn [2]: from statsmodels.gam.api import GLMGam, BSplines\n\n# import data\nIn [3]: from statsmodels.gam.tests.test_penalized import df_autos\n\n# create spline basis for weight and hp\nIn [4]: x_spline = df_autos[['weight', 'hp']]\n\nIn [5]: bs = BSplines(x_spline, df=[12, 10], degree=[3, 3])\n\n# penalization weight\nIn [6]: alpha = np.array([21833888.8, 6460.38479])\n\nIn [7]: gam_bs = GLMGam.from_formula('city_mpg ~ fuel + drive', data=df_autos,\n   ...:                              smoother=bs, alpha=alpha)\n   ...: \n\nIn [8]: res_bs = gam_bs.fit()\n\nIn [9]: print(res_bs.summary())\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:               city_mpg   No. Observations:                  203\nModel:                         GLMGam   Df Residuals:                   189.13\nModel Family:                Gaussian   Df Model:                        12.87\nLink Function:               identity   Scale:                          4.8825\nMethod:                         PIRLS   Log-Likelihood:                -441.81\nDate:                Wed, 02 Nov 2022   Deviance:                       923.45\nTime:                        17:13:15   Pearson chi2:                     923.\nNo. Iterations:                     3   Pseudo R-squ. (CS):             0.9996\nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept       51.9923      1.997     26.034      0.000      48.078      55.906\nfuel[T.gas]     -5.8099      0.727     -7.989      0.000      -7.235      -4.385\ndrive[T.fwd]     1.3910      0.819      1.699      0.089      -0.213       2.995\ndrive[T.rwd]     1.0638      0.842      1.263      0.207      -0.587       2.715\nweight_s0       -3.5556      0.959     -3.707      0.000      -5.436      -1.676\nweight_s1       -9.0876      1.750     -5.193      0.000     -12.518      -5.658\nweight_s2      -13.0303      1.827     -7.132      0.000     -16.611      -9.450\nweight_s3      -14.2641      1.854     -7.695      0.000     -17.897     -10.631\nweight_s4      -15.1805      1.892     -8.024      0.000     -18.889     -11.472\nweight_s5      -15.9557      1.963     -8.128      0.000     -19.803     -12.108\nweight_s6      -16.6297      2.038     -8.161      0.000     -20.624     -12.636\nweight_s7      -16.9928      2.045     -8.308      0.000     -21.002     -12.984\nweight_s8      -19.3480      2.367     -8.174      0.000     -23.987     -14.709\nweight_s9      -20.7978      2.455     -8.472      0.000     -25.609     -15.986\nweight_s10     -20.8062      2.443     -8.517      0.000     -25.594     -16.018\nhp_s0           -1.4473      0.558     -2.592      0.010      -2.542      -0.353\nhp_s1           -3.4228      1.012     -3.381      0.001      -5.407      -1.438\nhp_s2           -5.9026      1.251     -4.717      0.000      -8.355      -3.450\nhp_s3           -7.2389      1.352     -5.354      0.000      -9.889      -4.589\nhp_s4           -9.1052      1.384     -6.581      0.000     -11.817      -6.393\nhp_s5           -9.9865      1.525     -6.547      0.000     -12.976      -6.997\nhp_s6          -13.3639      2.228     -5.998      0.000     -17.731      -8.997\nhp_s7          -13.8902      3.194     -4.349      0.000     -20.150      -7.630\nhp_s8          -11.9752      2.556     -4.685      0.000     -16.985      -6.965\n================================================================================\n\n# plot smooth components\nIn [10]: res_bs.plot_partial(0, cpr=True)\nOut[10]: &lt;Figure size 640x480 with 1 Axes\n\nIn [11]: res_bs.plot_partial(1, cpr=True)\nOut[11]: &lt;Figure size 640x480 with 1 Axes\n\nIn [12]: alpha = np.array([8283989284.5829611, 14628207.58927821])\n\nIn [13]: gam_bs = GLMGam.from_formula('city_mpg ~ fuel + drive', data=df_autos,\n   ....:                              smoother=bs, alpha=alpha,\n   ....:                              family=sm.families.Poisson())\n   ....: \n\nIn [14]: res_bs = gam_bs.fit()\n\nIn [15]: print(res_bs.summary())\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:               city_mpg   No. Observations:                  203\nModel:                         GLMGam   Df Residuals:                   194.75\nModel Family:                 Poisson   Df Model:                         7.25\nLink Function:                    Log   Scale:                          1.0000\nMethod:                         PIRLS   Log-Likelihood:                -530.38\nDate:                Wed, 02 Nov 2022   Deviance:                       37.569\nTime:                        17:13:15   Pearson chi2:                     37.4\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.7715\nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept        3.9960      0.130     30.844      0.000       3.742       4.250\nfuel[T.gas]     -0.2398      0.057     -4.222      0.000      -0.351      -0.128\ndrive[T.fwd]     0.0386      0.075      0.513      0.608      -0.109       0.186\ndrive[T.rwd]     0.0309      0.078      0.395      0.693      -0.122       0.184\nweight_s0       -0.0811      0.030     -2.689      0.007      -0.140      -0.022\nweight_s1       -0.1938      0.063     -3.067      0.002      -0.318      -0.070\nweight_s2       -0.3160      0.082     -3.864      0.000      -0.476      -0.156\nweight_s3       -0.3735      0.090     -4.160      0.000      -0.549      -0.198\nweight_s4       -0.4187      0.096     -4.360      0.000      -0.607      -0.230\nweight_s5       -0.4645      0.103     -4.495      0.000      -0.667      -0.262\nweight_s6       -0.5092      0.112     -4.555      0.000      -0.728      -0.290\nweight_s7       -0.5469      0.119     -4.598      0.000      -0.780      -0.314\nweight_s8       -0.6211      0.137     -4.528      0.000      -0.890      -0.352\nweight_s9       -0.6866      0.153     -4.486      0.000      -0.987      -0.387\nweight_s10      -0.7370      0.174     -4.228      0.000      -1.079      -0.395\nhp_s0           -0.0247      0.010     -2.378      0.017      -0.045      -0.004\nhp_s1           -0.0557      0.022     -2.479      0.013      -0.100      -0.012\nhp_s2           -0.1046      0.038     -2.719      0.007      -0.180      -0.029\nhp_s3           -0.1438      0.050     -2.857      0.004      -0.242      -0.045\nhp_s4           -0.1919      0.063     -3.047      0.002      -0.315      -0.068\nhp_s5           -0.2567      0.079     -3.231      0.001      -0.412      -0.101\nhp_s6           -0.4152      0.120     -3.455      0.001      -0.651      -0.180\nhp_s7           -0.4889      0.152     -3.214      0.001      -0.787      -0.191\nhp_s8           -0.5470      0.195     -2.810      0.005      -0.928      -0.166\n================================================================================\n\n# Optimal penalization weights alpha can be obtaine through generalized\n# cross-validation or k-fold cross-validation.\n# The alpha above are from the unit tests against the R mgcv package.\nIn [16]: gam_bs.select_penweight()[0]\nOut[16]: array([8.28383793e+09, 1.46285472e+07])\n\nIn [17]: gam_bs.select_penweight_kfold()[0]\nOut[17]: (10000000.0, 15848.931924611108)",
            "code"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Additive Models (GAM)->Examples->References": [
        [
            "Hastie, Trevor, and Robert Tibshirani. 1986. Generalized Additive Models. Statistical Science 1 (3): 297-310.",
            "markdown"
        ],
        [
            "Wood, Simon N. 2006. Generalized Additive Models: An Introduction with R. Texts in Statistical Science. Boca Raton, FL: Chapman & Hall/CRC.",
            "markdown"
        ],
        [
            "Wood, Simon N. 2017. Generalized Additive Models: An Introduction with R. Second edition. Chapman & Hall/CRC Texts in Statistical Science. Boca Raton: CRC Press/Taylor & Francis Group.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Additive Models (GAM)->Module Reference": [
        [
            "Model Class",
            "markdown"
        ],
        [
            "Results Classes",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Additive Models (GAM)->Module Reference->Smooth Basis Functions": [
        [
            "Currently there is verified support for two spline bases",
            "markdown"
        ],
        [
            "statsmodels.gam.smooth_basis includes additional splines and a (global)\npolynomial smoother basis but those have not been verified yet.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Additive Models (GAM)->Module Reference->Families and Link Functions": [
        [
            "The distribution families in GLMGam are the same as for GLM and so are\nthe corresponding link functions.\nCurrent unit tests only cover Gaussian and Poisson, and GLMGam might not\nwork for all options that are available in GLM.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Robust Linear Models": [
        [
            "Robust linear models with support for the M-estimators listed under .",
            "markdown"
        ],
        [
            "See  for commands and arguments.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Robust Linear Models->Examples": [
        [
            "# Load modules and data\nIn [1]: import statsmodels.api as sm\n\nIn [2]: data = sm.datasets.stackloss.load()\n\nIn [3]: data.exog = sm.add_constant(data.exog)\n\n# Fit model and print summary\nIn [4]: rlm_model = sm.RLM(data.endog, data.exog, M=sm.robust.norms.HuberT())\n\nIn [5]: rlm_results = rlm_model.fit()\n\nIn [6]: print(rlm_results.params)\nconst       -41.026498\nAIRFLOW       0.829384\nWATERTEMP     0.926066\nACIDCONC     -0.127847\ndtype: float64",
            "code"
        ],
        [
            "Detailed examples can be found here:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Robust Linear Models->Technical Documentation->References": [
        [
            "PJ Huber. \u2018Robust Statistics\u2019 John Wiley and Sons, Inc., New York. 1981.",
            "markdown"
        ],
        [
            "PJ Huber. 1973, \u2018The 1972 Wald Memorial Lectures: Robust Regression: Asymptotics, Conjectures, and Monte Carlo.\u2019 The Annals of Statistics, 1.5, 799-821.",
            "markdown"
        ],
        [
            "R Venables, B Ripley. \u2018Modern Applied Statistics in S\u2019 Springer, New York,",
            "markdown"
        ],
        [
            "C Croux, PJ Rousseeuw, \u2018Time-efficient algorithms for two highly robust estimators of scale\u2019 Computational statistics. Physica, Heidelberg, 1992.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Robust Linear Models->Module Reference": [
        [
            "Model Classes",
            "markdown"
        ],
        [
            "Model Results",
            "markdown"
        ],
        [
            "Norms",
            "markdown"
        ],
        [
            "Scale",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Linear Mixed Effects Models": [
        [
            "Linear Mixed Effects models are used for regression analyses involving\ndependent data.  Such data arise when working with longitudinal and\nother study designs in which multiple observations are made on each\nsubject.  Some specific linear mixed effects models are",
            "markdown"
        ],
        [
            "Random intercepts models, where all responses in a group are\nadditively shifted by a value that is specific to the group.",
            "markdown"
        ],
        [
            "Random slopes models, where the responses in a group follow a\n(conditional) mean trajectory that is linear in the observed\ncovariates, with the slopes (and possibly intercepts) varying by\ngroup.",
            "markdown"
        ],
        [
            "Variance components models, where the levels of one or more\ncategorical covariates are associated with draws from distributions.\nThese random terms additively determine the conditional mean of each\nobservation based on its covariate values.",
            "markdown"
        ],
        [
            "The statsmodels implementation of LME is primarily group-based,\nmeaning that random effects must be independently-realized for\nresponses in different groups.  There are two types of random effects\nin our implementation of mixed models: (i) random coefficients\n(possibly vectors) that have an unknown covariance matrix, and (ii)\nrandom coefficients that are independent draws from a common\nunivariate distribution.  For both (i) and (ii), the random effects\ninfluence the conditional mean of a group through their matrix/vector\nproduct with a group-specific design matrix.",
            "markdown"
        ],
        [
            "A simple example of random coefficients, as in (i) above, is:\n\n\\[Y_{ij} = \\beta_0 + \\beta_1X_{ij} + \\gamma_{0i} + \\gamma_{1i}X_{ij} + \\epsilon_{ij}\\]",
            "markdown"
        ],
        [
            "Here, \\(Y_{ij}\\) is the \\(j^\\rm{th}\\) measured response for subject\n\\(i\\), and \\(X_{ij}\\) is a covariate for this response.  The\n\u201cfixed effects parameters\u201d \\(\\beta_0\\) and \\(\\beta_1\\) are\nshared by all subjects, and the errors \\(\\epsilon_{ij}\\) are\nindependent of everything else, and identically distributed (with mean\nzero).  The \u201crandom effects parameters\u201d \\(\\gamma_{0i}\\) and\n\\(\\gamma_{1i}\\) follow a bivariate distribution with mean zero,\ndescribed by three parameters: \\({\\rm var}(\\gamma_{0i})\\),\n\\({\\rm var}(\\gamma_{1i})\\), and \\({\\rm cov}(\\gamma_{0i},\n\\gamma_{1i})\\).  There is also a parameter for \\({\\rm\nvar}(\\epsilon_{ij})\\).",
            "markdown"
        ],
        [
            "A simple example of variance components, as in (ii) above, is:\n\n\\[Y_{ijk} = \\beta_0 + \\eta_{1i} + \\eta_{2j} + \\epsilon_{ijk}\\]",
            "markdown"
        ],
        [
            "Here, \\(Y_{ijk}\\) is the \\(k^\\rm{th}\\) measured response under\nconditions \\(i, j\\).  The only \u201cmean structure parameter\u201d is\n\\(\\beta_0\\).  The \\(\\eta_{1i}\\) are independent and\nidentically distributed with zero mean, and variance \\(\\tau_1^2\\),\nand the \\(\\eta_{2j}\\) are independent and identically distributed\nwith zero mean, and variance \\(\\tau_2^2\\).",
            "markdown"
        ],
        [
            "statsmodels MixedLM handles most non-crossed random effects models,\nand some crossed models.  To include crossed random effects in a\nmodel, it is necessary to treat the entire dataset as a single group.\nThe variance components arguments to the model can then be used to\ndefine models with various combinations of crossed and non-crossed\nrandom effects.",
            "markdown"
        ],
        [
            "The statsmodels LME framework currently supports post-estimation\ninference via Wald tests and confidence intervals on the coefficients,\nprofile likelihood analysis, likelihood ratio testing, and AIC.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Linear Mixed Effects Models->Examples": [
        [
            "In [1]: import statsmodels.api as sm\n\nIn [2]: import statsmodels.formula.api as smf\n\nIn [3]: data = sm.datasets.get_rdataset(\"dietox\", \"geepack\").data\n\nIn [4]: md = smf.mixedlm(\"Weight ~ Time\", data, groups=data[\"Pig\"])\n\nIn [5]: mdf = md.fit()\n\nIn [6]: print(mdf.summary())\n         Mixed Linear Model Regression Results\n========================================================\nModel:            MixedLM Dependent Variable: Weight    \nNo. Observations: 861     Method:             REML      \nNo. Groups:       72      Scale:              11.3669   \nMin. group size:  11      Log-Likelihood:     -2404.7753\nMax. group size:  12      Converged:          Yes       \nMean group size:  12.0                                  \n--------------------------------------------------------\n             Coef.  Std.Err.    z    P|z| [0.025 0.975]\n--------------------------------------------------------\nIntercept    15.724    0.788  19.952 0.000 14.179 17.268\nTime          6.943    0.033 207.939 0.000  6.877  7.008\nGroup Var    40.394    2.149                            \n========================================================",
            "code"
        ],
        [
            "Detailed examples can be found here",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "There are some notebook examples on the Wiki:",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Linear Mixed Effects Models->Technical Documentation": [
        [
            "The data are partitioned into disjoint groups.\nThe probability model for group \\(i\\) is:\n\n\\[Y = X\\beta + Z\\gamma + Q_1\\eta_1 + \\cdots + Q_k\\eta_k + \\epsilon\\]",
            "markdown"
        ],
        [
            "where",
            "markdown"
        ],
        [
            "\\(n_i\\) is the number of observations in group \\(i\\)",
            "markdown"
        ],
        [
            "\\(Y\\) is a \\(n_i\\) dimensional response vector",
            "markdown"
        ],
        [
            "\\(X\\) is a \\(n_i * k_{fe}\\) dimensional matrix of fixed effects\ncoefficients",
            "markdown"
        ],
        [
            "\\(\\beta\\) is a \\(k_{fe}\\)-dimensional vector of fixed effects slopes",
            "markdown"
        ],
        [
            "\\(Z\\) is a \\(n_i * k_{re}\\) dimensional matrix of random effects\ncoefficients",
            "markdown"
        ],
        [
            "\\(\\gamma\\) is a \\(k_{re}\\)-dimensional random vector with mean 0\nand covariance matrix \\(\\Psi\\); note that each group\ngets its own independent realization of gamma.",
            "markdown"
        ],
        [
            "\\(Q_j\\) is a \\(n_i \\times q_j\\) dimensional design matrix for the\n\\(j^\\rm{th}\\) variance component.",
            "markdown"
        ],
        [
            "\\(\\eta_j\\) is a \\(q_j\\)-dimensional random vector containing independent\nand identically distributed values with variance \\(\\tau_j^2\\).",
            "markdown"
        ],
        [
            "\\(\\epsilon\\) is a \\(n_i\\) dimensional vector of i.i.d normal\nerrors with mean 0 and variance \\(\\sigma^2\\); the \\(\\epsilon\\)\nvalues are independent both within and between groups",
            "markdown"
        ],
        [
            "\\(Y, X, \\{Q_j\\}\\) and \\(Z\\) must be entirely observed.  \\(\\beta\\),\n\\(\\Psi\\), and \\(\\sigma^2\\) are estimated using ML or REML estimation,\nand \\(\\gamma\\), \\(\\{\\eta_j\\}\\) and \\(\\epsilon\\) are\nrandom so define the probability model.",
            "markdown"
        ],
        [
            "The marginal mean structure is \\(E[Y|X,Z] = X*\\beta\\).  If only\nthe marginal mean structure is of interest, GEE is a good alternative\nto mixed models.",
            "markdown"
        ],
        [
            "Notation:",
            "markdown"
        ],
        [
            "\\(cov_{re}\\) is the random effects covariance matrix (referred\nto above as \\(\\Psi\\)) and \\(scale\\) is the (scalar) error\nvariance.  There is also a single estimated variance parameter\n\\(\\tau_j^2\\) for each variance component.  For a single group,\nthe marginal covariance matrix of endog given exog is\n\\(scale*I + Z * cov_{re} * Z\\), where \\(Z\\) is the design\nmatrix for the random effects in one group.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Linear Mixed Effects Models->Technical Documentation->References": [
        [
            "The primary reference for the implementation details is:",
            "markdown"
        ],
        [
            "MJ Lindstrom, DM Bates (1988).  Newton Raphson and EM algorithms for\nlinear mixed effects models for repeated measures data.  Journal of\nthe American Statistical Association. Volume 83, Issue 404, pages 1014-1022.",
            "markdown"
        ],
        [
            "See also this more recent document:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "All the likelihood, gradient, and Hessian calculations closely follow\nLindstrom and Bates.",
            "markdown"
        ],
        [
            "The following two documents are written more from the perspective of\nusers:",
            "markdown"
        ],
        [
            "checkout/www/lMMwR/lrgprt.pdf?revision=949&root=lme4&pathrev=1781",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Linear Mixed Effects Models->Module Reference": [
        [
            "The model class is:",
            "markdown"
        ],
        [
            "The result class is:",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Regression with Discrete Dependent Variable": [
        [
            "Regression models for limited and qualitative dependent variables. The module\ncurrently allows the estimation of models with binary (Logit, Probit), nominal\n(MNLogit), or count (Poisson, NegativeBinomial) data.",
            "markdown"
        ],
        [
            "Starting with version 0.9, this also includes new count models, that are still\nexperimental in 0.9, NegativeBinomialP, GeneralizedPoisson and zero-inflated\nmodels, ZeroInflatedPoisson, ZeroInflatedNegativeBinomialP and\nZeroInflatedGeneralizedPoisson.",
            "markdown"
        ],
        [
            "See  for commands and arguments.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Regression with Discrete Dependent Variable->Examples": [
        [
            "# Load the data from Spector and Mazzeo (1980)\nIn [1]: import statsmodels.api as sm\n\nIn [2]: spector_data = sm.datasets.spector.load_pandas()\n\nIn [3]: spector_data.exog = sm.add_constant(spector_data.exog)\n\n# Logit Model\nIn [4]: logit_mod = sm.Logit(spector_data.endog, spector_data.exog)\n\nIn [5]: logit_res = logit_mod.fit()\nOptimization terminated successfully.\n         Current function value: 0.402801\n         Iterations 7\n\nIn [6]: print(logit_res.summary())\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                  GRADE   No. Observations:                   32\nModel:                          Logit   Df Residuals:                       28\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 02 Nov 2022   Pseudo R-squ.:                  0.3740\nTime:                        17:12:32   Log-Likelihood:                -12.890\nconverged:                       True   LL-Null:                       -20.592\nCovariance Type:            nonrobust   LLR p-value:                  0.001502\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -13.0213      4.931     -2.641      0.008     -22.687      -3.356\nGPA            2.8261      1.263      2.238      0.025       0.351       5.301\nTUCE           0.0952      0.142      0.672      0.501      -0.182       0.373\nPSI            2.3787      1.065      2.234      0.025       0.292       4.465\n==============================================================================",
            "code"
        ],
        [
            "Detailed examples can be found here:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Regression with Discrete Dependent Variable->Technical Documentation": [
        [
            "Currently all models are estimated by Maximum Likelihood and assume\nindependently and identically distributed errors.",
            "markdown"
        ],
        [
            "All discrete regression models define the same methods and follow the same\nstructure, which is similar to the regression results but with some methods\nspecific to discrete models. Additionally some of them contain additional model\nspecific methods and attributes.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Regression with Discrete Dependent Variable->Technical Documentation->References": [
        [
            "General references for this class of models are:",
            "markdown"
        ],
        [
            "A.C. Cameron and P.K. Trivedi.  `Regression Analysis of Count Data`.\n    Cambridge, 1998\n\nG.S. Madalla. `Limited-Dependent and Qualitative Variables in Econometrics`.\n    Cambridge, 1983.\n\nW. Greene. `Econometric Analysis`. Prentice Hall, 5th. edition. 2003.",
            "code"
        ]
    ],
    "User Guide->Regression and Linear Models->Regression with Discrete Dependent Variable->Module Reference": [
        [
            "The specific model classes are:",
            "markdown"
        ],
        [
            "The cumulative link model for an ordinal dependent variable is currently\nin miscmodels as it subclasses GenericLikelihoodModel. This will change\nin future versions.",
            "markdown"
        ],
        [
            "The specific result classes are:",
            "markdown"
        ],
        [
            "DiscreteModel is a superclass of all discrete regression models. The\nestimation results are returned as an instance of one of the subclasses of\nDiscreteResults. Each category of models, binary, count and\nmultinomial, have their own intermediate level of model and results classes.\nThis intermediate classes are mostly to facilitate the implementation of the\nmethods and attributes defined by DiscreteModel and\nDiscreteResults.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Linear Mixed Effects Models": [
        [
            "Generalized Linear Mixed Effects (GLIMMIX) models are generalized\nlinear models with random effects in the linear predictors.\nstatsmodels currently supports estimation of binomial and Poisson\nGLIMMIX models using two Bayesian methods: the Laplace approximation\nto the posterior, and a variational Bayes approximation to the\nposterior.  Both methods provide point estimates (posterior means) and\nassessments of uncertainty (posterior standard deviation).",
            "markdown"
        ],
        [
            "The current implementation only supports independent random effects.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Linear Mixed Effects Models->Technical Documentation": [
        [
            "Unlike statsmodels mixed linear models, the GLIMMIX implementation is\nnot group-based.  Groups are created by interacting all random effects\nwith a categorical variable.  Note that this creates large, sparse\nrandom effects design matrices exog_vc.  Internally, exog_vc is\nconverted to a scipy sparse matrix.  When passing the arguments\ndirectly to the class initializer, a sparse matrix may be passed.\nWhen using formulas, a dense matrix is created then converted to\nsparse.  For very large problems, it may not be feasible to use\nformulas due to the size of this dense intermediate matrix.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Linear Mixed Effects Models->Technical Documentation->References": [
        [
            "Blei, Kucukelbir, McAuliffe (2017).  Variational Inference: A review\nfor Statisticians",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Generalized Linear Mixed Effects Models->Module Reference": [
        [
            "The model classes are:",
            "markdown"
        ],
        [
            "The result class is:",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->ANOVA": [
        [
            "Analysis of Variance models containing anova_lm for ANOVA analysis with a\nlinear OLSModel, and AnovaRM for repeated measures ANOVA, within ANOVA for\nbalanced data.",
            "markdown"
        ],
        [
            "Module Reference",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->ANOVA->Examples": [
        [
            "In [1]: import statsmodels.api as sm\n\nIn [2]: from statsmodels.formula.api import ols\n\nIn [3]: moore = sm.datasets.get_rdataset(\"Moore\", \"carData\",\n   ...:                                  cache=True) # load data\n   ...: \n\nIn [4]: data = moore.data\n\nIn [5]: data = data.rename(columns={\"partner.status\":\n   ...:                             \"partner_status\"}) # make name pythonic\n   ...: \n\nIn [6]: moore_lm = ols('conformity ~ C(fcategory, Sum)*C(partner_status, Sum)',\n   ...:                 data=data).fit()\n   ...: \n\nIn [7]: table = sm.stats.anova_lm(moore_lm, typ=2) # Type 2 ANOVA DataFrame\n\nIn [8]: print(table)\n                                              sum_sq    df          F    PR(F)\nC(fcategory, Sum)                          11.614700   2.0   0.276958  0.759564\nC(partner_status, Sum)                    212.213778   1.0  10.120692  0.002874\nC(fcategory, Sum):C(partner_status, Sum)  175.488928   2.0   4.184623  0.022572\nResidual                                  817.763961  39.0        NaN       NaN",
            "code"
        ],
        [
            "A more detailed example for anova_lm can be found here:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Other Models othermod": [
        [
            "contains model classes that do not fit into\nany other category, for example models for a response variable endog that\nhas support on the unit interval or is positive or non-negative.",
            "markdown"
        ],
        [
            "contains models that are, or will be fully developed\nin contrast to  which contains mainly examples\nfor the use of the generic likelihood model setup.",
            "markdown"
        ],
        [
            "Status is experimental. The api and implementation will need to adjust as we\nsupport more types of models, for example models with multiple exog and\nmultiple link functions.",
            "markdown"
        ]
    ],
    "User Guide->Regression and Linear Models->Other Models othermod->Interval Models betareg": [
        [
            "Models for continuous dependent variables that are in the unit interval such\nas fractions. These Models are estimated by full Maximum Likelihood.\nDependent variables on the unit interval can also be estimate by\nQuasi Maximum Likelihood using models for binary endog, such as Logit and\nGLM-Binomial. (The implementation of discrete.Probit assumes binary endog and\ncannot estimate a QMLE for continuous dependent variable.)",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa": [
        [
            "contains model classes and functions that are useful\nfor time series analysis. Basic models include univariate autoregressive models (AR),\nvector autoregressive models (VAR) and univariate autoregressive moving average models\n(ARMA). Non-linear models include Markov switching dynamic regression and\nautoregression. It also includes descriptive statistics for time series, for example autocorrelation, partial\nautocorrelation function and periodogram, as well as the corresponding theoretical properties\nof ARMA or related processes. It also includes methods to work with autoregressive and\nmoving average lag-polynomials.\nAdditionally, related statistical tests and some useful helper functions are available.",
            "markdown"
        ],
        [
            "Estimation is either done by exact or conditional Maximum Likelihood or conditional\nleast-squares, either using Kalman Filter or direct filters.",
            "markdown"
        ],
        [
            "Currently, functions and classes have to be imported from the corresponding module, but\nthe main classes will be made available in the statsmodels.tsa namespace. The module\nstructure is within statsmodels.tsa is",
            "markdown"
        ],
        [
            "stattools : empirical properties and tests, acf, pacf, granger-causality,\nadf unit root test, kpss test, bds test, ljung-box test and others.",
            "markdown"
        ],
        [
            "ar_model : univariate autoregressive process, estimation with conditional\nand exact maximum likelihood and conditional least-squares",
            "markdown"
        ],
        [
            "arima.model : univariate ARIMA process, estimation with alternative methods",
            "markdown"
        ],
        [
            "statespace : Comprehensive statespace model specification and estimation. See\nthe .",
            "markdown"
        ],
        [
            "vector_ar, var : vector autoregressive process (VAR) and vector error correction\nmodels, estimation, impulse response analysis, forecast error variance decompositions,\nand data visualization tools. See the .",
            "markdown"
        ],
        [
            "arma_process : properties of arma processes with given parameters, this\nincludes tools to convert between ARMA, MA and AR representation as well as\nacf, pacf, spectral density, impulse response function and similar",
            "markdown"
        ],
        [
            "sandbox.tsa.fftarma : similar to arma_process but working in frequency domain",
            "markdown"
        ],
        [
            "tsatools : additional helper functions, to create arrays of lagged variables,\nconstruct regressors for trend, detrend and similar.",
            "markdown"
        ],
        [
            "filters : helper function for filtering time series",
            "markdown"
        ],
        [
            "regime_switching : Markov switching dynamic regression and autoregression models",
            "markdown"
        ],
        [
            "Some additional functions that are also useful for time series analysis are in\nother parts of statsmodels, for example additional statistical tests.",
            "markdown"
        ],
        [
            "Some related functions are also available in matplotlib, nitime, and\nscikits.talkbox. Those functions are designed more for the use in signal\nprocessing where longer time series are available and work more often in the\nfrequency domain.",
            "markdown"
        ],
        [
            "Descriptive Statistics and Tests",
            "markdown"
        ],
        [
            "Regime switching models",
            "markdown"
        ],
        [
            "Time Series Filters",
            "markdown"
        ],
        [
            "TSA Tools",
            "markdown"
        ],
        [
            "VARMA Process",
            "markdown"
        ],
        [
            "Interpolation",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa->Estimation": [
        [
            "The following are the main estimation classes, which can be accessed through\nstatsmodels.tsa.api and their result classes",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa->Estimation->Univariate Autoregressive Processes (AR)": [
        [
            "The basic autoregressive model in Statsmodels is:",
            "markdown"
        ],
        [
            "The ar_model.AutoReg model estimates parameters using conditional MLE (OLS),\nand supports exogenous regressors (an AR-X model) and seasonal effects.",
            "markdown"
        ],
        [
            "AR-X and related models can also be fitted with the arima.ARIMA class and the\nSARIMAX class (using full MLE via the Kalman Filter).",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa->Estimation->Autoregressive Moving-Average Processes (ARMA) and Kalman Filter": [
        [
            "Basic ARIMA model and results classes are as follows:",
            "markdown"
        ],
        [
            "This model allows estimating parameters by various methods (including\nconditional MLE via the Hannan-Rissanen method and full MLE via the Kalman\nfilter). It is a special case of the SARIMAX model, and it includes a large\nnumber of inherited features from the  models\n(including prediction / forecasting, residual diagnostics, simulation and\nimpulse responses, etc.).",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa->Estimation->Exponential Smoothing": [
        [
            "Linear and non-linear exponential smoothing models are available:",
            "markdown"
        ],
        [
            "Separately, linear and non-linear exponential smoothing models have also been\nimplemented based on the \u201cinnovations\u201d state space approach. In addition to the\nusual support for parameter fitting, in-sample prediction, and out-of-sample\nforecasting, these models also support prediction intervals, simulation, and\nmore.",
            "markdown"
        ],
        [
            "Finally, linear exponential smoothing models have also been separately\nimplemented as a special case of the general state space framework (this is\nseparate from the \u201cinnovations\u201d state space approach described above). Although\nthis approach does not allow for the non-linear (multiplicative) exponential\nsmoothing models, it includes all features of \nmodels (including prediction / forecasting, residual diagnostics, simulation\nand impulse responses, etc.).",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa->ARMA Process": [
        [
            "The following are tools to work with the theoretical properties of an ARMA\nprocess for given lag-polynomials.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa->Autoregressive Distributed Lag (ARDL) Models": [
        [
            "Autoregressive Distributed Lag models span the space between\nautoregressive models ()\nand vector autoregressive models (VAR).",
            "markdown"
        ],
        [
            "The ardl.ARDL model estimates parameters using conditional MLE (OLS)\nand allows for both simple deterministic terms (trends and seasonal\ndummies) as well as complex deterministics using a\n.",
            "markdown"
        ],
        [
            "AR-X and related models can also be fitted with\n class (using full MLE via\nthe Kalman Filter).",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa->Error Correction Models (ECM)": [
        [
            "Error correction models are reparameterizations of ARDL models that\nregress the difference of the endogenous variable on the lagged levels\nof the endogenous variables and optional lagged differences of the\nexogenous variables.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa->Statespace Models": [
        [
            "See the .",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa->Vector ARs and Vector Error Correction Models": [
        [
            "See the",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa->Deterministic Processes": [
        [
            "Deterministic processes simplify creating deterministic sequences with time\ntrend or seasonal patterns. They also provide methods to simplify generating\ndeterministic terms for out-of-sample forecasting. A\n can be directly\nused with  to construct complex\ndeterministic dynamics and to forecast without constructing exogenous trends.",
            "markdown"
        ],
        [
            "Users who wish to write custom deterministic terms must subclass\n.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa->Forecasting Models->The Theta Model": [
        [
            "The Theta model is a simple forecasting method that combines a linear time\ntrend with a Simple Exponential Smoother (Assimakopoulos & Nikolopoulos).\nAn estimator for the parameters of the Theta model and methods to forecast\nare available in:",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa->Forecasting Models->Forecasting after STL Decomposition": [
        [
            "is commonly used to remove seasonal\ncomponents from a time series. The deseasonalized time series can then\nbe modeled using a any non-seasonal model, and forecasts are constructed\nby adding the forecast from the non-seasonal model to the estimates of\nthe seasonal component from the final full-cycle which are forecast using\na random-walk model.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series analysis tsa->Prediction Results": [
        [
            "Most forecasting methods support a get_prediction method that return\na PredictionResults object that contains both the prediction, its\nvariance and can construct a prediction interval.",
            "markdown"
        ],
        [
            "Results Class",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace": [
        [
            "contains classes and functions that are\nuseful for time series analysis using state space methods.",
            "markdown"
        ],
        [
            "A general state space model is of the form\n\n\\[\\begin{split}y_t & = Z_t \\alpha_t + d_t + \\varepsilon_t \\\\\n\\alpha_{t+1} & = T_t \\alpha_t + c_t + R_t \\eta_t \\\\\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(y_t\\) refers to the observation vector at time \\(t\\),\n\\(\\alpha_t\\) refers to the (unobserved) state vector at time\n\\(t\\), and where the irregular components are defined as\n\n\\[\\begin{split}\\varepsilon_t \\sim N(0, H_t) \\\\\n\\eta_t \\sim N(0, Q_t) \\\\\\end{split}\\]",
            "markdown"
        ],
        [
            "The remaining variables (\\(Z_t, d_t, H_t, T_t, c_t, R_t, Q_t\\)) in the\nequations are matrices describing the process. Their variable names and\ndimensions are as follows",
            "markdown"
        ],
        [
            "Z : design \\((k\\_endog \\times k\\_states \\times nobs)\\)",
            "markdown"
        ],
        [
            "d : obs_intercept \\((k\\_endog \\times nobs)\\)",
            "markdown"
        ],
        [
            "H : obs_cov \\((k\\_endog \\times k\\_endog \\times nobs)\\)",
            "markdown"
        ],
        [
            "T : transition \\((k\\_states \\times k\\_states \\times nobs)\\)",
            "markdown"
        ],
        [
            "c : state_intercept \\((k\\_states \\times nobs)\\)",
            "markdown"
        ],
        [
            "R : selection \\((k\\_states \\times k\\_posdef \\times nobs)\\)",
            "markdown"
        ],
        [
            "Q : state_cov \\((k\\_posdef \\times k\\_posdef \\times nobs)\\)",
            "markdown"
        ],
        [
            "In the case that one of the matrices is time-invariant (so that, for\nexample, \\(Z_t = Z_{t+1} ~ \\forall ~ t\\)), its last dimension may\nbe of size \\(1\\) rather than size nobs.",
            "markdown"
        ],
        [
            "This generic form encapsulates many of the most popular linear time series\nmodels (see below) and is very flexible, allowing estimation with missing\nobservations, forecasting, impulse response functions, and much more.",
            "markdown"
        ],
        [
            "<strong>Example: AR(2) model</strong>",
            "markdown"
        ],
        [
            "An autoregressive model is a good introductory example to putting models in\nstate space form. Recall that an AR(2) model is often written as:\n\n\\[y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\epsilon_t,\n\\quad \\epsilon_t \\sim N(0, \\sigma^2)\\]",
            "markdown"
        ],
        [
            "This can be put into state space form in the following way:\n\n\\[\\begin{split}y_t & = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\alpha_t \\\\\n\\alpha_{t+1} & = \\begin{bmatrix}\n   \\phi_1 & \\phi_2 \\\\\n        1 &      0\n\\end{bmatrix} \\alpha_t + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\eta_t\\end{split}\\]",
            "markdown"
        ],
        [
            "Where\n\n\\[Z_t \\equiv Z = \\begin{bmatrix} 1 & 0 \\end{bmatrix}\\]",
            "markdown"
        ],
        [
            "and\n\n\\[\\begin{split}T_t \\equiv T & = \\begin{bmatrix}\n   \\phi_1 & \\phi_2 \\\\\n        1 &      0\n\\end{bmatrix} \\\\\nR_t \\equiv R & = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\\\\n\\eta_t \\equiv \\epsilon_{t+1} & \\sim N(0, \\sigma^2)\\end{split}\\]",
            "markdown"
        ],
        [
            "There are three unknown parameters in this model:\n\\(\\phi_1, \\phi_2, \\sigma^2\\).",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Models and Estimation": [
        [
            "The following are the main estimation classes, which can be accessed through\nstatsmodels.tsa.statespace.api and their result classes.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Models and Estimation->Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX)": [
        [
            "The SARIMAX class is an example of a fully fledged model created using the\nstatespace backend for estimation. SARIMAX can be used very similarly to\n models, but works on a wider range of models by adding the\nestimation of additive and multiplicative seasonal effects, as well as\narbitrary trend polynomials.",
            "markdown"
        ],
        [
            "For an example of the use of this model, see the\n\nor the very brief code snippet below:",
            "markdown"
        ],
        [
            "# Load the statsmodels api\nimport statsmodels.api as sm\n\n# Load your dataset\nendog = pd.read_csv('your/dataset/here.csv')\n\n# We could fit an AR(2) model, described above\nmod_ar2 = sm.tsa.SARIMAX(endog, order=(2,0,0))\n# Note that mod_ar2 is an instance of the SARIMAX class\n\n# Fit the model via maximum likelihood\nres_ar2 = mod_ar2.fit()\n# Note that res_ar2 is an instance of the SARIMAXResults class\n\n# Show the summary of results\nprint(res_ar2.summary())\n\n# We could also fit a more complicated model with seasonal components.\n# As an example, here is an SARIMA(1,1,1) x (0,1,1,4):\nmod_sarimax = sm.tsa.SARIMAX(endog, order=(1,1,1),\n                             seasonal_order=(0,1,1,4))\nres_sarimax = mod_sarimax.fit()\n\n# Show the summary of results\nprint(res_sarimax.summary())",
            "code"
        ],
        [
            "The results object has many of the attributes and methods you would expect from\nother statsmodels results objects, including standard errors, z-statistics,\nand prediction / forecasting.",
            "markdown"
        ],
        [
            "Behind the scenes, the SARIMAX model creates the design and transition\nmatrices (and sometimes some of the other matrices) based on the model\nspecification.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Models and Estimation->Unobserved Components": [
        [
            "The UnobservedComponents class is another example of a statespace model.",
            "markdown"
        ],
        [
            "For examples of the use of this model, see the  or a notebook on using the unobserved components model to  or the very brief code snippet below:",
            "markdown"
        ],
        [
            "# Load the statsmodels api\nimport statsmodels.api as sm\n\n# Load your dataset\nendog = pd.read_csv('your/dataset/here.csv')\n\n# Fit a local level model\nmod_ll = sm.tsa.UnobservedComponents(endog, 'local level')\n# Note that mod_ll is an instance of the UnobservedComponents class\n\n# Fit the model via maximum likelihood\nres_ll = mod_ll.fit()\n# Note that res_ll is an instance of the UnobservedComponentsResults class\n\n# Show the summary of results\nprint(res_ll.summary())\n\n# Show a plot of the estimated level and trend component series\nfig_ll = res_ll.plot_components()\n\n# We could further add a damped stochastic cycle as follows\nmod_cycle = sm.tsa.UnobservedComponents(endog, 'local level', cycle=True,\n                                        damped_cycle=True,\n                                        stochastic_cycle=True)\nres_cycle = mod_cycle.fit()\n\n# Show the summary of results\nprint(res_cycle.summary())\n\n# Show a plot of the estimated level, trend, and cycle component series\nfig_cycle = res_cycle.plot_components()",
            "code"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Models and Estimation->Vector Autoregressive Moving-Average with eXogenous regressors (VARMAX)": [
        [
            "The VARMAX class is an example of a multivariate statespace model.",
            "markdown"
        ],
        [
            "For an example of the use of this model, see the  or the very brief code snippet below:",
            "markdown"
        ],
        [
            "# Load the statsmodels api\nimport statsmodels.api as sm\n\n# Load your (multivariate) dataset\nendog = pd.read_csv('your/dataset/here.csv')\n\n# Fit a local level model\nmod_var1 = sm.tsa.VARMAX(endog, order=(1,0))\n# Note that mod_var1 is an instance of the VARMAX class\n\n# Fit the model via maximum likelihood\nres_var1 = mod_var1.fit()\n# Note that res_var1 is an instance of the VARMAXResults class\n\n# Show the summary of results\nprint(res_var1.summary())\n\n# Construct impulse responses\nirfs = res_ll.impulse_responses(steps=10)",
            "code"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Models and Estimation->Dynamic Factor Models": [
        [
            "Statsmodels has two classes that support dynamic factor models:\nDynamicFactorMQ and DynamicFactor. Each of these models has strengths, but\nin general the DynamicFactorMQ class is recommended. This is because it fits\nparameters using the Expectation-Maximization (EM) algorithm, which is more\nrobust and can handle including hundreds of observed series. In addition, it\nallows customization of which variables load on which factors. However, it does\nnot yet support including exogenous variables, while DynamicFactor does\nsupport that feature.",
            "markdown"
        ],
        [
            "For an example of the DynamicFactorMQ class, see the very brief code snippet below:",
            "markdown"
        ],
        [
            "# Load the statsmodels api\nimport statsmodels.api as sm\n\n# Load your dataset\nendog = pd.read_csv('your/dataset/here.csv')\n\n# Create a dynamic factor model\nmod_dfm = sm.tsa.DynamicFactorMQ(endog, k_factors=1, factor_order=2)\n# Note that mod_dfm is an instance of the DynamicFactorMQ class\n\n# Fit the model via maximum likelihood, using the EM algorithm\nres_dfm = mod_dfm.fit()\n# Note that res_dfm is an instance of the DynamicFactorMQResults class\n\n# Show the summary of results\nprint(res_ll.summary())\n\n# Show a plot of the r^2 values from regressions of\n# individual estimated factors on endogenous variables.\nfig_dfm = res_ll.plot_coefficients_of_determination()",
            "code"
        ],
        [
            "The DynamicFactor class is suitable for models with a smaller number of\nobserved variables",
            "markdown"
        ],
        [
            "For an example of the use of the DynamicFactor model, see the",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Models and Estimation->Linear Exponential Smoothing Models": [
        [
            "The ExponentialSmoothing class is an implementation of linear exponential\nsmoothing models using a state space approach.",
            "markdown"
        ],
        [
            "<strong>Note</strong>: this model is available at sm.tsa.statespace.ExponentialSmoothing;\nit is not the same as the model available at sm.tsa.ExponentialSmoothing.\nSee below for details of the differences between these classes.",
            "markdown"
        ],
        [
            "A very brief code snippet follows:",
            "markdown"
        ],
        [
            "# Load the statsmodels api\nimport statsmodels.api as sm\n\n# Load your dataset\nendog = pd.read_csv('your/dataset/here.csv')\n\n# Simple exponential smoothing, denoted (A,N,N)\nmod_ses = sm.tsa.statespace.ExponentialSmoothing(endog)\nres_ses = mod_ses.fit()\n\n# Holt's linear method, denoted (A,A,N)\nmod_h = sm.tsa.statespace.ExponentialSmoothing(endog, trend=True)\nres_h = mod_h.fit()\n\n# Damped trend model, denoted (A,Ad,N)\nmod_dt = sm.tsa.statespace.ExponentialSmoothing(endog, trend=True,\n                                                damped_trend=True)\nres_dt = mod_dt.fit()\n\n# Holt-Winters' trend and seasonality method, denoted (A,A,A)\n# (assuming that `endog` has a seasonal periodicity of 4, for example if it\n# is quarterly data).\nmod_hw = sm.tsa.statespace.ExponentialSmoothing(endog, trend=True,\n                                                seasonal=4)\nres_hw = mod_hw.fit()",
            "code"
        ],
        [
            "<strong>Differences between Statsmodels\u2019 exponential smoothing model classes</strong>",
            "markdown"
        ],
        [
            "There are several differences between this model class, available at\nsm.tsa.statespace.ExponentialSmoothing, and the model class available at\nsm.tsa.ExponentialSmoothing.",
            "markdown"
        ],
        [
            "This model class only supports linear exponential smoothing models, while\nsm.tsa.ExponentialSmoothing also supports multiplicative models.",
            "markdown"
        ],
        [
            "This model class puts the exponential smoothing models into state space form\nand then applies the Kalman filter to estimate the states, while\nsm.tsa.ExponentialSmoothing is based on exponential smoothing recursions.\nIn some cases, this can mean that estimating parameters with this model class\nwill be somewhat slower than with sm.tsa.ExponentialSmoothing.",
            "markdown"
        ],
        [
            "This model class can produce confidence intervals for forecasts, based on an\nassumption of Gaussian errors, while sm.tsa.ExponentialSmoothing does not\nsupport confidence intervals.",
            "markdown"
        ],
        [
            "This model class supports concentrating initial values out of the objective\nfunction, which can improve performance when there are many initial states to\nestimate (for example when the seasonal periodicity is large).",
            "markdown"
        ],
        [
            "This model class supports many advanced features available to state space\nmodels, such as diagnostics and fixed parameters.",
            "markdown"
        ],
        [
            "<strong>Note</strong>: this class is based on a \u201cmultiple sources of error\u201d (MSOE) state\nspace formulation and not a \u201csingle source of error\u201d (SSOE) formulation.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Models and Estimation->Custom state space models": [
        [
            "The true power of the state space model is to allow the creation and estimation\nof custom models. Usually that is done by extending the following two classes,\nwhich bundle all of state space representation, Kalman filtering, and maximum\nlikelihood fitting functionality for estimation and results output.",
            "markdown"
        ],
        [
            "For a basic example demonstrating creating and estimating a custom state space\nmodel, see the .\nFor a more sophisticated example, see the source code for the SARIMAX and\nSARIMAXResults classes, which are built by extending MLEModel and\nMLEResults.",
            "markdown"
        ],
        [
            "In simple cases, the model can be constructed entirely using the MLEModel\nclass. For example, the AR(2) model from above could be constructed and\nestimated using only the following code:",
            "markdown"
        ],
        [
            "import numpy as np\nfrom scipy.signal import lfilter\nimport statsmodels.api as sm\n\n# True model parameters\nnobs = int(1e3)\ntrue_phi = np.r_[0.5, -0.2]\ntrue_sigma = 1**0.5\n\n# Simulate a time series\nnp.random.seed(1234)\ndisturbances = np.random.normal(0, true_sigma, size=(nobs,))\nendog = lfilter([1], np.r_[1, -true_phi], disturbances)\n\n# Construct the model\nclass AR2(sm.tsa.statespace.MLEModel):\n    def __init__(self, endog):\n        # Initialize the state space model\n        super(AR2, self).__init__(endog, k_states=2, k_posdef=1,\n                                  initialization='stationary')\n\n        # Setup the fixed components of the state space representation\n        self['design'] = [1, 0]\n        self['transition'] = [[0, 0],\n                                  [1, 0]]\n        self['selection', 0, 0] = 1\n\n    # Describe how parameters enter the model\n    def update(self, params, transformed=True, **kwargs):\n        params = super(AR2, self).update(params, transformed, **kwargs)\n\n        self['transition', 0, :] = params[:2]\n        self['state_cov', 0, 0] = params[2]\n\n    # Specify start parameters and parameter names\n    @property\n    def start_params(self):\n        return [0,0,1]  # these are very simple\n\n# Create and fit the model\nmod = AR2(endog)\nres = mod.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "This results in the following summary table:",
            "markdown"
        ],
        [
            "Statespace Model Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                 1000\nModel:                            AR2   Log Likelihood               -1389.437\nDate:                Wed, 26 Oct 2016   AIC                           2784.874\nTime:                        00:42:03   BIC                           2799.598\nSample:                             0   HQIC                          2790.470\n                               - 1000\nCovariance Type:                  opg\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nparam.0        0.4395      0.030     14.730      0.000       0.381       0.498\nparam.1       -0.2055      0.032     -6.523      0.000      -0.267      -0.144\nparam.2        0.9425      0.042     22.413      0.000       0.860       1.025\n===================================================================================\nLjung-Box (Q):                       24.25   Jarque-Bera (JB):                 0.22\nProb(Q):                              0.98   Prob(JB):                         0.90\nHeteroskedasticity (H):               1.05   Skew:                            -0.04\nProb(H) (two-sided):                  0.66   Kurtosis:                         3.02\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "The results object has many of the attributes and methods you would expect from\nother statsmodels results objects, including standard errors, z-statistics,\nand prediction / forecasting.",
            "markdown"
        ],
        [
            "More advanced usage is possible, including specifying parameter\ntransformations, and specifying names for parameters for a more informative\noutput summary.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Overview of usage": [
        [
            "All state space models follow the typical Statsmodels pattern:",
            "markdown"
        ],
        [
            "Construct a <strong>model instance</strong> with an input dataset",
            "markdown"
        ],
        [
            "Apply parameters to the model (for example, using fit) to construct a <strong>results instance</strong>",
            "markdown"
        ],
        [
            "Interact with the results instance to examine the estimated parameters, explore residual diagnostics, and produce forecasts, simulations, or impulse responses.",
            "markdown"
        ],
        [
            "An example of this pattern is as follows:",
            "markdown"
        ],
        [
            "# Load in the example macroeconomic dataset\ndta = sm.datasets.macrodata.load_pandas().data\n# Make sure we have an index with an associated frequency, so that\n# we can refer to time periods with date strings or timestamps\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')\n\n# Step 1: construct an SARIMAX model for US inflation data\nmodel = sm.tsa.SARIMAX(dta.infl, order=(4, 0, 0), trend='c')\n\n# Step 2: fit the model's parameters by maximum likelihood\nresults = model.fit()\n\n# Step 3: explore / use results\n\n# - Print a table summarizing estimation results\nprint(results.summary())\n\n# - Print only the estimated parameters\nprint(results.params)\n\n# - Create diagnostic figures based on standardized residuals:\n#   (1) time series graph\n#   (2) histogram\n#   (3) Q-Q plot\n#   (4) correlogram\nresults.plot_diagnostics()\n\n# - Examine diagnostic hypothesis tests\n# Jarque-Bera: [test_statistic, pvalue, skewness, kurtosis]\nprint(results.test_normality(method='jarquebera'))\n# Goldfeld-Quandt type test: [test_statistic, pvalue]\nprint(results.test_heteroskedasticity(method='breakvar'))\n# Ljung-Box test: [test_statistic, pvalue] for each lag\nprint(results.test_serial_correlation(method='ljungbox'))\n\n# - Forecast the next 4 values\nprint(results.forecast(4))\n\n# - Forecast until 2020Q4\nprint(results.forecast('2020Q4'))\n\n# - Plot in-sample dynamic prediction starting in 2005Q1\n#   and out-of-sample forecasts until 2010Q4 along with\n#   90% confidence intervals\npredict_results = results.get_prediction(start='2005Q1', end='2010Q4', dynamic=True)\npredict_df = predict_results.summary_frame(alpha=0.10)\nfig, ax = plt.subplots()\npredict_df['mean'].plot(ax=ax)\nax.fill_between(predict_df.index, predict_df['mean_ci_lower'],\n                predict_df['mean_ci_upper'], alpha=0.2)\n\n# - Simulate two years of new data after the end of the sample\nprint(results.simulate(8, anchor='end'))\n\n# - Impulse responses for two years\nprint(results.impulse_responses(8))",
            "code"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Basic methods and attributes for estimation / filtering / smoothing": [
        [
            "The most-used methods for a state space model are:",
            "markdown"
        ],
        [
            "- estimate parameters via maximum\nlikelihood and return a results object (this object will have also performed\nKalman filtering and smoothing at the estimated parameters). This is the most\ncommonly used method.",
            "markdown"
        ],
        [
            "- return a results object\nassociated with a given vector of parameters after performing Kalman\nfiltering and smoothing",
            "markdown"
        ],
        [
            "- compute the log-likelihood\nof the data using a given vector of parameters",
            "markdown"
        ],
        [
            "Some useful attributes of a state space model are:",
            "markdown"
        ],
        [
            "- names of the\nparameters used by the model",
            "markdown"
        ],
        [
            "- names of the\nelements of the (unobserved) state vector",
            "markdown"
        ],
        [
            "- initial parameter\nestimates used a starting values for numerical maximum likelihood\noptimization",
            "markdown"
        ],
        [
            "Other methods that are used less often are:",
            "markdown"
        ],
        [
            "- return a results object\nassociated with a given vector of parameters after only performing Kalman\nfiltering (but not smoothing)",
            "markdown"
        ],
        [
            "-\nreturn an object that can perform simulation smoothing",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Output and postestimation methods and attributes": [
        [
            "Commonly used methods include:",
            "markdown"
        ],
        [
            "- construct a table that\npresents model fit statistics, estimated parameters, and other summary output",
            "markdown"
        ],
        [
            "- compute in-sample\npredictions and out-of-sample forecasts (point estimates only)",
            "markdown"
        ],
        [
            "- compute\nin-sample predictions and out-of-sample forecasts, including confidence\nintervals",
            "markdown"
        ],
        [
            "- compute out-of-sample\nforecasts (point estimates only) (this is a convenience wrapper around\npredict)",
            "markdown"
        ],
        [
            "- compute\nout-of-sample forecasts, including confidence intervals (this is a\nconvenience wrapper around get_prediction)",
            "markdown"
        ],
        [
            "- simulate new data\naccording to the state space model",
            "markdown"
        ],
        [
            "-\ncompute impulse responses from the state space model",
            "markdown"
        ],
        [
            "Commonly used attributes include:",
            "markdown"
        ],
        [
            "params - estimated parameters",
            "markdown"
        ],
        [
            "- standard errors of estimated\nparameters",
            "markdown"
        ],
        [
            "- p-values associated with\nestimated parameters",
            "markdown"
        ],
        [
            "- log-likelihood of the data at\nthe estimated parameters",
            "markdown"
        ],
        [
            ",\n, and\n - sum of squared errors,\nmean square error, and mean absolute error",
            "markdown"
        ],
        [
            "Information criteria, including: ,\n,\n, and",
            "markdown"
        ],
        [
            "- fitted values\nfrom the model (note that these are one-step-ahead predictions)",
            "markdown"
        ],
        [
            "- residuals from the model (note\nthat these are one-step-ahead prediction errors)",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Output and postestimation methods and attributes->Estimates and covariances of the unobserved state": [
        [
            "It can be useful to compute estimates of the unobserved state vector\nconditional on the observed data. These are available in the results object\n, which contains the following\nelements:",
            "markdown"
        ],
        [
            "states.filtered - filtered (one-sided) estimates of the state vector. The\nestimate of the state vector at time t is based on the observed data up\nto and including time t.",
            "markdown"
        ],
        [
            "states.smoothed - smoothed (two-sided) estimates of the state vector. The\nestimate of the state vector at time t is based on all observed data in\nthe sample.",
            "markdown"
        ],
        [
            "states.filtered_cov - filtered (one-sided) covariance of the state vector",
            "markdown"
        ],
        [
            "states.smoothed_cov - smoothed (two-sided) covariance of the state vector",
            "markdown"
        ],
        [
            "Each of these elements are Pandas DataFrame objects.",
            "markdown"
        ],
        [
            "As an example, in a \u201clocal level + seasonal\u201d model estimated via the\nUnobservedComponents components class we can get an estimates of the\nunderlying level and seasonal movements of a series over time.",
            "markdown"
        ],
        [
            "fig, axes = plt.subplots(3, 1, figsize=(8, 8))\n\n# Retrieve monthly retail sales for clothing\nfrom pandas_datareader.data import DataReader\nclothing = DataReader('MRTSSM4481USN', 'fred', start='1992').asfreq('MS')['MRTSSM4481USN']\n\n# Construct a local level + seasonal model\nmodel = sm.tsa.UnobservedComponents(clothing, 'llevel', seasonal=12)\nresults = model.fit()\n\n# Plot the data, the level, and seasonal\nclothing.plot(ax=axes[0])\nresults.states.smoothed['level'].plot(ax=axes[1])\nresults.states.smoothed['seasonal'].plot(ax=axes[2])",
            "code"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Output and postestimation methods and attributes->Residual diagnostics": [
        [
            "Three diagnostic tests are available after estimation of any statespace model,\nwhether built in or custom, to help assess whether the model conforms to the\nunderlying statistical assumptions. These tests are:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "A number of standard plots of regression residuals are available for the same\npurpose. These can be produced using the command\n.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Output and postestimation methods and attributes->Applying estimated parameters to an updated or different dataset": [
        [
            "There are three methods that can be used to apply estimated parameters from a\nresults object to an updated or different dataset:",
            "markdown"
        ],
        [
            "- retrieve a new results\nobject with additional observations that follow after the end of the current\nsample appended to it (so the new results object contains both the current\nsample and the additional observations)",
            "markdown"
        ],
        [
            "- retrieve a new results\nobject for additional observations that follow after end of the current\nsample (so the new results object contains only the new observations but NOT\nthe current sample)",
            "markdown"
        ],
        [
            "- retrieve a new results object\nfor a completely different dataset",
            "markdown"
        ],
        [
            "One cross-validation exercise on time-series data involves fitting a model\u2019s\nparameters based on a training sample (observations through time t) and\nthen evaluating the fit of the model using a test sample (observations t+1,\nt+2, \u2026). This can be conveniently done using either apply or extend. In\nthe example below, we use the extend method.",
            "markdown"
        ],
        [
            "# Load in the example macroeconomic dataset\ndta = sm.datasets.macrodata.load_pandas().data\n# Make sure we have an index with an associated frequency, so that\n# we can refer to time periods with date strings or timestamps\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')\n\n# Separate inflation data into a training and test dataset\ntraining_endog = dta['infl'].iloc[:-1]\ntest_endog = dta['infl'].iloc[-1:]\n\n# Fit an SARIMAX model for inflation\ntraining_model = sm.tsa.SARIMAX(training_endog, order=(4, 0, 0))\ntraining_results = training_model.fit()\n\n# Extend the results to the test observations\ntest_results = training_results.extend(test_endog)\n\n# Print the sum of squared errors in the test sample,\n# based on parameters computed using only the training sample\nprint(test_results.sse)",
            "code"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Output and postestimation methods and attributes->Understanding the Impact of Data Revisions": [
        [
            "Statespace model results expose a news method that\ncan be used to understand the impact of data revisions \u2013 news \u2013 on model\nparameters.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Additional options and tools": [
        [
            "All state space models have the following options and tools:",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Additional options and tools->Holding some parameters fixed and estimating the rest": [
        [
            "The  method\nallows fixing some parameters to known values and then estimating the rest via\nmaximum likelihood. An example of this is:",
            "markdown"
        ],
        [
            "# Construct a model\nmodel = sm.tsa.SARIMAX(endog, order=(1, 0, 0))\n\n# To find out the parameter names, use:\nprint(model.param_names)\n\n# Fit the model with a fixed value for the AR(1) coefficient:\nresults = model.fit_constrained({'ar.L1': 0.5})",
            "code"
        ],
        [
            "Alternatively, you can use the\n context manager:",
            "markdown"
        ],
        [
            "# Construct a model\nmodel = sm.tsa.SARIMAX(endog, order=(1, 0, 0))\n\n# Fit the model with a fixed value for the AR(1) coefficient using the\n# context manager\nwith model.fix_params({'ar.L1': 0.5}):\n    results = model.fit()",
            "code"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Additional options and tools->Low memory options": [
        [
            "When the observed dataset is very large and / or the state vector of the model\nis high-dimensional (for example when considering long seasonal effects), the\ndefault memory requirements can be too large. For this reason, the fit,\nfilter, and smooth methods accept an optional low_memory=True argument,\nwhich can considerably reduce memory requirements and speed up model fitting.",
            "markdown"
        ],
        [
            "Note that when using low_memory=True, not all results objects will be\navailable. However, residual diagnostics, in-sample (non-dynamic) prediction,\nand out-of-sample forecasting are all still available.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Low-level state space representation and Kalman filtering": [
        [
            "While creation of custom models will almost always be done by extending\nMLEModel and MLEResults, it can be useful to understand the superstructure\nbehind those classes.",
            "markdown"
        ],
        [
            "Maximum likelihood estimation requires evaluating the likelihood function of\nthe model, and for models in state space form the likelihood function is\nevaluated as a byproduct of running the Kalman filter.",
            "markdown"
        ],
        [
            "There are two classes used by MLEModel that facilitate specification of the\nstate space model and Kalman filtering: Representation and KalmanFilter.",
            "markdown"
        ],
        [
            "The Representation class is the piece where the state space model\nrepresentation is defined. In simple terms, it holds the state space matrices\n(design, obs_intercept, etc.; see the introduction to state space models,\nabove) and allows their manipulation.",
            "markdown"
        ],
        [
            "FrozenRepresentation is the most basic results-type class, in that it takes a\n\u201csnapshot\u201d of the state space representation at any given time. See the class\ndocumentation for the full list of available attributes.",
            "markdown"
        ],
        [
            "The KalmanFilter class is a subclass of Representation that provides\nfiltering capabilities. Once the state space representation matrices have been\nconstructed, the \nmethod can be called, producing a FilterResults instance; FilterResults is\na subclass of FrozenRepresentation.",
            "markdown"
        ],
        [
            "The FilterResults class not only holds a frozen representation of the state\nspace model (the design, transition, etc. matrices, as well as model\ndimensions, etc.) but it also holds the filtering output, including the\nfiltered state and\nloglikelihood (see the class documentation for the full list of available\nresults). It also provides a\n method, which allows\nin-sample prediction or out-of-sample forecasting. A similar method,\npredict, provides\nadditional prediction or forecasting results, including confidence intervals.",
            "markdown"
        ],
        [
            "The KalmanSmoother class is a subclass of KalmanFilter that provides\nsmoothing capabilities. Once the state space representation matrices have been\nconstructed, the \nmethod can be called, producing a SmootherResults instance; SmootherResults\nis a subclass of FilterResults.",
            "markdown"
        ],
        [
            "The SmootherResults class holds all the output from FilterResults, but\nalso includes smoothing output, including the\nsmoothed state and\nloglikelihood (see the class documentation for the full list of available\nresults). Whereas \u201cfiltered\u201d output at time t refers to estimates conditional\non observations up through time t, \u201csmoothed\u201d output refers to estimates\nconditional on the entire set of observations in the dataset.",
            "markdown"
        ],
        [
            "The SimulationSmoother class is a subclass of KalmanSmoother that further\nprovides simulation and simulation smoothing capabilities. The\n\nmethod can be called, producing a SimulationSmoothResults instance.",
            "markdown"
        ],
        [
            "The SimulationSmoothResults class has a simulate method, that allows\nperforming simulation smoothing to draw from the joint posterior of the state\nvector. This is useful for Bayesian estimation of state space models via Gibbs\nsampling.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Statespace Tools": [
        [
            "There are a variety of tools used for state space modeling or by the SARIMAX\nclass:",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar": [
        [
            "contains methods that are useful\nfor simultaneously modeling and analyzing multiple time series using\n and\n.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes": [
        [
            "We are interested in modeling a \\(T \\times K\\) multivariate time series\n\\(Y\\), where \\(T\\) denotes the number of observations and \\(K\\) the\nnumber of variables. One way of estimating relationships between the time series\nand their lagged values is the vector autoregression process:\n\n\\[ \\begin{align}\\begin{aligned}Y_t = \\nu + A_1 Y_{t-1} + \\ldots + A_p Y_{t-p} + u_t\\\\u_t \\sim {\\sf Normal}(0, \\Sigma_u)\\end{aligned}\\end{align} \\]",
            "markdown"
        ],
        [
            "where \\(A_i\\) is a \\(K \\times K\\) coefficient matrix.",
            "markdown"
        ],
        [
            "We follow in large part the methods and notation of ,\nwhich we will not develop here.",
            "markdown"
        ],
        [
            "Class Reference",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Model fitting": [
        [
            "Note",
            "markdown"
        ],
        [
            "The classes referenced below are accessible via the\nstatsmodels.tsa.api module.",
            "markdown"
        ],
        [
            "To estimate a VAR model, one must first create the model using an ndarray of\nhomogeneous or structured dtype. When using a structured or record array, the\nclass will use the passed variable names. Otherwise they can be passed\nexplicitly:",
            "markdown"
        ],
        [
            "# some example data\nIn [1]: import numpy as np\n\nIn [2]: import pandas\n\nIn [3]: import statsmodels.api as sm\n\nIn [4]: from statsmodels.tsa.api import VAR\n\nIn [5]: mdata = sm.datasets.macrodata.load_pandas().data\n\n# prepare the dates index\nIn [6]: dates = mdata[['year', 'quarter']].astype(int).astype(str)\n\nIn [7]: quarterly = dates[\"year\"] + \"Q\" + dates[\"quarter\"]\n\nIn [8]: from statsmodels.tsa.base.datetools import dates_from_str\n\nIn [9]: quarterly = dates_from_str(quarterly)\n\nIn [10]: mdata = mdata[['realgdp','realcons','realinv']]\n\nIn [11]: mdata.index = pandas.DatetimeIndex(quarterly)\n\nIn [12]: data = np.log(mdata).diff().dropna()\n\n# make a VAR model\nIn [13]: model = VAR(data)",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The  class assumes that the passed time series are\nstationary. Non-stationary or trending data can often be transformed to be\nstationary by first-differencing or some other method. For direct analysis of\nnon-stationary time series, a standard stable VAR(p) model is not\nappropriate.",
            "markdown"
        ],
        [
            "To actually do the estimation, call the fit method with the desired lag\norder. Or you can have the model select a lag order based on a standard\ninformation criterion (see below):",
            "markdown"
        ],
        [
            "In [14]: results = model.fit(2)\n\nIn [15]: results.summary()\nOut[15]: \n  Summary of Regression Results   \n==================================\nModel:                         VAR\nMethod:                        OLS\nDate:           Wed, 02, Nov, 2022\nTime:                     17:12:57\n--------------------------------------------------------------------\nNo. of Equations:         3.00000    BIC:                   -27.5830\nNobs:                     200.000    HQIC:                  -27.7892\nLog likelihood:           1962.57    FPE:                7.42129e-13\nAIC:                     -27.9293    Det(Omega_mle):     6.69358e-13\n--------------------------------------------------------------------\nResults for equation realgdp\n==============================================================================\n                 coefficient       std. error           t-stat            prob\n------------------------------------------------------------------------------\nconst               0.001527         0.001119            1.365           0.172\nL1.realgdp         -0.279435         0.169663           -1.647           0.100\nL1.realcons         0.675016         0.131285            5.142           0.000\nL1.realinv          0.033219         0.026194            1.268           0.205\nL2.realgdp          0.008221         0.173522            0.047           0.962\nL2.realcons         0.290458         0.145904            1.991           0.047\nL2.realinv         -0.007321         0.025786           -0.284           0.776\n==============================================================================\n\nResults for equation realcons\n==============================================================================\n                 coefficient       std. error           t-stat            prob\n------------------------------------------------------------------------------\nconst               0.005460         0.000969            5.634           0.000\nL1.realgdp         -0.100468         0.146924           -0.684           0.494\nL1.realcons         0.268640         0.113690            2.363           0.018\nL1.realinv          0.025739         0.022683            1.135           0.257\nL2.realgdp         -0.123174         0.150267           -0.820           0.412\nL2.realcons         0.232499         0.126350            1.840           0.066\nL2.realinv          0.023504         0.022330            1.053           0.293\n==============================================================================\n\nResults for equation realinv\n==============================================================================\n                 coefficient       std. error           t-stat            prob\n------------------------------------------------------------------------------\nconst              -0.023903         0.005863           -4.077           0.000\nL1.realgdp         -1.970974         0.888892           -2.217           0.027\nL1.realcons         4.414162         0.687825            6.418           0.000\nL1.realinv          0.225479         0.137234            1.643           0.100\nL2.realgdp          0.380786         0.909114            0.419           0.675\nL2.realcons         0.800281         0.764416            1.047           0.295\nL2.realinv         -0.124079         0.135098           -0.918           0.358\n==============================================================================\n\nCorrelation matrix of residuals\n             realgdp  realcons   realinv\nrealgdp     1.000000  0.603316  0.750722\nrealcons    0.603316  1.000000  0.131951\nrealinv     0.750722  0.131951  1.000000",
            "code"
        ],
        [
            "Several ways to visualize the data using matplotlib are available.",
            "markdown"
        ],
        [
            "Plotting input time series:",
            "markdown"
        ],
        [
            "In [16]: results.plot()\nOut[16]: &lt;Figure size 1000x1000 with 3 Axes\n\n\n<img alt=\"_images/var_plot_input.png\" src=\"_images/var_plot_input.png\"/>",
            "code"
        ],
        [
            "Plotting time series autocorrelation function:",
            "markdown"
        ],
        [
            "In [17]: results.plot_acorr()\nOut[17]: &lt;Figure size 1000x1000 with 9 Axes\n\n\n<img alt=\"_images/var_plot_acorr.png\" src=\"_images/var_plot_acorr.png\"/>",
            "code"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Lag order selection": [
        [
            "Choice of lag order can be a difficult problem. Standard analysis employs\nlikelihood test or information criteria-based order selection. We have\nimplemented the latter, accessible through the  class:",
            "markdown"
        ],
        [
            "In [18]: model.select_order(15)\nOut[18]: &lt;statsmodels.tsa.vector_ar.var_model.LagOrderResults at 0x7f6bfc21e830",
            "code"
        ],
        [
            "When calling the fit function, one can pass a maximum number of lags and the\norder criterion to use for order selection:",
            "markdown"
        ],
        [
            "In [19]: results = model.fit(maxlags=15, ic='aic')",
            "code"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Forecasting": [
        [
            "The linear predictor is the optimal h-step ahead forecast in terms of\nmean-squared error:\n\n\\[y_t(h) = \\nu + A_1 y_t(h \u2212 1) + \\cdots + A_p y_t(h \u2212 p)\\]",
            "markdown"
        ],
        [
            "We can use the forecast function to produce this forecast. Note that we have\nto specify the \u201cinitial value\u201d for the forecast:",
            "markdown"
        ],
        [
            "In [20]: lag_order = results.k_ar\n\nIn [21]: results.forecast(data.values[-lag_order:], 5)\nOut[21]: \narray([[ 0.00616044,  0.00500006,  0.00916198],\n       [ 0.00427559,  0.00344836, -0.00238478],\n       [ 0.00416634,  0.0070728 , -0.01193629],\n       [ 0.00557873,  0.00642784,  0.00147152],\n       [ 0.00626431,  0.00666715,  0.00379567]])",
            "code"
        ],
        [
            "The forecast_interval function will produce the above forecast along with\nasymptotic standard errors. These can be visualized using the plot_forecast\nfunction:",
            "markdown"
        ],
        [
            "In [22]: results.plot_forecast(10)\nOut[22]: &lt;Figure size 1000x1000 with 3 Axes\n\n\n<img alt=\"_images/var_forecast.png\" src=\"_images/var_forecast.png\"/>",
            "code"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Post-estimation Analysis": [
        [
            "Several process properties and additional results after\nestimation are available for vector autoregressive processes.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->Impulse Response Analysis": [
        [
            "Impulse responses are of interest in econometric studies: they are the\nestimated responses to a unit impulse in one of the variables. They are computed\nin practice using the MA(\\(\\infty\\)) representation of the VAR(p) process:\n\n\\[Y_t = \\mu + \\sum_{i=0}^\\infty \\Phi_i u_{t-i}\\]",
            "markdown"
        ],
        [
            "We can perform an impulse response analysis by calling the irf function on a\nVARResults object:",
            "markdown"
        ],
        [
            "In [23]: irf = results.irf(10)",
            "code"
        ],
        [
            "These can be visualized using the plot function, in either orthogonalized or\nnon-orthogonalized form. Asymptotic standard errors are plotted by default at\nthe 95% significance level, which can be modified by the user.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Orthogonalization is done using the Cholesky decomposition of the estimated\nerror covariance matrix \\(\\hat \\Sigma_u\\) and hence interpretations may\nchange depending on variable ordering.",
            "markdown"
        ],
        [
            "In [24]: irf.plot(orth=False)\nOut[24]: &lt;Figure size 1000x1000 with 9 Axes\n\n\n<img alt=\"_images/var_irf.png\" src=\"_images/var_irf.png\"/>",
            "code"
        ],
        [
            "Note the plot function is flexible and can plot only variables of interest if\nso desired:",
            "markdown"
        ],
        [
            "In [25]: irf.plot(impulse='realgdp')\nOut[25]: &lt;Figure size 1000x1000 with 3 Axes\n\n\n<img alt=\"_images/var_realgdp.png\" src=\"_images/var_realgdp.png\"/>",
            "code"
        ],
        [
            "The cumulative effects \\(\\Psi_n = \\sum_{i=0}^n \\Phi_i\\) can be plotted with\nthe long run effects as follows:",
            "markdown"
        ],
        [
            "In [26]: irf.plot_cum_effects(orth=False)\nOut[26]: &lt;Figure size 1000x1000 with 9 Axes\n\n\n<img alt=\"_images/var_irf_cum.png\" src=\"_images/var_irf_cum.png\"/>",
            "code"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->Forecast Error Variance Decomposition (FEVD)": [
        [
            "Forecast errors of component j on k in an i-step ahead forecast can be\ndecomposed using the orthogonalized impulse responses \\(\\Theta_i\\):\n\n\\[ \\begin{align}\\begin{aligned}\\omega_{jk, i} = \\sum_{i=0}^{h-1} (e_j^\\prime \\Theta_i e_k)^2 / \\mathrm{MSE}_j(h)\\\\\\mathrm{MSE}_j(h) = \\sum_{i=0}^{h-1} e_j^\\prime \\Phi_i \\Sigma_u \\Phi_i^\\prime e_j\\end{aligned}\\end{align} \\]",
            "markdown"
        ],
        [
            "These are computed via the fevd function up through a total number of steps ahead:",
            "markdown"
        ],
        [
            "In [27]: fevd = results.fevd(5)\n\nIn [28]: fevd.summary()\nFEVD for realgdp\n      realgdp  realcons   realinv\n0    1.000000  0.000000  0.000000\n1    0.864889  0.129253  0.005858\n2    0.816725  0.177898  0.005378\n3    0.793647  0.197590  0.008763\n4    0.777279  0.208127  0.014594\n\nFEVD for realcons\n      realgdp  realcons   realinv\n0    0.359877  0.640123  0.000000\n1    0.358767  0.635420  0.005813\n2    0.348044  0.645138  0.006817\n3    0.319913  0.653609  0.026478\n4    0.317407  0.652180  0.030414\n\nFEVD for realinv\n      realgdp  realcons   realinv\n0    0.577021  0.152783  0.270196\n1    0.488158  0.293622  0.218220\n2    0.478727  0.314398  0.206874\n3    0.477182  0.315564  0.207254\n4    0.466741  0.324135  0.209124",
            "code"
        ],
        [
            "They can also be visualized through the returned FEVD object:",
            "markdown"
        ],
        [
            "In [29]: results.fevd(20).plot()\nOut[29]: &lt;Figure size 1000x1000 with 3 Axes\n\n\n<img alt=\"_images/var_fevd.png\" src=\"_images/var_fevd.png\"/>",
            "code"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->Statistical tests": [
        [
            "A number of different methods are provided to carry out hypothesis tests about\nthe model results and also the validity of the model assumptions (normality,\nwhiteness / \u201ciid-ness\u201d of errors, etc.).",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->Statistical tests->Granger causality": [
        [
            "One is often interested in whether a variable or group of variables is \u201ccausal\u201d\nfor another variable, for some definition of \u201ccausal\u201d. In the context of VAR\nmodels, one can say that a set of variables are Granger-causal within one of the\nVAR equations. We will not detail the mathematics or definition of Granger\ncausality, but leave it to the reader. The  object has the\ntest_causality method for performing either a Wald (\\(\\chi^2\\)) test or an\nF-test.",
            "markdown"
        ],
        [
            "In [30]: results.test_causality('realgdp', ['realinv', 'realcons'], kind='f')\nOut[30]: &lt;statsmodels.tsa.vector_ar.hypothesis_test_results.CausalityTestResults at 0x7f6bf7aa7790",
            "code"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->Statistical tests->Normality": [
        [
            "As pointed out in the beginning of this document, the white noise component\n\\(u_t\\) is assumed to be normally distributed. While this assumption\nis not required for parameter estimates to be consistent or asymptotically\nnormal, results are generally more reliable in finite samples when residuals\nare Gaussian white noise. To test whether this assumption is consistent with\na data set,  offers the test_normality method.",
            "markdown"
        ],
        [
            "In [31]: results.test_normality()\nOut[31]: &lt;statsmodels.tsa.vector_ar.hypothesis_test_results.NormalityTestResults at 0x7f6bf7dbf850",
            "code"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->Statistical tests->Whiteness of residuals": [
        [
            "To test the whiteness of the estimation residuals (this means absence of\nsignificant residual autocorrelations) one can use the test_whiteness\nmethod of .",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->Structural Vector Autoregressions": [
        [
            "There are a matching set of classes that handle some types of Structural VAR models.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->Vector Error Correction Models (VECM)": [
        [
            "Vector Error Correction Models are used to study short-run deviations from\none or more permanent stochastic trends (unit roots). A VECM models the\ndifference of a vector of time series by imposing structure that is implied\nby the assumed number of stochastic trends. VECM is used to\nspecify and estimate these models.",
            "markdown"
        ],
        [
            "A VECM(\\(k_{ar}-1\\)) has the following form\n\n\\[\\Delta y_t = \\Pi y_{t-1} + \\Gamma_1 \\Delta y_{t-1} + \\ldots\n               + \\Gamma_{k_{ar}-1} \\Delta y_{t-k_{ar}+1} + u_t\\]",
            "markdown"
        ],
        [
            "where\n\n\\[\\Pi = \\alpha \\beta'\\]",
            "markdown"
        ],
        [
            "as described in chapter 7 of .",
            "markdown"
        ],
        [
            "A VECM(\\(k_{ar} - 1\\)) with deterministic terms has the form\n\n\\[\\begin{split}\\Delta y_t = \\alpha \\begin{pmatrix}\\beta' & \\eta'\\end{pmatrix} \\begin{pmatrix}y_{t-1} \\\\\n             D^{co}_{t-1}\\end{pmatrix} + \\Gamma_1 \\Delta y_{t-1} + \\dots + \\Gamma_{k_{ar}-1} \\Delta y_{t-k_{ar}+1} + C D_t + u_t.\\end{split}\\]",
            "markdown"
        ],
        [
            "In \\(D^{co}_{t-1}\\) we have the deterministic terms which are inside\nthe cointegration relation (or restricted to the cointegration relation).\n\\(\\eta\\) is the corresponding estimator. To pass a deterministic term\ninside the cointegration relation, we can use the exog_coint argument.\nFor the two special cases of an intercept and a linear trend there exists\na simpler way to declare these terms: we can pass \"ci\" and \"li\"\nrespectively to the deterministic argument. So for an intercept inside\nthe cointegration relation we can either pass \"ci\" as deterministic\nor np.ones(len(data)) as exog_coint if data is passed as the\nendog argument. This ensures that \\(D_{t-1}^{co} = 1\\) for all\n\\(t\\).",
            "markdown"
        ],
        [
            "We can also use deterministic terms outside the cointegration relation.\nThese are defined in \\(D_t\\) in the formula above with the\ncorresponding estimators in the matrix \\(C\\). We specify such terms by\npassing them to the exog argument. For an intercept and/or linear trend\nwe again have the possibility to use deterministic alternatively. For\nan intercept we pass \"co\" and for a linear trend we pass \"lo\" where\nthe o stands for outside.",
            "markdown"
        ],
        [
            "The following table shows the five cases considered in . The last\ncolumn indicates which string to pass to the deterministic argument for\neach of these cases.",
            "markdown"
        ]
    ],
    "User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->References": [
        [
            "L\u00fctkepohl, H. 2005. New Introduction to Multiple Time Series Analysis. Springer.\n\n\n[]",
            "markdown"
        ],
        [
            "Johansen, S. 1995. Likelihood-Based Inference in Cointegrated *\n*Vector Autoregressive Models. Oxford University Press.",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Methods for Survival and Duration Analysis": [
        [
            "implements several standard methods for\nworking with censored data.  These methods are most commonly used when\nthe data consist of durations between an origin time point and the\ntime at which some event of interest occurred.  A typical example is a\nmedical study in which the origin is the time at which a subject is\ndiagnosed with some condition, and the event of interest is death (or\ndisease progression, recovery, etc.).",
            "markdown"
        ],
        [
            "Currently only right-censoring is handled.  Right censoring occurs\nwhen we know that an event occurred after a given time t, but we do\nnot know the exact event time.",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Methods for Survival and Duration Analysis->Survival function estimation and inference": [
        [
            "The statsmodels.api.SurvfuncRight class can be used to\nestimate a survival function using data that may be right censored.\nSurvfuncRight implements several inference procedures including\nconfidence intervals for survival distribution quantiles, pointwise\nand simultaneous confidence bands for the survival function, and\nplotting procedures.  The duration.survdiff function provides\ntesting procedures for comparing survival distributions.",
            "markdown"
        ],
        [
            "Here we create a SurvfuncRight object using data from the\nflchain study, which is available through the R datasets repository.\nWe fit the survival distribution only for the female subjects.",
            "markdown"
        ],
        [
            "import statsmodels.api as sm\n\ndata = sm.datasets.get_rdataset(\"flchain\", \"survival\").data\ndf = data.loc[data.sex == \"F\", :]\nsf = sm.SurvfuncRight(df[\"futime\"], df[\"death\"])",
            "code"
        ],
        [
            "The main features of the fitted survival distribution can be seen by\ncalling the summary method:",
            "markdown"
        ],
        [
            "sf.summary().head()",
            "code"
        ],
        [
            "We can obtain point estimates and confidence intervals for quantiles\nof the survival distribution.  Since only around 30% of the subjects\ndied during this study, we can only estimate quantiles below the 0.3\nprobability point:",
            "markdown"
        ],
        [
            "sf.quantile(0.25)\nsf.quantile_ci(0.25)",
            "code"
        ],
        [
            "To plot a single survival function, call the plot method:",
            "markdown"
        ],
        [
            "sf.plot()",
            "code"
        ],
        [
            "Since this is a large dataset with a lot of censoring, we may wish\nto not plot the censoring symbols:",
            "markdown"
        ],
        [
            "fig = sf.plot()\nax = fig.get_axes()[0]\npt = ax.get_lines()[1]\npt.set_visible(False)",
            "code"
        ],
        [
            "We can also add a 95% simultaneous confidence band to the plot.\nTypically these bands only plotted for central part of the\ndistribution.",
            "markdown"
        ],
        [
            "fig = sf.plot()\nlcb, ucb = sf.simultaneous_cb()\nax = fig.get_axes()[0]\nax.fill_between(sf.surv_times, lcb, ucb, color='lightgrey')\nax.set_xlim(365, 365*10)\nax.set_ylim(0.7, 1)\nax.set_ylabel(\"Proportion alive\")\nax.set_xlabel(\"Days since enrollment\")",
            "code"
        ],
        [
            "Here we plot survival functions for two groups (females and males) on\nthe same axes:",
            "markdown"
        ],
        [
            "gb = data.groupby(\"sex\")\nax = plt.axes()\nsexes = []\nfor g in gb:\n    sexes.append(g[0])\n    sf = sm.SurvfuncRight(g[1][\"futime\"], g[1][\"death\"])\n    sf.plot(ax)\nli = ax.get_lines()\nli[1].set_visible(False)\nli[3].set_visible(False)\nplt.figlegend((li[0], li[2]), sexes, \"center right\")\nplt.ylim(0.6, 1)\nax.set_ylabel(\"Proportion alive\")\nax.set_xlabel(\"Days since enrollment\")",
            "code"
        ],
        [
            "We can formally compare two survival distributions with survdiff,\nwhich implements several standard nonparametric procedures.  The\ndefault procedure is the logrank test:",
            "markdown"
        ],
        [
            "stat, pv = sm.duration.survdiff(data.futime, data.death, data.sex)",
            "code"
        ],
        [
            "Here are some of the other testing procedures implemented by survdiff:",
            "markdown"
        ],
        [
            "# Fleming-Harrington with p=1, i.e. weight by pooled survival time\nstat, pv = sm.duration.survdiff(data.futime, data.death, data.sex, weight_type='fh', fh_p=1)\n\n# Gehan-Breslow, weight by number at risk\nstat, pv = sm.duration.survdiff(data.futime, data.death, data.sex, weight_type='gb')\n\n# Tarone-Ware, weight by the square root of the number at risk\nstat, pv = sm.duration.survdiff(data.futime, data.death, data.sex, weight_type='tw')",
            "code"
        ]
    ],
    "User Guide->Other Models->Methods for Survival and Duration Analysis->Regression methods": [
        [
            "Proportional hazard regression models (\u201cCox models\u201d) are a regression\ntechnique for censored data.  They allow variation in the time to an\nevent to be explained in terms of covariates, similar to what is done\nin a linear or generalized linear regression model.  These models\nexpress the covariate effects in terms of \u201chazard ratios\u201d, meaning the\nthe hazard (instantaneous event rate) is multiplied by a given factor\ndepending on the value of the covariates.",
            "markdown"
        ],
        [
            "import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\ndata = sm.datasets.get_rdataset(\"flchain\", \"survival\").data\ndel data[\"chapter\"]\ndata = data.dropna()\ndata[\"lam\"] = data[\"lambda\"]\ndata[\"female\"] = (data[\"sex\"] == \"F\").astype(int)\ndata[\"year\"] = data[\"sample.yr\"] - min(data[\"sample.yr\"])\nstatus = data[\"death\"].values\n\nmod = smf.phreg(\"futime ~ 0 + age + female + creatinine + \"\n                \"np.sqrt(kappa) + np.sqrt(lam) + year + mgus\",\n                data, status=status, ties=\"efron\")\nrslt = mod.fit()\nprint(rslt.summary())",
            "code"
        ],
        [
            "See  for more detailed examples.",
            "markdown"
        ],
        [
            "There are some notebook examples on the Wiki:",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Methods for Survival and Duration Analysis->Regression methods->References": [
        [
            "References for Cox proportional hazards regression model:",
            "markdown"
        ],
        [
            "T Therneau (1996). Extending the Cox model. Technical report.\nhttp://www.mayo.edu/research/documents/biostat-58pdf/DOC-10027288\n\nG Rodriguez (2005). Non-parametric estimation in survival models.\nhttp://data.princeton.edu/pop509/NonParametricSurvival.pdf\n\nB Gillespie (2006). Checking the assumptions in the Cox proportional\nhazards model.\nhttp://www.mwsug.org/proceedings/2006/stats/MWSUG-2006-SD08.pdf",
            "code"
        ]
    ],
    "User Guide->Other Models->Methods for Survival and Duration Analysis->Module Reference": [
        [
            "The class for working with survival distributions is:",
            "markdown"
        ],
        [
            "The proportional hazards regression model class is:",
            "markdown"
        ],
        [
            "The proportional hazards regression result class is:",
            "markdown"
        ],
        [
            "The primary helper class is:",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Nonparametric Methods nonparametric": [
        [
            "This section collects various methods in nonparametric statistics. This\nincludes kernel density estimation for univariate and multivariate data,\nkernel regression and locally weighted scatterplot smoothing (lowess).",
            "markdown"
        ],
        [
            "sandbox.nonparametric contains additional functions that are work in progress\nor do not have unit tests yet. We are planning to include here nonparametric\ndensity estimators, especially based on kernel or orthogonal polynomials,\nsmoothers, and tools for nonparametric models and methods in other parts of\nstatsmodels.",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Nonparametric Methods nonparametric->Kernel density estimation": [
        [
            "The kernel density estimation (KDE) functionality is split between univariate\nand multivariate estimation, which are implemented in quite different ways.",
            "markdown"
        ],
        [
            "Univariate estimation (as provided by KDEUnivariate) uses FFT transforms,\nwhich makes it quite fast.  Therefore it should be preferred for continuous,\nunivariate data if speed is important.  It supports using different kernels;\nbandwidth estimation is done only by a rule of thumb (Scott or Silverman).",
            "markdown"
        ],
        [
            "Multivariate estimation (as provided by KDEMultivariate) uses product\nkernels.   It supports least squares and maximum likelihood cross-validation\nfor bandwidth estimation, as well as estimating mixed continuous, ordered and\nunordered data.  The default kernels (Gaussian, Wang-Ryzin and\nAitchison-Aitken) cannot be altered at the moment however.  Direct estimation\nof the conditional density (\\(P(X | Y) = P(X, Y) / P(Y)\\)) is supported\nby KDEMultivariateConditional.",
            "markdown"
        ],
        [
            "KDEMultivariate can do univariate estimation as well, but is up to two orders\nof magnitude slower than KDEUnivariate.",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Nonparametric Methods nonparametric->Kernel regression": [
        [
            "Kernel regression (as provided by KernelReg) is based on the same product\nkernel approach as KDEMultivariate, and therefore has the same set of\nfeatures (mixed data, cross-validated bandwidth estimation, kernels) as\ndescribed above for KDEMultivariate.  Censored regression is provided by\nKernelCensoredReg.",
            "markdown"
        ],
        [
            "Note that code for semi-parametric partial linear models and single index\nmodels, based on KernelReg, can be found in the sandbox.",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Nonparametric Methods nonparametric->References": [
        [
            "B.W. Silverman, \u201cDensity Estimation for Statistics and Data Analysis\u201d",
            "markdown"
        ],
        [
            "J.S. Racine, \u201cNonparametric Econometrics: A Primer,\u201d Foundation and\nTrends in Econometrics, Vol. 3, No. 1, pp. 1-88, 2008.",
            "markdown"
        ],
        [
            "Q. Li and J.S. Racine, \u201cNonparametric econometrics: theory and practice\u201d,\nPrinceton University Press, 2006.",
            "markdown"
        ],
        [
            "Hastie, Tibshirani and Friedman, \u201cThe Elements of Statistical Learning:\nData Mining, Inference, and Prediction\u201d, Springer, 2009.",
            "markdown"
        ],
        [
            "Racine, J., Li, Q. \u201cNonparametric Estimation of Distributions\nwith Categorical and Continuous Data.\u201d Working Paper. (2000)",
            "markdown"
        ],
        [
            "Racine, J. Li, Q. \u201cKernel Estimation of Multivariate Conditional\nDistributions Annals of Economics and Finance 5, 211-235 (2004)",
            "markdown"
        ],
        [
            "Liu, R., Yang, L. \u201cKernel estimation of multivariate\ncumulative distribution function.\u201d Journal of Nonparametric Statistics\n(2008)",
            "markdown"
        ],
        [
            "Li, R., Ju, G. \u201cNonparametric Estimation of Multivariate CDF\nwith Categorical and Continuous Data.\u201d Working Paper",
            "markdown"
        ],
        [
            "Li, Q., Racine, J. \u201cCross-validated local linear nonparametric\nregression\u201d Statistica Sinica 14(2004), pp. 485-512",
            "markdown"
        ],
        [
            "Racine, J.: \u201cConsistent Significance Testing for Nonparametric\nRegression\u201d Journal of Business & Economics Statistics",
            "markdown"
        ],
        [
            "Racine, J., Hart, J., Li, Q., \u201cTesting the Significance of\nCategorical Predictor Variables in Nonparametric Regression\nModels\u201d, 2006, Econometric Reviews 25, 523-544",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Nonparametric Methods nonparametric->Module Reference": [
        [
            "The public functions and classes are",
            "markdown"
        ],
        [
            "helper functions for kernel bandwidths",
            "markdown"
        ],
        [
            "There are some examples for nonlinear functions in\nstatsmodels.nonparametric.dgp_examples",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Nonparametric Methods nonparametric->Asymmetric Kernels": [
        [
            "Asymmetric kernels like beta for the unit interval and gamma for positive\nvalued random variables avoid problems at the boundary of the support of the\ndistribution.",
            "markdown"
        ],
        [
            "Statsmodels has preliminary support for estimating density and cumulative\ndistribution function using kernels for the unit interval, beta or the\npositive real line, all other kernels.",
            "markdown"
        ],
        [
            "Several of the kernels for the positive real line assume that the density at\nthe zero boundary is zero. The gamma kernel also allows the case of positive\nor unbound density at the zero boundary.",
            "markdown"
        ],
        [
            "There are currently no defaults and no support for choosing the bandwidth. the\nuser has to provide the bandwidth.",
            "markdown"
        ],
        [
            "The functions to compute kernel density and kernel cdf are",
            "markdown"
        ],
        [
            "The available kernel functions for pdf and cdf are",
            "markdown"
        ],
        [
            "The sandbox.nonparametric contains additional insufficiently tested classes\nfor testing functional form and for semi-linear and single index models.",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Generalized Method of Moments gmm": [
        [
            "statsmodels.gmm contains model classes and functions that are based on\nestimation with Generalized Method of Moments.\nCurrently the general non-linear case is implemented. An example class for the standard\nlinear instrumental variable model is included. This has been introduced as a test case, it\nworks correctly but it does not take the linear structure into account. For the linear\ncase we intend to introduce a specific implementation which will be faster and numerically\nmore accurate.",
            "markdown"
        ],
        [
            "Currently, GMM takes arbitrary non-linear moment conditions and calculates the estimates\neither for a given weighting matrix or iteratively by alternating between estimating\nthe optimal weighting matrix and estimating the parameters. Implementing models with\ndifferent moment conditions is done by subclassing GMM. In the minimal implementation\nonly the moment conditions, momcond have to be defined.",
            "markdown"
        ],
        [
            "Module Reference",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Other Models miscmodels": [
        [
            "contains model classes and that do not yet fit into\nany other category, or are basic implementations that are not yet polished and will most\nlikely still change. Some of these models were written as examples for the generic\nmaximum likelihood framework, and there will be others that might be based on general\nmethod of moments.",
            "markdown"
        ],
        [
            "The models in this category have been checked for basic cases, but might be more exposed\nto numerical problems than the complete implementation. For example, count.Poisson has\nbeen added using only the generic maximum likelihood framework, the standard errors\nare based on the numerical evaluation of the Hessian, while discretemod.Poisson uses\nanalytical Gradients and Hessian and will be more precise, especially in cases when there\nis strong multicollinearity.\nOn the other hand, by subclassing GenericLikelihoodModel, it is easy to add new models,\nanother example can be seen in the zero inflated Poisson model, miscmodels.count.",
            "markdown"
        ],
        [
            "Count Models count",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Other Models miscmodels->Linear Model with t-distributed errors": [
        [
            "This is a class that shows that a new model can be defined by only specifying the\nmethod for the loglikelihood. All result statistics are inherited from the generic\nlikelihood model and result classes. The results have been checked against R for a\nsimple case.",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Multivariate Statistics multivariate": [
        [
            "This section includes methods and algorithms from multivariate statistics.",
            "markdown"
        ],
        [
            "Principal Component Analysis",
            "markdown"
        ],
        [
            "Factor Analysis",
            "markdown"
        ],
        [
            "Factor Rotation",
            "markdown"
        ],
        [
            "Canonical Correlation",
            "markdown"
        ],
        [
            "MANOVA",
            "markdown"
        ]
    ],
    "User Guide->Other Models->Multivariate Statistics multivariate->MultivariateOLS": [
        [
            "_MultivariateOLS is a model class with limited features. Currently it\nsupports multivariate hypothesis tests and is used as backend for MANOVA.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats": [
        [
            "This section collects various statistical tests and tools.\nSome can be used independently of any models, some are intended as extension to the\nmodels and model results.",
            "markdown"
        ],
        [
            "API Warning: The functions and objects in this category are spread out in\nvarious modules and might still be moved around. We expect that in future the\nstatistical tests will return class instances with more informative reporting\ninstead of only the raw numbers.",
            "markdown"
        ],
        [
            "Non-Parametric Tests",
            "markdown"
        ],
        [
            "Descriptive Statistics",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Residual Diagnostics and Specification Tests->Outliers and influence measures": [
        [
            "See also the notes on",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Sandwich Robust Covariances": [
        [
            "The following functions calculate covariance matrices and standard errors for\nthe parameter estimates that are robust to heteroscedasticity and\nautocorrelation in the errors. Similar to the methods that are available\nfor the LinearModelResults, these methods are designed for use with OLS.",
            "markdown"
        ],
        [
            "The following are standalone versions of the heteroscedasticity robust\nstandard errors attached to LinearModelResults",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Goodness of Fit Tests and Measures": [
        [
            "some tests for goodness of fit for univariate distributions",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Interrater Reliability and Agreement": [
        [
            "The main function that statsmodels has currently available for interrater\nagreement measures and tests is Cohen\u2019s Kappa. Fleiss\u2019 Kappa is currently\nonly implemented as a measures but without associated results statistics.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Multiple Tests and Multiple Comparison Procedures": [
        [
            "multipletests is a function for p-value correction, which also includes p-value\ncorrection based on fdr in fdrcorrection.\ntukeyhsd performs simultaneous testing for the comparison of (independent) means.\nThese three functions are verified.\nGroupsStats and MultiComparison are convenience classes to multiple comparisons similar\nto one way ANOVA, but still in development",
            "markdown"
        ],
        [
            "The following functions are not (yet) public",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Basic Statistics and t-Tests with frequency weights": [
        [
            "Besides basic statistics, like mean, variance, covariance and correlation for\ndata with case weights, the classes here provide one and two sample tests\nfor means. The t-tests have more options than those in scipy.stats, but are\nmore restrictive in the shape of the arrays. Confidence intervals for means\nare provided based on the same assumptions as the t-tests.",
            "markdown"
        ],
        [
            "Additionally, tests for equivalence of means are available for one sample and\nfor two, either paired or independent, samples. These tests are based on TOST,\ntwo one-sided tests, which have as null hypothesis that the means are not\n\u201cclose\u201d to each other.",
            "markdown"
        ],
        [
            "weightstats also contains tests and confidence intervals based on summary\ndata",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Power and Sample Size Calculations": [
        [
            "The power module currently implements power and sample size calculations\nfor the t-tests, normal based test, F-tests and Chisquare goodness of fit test.\nThe implementation is class based, but the module also provides\nthree shortcut functions, tt_solve_power, tt_ind_solve_power and\nzt_ind_solve_power to solve for any one of the parameters of the power\nequations.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Proportion": [
        [
            "Also available are hypothesis test, confidence intervals and effect size for\nproportions that can be used with NormalIndPower.",
            "markdown"
        ],
        [
            "Statistics for two independent samples\nStatus: experimental, API might change, added in 0.12",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Rates": [
        [
            "Statistical functions for rates. This currently includes hypothesis tests for\ntwo independent samples.",
            "markdown"
        ],
        [
            "Status: experimental, API might change, added in 0.12",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Multivariate": [
        [
            "Statistical functions for multivariate samples.",
            "markdown"
        ],
        [
            "This includes hypothesis test and confidence intervals for mean of sample\nof multivariate observations and hypothesis tests for the structure of a\ncovariance matrix.",
            "markdown"
        ],
        [
            "Status: experimental, API might change, added in 0.12",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Oneway Anova": [
        [
            "Hypothesis test, confidence intervals and effect size for oneway analysis of\nk samples.",
            "markdown"
        ],
        [
            "Status: experimental, API might change, added in 0.12",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Robust, Trimmed Statistics": [
        [
            "Statistics for samples that are trimmed at a fixed fraction. This includes\nclass TrimmedMean for one sample statistics. It is used in stats.oneway\nfor trimmed \u201cYuen\u201d Anova.",
            "markdown"
        ],
        [
            "Status: experimental, API might change, added in 0.12",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Moment Helpers": [
        [
            "When there are missing values, then it is possible that a correlation or\ncovariance matrix is not positive semi-definite. The following\nfunctions can be used to find a correlation or covariance matrix that is\npositive definite and close to the original matrix.\nAdditional functions estimate spatial covariance matrix and regularized\ninverse covariance or precision matrix.",
            "markdown"
        ],
        [
            "These are utility functions to convert between central and non-central moments, skew,\nkurtosis and cummulants.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Mediation Analysis": [
        [
            "Mediation analysis focuses on the relationships among three key variables:\nan \u2018outcome\u2019, a \u2018treatment\u2019, and a \u2018mediator\u2019. Since mediation analysis is a\nform of causal inference, there are several assumptions involved that are\ndifficult or impossible to verify. Ideally, mediation analysis is conducted in\nthe context of an experiment such as this one in which the treatment is\nrandomly assigned. It is also common for people to conduct mediation analyses\nusing observational data in which the treatment may be thought of as an\n\u2018exposure\u2019. The assumptions behind mediation analysis are even more difficult\nto verify in an observational setting.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Oaxaca-Blinder Decomposition": [
        [
            "The Oaxaca-Blinder, or Blinder-Oaxaca as some call it, decomposition attempts to explain\ngaps in means of groups. It uses the linear models of two given regression equations to\nshow what is explained by regression coefficients and known data and what is unexplained\nusing the same data. There are two types of Oaxaca-Blinder decompositions, the two-fold\nand the three-fold, both of which can and are used in Economics Literature to discuss\ndifferences in groups. This method helps classify discrimination or unobserved effects.\nThis function attempts to port the functionality of the oaxaca command in STATA to Python.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Distance Dependence Measures": [
        [
            "Distance dependence measures and the Distance Covariance (dCov) test.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Statistics stats->Meta-Analysis": [
        [
            "Functions for basic meta-analysis of a collection of sample statistics.",
            "markdown"
        ],
        [
            "Examples can be found in the notebook",
            "markdown"
        ],
        [
            "</blockquote>",
            "markdown"
        ],
        [
            "Status: experimental, API might change, added in 0.12",
            "markdown"
        ],
        [
            "The module also includes internal functions to compute random effects\nvariance.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Contingency tables": [
        [
            "statsmodels supports a variety of approaches for analyzing contingency\ntables, including methods for assessing independence, symmetry,\nhomogeneity, and methods for working with collections of tables from a\nstratified population.",
            "markdown"
        ],
        [
            "The methods described here are mainly for two-way tables.  Multi-way\ntables can be analyzed using log-linear models.  statsmodels does not\ncurrently have a dedicated API for loglinear modeling, but Poisson\nregression in statsmodels.genmod.GLM can be used for this\npurpose.",
            "markdown"
        ],
        [
            "A contingency table is a multi-way table that describes a data set in\nwhich each observation belongs to one category for each of several\nvariables.  For example, if there are two variables, one with\n\\(r\\) levels and one with \\(c\\) levels, then we have a\n\\(r \\times c\\) contingency table.  The table can be described in\nterms of the number of observations that fall into a given cell of the\ntable, e.g. \\(T_{ij}\\) is the number of observations that have\nlevel \\(i\\) for the first variable and level \\(j\\) for the\nsecond variable.  Note that each variable must have a finite number of\nlevels (or categories), which can be either ordered or unordered.  In\ndifferent contexts, the variables defining the axes of a contingency\ntable may be called <strong>categorical variables</strong> or <strong>factor variables</strong>.\nThey may be either <strong>nominal</strong> (if their levels are unordered) or\n<strong>ordinal</strong> (if their levels are ordered).",
            "markdown"
        ],
        [
            "The underlying population for a contingency table is described by a\n<strong>distribution table</strong> \\(P_{i, j}\\).  The elements of \\(P\\)\nare probabilities, and the sum of all elements in \\(P\\) is 1.\nMethods for analyzing contingency tables use the data in \\(T\\) to\nlearn about properties of \\(P\\).",
            "markdown"
        ],
        [
            "The statsmodels.stats.Table is the most basic class for\nworking with contingency tables.  We can create a Table object\ndirectly from any rectangular array-like object containing the\ncontingency table cell counts:",
            "markdown"
        ],
        [
            "In [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: import statsmodels.api as sm\n\nIn [4]: df = sm.datasets.get_rdataset(\"Arthritis\", \"vcd\").data\n\nIn [5]: tab = pd.crosstab(df['Treatment'], df['Improved'])\n\nIn [6]: tab = tab.loc[:, [\"None\", \"Some\", \"Marked\"]]\n\nIn [7]: table = sm.stats.Table(tab)",
            "code"
        ],
        [
            "Alternatively, we can pass the raw data and let the Table class\nconstruct the array of cell counts for us:",
            "markdown"
        ],
        [
            "In [8]: data = df[[\"Treatment\", \"Improved\"]]\n\nIn [9]: table = sm.stats.Table.from_data(data)",
            "code"
        ],
        [
            "Module Reference",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Contingency tables->Independence": [
        [
            "<strong>Independence</strong> is the property that the row and column factors occur\nindependently. <strong>Association</strong> is the lack of independence.  If the\njoint distribution is independent, it can be written as the outer\nproduct of the row and column marginal distributions:\n\n\\[P_{ij} = \\sum_k P_{ij} \\cdot \\sum_k P_{kj} \\quad \\text{for all} \\quad  i, j\\]",
            "markdown"
        ],
        [
            "We can obtain the best-fitting independent distribution for our\nobserved data, and then view residuals which identify particular cells\nthat most strongly violate independence:",
            "markdown"
        ],
        [
            "In [10]: print(table.table_orig)\nImproved   Marked  None  Some\nTreatment                    \nPlacebo         7    29     7\nTreated        21    13     7\n\nIn [11]: print(table.fittedvalues)\nImproved      Marked  None      Some\nTreatment                           \nPlacebo    14.333333  21.5  7.166667\nTreated    13.666667  20.5  6.833333\n\nIn [12]: print(table.resid_pearson)\nImproved     Marked      None      Some\nTreatment                              \nPlacebo   -1.936992  1.617492 -0.062257\nTreated    1.983673 -1.656473  0.063758",
            "code"
        ],
        [
            "In this example, compared to a sample from a population in which the\nrows and columns are independent, we have too many observations in the\nplacebo/no improvement and treatment/marked improvement cells, and too\nfew observations in the placebo/marked improvement and treated/no\nimprovement cells.  This reflects the apparent benefits of the\ntreatment.",
            "markdown"
        ],
        [
            "If the rows and columns of a table are unordered (i.e. are nominal\nfactors), then the most common approach for formally assessing\nindependence is using Pearson\u2019s \\(\\chi^2\\) statistic.  It\u2019s often\nuseful to look at the cell-wise contributions to the \\(\\chi^2\\)\nstatistic to see where the evidence for dependence is coming from.",
            "markdown"
        ],
        [
            "In [13]: rslt = table.test_nominal_association()\n\nIn [14]: print(rslt.pvalue)\n0.0014626434089526352\n\nIn [15]: print(table.chi2_contribs)\nImproved     Marked      None      Some\nTreatment                              \nPlacebo    3.751938  2.616279  0.003876\nTreated    3.934959  2.743902  0.004065",
            "code"
        ],
        [
            "For tables with ordered row and column factors, we can us the <strong>linear\nby linear</strong> association test to obtain more power against alternative\nhypotheses that respect the ordering.  The test statistic for the\nlinear by linear association test is\n\n\\[\\sum_k r_i c_j T_{ij}\\]",
            "markdown"
        ],
        [
            "where \\(r_i\\) and \\(c_j\\) are row and column scores.  Often\nthese scores are set to the sequences 0, 1, \u2026.  This gives the\n\u2018Cochran-Armitage trend test\u2019.",
            "markdown"
        ],
        [
            "In [16]: rslt = table.test_ordinal_association()\n\nIn [17]: print(rslt.pvalue)\n0.023644578093923983",
            "code"
        ],
        [
            "We can assess the association in a \\(r\\times x\\) table by\nconstructing a series of \\(2\\times 2\\) tables and calculating\ntheir odds ratios.  There are two ways to do this.  The <strong>local odds\nratios</strong> construct \\(2\\times 2\\) tables from adjacent row and\ncolumn categories.",
            "markdown"
        ],
        [
            "In [18]: print(table.local_oddsratios)\nImproved     Marked      None  Some\nTreatment                          \nPlacebo    0.149425  2.230769   NaN\nTreated         NaN       NaN   NaN\n\nIn [19]: taloc = sm.stats.Table2x2(np.asarray([[7, 29], [21, 13]]))\n\nIn [20]: print(taloc.oddsratio)\n0.14942528735632185\n\nIn [21]: taloc = sm.stats.Table2x2(np.asarray([[29, 7], [13, 7]]))\n\nIn [22]: print(taloc.oddsratio)\n2.230769230769231",
            "code"
        ],
        [
            "The <strong>cumulative odds ratios</strong> construct \\(2\\times 2\\) tables by\ndichotomizing the row and column factors at each possible point.",
            "markdown"
        ],
        [
            "In [23]: print(table.cumulative_oddsratios)\nImproved     Marked      None  Some\nTreatment                          \nPlacebo    0.185185  1.058824   NaN\nTreated         NaN       NaN   NaN\n\nIn [24]: tab1 = np.asarray([[7, 29 + 7], [21, 13 + 7]])\n\nIn [25]: tacum = sm.stats.Table2x2(tab1)\n\nIn [26]: print(tacum.oddsratio)\n0.18518518518518517\n\nIn [27]: tab1 = np.asarray([[7 + 29, 7], [21 + 13, 7]])\n\nIn [28]: tacum = sm.stats.Table2x2(tab1)\n\nIn [29]: print(tacum.oddsratio)\n1.0588235294117647",
            "code"
        ],
        [
            "A mosaic plot is a graphical approach to informally assessing\ndependence in two-way tables.",
            "markdown"
        ],
        [
            "In [30]: from statsmodels.graphics.mosaicplot import mosaic\n\nIn [31]: fig, _ = mosaic(data, index=[\"Treatment\", \"Improved\"])",
            "code"
        ]
    ],
    "User Guide->Statistics and Tools->Contingency tables->Symmetry and homogeneity": [
        [
            "<strong>Symmetry</strong> is the property that \\(P_{i, j} = P_{j, i}\\) for\nevery \\(i\\) and \\(j\\).  <strong>Homogeneity</strong> is the property that\nthe marginal distribution of the row factor and the column factor are\nidentical, meaning that\n\n\\[\\sum_j P_{ij} = \\sum_j P_{ji} \\forall i\\]",
            "markdown"
        ],
        [
            "Note that for these properties to be applicable the table \\(P\\)\n(and \\(T\\)) must be square, and the row and column categories must\nbe identical and must occur in the same order.",
            "markdown"
        ],
        [
            "To illustrate, we load a data set, create a contingency table, and\ncalculate the row and column margins.  The  class\ncontains methods for analyzing \\(r \\times c\\) contingency tables.\nThe data set loaded below contains assessments of visual acuity in\npeople\u2019s left and right eyes.  We first load the data and create a\ncontingency table.",
            "markdown"
        ],
        [
            "In [32]: df = sm.datasets.get_rdataset(\"VisualAcuity\", \"vcd\").data\n\nIn [33]: df = df.loc[df.gender == \"female\", :]\n\nIn [34]: tab = df.set_index(['left', 'right'])\n\nIn [35]: del tab[\"gender\"]\n\nIn [36]: tab = tab.unstack()\n\nIn [37]: tab.columns = tab.columns.get_level_values(1)\n\nIn [38]: print(tab)\nright     1     2     3    4\nleft                        \n1      1520   234   117   36\n2       266  1512   362   82\n3       124   432  1772  179\n4        66    78   205  492",
            "code"
        ],
        [
            "Next we create a  object from the contingency\ntable.",
            "markdown"
        ],
        [
            "In [39]: sqtab = sm.stats.SquareTable(tab)\n\nIn [40]: row, col = sqtab.marginal_probabilities\n\nIn [41]: print(row)\nright\n1    0.255049\n2    0.297178\n3    0.335295\n4    0.112478\ndtype: float64\n\nIn [42]: print(col)\nright\n1    0.264277\n2    0.301725\n3    0.328474\n4    0.105524\ndtype: float64",
            "code"
        ],
        [
            "The summary method prints results for the symmetry and homogeneity\ntesting procedures.",
            "markdown"
        ],
        [
            "In [43]: print(sqtab.summary())\n            Statistic P-value DF\n--------------------------------\nSymmetry       19.107   0.004  6\nHomogeneity    11.957   0.008  3\n--------------------------------",
            "code"
        ],
        [
            "If we had the individual case records in a dataframe called data,\nwe could also perform the same analysis by passing the raw data using\nthe SquareTable.from_data class method.",
            "markdown"
        ],
        [
            "sqtab = sm.stats.SquareTable.from_data(data[['left', 'right']])\nprint(sqtab.summary())",
            "code"
        ]
    ],
    "User Guide->Statistics and Tools->Contingency tables->A single 2x2 table": [
        [
            "Several methods for working with individual 2x2 tables are provided in\nthe sm.stats.Table2x2 class.  The summary method displays\nseveral measures of association between the rows and columns of the\ntable.",
            "markdown"
        ],
        [
            "In [44]: table = np.asarray([[35, 21], [25, 58]])\n\nIn [45]: t22 = sm.stats.Table2x2(table)\n\nIn [46]: print(t22.summary())\n               Estimate   SE   LCB   UCB  p-value\n-------------------------------------------------\nOdds ratio        3.867       1.890 7.912   0.000\nLog odds ratio    1.352 0.365 0.636 2.068   0.000\nRisk ratio        2.075       1.411 3.051   0.000\nLog risk ratio    0.730 0.197 0.345 1.115   0.000\n-------------------------------------------------",
            "code"
        ],
        [
            "Note that the risk ratio is not symmetric so different results will be\nobtained if the transposed table is analyzed.",
            "markdown"
        ],
        [
            "In [47]: table = np.asarray([[35, 21], [25, 58]])\n\nIn [48]: t22 = sm.stats.Table2x2(table.T)\n\nIn [49]: print(t22.summary())\n               Estimate   SE   LCB   UCB  p-value\n-------------------------------------------------\nOdds ratio        3.867       1.890 7.912   0.000\nLog odds ratio    1.352 0.365 0.636 2.068   0.000\nRisk ratio        2.194       1.436 3.354   0.000\nLog risk ratio    0.786 0.216 0.362 1.210   0.000\n-------------------------------------------------",
            "code"
        ]
    ],
    "User Guide->Statistics and Tools->Contingency tables->Stratified 2x2 tables": [
        [
            "Stratification occurs when we have a collection of contingency tables\ndefined by the same row and column factors.  In the example below, we\nhave a collection of 2x2 tables reflecting the joint distribution of\nsmoking and lung cancer in each of several regions of China.  It is\npossible that the tables all have a common odds ratio, even while the\nmarginal probabilities vary among the strata.  The \u2018Breslow-Day\u2019\nprocedure tests whether the data are consistent with a common odds\nratio.  It appears below as the Test of constant OR.  The\nMantel-Haenszel procedure tests whether this common odds ratio is\nequal to one.  It appears below as the Test of OR=1.  It is also\npossible to estimate the common odds and risk ratios and obtain\nconfidence intervals for them.  The summary method displays all of\nthese results.  Individual results can be obtained from the class\nmethods and attributes.",
            "markdown"
        ],
        [
            "In [50]: data = sm.datasets.china_smoking.load_pandas()\n\nIn [51]: mat = np.asarray(data.data)\n\nIn [52]: tables = [np.reshape(x.tolist(), (2, 2)) for x in mat]\n\nIn [53]: st = sm.stats.StratifiedTable(tables)\n\nIn [54]: print(st.summary())\n                   Estimate   LCB    UCB \n-----------------------------------------\nPooled odds           2.174   1.984 2.383\nPooled log odds       0.777   0.685 0.868\nPooled risk ratio     1.519              \n                                         \n                 Statistic P-value \n-----------------------------------\nTest of OR=1       280.138   0.000 \nTest constant OR     5.200   0.636 \n                       \n-----------------------\nNumber of tables    8  \nMin n             213  \nMax n            2900  \nAvg n            1052  \nTotal n          8419  \n-----------------------",
            "code"
        ]
    ],
    "User Guide->Statistics and Tools->Contingency tables->See also": [
        [
            "has several functions for analyzing contingency tables,\nincluding Fisher\u2019s exact test which is not currently in statsmodels.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Multiple Imputation with Chained Equations": [
        [
            "The MICE module allows most statsmodels models to be fit to a dataset\nwith missing values on the independent and/or dependent variables, and\nprovides rigorous standard errors for the fitted parameters.  The\nbasic idea is to treat each variable with missing values as the\ndependent variable in a regression, with some or all of the remaining\nvariables as its predictors.  The MICE procedure cycles through these\nmodels, fitting each in turn, then uses a procedure called \u201cpredictive\nmean matching\u201d (PMM) to generate random draws from the predictive\ndistributions determined by the fitted models.  These random draws\nbecome the imputed values for one imputed data set.",
            "markdown"
        ],
        [
            "By default, each variable with missing variables is modeled using a\nlinear regression with main effects for all other variables in the\ndata set.  Note that even when the imputation model is linear, the PMM\nprocedure preserves the domain of each variable.  Thus, for example,\nif all observed values for a given variable are positive, all imputed\nvalues for the variable will always be positive.  The user also has\nthe option to specify which model is used to produce imputed values\nfor each variable.",
            "markdown"
        ],
        [
            "Classes",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Multiple Imputation with Chained Equations->Implementation Details": [
        [
            "Internally, this function uses\n.\nAnything that returns True from this function will be treated as missing data.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Empirical Likelihood emplike->Introduction": [
        [
            "Empirical likelihood is a method of nonparametric inference and estimation that lifts the\nobligation of having to specify a family of underlying distributions.  Moreover, empirical\nlikelihood methods do not require re-sampling but still\nuniquely determine confidence regions whose shape mirrors the shape of the data.\nIn essence, empirical likelihood attempts to combine the benefits of parametric\nand nonparametric methods while limiting their shortcomings.  The main difficulties  of\nempirical likelihood is the computationally intensive methods required to conduct inference.\n attempts to provide a user-friendly interface that allows the\nend user to effectively conduct empirical likelihood analysis without having to concern\nthemselves with the computational burdens.",
            "markdown"
        ],
        [
            "Currently, emplike provides methods to conduct hypothesis tests and form confidence\nintervals for descriptive statistics.  Empirical likelihood estimation and inference\nin a regression, accelerated failure time and instrumental variable model are\ncurrently under development.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Empirical Likelihood emplike->Introduction->References": [
        [
            "The main reference for empirical likelihood is:",
            "markdown"
        ],
        [
            "Owen, A.B. \"Empirical Likelihood.\" Chapman and Hall, 2001.",
            "code"
        ]
    ],
    "User Guide->Statistics and Tools->Empirical Likelihood emplike->Examples": [
        [
            "In [1]: import numpy as np\n\nIn [2]: import statsmodels.api as sm\n\n# Generate Data\nIn [3]: x = np.random.standard_normal(50)\n\n# initiate EL\nIn [4]: el = sm.emplike.DescStat(x)\n\n# confidence interval for the mean\nIn [5]: el.ci_mean()\nOut[5]: (-0.4010132937306379, 0.22509889228965327)\n\n# test variance is 1\nIn [6]: el.test_var(1)\nOut[6]: (0.923010006842353, 0.33668588196483373)",
            "code"
        ]
    ],
    "User Guide->Statistics and Tools->Empirical Likelihood emplike": [
        [
            "Module Reference",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Distributions": [
        [
            "This section collects various additional functions and methods for statistical\ndistributions.",
            "markdown"
        ],
        [
            "Empirical Distributions",
            "markdown"
        ],
        [
            "Helper Functions",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Distributions->Count Distributions": [
        [
            "The discrete module contains classes for count distributions that are based\non discretizing a continuous distribution, and specific count distributions\nthat are not available in scipy.distributions like generalized poisson and\nzero-inflated count models.",
            "markdown"
        ],
        [
            "The latter are mainly in support of the corresponding models in\nstatsmodels.discrete. Some methods are not specifically implemented and will\nuse potentially slow inherited generic methods.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Distributions->Copula": [
        [
            "The copula sub-module provides classes to model the dependence between\nparameters. Copulae are used to construct a multivariate joint distribution and\nprovide a set of functions like sampling, PDF, CDF.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Distributions->Distribution Extras": [
        [
            "Skew Distributions",
            "markdown"
        ],
        [
            "Distributions based on Gram-Charlier expansion",
            "markdown"
        ],
        [
            "cdf of multivariate normal wrapper for scipy.stats",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Distributions->Univariate Distributions by non-linear Transformations": [
        [
            "Univariate distributions can be generated from a non-linear transformation of an\nexisting univariate distribution. Transf_gen is a class that can generate a new\ndistribution from a monotonic transformation, TransfTwo_gen can use hump-shaped\nor u-shaped transformation, such as abs or square. The remaining objects are\nspecial cases.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Graphics": [
        [
            "Goodness of Fit Plots",
            "markdown"
        ],
        [
            "Boxplots",
            "markdown"
        ],
        [
            "Correlation Plots",
            "markdown"
        ],
        [
            "Dot Plots",
            "markdown"
        ],
        [
            "Functional Plots",
            "markdown"
        ],
        [
            "Regression Plots",
            "markdown"
        ],
        [
            "Time Series Plots",
            "markdown"
        ],
        [
            "Other Plots",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Input-Output iolib": [
        [
            "statsmodels offers some functions for input and output. These include a\nreader for STATA files, a class for generating tables for printing in several\nformats and two helper functions for pickling.",
            "markdown"
        ],
        [
            "Users can also leverage the powerful input/output functions provided by . Among other things, pandas (a statsmodels dependency) allows reading and writing to Excel, CSV, and HDF5 (PyTables).",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Input-Output iolib->Examples": [
        [
            "</blockquote>",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Input-Output iolib->Module Reference": [
        [
            "The following are classes and functions used to return the summary of\nestimation results, and mostly intended for internal use. There are currently\ntwo versions for creating summaries.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Tools": [
        [
            "Our tool collection contains some convenience functions for users and\nfunctions that were written mainly for internal use.",
            "markdown"
        ],
        [
            "Additional to this tools directory, several other subpackages have their own\ntools modules, for example statsmodels.tsa.tsatools",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Tools->Module Reference->Basic tools tools": [
        [
            "These are basic and miscellaneous tools. The full import path is\nstatsmodels.tools.tools.",
            "markdown"
        ],
        [
            "The next group are mostly helper functions that are not separately tested or\ninsufficiently tested.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Tools->Module Reference": [
        [
            "Numerical Differentiation",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Tools->Module Reference->Measure for fit performance eval_measures": [
        [
            "The first group of function in this module are standalone versions of\ninformation criteria, aic bic and hqic. The function with _sigma suffix\ntake the error sum of squares as argument, those without, take the value\nof the log-likelihood, llf, as argument.",
            "markdown"
        ],
        [
            "The second group of function are measures of fit or prediction performance,\nwhich are mostly one liners to be used as helper functions. All of those\ncalculate a performance or distance statistic for the difference between two\narrays. For example in the case of Monte Carlo or cross-validation, the first\narray would be the estimation results for the different replications or draws,\nwhile the second array would be the true or observed values.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Working with Large Data Sets": [
        [
            "Big data is something of a buzzword in the modern world. While statsmodels\nworks well with small and moderately-sized data sets that can be loaded in\nmemory\u2013perhaps tens of thousands of observations\u2013use cases exist with\nmillions of observations or more. Depending your use case, statsmodels may or\nmay not be a sufficient tool.",
            "markdown"
        ],
        [
            "statsmodels and most of the software stack it is written on operates in\nmemory. Resultantly, building models on larger data sets can be challenging\nor even impractical. With that said, there are 2 general strategies for\nbuilding models on larger data sets with statsmodels.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Working with Large Data Sets->Divide and Conquer - Distributing Jobs": [
        [
            "If your system is capable of loading all the data, but the analysis you are\nattempting to perform is slow, you might be able to build models on horizontal\nslices of the data and then aggregate the individual models once fit.",
            "markdown"
        ],
        [
            "A current limitation of this approach is that it generally does not support\n so constructing your\ndesign matrix (known as exog) in statsmodels, is a little challenging.",
            "markdown"
        ],
        [
            "A detailed example is available\n.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Working with Large Data Sets->Subsetting your data": [
        [
            "If your entire data set is too large to store in memory, you might try storing\nit in a columnar container like \nor . Using the patsy formula\ninterface, statsmodels will use the __getitem__ function (i.e. data[\u2018Item\u2019])\nto pull only the specified columns.",
            "markdown"
        ],
        [
            "import pyarrow as pa\nimport pyarrow.parquet as pq\nimport statsmodels.formula.api as smf\n\nclass DataSet(dict):\n    def __init__(self, path):\n        self.parquet = pq.ParquetFile(path)\n\n    def __getitem__(self, key):\n        try:\n            return self.parquet.read([key]).to_pandas()[key]\n        except:\n            raise KeyError\n\nLargeData = DataSet('LargeData.parquet')\n\nres = smf.ols('Profit ~ Sugar + Power + Women', data=LargeData).fit()",
            "code"
        ],
        [
            "Additionally, you can add code to this example DataSet object to return only\na subset of the rows until you have built a good model. Then, you can refit\nyour final model on more data.",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Optimization": [
        [
            "statsmodels uses three types of algorithms for the estimation of the parameters\nof a model.",
            "markdown"
        ],
        [
            "Basic linear models such as  are directly\nestimated using appropriate linear algebra.",
            "markdown"
        ],
        [
            "and , use iteratively re-weighted\nleast squares. However, you can optionally select one of the scipy\noptimizers discussed below.",
            "markdown"
        ],
        [
            "For all other models, we use\n\nfrom .\n\n</blockquote>",
            "markdown"
        ],
        [
            "Where practical, certain models allow for the optional selection of a\nscipy optimizer. A particular scipy optimizer might be default or an option.\nDepending on the model and the data, choosing an appropriate scipy optimizer\nenables avoidance of a local minima, fitting models in less time, or fitting a\nmodel with less memory.",
            "markdown"
        ],
        [
            "statsmodels supports the following optimizers along with keyword arguments\nassociated with that specific optimizer:",
            "markdown"
        ],
        [
            "newton - Newton-Raphson iteration. While not directly from scipy, we\nconsider it an optimizer because only the score and hessian are required.\n\n\ntolfloat",
            "markdown"
        ],
        [
            "Relative error in params acceptable for convergence.\n\n</dl>\n</blockquote>",
            "markdown"
        ],
        [
            "nm - scipy\u2019s fmin_nm\n\n\nxtolfloat",
            "markdown"
        ],
        [
            "Relative error in params acceptable for convergence\n\nftolfloat",
            "markdown"
        ],
        [
            "Relative error in loglike(params) acceptable for\nconvergence\n\nmaxfunint",
            "markdown"
        ],
        [
            "Maximum number of function evaluations to make.\n\n</dl>\n</blockquote>",
            "markdown"
        ],
        [
            "bfgs - Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno optimization, scipy\u2019s\nfmin_bfgs.\n\n\ngtolfloat",
            "markdown"
        ],
        [
            "Stop when norm of gradient is less than gtol.\n\nnormfloat",
            "markdown"
        ],
        [
            "Order of norm (np.Inf is max, -np.Inf is min)\n\nepsilon",
            "markdown"
        ],
        [
            "If fprime is approximated, use this value for the step\nsize. Only relevant if LikelihoodModel.score is None.\n\n</dl>\n</blockquote>",
            "markdown"
        ],
        [
            "lbfgs - A more memory-efficient (limited memory) implementation of\nbfgs. Scipy\u2019s fmin_l_bfgs_b.\n\n\nmint",
            "markdown"
        ],
        [
            "The maximum number of variable metric corrections used to\ndefine the limited memory matrix. (The limited memory BFGS\nmethod does not store the full hessian but uses this many\nterms in an approximation to it.)\n\npgtolfloat",
            "markdown"
        ],
        [
            "The iteration will stop when\nmax{|proj g_i | i = 1, ..., n} &lt;= pgtol where pg_i is\nthe i-th component of the projected gradient.\n\nfactrfloat",
            "markdown"
        ],
        [
            "The iteration stops when\n(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} &lt;= factr * eps,\nwhere eps is the machine precision, which is automatically\ngenerated by the code. Typical values for factr are: 1e12\nfor low accuracy; 1e7 for moderate accuracy; 10.0 for\nextremely high accuracy. See Notes for relationship to\nftol, which is exposed (instead of factr) by the\nscipy.optimize.minimize interface to L-BFGS-B.\n\nmaxfunint",
            "markdown"
        ],
        [
            "Maximum number of iterations.\n\nepsilonfloat",
            "markdown"
        ],
        [
            "Step size used when approx_grad is True, for numerically\ncalculating the gradient\n\napprox_gradbool",
            "markdown"
        ],
        [
            "Whether to approximate the gradient numerically (in which\ncase func returns only the function value).\n\n</dl>\n</blockquote>",
            "markdown"
        ],
        [
            "cg - Conjugate gradient optimization. Scipy\u2019s fmin_cg.\n\n\ngtolfloat",
            "markdown"
        ],
        [
            "Stop when norm of gradient is less than gtol.\n\nnormfloat",
            "markdown"
        ],
        [
            "Order of norm (np.Inf is max, -np.Inf is min)\n\nepsilonfloat",
            "markdown"
        ],
        [
            "If fprime is approximated, use this value for the step\nsize. Can be scalar or vector.  Only relevant if\nLikelihoodmodel.score is None.\n\n</dl>\n</blockquote>",
            "markdown"
        ],
        [
            "ncg - Newton conjugate gradient. Scipy\u2019s fmin_ncg.\n\n\nfhess_pcallable f\u2019(x, *args)",
            "markdown"
        ],
        [
            "Function which computes the Hessian of f times an arbitrary\nvector, p.  Should only be supplied if\nLikelihoodModel.hessian is None.\n\navextolfloat",
            "markdown"
        ],
        [
            "Stop when the average relative error in the minimizer\nfalls below this amount.\n\nepsilonfloat or ndarray",
            "markdown"
        ],
        [
            "If fhess is approximated, use this value for the step size.\nOnly relevant if Likelihoodmodel.hessian is None.\n\n</dl>\n</blockquote>",
            "markdown"
        ],
        [
            "powell - Powell\u2019s method. Scipy\u2019s fmin_powell.\n\n\nxtolfloat",
            "markdown"
        ],
        [
            "Line-search error tolerance\n\nftolfloat",
            "markdown"
        ],
        [
            "Relative error in loglike(params) for acceptable for\nconvergence.\n\nmaxfunint",
            "markdown"
        ],
        [
            "Maximum number of function evaluations to make.\n\nstart_direcndarray",
            "markdown"
        ],
        [
            "Initial direction set.\n\n</dl>\n</blockquote>",
            "markdown"
        ],
        [
            "basinhopping - Basin hopping. This is part of scipy\u2019s basinhopping\ntools.\n\n\nniterinteger",
            "markdown"
        ],
        [
            "The number of basin hopping iterations.\n\nniter_successinteger",
            "markdown"
        ],
        [
            "Stop the run if the global minimum candidate remains the\nsame for this number of iterations.\n\nTfloat",
            "markdown"
        ],
        [
            "The \u201ctemperature\u201d parameter for the accept or reject\ncriterion. Higher \u201ctemperatures\u201d mean that larger jumps\nin function value will be accepted. For best results\nT should be comparable to the separation (in function\nvalue) between local minima.\n\nstepsizefloat",
            "markdown"
        ],
        [
            "Initial step size for use in the random displacement.\n\nintervalinteger",
            "markdown"
        ],
        [
            "The interval for how often to update the stepsize.\n\nminimizerdict",
            "markdown"
        ],
        [
            "Extra keyword arguments to be passed to the minimizer\nscipy.optimize.minimize(), for example \u2018method\u2019 - the\nminimization method (e.g. \u2018L-BFGS-B\u2019), or \u2018tol\u2019 - the\ntolerance for termination. Other arguments are mapped from\nexplicit argument of fit:\n- args &lt;- fargs\n- jac &lt;- score\n- hess &lt;- hess\n\n</dl>\n</blockquote>",
            "markdown"
        ],
        [
            "minimize - Allows the use of any scipy optimizer.\n\nmin_methodstr, optional",
            "markdown"
        ],
        [
            "Name of minimization method to use.\nAny method specific arguments can be passed directly.\nFor a list of methods and their arguments, see\ndocumentation of scipy.optimize.minimize.\nIf no method is specified, then BFGS is used.\n\n</dl>",
            "markdown"
        ]
    ],
    "User Guide->Statistics and Tools->Optimization->Model Class": [
        [
            "Generally, there is no need for an end-user to directly call these functions\nand classes. However, we provide the class because the different optimization\ntechniques have unique keyword arguments that may be useful to the user.",
            "markdown"
        ]
    ],
    "User Guide->Data Sets->The Datasets Package": [
        [
            "statsmodels provides data sets (i.e. data and meta-data) for use in\nexamples, tutorials, model testing, etc.",
            "markdown"
        ],
        [
            "Using Datasets from Stata",
            "markdown"
        ],
        [
            "R Datasets Function Reference",
            "markdown"
        ],
        [
            "Available Datasets",
            "markdown"
        ]
    ],
    "User Guide->Data Sets->The Datasets Package->Using Datasets from R": [
        [
            "The  gives access to the datasets available in R\u2019s core datasets package and many other common R packages. All of these datasets are available to statsmodels by using the  function. The actual data is accessible by the data attribute. For example:",
            "markdown"
        ],
        [
            "In [1]: import statsmodels.api as sm\n\nIn [2]: duncan_prestige = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\n\nIn [3]: print(duncan_prestige.__doc__)\n.. container::\n\n   ====== ===============\n   Duncan R Documentation\n   ====== ===============\n\n   .. rubric:: Duncan's Occupational Prestige Data\n      :name: duncans-occupational-prestige-data\n\n   .. rubric:: Description\n      :name: description\n\n   The ``Duncan`` data frame has 45 rows and 4 columns. Data on the\n   prestige and other characteristics of 45 U. S. occupations in 1950.\n\n   .. rubric:: Usage\n      :name: usage\n\n   ::\n\n      Duncan\n\n   .. rubric:: Format\n      :name: format\n\n   This data frame contains the following columns:\n\n   type\n      Type of occupation. A factor with the following levels: ``prof``,\n      professional and managerial; ``wc``, white-collar; ``bc``,\n      blue-collar.\n\n   income\n      Percentage of occupational incumbents in the 1950 US Census who\n      earned $3,500 or more per year (about $36,000 in 2017 US dollars).\n\n   education\n      Percentage of occupational incumbents in 1950 who were high school\n      graduates (which, were we cynical, we would say is roughly\n      equivalent to a PhD in 2017)\n\n   prestige\n      Percentage of respondents in a social survey who rated the\n      occupation as \u201cgood\u201d or better in prestige\n\n   .. rubric:: Source\n      :name: source\n\n   Duncan, O. D. (1961) A socioeconomic index for all occupations. In\n   Reiss, A. J., Jr. (Ed.) *Occupations and Social Status.* Free Press\n   [Table VI-1].\n\n   .. rubric:: References\n      :name: references\n\n   Fox, J. (2016) *Applied Regression Analysis and Generalized Linear\n   Models*, Third Edition. Sage.\n\n   Fox, J. and Weisberg, S. (2019) *An R Companion to Applied\n   Regression*, Third Edition, Sage.\n\n\nIn [4]: duncan_prestige.data.head(5)\nOut[4]: \n            type  income  education  prestige\naccountant  prof      62         86        82\npilot       prof      72         76        83\narchitect   prof      75         92        90\nauthor      prof      55         90        76\nchemist     prof      64         86        90",
            "code"
        ]
    ],
    "User Guide->Data Sets->The Datasets Package->Usage": [
        [
            "Load a dataset:",
            "markdown"
        ],
        [
            "In [5]: import statsmodels.api as sm\n\nIn [6]: data = sm.datasets.longley.load_pandas()",
            "code"
        ],
        [
            "The Dataset object follows the bunch pattern. The full dataset is available\nin the data attribute.",
            "markdown"
        ],
        [
            "In [7]: data.data\nOut[7]: \n     TOTEMP  GNPDEFL       GNP   UNEMP   ARMED       POP    YEAR\n0   60323.0     83.0  234289.0  2356.0  1590.0  107608.0  1947.0\n1   61122.0     88.5  259426.0  2325.0  1456.0  108632.0  1948.0\n2   60171.0     88.2  258054.0  3682.0  1616.0  109773.0  1949.0\n3   61187.0     89.5  284599.0  3351.0  1650.0  110929.0  1950.0\n4   63221.0     96.2  328975.0  2099.0  3099.0  112075.0  1951.0\n5   63639.0     98.1  346999.0  1932.0  3594.0  113270.0  1952.0\n6   64989.0     99.0  365385.0  1870.0  3547.0  115094.0  1953.0\n7   63761.0    100.0  363112.0  3578.0  3350.0  116219.0  1954.0\n8   66019.0    101.2  397469.0  2904.0  3048.0  117388.0  1955.0\n9   67857.0    104.6  419180.0  2822.0  2857.0  118734.0  1956.0\n10  68169.0    108.4  442769.0  2936.0  2798.0  120445.0  1957.0\n11  66513.0    110.8  444546.0  4681.0  2637.0  121950.0  1958.0\n12  68655.0    112.6  482704.0  3813.0  2552.0  123366.0  1959.0\n13  69564.0    114.2  502601.0  3931.0  2514.0  125368.0  1960.0\n14  69331.0    115.7  518173.0  4806.0  2572.0  127852.0  1961.0\n15  70551.0    116.9  554894.0  4007.0  2827.0  130081.0  1962.0",
            "code"
        ],
        [
            "Most datasets hold convenient representations of the data in the attributes endog and exog:",
            "markdown"
        ],
        [
            "In [8]: data.endog.iloc[:5]\nOut[8]: \n0    60323.0\n1    61122.0\n2    60171.0\n3    61187.0\n4    63221.0\nName: TOTEMP, dtype: float64\n\nIn [9]: data.exog.iloc[:5,:]\nOut[9]: \n   GNPDEFL       GNP   UNEMP   ARMED       POP    YEAR\n0     83.0  234289.0  2356.0  1590.0  107608.0  1947.0\n1     88.5  259426.0  2325.0  1456.0  108632.0  1948.0\n2     88.2  258054.0  3682.0  1616.0  109773.0  1949.0\n3     89.5  284599.0  3351.0  1650.0  110929.0  1950.0\n4     96.2  328975.0  2099.0  3099.0  112075.0  1951.0",
            "code"
        ],
        [
            "Univariate datasets, however, do not have an exog attribute.",
            "markdown"
        ],
        [
            "Variable names can be obtained by typing:",
            "markdown"
        ],
        [
            "In [10]: data.endog_name\nOut[10]: 'TOTEMP'\n\nIn [11]: data.exog_name\nOut[11]: ['GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP', 'YEAR']",
            "code"
        ],
        [
            "If the dataset does not have a clear interpretation of what should be an\nendog and exog, then you can always access the data or raw_data\nattributes. This is the case for the macrodata dataset, which is a collection\nof US macroeconomic data rather than a dataset with a specific example in mind.\nThe data attribute contains a record array of the full dataset and the\nraw_data attribute contains an ndarray with the names of the columns given\nby the names attribute.",
            "markdown"
        ],
        [
            "In [12]: type(data.data)\nOut[12]: pandas.core.frame.DataFrame\n\nIn [13]: type(data.raw_data)\nOut[13]: pandas.core.frame.DataFrame\n\nIn [14]: data.names\nOut[14]: ['TOTEMP', 'GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP', 'YEAR']",
            "code"
        ]
    ],
    "User Guide->Data Sets->The Datasets Package->Usage->Loading data as pandas objects": [
        [
            "For many users it may be preferable to get the datasets as a pandas DataFrame or\nSeries object. Each of the dataset modules is equipped with a load_pandas\nmethod which returns a Dataset instance with the data readily available as pandas objects:",
            "markdown"
        ],
        [
            "In [15]: data = sm.datasets.longley.load_pandas()\n\nIn [16]: data.exog\nOut[16]: \n    GNPDEFL       GNP   UNEMP   ARMED       POP    YEAR\n0      83.0  234289.0  2356.0  1590.0  107608.0  1947.0\n1      88.5  259426.0  2325.0  1456.0  108632.0  1948.0\n2      88.2  258054.0  3682.0  1616.0  109773.0  1949.0\n3      89.5  284599.0  3351.0  1650.0  110929.0  1950.0\n4      96.2  328975.0  2099.0  3099.0  112075.0  1951.0\n5      98.1  346999.0  1932.0  3594.0  113270.0  1952.0\n6      99.0  365385.0  1870.0  3547.0  115094.0  1953.0\n7     100.0  363112.0  3578.0  3350.0  116219.0  1954.0\n8     101.2  397469.0  2904.0  3048.0  117388.0  1955.0\n9     104.6  419180.0  2822.0  2857.0  118734.0  1956.0\n10    108.4  442769.0  2936.0  2798.0  120445.0  1957.0\n11    110.8  444546.0  4681.0  2637.0  121950.0  1958.0\n12    112.6  482704.0  3813.0  2552.0  123366.0  1959.0\n13    114.2  502601.0  3931.0  2514.0  125368.0  1960.0\n14    115.7  518173.0  4806.0  2572.0  127852.0  1961.0\n15    116.9  554894.0  4007.0  2827.0  130081.0  1962.0\n\nIn [17]: data.endog\nOut[17]: \n0     60323.0\n1     61122.0\n2     60171.0\n3     61187.0\n4     63221.0\n5     63639.0\n6     64989.0\n7     63761.0\n8     66019.0\n9     67857.0\n10    68169.0\n11    66513.0\n12    68655.0\n13    69564.0\n14    69331.0\n15    70551.0\nName: TOTEMP, dtype: float64",
            "code"
        ],
        [
            "The full DataFrame is available in the data attribute of the Dataset object",
            "markdown"
        ],
        [
            "In [18]: data.data\nOut[18]: \n     TOTEMP  GNPDEFL       GNP   UNEMP   ARMED       POP    YEAR\n0   60323.0     83.0  234289.0  2356.0  1590.0  107608.0  1947.0\n1   61122.0     88.5  259426.0  2325.0  1456.0  108632.0  1948.0\n2   60171.0     88.2  258054.0  3682.0  1616.0  109773.0  1949.0\n3   61187.0     89.5  284599.0  3351.0  1650.0  110929.0  1950.0\n4   63221.0     96.2  328975.0  2099.0  3099.0  112075.0  1951.0\n5   63639.0     98.1  346999.0  1932.0  3594.0  113270.0  1952.0\n6   64989.0     99.0  365385.0  1870.0  3547.0  115094.0  1953.0\n7   63761.0    100.0  363112.0  3578.0  3350.0  116219.0  1954.0\n8   66019.0    101.2  397469.0  2904.0  3048.0  117388.0  1955.0\n9   67857.0    104.6  419180.0  2822.0  2857.0  118734.0  1956.0\n10  68169.0    108.4  442769.0  2936.0  2798.0  120445.0  1957.0\n11  66513.0    110.8  444546.0  4681.0  2637.0  121950.0  1958.0\n12  68655.0    112.6  482704.0  3813.0  2552.0  123366.0  1959.0\n13  69564.0    114.2  502601.0  3931.0  2514.0  125368.0  1960.0\n14  69331.0    115.7  518173.0  4806.0  2572.0  127852.0  1961.0\n15  70551.0    116.9  554894.0  4007.0  2827.0  130081.0  1962.0",
            "code"
        ],
        [
            "With pandas integration in the estimation classes, the metadata will be attached\nto model results:",
            "markdown"
        ],
        [
            "In [19]: y, x = data.endog, data.exog\n\nIn [20]: res = sm.OLS(y, x).fit()\n\nIn [21]: res.params\nOut[21]: \nGNPDEFL   -52.993570\nGNP         0.071073\nUNEMP      -0.423466\nARMED      -0.572569\nPOP        -0.414204\nYEAR       48.417866\ndtype: float64\n\nIn [22]: res.summary()\nOut[22]: \n&lt;class 'statsmodels.iolib.summary.Summary'\n\"\"\"\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                 TOTEMP   R-squared (uncentered):                   1.000\nModel:                            OLS   Adj. R-squared (uncentered):              1.000\nMethod:                 Least Squares   F-statistic:                          5.052e+04\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):                    8.20e-22\nTime:                        17:12:36   Log-Likelihood:                         -117.56\nNo. Observations:                  16   AIC:                                      247.1\nDf Residuals:                      10   BIC:                                      251.8\nDf Model:                           6                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGNPDEFL      -52.9936    129.545     -0.409      0.691    -341.638     235.650\nGNP            0.0711      0.030      2.356      0.040       0.004       0.138\nUNEMP         -0.4235      0.418     -1.014      0.335      -1.354       0.507\nARMED         -0.5726      0.279     -2.052      0.067      -1.194       0.049\nPOP           -0.4142      0.321     -1.289      0.226      -1.130       0.302\nYEAR          48.4179     17.689      2.737      0.021       9.003      87.832\n==============================================================================\nOmnibus:                        1.443   Durbin-Watson:                   1.277\nProb(Omnibus):                  0.486   Jarque-Bera (JB):                0.605\nSkew:                           0.476   Prob(JB):                        0.739\nKurtosis:                       3.031   Cond. No.                     4.56e+05\n==============================================================================\n\nNotes:\n[1] R\u00b2 is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[3] The condition number is large, 4.56e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
            "code"
        ]
    ],
    "User Guide->Data Sets->The Datasets Package->Usage->Extra Information": [
        [
            "If you want to know more about the dataset itself, you can access the\nfollowing, again using the Longley dataset as an example",
            "markdown"
        ],
        [
            "dir(sm.datasets.longley)[:6]\n['COPYRIGHT', 'DESCRLONG', 'DESCRSHORT', 'NOTE', 'SOURCE', 'TITLE']",
            "code"
        ]
    ],
    "User Guide->Data Sets->The Datasets Package->Additional information": [
        [
            "The idea for a datasets package was originally proposed by David Cournapeau.",
            "markdown"
        ],
        [
            "To add datasets, see the .",
            "markdown"
        ]
    ],
    "User Guide->Sandbox->Sandbox": [
        [
            "This sandbox contains code that is for various reasons not ready to be\nincluded in statsmodels proper. It contains modules from the old stats.models\ncode that have not been tested, verified and updated to the new statsmodels\nstructure: cox survival model, mixed effects model with repeated measures,\ngeneralized additive model and the formula framework. The sandbox also\ncontains code that is currently being worked on until it fits the pattern\nof statsmodels or is sufficiently tested.",
            "markdown"
        ],
        [
            "All sandbox modules have to be explicitly imported to indicate that they are\nnot yet part of the core of statsmodels. The quality and testing of the\nsandbox code varies widely.",
            "markdown"
        ]
    ],
    "User Guide->Sandbox->Sandbox->Examples": [
        [
            "There are some examples in the sandbox.examples folder. Additional\nexamples are directly included in the modules and in subfolders of\nthe sandbox.",
            "markdown"
        ]
    ],
    "User Guide->Sandbox->Sandbox->Module Reference->Time Series analysis tsa": [
        [
            "In this part we develop models and functions that will be useful for time\nseries analysis. Most of the models and function have been moved to\n.",
            "markdown"
        ]
    ],
    "User Guide->Sandbox->Sandbox->Module Reference->Time Series analysis tsa->Moving Window Statistics": [
        [
            "Most moving window statistics, like rolling mean, moments (up to 4th order), min,\nmax, mean, and variance, are covered by the functions for  in Pandas.",
            "markdown"
        ]
    ],
    "User Guide->Sandbox->Sandbox->Module Reference->Regression and ANOVA": [
        [
            "The following two ANOVA functions are fully tested against the NIST test data\nfor balanced one-way ANOVA. anova_oneway follows the same pattern as the\noneway anova function in scipy.stats but with higher precision for badly\nscaled problems. anova_ols produces the same results as the one way anova\nhowever using the OLS model class. It also verifies against the NIST tests,\nwith some problems in the worst scaled cases. It shows how to do simple ANOVA\nusing statsmodels in three lines and is also best taken as a recipe.",
            "markdown"
        ],
        [
            "The following are helper functions for working with dummy variables and\ngenerating ANOVA results with OLS. They are best considered as recipes since\nthey were written with a specific use in mind. These function will eventually\nbe rewritten or reorganized.",
            "markdown"
        ],
        [
            "The following are helper functions for group statistics where groups are\ndefined by a label array. The qualifying comments for the previous group\napply also to this group of functions.",
            "markdown"
        ],
        [
            "Additional to these functions, sandbox regression still contains several\nexamples, that are illustrative of the use of the regression models of\nstatsmodels.",
            "markdown"
        ]
    ],
    "User Guide->Sandbox->Sandbox->Module Reference->Systems of Regression Equations and Simultaneous Equations": [
        [
            "The following are for fitting systems of equations models.  Though the returned\nparameters have been verified as accurate, this code is still very\nexperimental, and the usage of the models will very likely change significantly\nbefore they are added to the main codebase.",
            "markdown"
        ]
    ],
    "User Guide->Sandbox->Sandbox->Module Reference->Miscellaneous": [
        [
            "Descriptive Statistics Printing",
            "markdown"
        ]
    ],
    "User Guide->Sandbox->Sandbox->Module Reference->Original stats.models": [
        [
            "None of these are fully working. The formula framework is used by cox and\nmixed.",
            "markdown"
        ],
        [
            "<strong>Mixed Effects Model with Repeated Measures using an EM Algorithm</strong>",
            "markdown"
        ],
        [
            "statsmodels.sandbox.mixed",
            "markdown"
        ],
        [
            "<strong>Cox Proportional Hazards Model</strong>",
            "markdown"
        ],
        [
            "statsmodels.sandbox.cox",
            "markdown"
        ],
        [
            "<strong>Generalized Additive Models</strong>",
            "markdown"
        ],
        [
            "statsmodels.sandbox.gam",
            "markdown"
        ],
        [
            "<strong>Formula</strong>",
            "markdown"
        ],
        [
            "statsmodels.sandbox.formula",
            "markdown"
        ]
    ],
    "Examples": [
        [
            "This page provides a series of examples, tutorials and recipes to help you get\nstarted with statsmodels. Each of the examples shown here is made available\nas an IPython Notebook and as a plain python script on the statsmodels github\nrepository.",
            "markdown"
        ],
        [
            "We also encourage users to submit their own examples, tutorials or cool\nstatsmodels trick to the Examples wiki page",
            "markdown"
        ],
        [
            "Ordinary Least Squares",
            "markdown"
        ],
        [
            "Generalized Least Squares",
            "markdown"
        ],
        [
            "Quantile Regression",
            "markdown"
        ],
        [
            "Recursive Least Squares",
            "markdown"
        ],
        [
            "Rolling Least Squares",
            "markdown"
        ],
        [
            "Regression Diagnostics",
            "markdown"
        ],
        [
            "Weighted Least Squares",
            "markdown"
        ],
        [
            "Linear Mixed-Effects",
            "markdown"
        ],
        [
            "Variance Component Analysis",
            "markdown"
        ],
        [
            "Regression Plots",
            "markdown"
        ],
        [
            "Categorical Interactions",
            "markdown"
        ],
        [
            "Box Plots",
            "markdown"
        ],
        [
            "Getting Started",
            "markdown"
        ],
        [
            "Fair\u2019s Affairs Data",
            "markdown"
        ],
        [
            "Ordinal Regression",
            "markdown"
        ],
        [
            "Univariate Kernel Density Estimator",
            "markdown"
        ],
        [
            "Lowess Regression",
            "markdown"
        ],
        [
            "Generalized Linear Models Overview",
            "markdown"
        ],
        [
            "Using Formulas with GLMs",
            "markdown"
        ],
        [
            "Weighting Observations with GLMs",
            "markdown"
        ],
        [
            "Influence Measures for GLMs",
            "markdown"
        ],
        [
            "Quasi-binomial regression",
            "markdown"
        ],
        [
            "M-estimators for Robust Regression",
            "markdown"
        ],
        [
            "Comparing OLS and RLM",
            "markdown"
        ],
        [
            "GEE Nested Covariance Structure",
            "markdown"
        ],
        [
            "GEE Score Tests",
            "markdown"
        ],
        [
            "ANOVA",
            "markdown"
        ],
        [
            "Meta-Analysis in statsmodels",
            "markdown"
        ],
        [
            "Mediation analysis with duration data",
            "markdown"
        ],
        [
            "Copulas",
            "markdown"
        ],
        [
            "Autoregressions",
            "markdown"
        ],
        [
            "Autoregressive Distributed Lag Models",
            "markdown"
        ],
        [
            "Deterministic Terms",
            "markdown"
        ],
        [
            "ARMA: Sunspots Data",
            "markdown"
        ],
        [
            "ARMA: Artificial Data",
            "markdown"
        ],
        [
            "Time Series Filters",
            "markdown"
        ],
        [
            "Markov switching dynamic regression",
            "markdown"
        ],
        [
            "Markov switching autoregression",
            "markdown"
        ],
        [
            "Exponential Smoothing",
            "markdown"
        ],
        [
            "Seasonal Decomposition",
            "markdown"
        ],
        [
            "Stationarity and detrending (ADF/KPSS)",
            "markdown"
        ],
        [
            "SARIMAX: Introduction",
            "markdown"
        ],
        [
            "SARIMAX: Model selection, missing data",
            "markdown"
        ],
        [
            "SARIMAX: Frequently Asked Questions (FAQ)",
            "markdown"
        ],
        [
            "VARMAX: Introduction",
            "markdown"
        ],
        [
            "Dynamic Factor Models: Application",
            "markdown"
        ],
        [
            "Unobserved Components: Application",
            "markdown"
        ],
        [
            "Trends and cycles in unemployment",
            "markdown"
        ],
        [
            "State space modeling: Local Linear Trends",
            "markdown"
        ],
        [
            "Statespace ARMA: Sunspots Data",
            "markdown"
        ],
        [
            "Seasonality in Time Series Data",
            "markdown"
        ],
        [
            "Fixed / constrained parameters in state space models",
            "markdown"
        ],
        [
            "TVP-VAR, MCMC, and sparse simulation smoothing",
            "markdown"
        ],
        [
            "SARIMAX estimation with Bayesian methods",
            "markdown"
        ],
        [
            "Forecasting, updating datasets, and the \u201cnews\u201d",
            "markdown"
        ],
        [
            "Statespace: Custom Models",
            "markdown"
        ],
        [
            "ETS models",
            "markdown"
        ],
        [
            "State space models: concentrating out the scale",
            "markdown"
        ],
        [
            "State space models: Chandrasekhar recursions",
            "markdown"
        ],
        [
            "Forecasting using the Theta Model",
            "markdown"
        ],
        [
            "Principal Component Analysis",
            "markdown"
        ],
        [
            "Contrasts",
            "markdown"
        ],
        [
            "Formulas",
            "markdown"
        ],
        [
            "Prediction",
            "markdown"
        ],
        [
            "Forecasting in statsmodels",
            "markdown"
        ],
        [
            "Generic Maximum Likelihood",
            "markdown"
        ],
        [
            "Dates in Time-Series Models",
            "markdown"
        ],
        [
            "Least squares fitting of models to data",
            "markdown"
        ],
        [
            "Distributed Estimations",
            "markdown"
        ]
    ],
    "Examples->Linear Regression Models->Ordinary Least Squares": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(9876789)",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Ordinary Least Squares->OLS estimation": [
        [
            "Artificial data:",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "nsample = 100\nx = np.linspace(0, 10, 100)\nX = np.column_stack((x, x ** 2))\nbeta = np.array([1, 0.1, 10])\ne = np.random.normal(size=nsample)",
            "code"
        ],
        [
            "Our model needs an intercept so we add a column of 1s:",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "X = sm.add_constant(X)\ny = np.dot(X, beta) + e",
            "code"
        ],
        [
            "Fit and summary:",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "model = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                      y   R-squared:                       1.000\nModel:                            OLS   Adj. R-squared:                  1.000\nMethod:                 Least Squares   F-statistic:                 4.020e+06\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):          2.83e-239\nTime:                        17:11:19   Log-Likelihood:                -146.51\nNo. Observations:                 100   AIC:                             299.0\nDf Residuals:                      97   BIC:                             306.8\nDf Model:                           2\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.3423      0.313      4.292      0.000       0.722       1.963\nx1            -0.0402      0.145     -0.278      0.781      -0.327       0.247\nx2            10.0103      0.014    715.745      0.000       9.982      10.038\n==============================================================================\nOmnibus:                        2.042   Durbin-Watson:                   2.274\nProb(Omnibus):                  0.360   Jarque-Bera (JB):                1.875\nSkew:                           0.234   Prob(JB):                        0.392\nKurtosis:                       2.519   Cond. No.                         144.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "Quantities of interest can be extracted directly from the fitted model. Type dir(results) for a full list. Here are some examples:",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "print(\"Parameters: \", results.params)\nprint(\"R2: \", results.rsquared)",
            "code"
        ],
        [
            "Parameters:  [ 1.34233516 -0.04024948 10.01025357]\nR2:  0.9999879365025871",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Ordinary Least Squares->OLS non-linear curve but linear in parameters": [
        [
            "We simulate artificial data with a non-linear relationship between x and y:",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "nsample = 50\nsig = 0.5\nx = np.linspace(0, 20, nsample)\nX = np.column_stack((x, np.sin(x), (x - 5) ** 2, np.ones(nsample)))\nbeta = [0.5, 0.5, -0.02, 5.0]\n\ny_true = np.dot(X, beta)\ny = y_true + sig * np.random.normal(size=nsample)",
            "code"
        ],
        [
            "Fit and summary:",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "res = sm.OLS(y, X).fit()\nprint(res.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                      y   R-squared:                       0.933\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     211.8\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           6.30e-27\nTime:                        17:11:19   Log-Likelihood:                -34.438\nNo. Observations:                  50   AIC:                             76.88\nDf Residuals:                      46   BIC:                             84.52\nDf Model:                           3\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             0.4687      0.026     17.751      0.000       0.416       0.522\nx2             0.4836      0.104      4.659      0.000       0.275       0.693\nx3            -0.0174      0.002     -7.507      0.000      -0.022      -0.013\nconst          5.2058      0.171     30.405      0.000       4.861       5.550\n==============================================================================\nOmnibus:                        0.655   Durbin-Watson:                   2.896\nProb(Omnibus):                  0.721   Jarque-Bera (JB):                0.360\nSkew:                           0.207   Prob(JB):                        0.835\nKurtosis:                       3.026   Cond. No.                         221.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "Extract other quantities of interest:",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "print(\"Parameters: \", res.params)\nprint(\"Standard errors: \", res.bse)\nprint(\"Predicted values: \", res.predict())",
            "code"
        ],
        [
            "Parameters:  [ 0.46872448  0.48360119 -0.01740479  5.20584496]\nStandard errors:  [0.02640602 0.10380518 0.00231847 0.17121765]\nPredicted values:  [ 4.77072516  5.22213464  5.63620761  5.98658823  6.25643234  6.44117491\n  6.54928009  6.60085051  6.62432454  6.6518039   6.71377946  6.83412169\n  7.02615877  7.29048685  7.61487206  7.97626054  8.34456611  8.68761335\n  8.97642389  9.18997755  9.31866582  9.36587056  9.34740836  9.28893189\n  9.22171529  9.17751587  9.1833565   9.25708583  9.40444579  9.61812821\n  9.87897556 10.15912843 10.42660281 10.65054491 10.8063004  10.87946503\n 10.86825119 10.78378163 10.64826203 10.49133265 10.34519853 10.23933827\n 10.19566084 10.22490593 10.32487947 10.48081414 10.66779556 10.85485568\n 11.01006072 11.10575781]",
            "code"
        ],
        [
            "Draw a plot to compare the true relationship to OLS predictions. Confidence intervals around the predictions are built using the wls_prediction_std command.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "pred_ols = res.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.plot(x, y, \"o\", label=\"data\")\nax.plot(x, y_true, \"b-\", label=\"True\")\nax.plot(x, res.fittedvalues, \"r--.\", label=\"OLS\")\nax.plot(x, iv_u, \"r--\")\nax.plot(x, iv_l, \"r--\")\nax.legend(loc=\"best\")",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f788757a290\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ols_18_1.png\" src=\"../../../_images/examples_notebooks_generated_ols_18_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables": [
        [
            "We generate some artificial data. There are 3 groups which will be modelled using dummy variables. Group 0 is the omitted/benchmark category.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "nsample = 50\ngroups = np.zeros(nsample, int)\ngroups[20:40] = 1\ngroups[40:] = 2\n# dummy = (groups[:,None] == np.unique(groups)).astype(float)\n\ndummy = pd.get_dummies(groups).values\nx = np.linspace(0, 20, nsample)\n# drop reference category\nX = np.column_stack((x, dummy[:, 1:]))\nX = sm.add_constant(X, prepend=False)\n\nbeta = [1.0, 3, -3, 10]\ny_true = np.dot(X, beta)\ne = np.random.normal(size=nsample)\ny = y_true + e",
            "code"
        ],
        [
            "Inspect the data:",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "print(X[:5, :])\nprint(y[:5])\nprint(groups)\nprint(dummy[:5, :])",
            "code"
        ],
        [
            "[[0.         0.         0.         1.        ]\n [0.40816327 0.         0.         1.        ]\n [0.81632653 0.         0.         1.        ]\n [1.2244898  0.         0.         1.        ]\n [1.63265306 0.         0.         1.        ]]\n[ 9.28223335 10.50481865 11.84389206 10.38508408 12.37941998]\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 2 2 2 2 2 2 2 2 2 2]\n[[1 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]]",
            "code"
        ],
        [
            "Fit and summary:",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "res2 = sm.OLS(y, X).fit()\nprint(res2.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                      y   R-squared:                       0.978\nModel:                            OLS   Adj. R-squared:                  0.976\nMethod:                 Least Squares   F-statistic:                     671.7\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           5.69e-38\nTime:                        17:11:19   Log-Likelihood:                -64.643\nNo. Observations:                  50   AIC:                             137.3\nDf Residuals:                      46   BIC:                             144.9\nDf Model:                           3\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             0.9999      0.060     16.689      0.000       0.879       1.121\nx2             2.8909      0.569      5.081      0.000       1.746       4.036\nx3            -3.2232      0.927     -3.477      0.001      -5.089      -1.357\nconst         10.1031      0.310     32.573      0.000       9.479      10.727\n==============================================================================\nOmnibus:                        2.831   Durbin-Watson:                   1.998\nProb(Omnibus):                  0.243   Jarque-Bera (JB):                1.927\nSkew:                          -0.279   Prob(JB):                        0.382\nKurtosis:                       2.217   Cond. No.                         96.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "Draw a plot to compare the true relationship to OLS predictions:",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "pred_ols2 = res2.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.plot(x, y, \"o\", label=\"Data\")\nax.plot(x, y_true, \"b-\", label=\"True\")\nax.plot(x, res2.fittedvalues, \"r--.\", label=\"Predicted\")\nax.plot(x, iv_u, \"r--\")\nax.plot(x, iv_l, \"r--\")\nlegend = ax.legend(loc=\"best\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ols_26_0.png\" src=\"../../../_images/examples_notebooks_generated_ols_26_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->F test": [
        [
            "We want to test the hypothesis that both coefficients on the dummy variables are equal to zero, that is, \\(R \\times \\beta = 0\\). An F test leads us to strongly reject the null hypothesis of identical constant in the 3 groups:",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "R = [[0, 1, 0, 0], [0, 0, 1, 0]]\nprint(np.array(R))\nprint(res2.f_test(R))",
            "code"
        ],
        [
            "[[0 1 0 0]\n [0 0 1 0]]\n&lt;F test: F=145.49268198027985, p=1.2834419617282554e-20, df_denom=46, df_num=2",
            "code"
        ],
        [
            "You can also use formula-like syntax to test hypotheses",
            "markdown"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "print(res2.f_test(\"x2 = x3 = 0\"))",
            "code"
        ],
        [
            "&lt;F test: F=145.49268198028003, p=1.2834419617282254e-20, df_denom=46, df_num=2",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Small group effects": [
        [
            "If we generate artificial data with smaller group effects, the T test can no longer reject the Null hypothesis:",
            "markdown"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "beta = [1.0, 0.3, -0.0, 10]\ny_true = np.dot(X, beta)\ny = y_true + np.random.normal(size=nsample)\n\nres3 = sm.OLS(y, X).fit()",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "print(res3.f_test(R))",
            "code"
        ],
        [
            "&lt;F test: F=1.2249111925408935, p=0.3031864410631272, df_denom=46, df_num=2",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "print(res3.f_test(\"x2 = x3 = 0\"))",
            "code"
        ],
        [
            "&lt;F test: F=1.2249111925408938, p=0.3031864410631272, df_denom=46, df_num=2",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity": [
        [
            "The Longley dataset is well known to have high multicollinearity. That is, the exogenous predictors are highly correlated. This is problematic because it can affect the stability of our coefficient estimates as we make minor changes to model specification.",
            "markdown"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "from statsmodels.datasets.longley import load_pandas\n\ny = load_pandas().endog\nX = load_pandas().exog\nX = sm.add_constant(X)",
            "code"
        ],
        [
            "Fit and summary:",
            "markdown"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                 TOTEMP   R-squared:                       0.995\nModel:                            OLS   Adj. R-squared:                  0.992\nMethod:                 Least Squares   F-statistic:                     330.3\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           4.98e-10\nTime:                        17:11:20   Log-Likelihood:                -109.62\nNo. Observations:                  16   AIC:                             233.2\nDf Residuals:                       9   BIC:                             238.6\nDf Model:                           6\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst      -3.482e+06    8.9e+05     -3.911      0.004    -5.5e+06   -1.47e+06\nGNPDEFL       15.0619     84.915      0.177      0.863    -177.029     207.153\nGNP           -0.0358      0.033     -1.070      0.313      -0.112       0.040\nUNEMP         -2.0202      0.488     -4.136      0.003      -3.125      -0.915\nARMED         -1.0332      0.214     -4.822      0.001      -1.518      -0.549\nPOP           -0.0511      0.226     -0.226      0.826      -0.563       0.460\nYEAR        1829.1515    455.478      4.016      0.003     798.788    2859.515\n==============================================================================\nOmnibus:                        0.749   Durbin-Watson:                   2.559\nProb(Omnibus):                  0.688   Jarque-Bera (JB):                0.684\nSkew:                           0.420   Prob(JB):                        0.710\nKurtosis:                       2.434   Cond. No.                     4.86e+09\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.86e+09. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/scipy/stats/stats.py:1541: UserWarning: kurtosistest only valid for n=20 ... continuing anyway, n=16\n  warnings.warn(\"kurtosistest only valid for n=20 ... continuing \"",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Condition number": [
        [
            "One way to assess multicollinearity is to compute the condition number. Values over 20 are worrisome (see Greene 4.9). The first step is to normalize the independent variables to have unit length:",
            "markdown"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "norm_x = X.values\nfor i, name in enumerate(X):\n    if name == \"const\":\n        continue\n    norm_x[:, i] = X[name] / np.linalg.norm(X[name])\nnorm_xtx = np.dot(norm_x.T, norm_x)",
            "code"
        ],
        [
            "Then, we take the square root of the ratio of the biggest to the smallest eigen values.",
            "markdown"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "eigs = np.linalg.eigvals(norm_xtx)\ncondition_number = np.sqrt(eigs.max() / eigs.min())\nprint(condition_number)",
            "code"
        ],
        [
            "56240.86912116517",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Dropping an observation": [
        [
            "Greene also points out that dropping a single observation can have a dramatic effect on the coefficient estimates:",
            "markdown"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "ols_results2 = sm.OLS(y.iloc[:14], X.iloc[:14]).fit()\nprint(\n    \"Percentage change %4.2f%%\\n\"\n    * 7\n    % tuple(\n        [\n            i\n            for i in (ols_results2.params - ols_results.params)\n            / ols_results.params\n            * 100\n        ]\n    )\n)",
            "code"
        ],
        [
            "Percentage change 4.55%\nPercentage change -2228.01%\nPercentage change 154304695.31%\nPercentage change 1366329.02%\nPercentage change 1112549.36%\nPercentage change 92708715.91%\nPercentage change 817944.26%",
            "code"
        ],
        [
            "We can also look at formal statistics for this such as the DFBETAS \u2013 a standardized measure of how much each coefficient changes when that observation is left out.",
            "markdown"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "infl = ols_results.get_influence()",
            "code"
        ],
        [
            "In general we may consider DBETAS in absolute value greater than \\(2/\\sqrt{N}\\) to be influential observations",
            "markdown"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "2.0 / len(X) ** 0.5",
            "code"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "0.5",
            "code"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "print(infl.summary_frame().filter(regex=\"dfb\"))",
            "code"
        ],
        [
            "dfb_const  dfb_GNPDEFL       dfb_GNP     dfb_UNEMP     dfb_ARMED  \\\n0   -0.016406  -169.822675  1.673981e+06  54490.318088  51447.824036\n1   -0.020608  -187.251727  1.829990e+06  54495.312977  52659.808664\n2   -0.008382   -65.417834  1.587601e+06  52002.330476  49078.352378\n3    0.018093   288.503914  1.155359e+06  56211.331922  60350.723082\n4    1.871260  -171.109595  4.498197e+06  82532.785818  71034.429294\n5   -0.321373  -104.123822  1.398891e+06  52559.760056  47486.527649\n6    0.315945  -169.413317  2.364827e+06  59754.651394  50371.817827\n7    0.015816   -69.343793  1.641243e+06  51849.056936  48628.749338\n8   -0.004019   -86.903523  1.649443e+06  52023.265116  49114.178265\n9   -1.018242  -201.315802  1.371257e+06  56432.027292  53997.742487\n10   0.030947   -78.359439  1.658753e+06  52254.848135  49341.055289\n11   0.005987  -100.926843  1.662425e+06  51744.606934  48968.560299\n12  -0.135883   -32.093127  1.245487e+06  50203.467593  51148.376274\n13   0.032736   -78.513866  1.648417e+06  52509.194459  50212.844641\n14   0.305868   -16.833121  1.829996e+06  60975.868083  58263.878679\n15  -0.538323   102.027105  1.344844e+06  54721.897640  49660.474568\n\n          dfb_POP      dfb_YEAR\n0   207954.113588 -31969.158503\n1    25343.938290 -29760.155888\n2   107465.770565 -29593.195253\n3   456190.215133 -36213.129569\n4  -389122.401699 -49905.782854\n5   144354.586054 -28985.057609\n6  -107413.074918 -32984.462465\n7    92843.959345 -29724.975873\n8    83931.635336 -29563.619222\n9    18392.575057 -29203.217108\n10   93617.648517 -29846.022426\n11   95414.217290 -29690.904188\n12  258559.048569 -29296.334617\n13  104434.061226 -30025.564763\n14  275103.677860 -36060.612522\n15 -110176.960671 -28053.834556",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:696: RuntimeWarning: invalid value encountered in sqrt\n  return self.resid / sigma / np.sqrt(1 - hii)\n/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:737: RuntimeWarning: invalid value encountered in sqrt\n  dffits_ = self.resid_studentized_internal * np.sqrt(hii / (1 - hii))\n/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:766: RuntimeWarning: invalid value encountered in sqrt\n  dffits_ = self.resid_studentized_external * np.sqrt(hii / (1 - hii))",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Generalized Least Squares": [
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np\n\nimport statsmodels.api as sm",
            "code"
        ],
        [
            "The Longley dataset is a time series dataset:",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "data = sm.datasets.longley.load()\ndata.exog = sm.add_constant(data.exog)\nprint(data.exog.head())",
            "code"
        ],
        [
            "const  GNPDEFL       GNP   UNEMP   ARMED       POP    YEAR\n0    1.0     83.0  234289.0  2356.0  1590.0  107608.0  1947.0\n1    1.0     88.5  259426.0  2325.0  1456.0  108632.0  1948.0\n2    1.0     88.2  258054.0  3682.0  1616.0  109773.0  1949.0\n3    1.0     89.5  284599.0  3351.0  1650.0  110929.0  1950.0\n4    1.0     96.2  328975.0  2099.0  3099.0  112075.0  1951.0",
            "code"
        ],
        [
            "Let\u2019s assume that the data is heteroskedastic and that we know the nature of the heteroskedasticity. We can then define sigma and use it to give us a GLS model",
            "markdown"
        ],
        [
            "First we will obtain the residuals from an OLS fit",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "ols_resid = sm.OLS(data.endog, data.exog).fit().resid",
            "code"
        ],
        [
            "Assume that the error terms follow an AR(1) process with a trend:",
            "markdown"
        ],
        [
            "\\(\\epsilon_i = \\beta_0 + \\rho\\epsilon_{i-1} + \\eta_i\\)",
            "markdown"
        ],
        [
            "where \\(\\eta \\sim N(0,\\Sigma^2)\\)",
            "markdown"
        ],
        [
            "and that \\(\\rho\\) is simply the correlation of the residual a consistent estimator for rho is to regress the residuals on the lagged residuals",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "resid_fit = sm.OLS(\n    np.asarray(ols_resid)[1:], sm.add_constant(np.asarray(ols_resid)[:-1])\n).fit()\nprint(resid_fit.tvalues[1])\nprint(resid_fit.pvalues[1])",
            "code"
        ],
        [
            "-1.4390229839793167\n0.17378444788655237",
            "code"
        ],
        [
            "While we do not have strong evidence that the errors follow an AR(1) process we continue",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "rho = resid_fit.params[1]",
            "code"
        ],
        [
            "As we know, an AR(1) process means that near-neighbors have a stronger relation so we can give this structure by using a toeplitz matrix",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "from scipy.linalg import toeplitz\n\ntoeplitz(range(5))",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "array([[0, 1, 2, 3, 4],\n       [1, 0, 1, 2, 3],\n       [2, 1, 0, 1, 2],\n       [3, 2, 1, 0, 1],\n       [4, 3, 2, 1, 0]])",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "order = toeplitz(range(len(ols_resid)))",
            "code"
        ],
        [
            "so that our error covariance structure is actually rho**order which defines an autocorrelation structure",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "sigma = rho ** order\ngls_model = sm.GLS(data.endog, data.exog, sigma=sigma)\ngls_results = gls_model.fit()",
            "code"
        ],
        [
            "Of course, the exact rho in this instance is not known so it it might make more sense to use feasible gls, which currently only has experimental support.",
            "markdown"
        ],
        [
            "We can use the GLSAR model with one lag, to get to a similar result:",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "glsar_model = sm.GLSAR(data.endog, data.exog, 1)\nglsar_results = glsar_model.iterative_fit(1)\nprint(glsar_results.summary())",
            "code"
        ],
        [
            "GLSAR Regression Results\n==============================================================================\nDep. Variable:                 TOTEMP   R-squared:                       0.996\nModel:                          GLSAR   Adj. R-squared:                  0.992\nMethod:                 Least Squares   F-statistic:                     295.2\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           6.09e-09\nTime:                        17:06:55   Log-Likelihood:                -102.04\nNo. Observations:                  15   AIC:                             218.1\nDf Residuals:                       8   BIC:                             223.0\nDf Model:                           6\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst      -3.468e+06   8.72e+05     -3.979      0.004   -5.48e+06   -1.46e+06\nGNPDEFL       34.5568     84.734      0.408      0.694    -160.840     229.953\nGNP           -0.0343      0.033     -1.047      0.326      -0.110       0.041\nUNEMP         -1.9621      0.481     -4.083      0.004      -3.070      -0.854\nARMED         -1.0020      0.211     -4.740      0.001      -1.489      -0.515\nPOP           -0.0978      0.225     -0.435      0.675      -0.616       0.421\nYEAR        1823.1829    445.829      4.089      0.003     795.100    2851.266\n==============================================================================\nOmnibus:                        1.960   Durbin-Watson:                   2.554\nProb(Omnibus):                  0.375   Jarque-Bera (JB):                1.423\nSkew:                           0.713   Prob(JB):                        0.491\nKurtosis:                       2.508   Cond. No.                     4.80e+09\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.8e+09. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/scipy/stats/stats.py:1541: UserWarning: kurtosistest only valid for n=20 ... continuing anyway, n=15\n  warnings.warn(\"kurtosistest only valid for n=20 ... continuing \"",
            "code"
        ],
        [
            "Comparing gls and glsar results, we see that there are some small differences in the parameter estimates and the resulting standard errors of the parameter estimate. This might be do to the numerical differences in the algorithm, e.g.\u00a0the treatment of initial conditions, because of the small number of observations in the longley dataset.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "print(gls_results.params)\nprint(glsar_results.params)\nprint(gls_results.bse)\nprint(glsar_results.bse)",
            "code"
        ],
        [
            "const     -3.797855e+06\nGNPDEFL   -1.276565e+01\nGNP       -3.800132e-02\nUNEMP     -2.186949e+00\nARMED     -1.151776e+00\nPOP       -6.805356e-02\nYEAR       1.993953e+03\ndtype: float64\nconst     -3.467961e+06\nGNPDEFL    3.455678e+01\nGNP       -3.434101e-02\nUNEMP     -1.962144e+00\nARMED     -1.001973e+00\nPOP       -9.780460e-02\nYEAR       1.823183e+03\ndtype: float64\nconst      670688.699308\nGNPDEFL        69.430807\nGNP             0.026248\nUNEMP           0.382393\nARMED           0.165253\nPOP             0.176428\nYEAR          342.634628\ndtype: float64\nconst      871584.051696\nGNPDEFL        84.733715\nGNP             0.032803\nUNEMP           0.480545\nARMED           0.211384\nPOP             0.224774\nYEAR          445.828748\ndtype: float64",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Quantile Regression": [
        [
            "This example page shows how to use statsmodels\u2019 QuantReg class to replicate parts of the analysis published in",
            "markdown"
        ],
        [
            "Koenker, Roger and Kevin F. Hallock. \u201cQuantile Regression\u201d. Journal of Economic Perspectives, Volume 15, Number 4, Fall 2001, Pages 143\u2013156",
            "markdown"
        ],
        [
            "We are interested in the relationship between income and expenditures on food for a sample of working class Belgian households in 1857 (the Engel data).",
            "markdown"
        ]
    ],
    "Examples->Linear Regression Models->Quantile Regression->Setup": [
        [
            "We first need to load some modules and to retrieve the data. Conveniently, the Engel dataset is shipped with statsmodels.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\ndata = sm.datasets.engel.load_pandas().data\ndata.head()",
            "code"
        ],
        [
            "[2]:",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Quantile Regression->Least Absolute Deviation": [
        [
            "The LAD model is a special case of quantile regression where q=0.5",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "mod = smf.quantreg(\"foodexp ~ income\", data)\nres = mod.fit(q=0.5)\nprint(res.summary())",
            "code"
        ],
        [
            "QuantReg Regression Results\n==============================================================================\nDep. Variable:                foodexp   Pseudo R-squared:               0.6206\nModel:                       QuantReg   Bandwidth:                       64.51\nMethod:                 Least Squares   Sparsity:                        209.3\nDate:                Wed, 02 Nov 2022   No. Observations:                  235\nTime:                        17:09:43   Df Residuals:                      233\n                                        Df Model:                            1\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     81.4823     14.634      5.568      0.000      52.649     110.315\nincome         0.5602      0.013     42.516      0.000       0.534       0.586\n==============================================================================\n\nThe condition number is large, 2.38e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Quantile Regression->Visualizing the results": [
        [
            "We estimate the quantile regression model for many quantiles between .05 and .95, and compare best fit line from each of these models to Ordinary Least Squares results.",
            "markdown"
        ]
    ],
    "Examples->Linear Regression Models->Quantile Regression->Visualizing the results->Prepare data for plotting": [
        [
            "For convenience, we place the quantile regression results in a Pandas DataFrame, and the OLS results in a dictionary.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "quantiles = np.arange(0.05, 0.96, 0.1)\n\n\ndef fit_model(q):\n    res = mod.fit(q=q)\n    return [q, res.params[\"Intercept\"], res.params[\"income\"]] + res.conf_int().loc[\n        \"income\"\n    ].tolist()\n\n\nmodels = [fit_model(x) for x in quantiles]\nmodels = pd.DataFrame(models, columns=[\"q\", \"a\", \"b\", \"lb\", \"ub\"])\n\nols = smf.ols(\"foodexp ~ income\", data).fit()\nols_ci = ols.conf_int().loc[\"income\"].tolist()\nols = dict(\n    a=ols.params[\"Intercept\"], b=ols.params[\"income\"], lb=ols_ci[0], ub=ols_ci[1]\n)\n\nprint(models)\nprint(ols)",
            "code"
        ],
        [
            "q           a         b        lb        ub\n0  0.05  124.880097  0.343361  0.268632  0.418090\n1  0.15  111.693660  0.423708  0.382780  0.464636\n2  0.25   95.483539  0.474103  0.439900  0.508306\n3  0.35  105.841294  0.488901  0.457759  0.520043\n4  0.45   81.083647  0.552428  0.525021  0.579835\n5  0.55   89.661370  0.565601  0.540955  0.590247\n6  0.65   74.033435  0.604576  0.582169  0.626982\n7  0.75   62.396584  0.644014  0.622411  0.665617\n8  0.85   52.272216  0.677603  0.657383  0.697823\n9  0.95   64.103964  0.709069  0.687831  0.730306\n{'a': 147.47538852370573, 'b': 0.48517842367692354, 'lb': 0.4568738130184233, 'ub': 0.5134830343354237}",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Quantile Regression->Visualizing the results->First plot": [
        [
            "This plot compares best fit lines for 10 quantile regression models to the least squares fit. As Koenker and Hallock (2001) point out, we see that:",
            "markdown"
        ],
        [
            "Food expenditure increases with income",
            "markdown"
        ],
        [
            "The dispersion of food expenditure increases with income",
            "markdown"
        ],
        [
            "The least squares estimates fit low income observations quite poorly (i.e.\u00a0the OLS line passes over most low income households)",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "x = np.arange(data.income.min(), data.income.max(), 50)\nget_y = lambda a, b: a + b * x\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor i in range(models.shape[0]):\n    y = get_y(models.a[i], models.b[i])\n    ax.plot(x, y, linestyle=\"dotted\", color=\"grey\")\n\ny = get_y(ols[\"a\"], ols[\"b\"])\n\nax.plot(x, y, color=\"red\", label=\"OLS\")\nax.scatter(data.income, data.foodexp, alpha=0.2)\nax.set_xlim((240, 3000))\nax.set_ylim((240, 2000))\nlegend = ax.legend()\nax.set_xlabel(\"Income\", fontsize=16)\nax.set_ylabel(\"Food expenditure\", fontsize=16)",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "Text(0, 0.5, 'Food expenditure')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_quantile_regression_10_1.png\" src=\"../../../_images/examples_notebooks_generated_quantile_regression_10_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Quantile Regression->Visualizing the results->Second plot": [
        [
            "The dotted black lines form 95% point-wise confidence band around 10 quantile regression estimates (solid black line). The red lines represent OLS regression results along with their 95% confidence interval.",
            "markdown"
        ],
        [
            "In most cases, the quantile regression point estimates lie outside the OLS confidence interval, which suggests that the effect of income on food expenditure may not be constant across the distribution.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "n = models.shape[0]\np1 = plt.plot(models.q, models.b, color=\"black\", label=\"Quantile Reg.\")\np2 = plt.plot(models.q, models.ub, linestyle=\"dotted\", color=\"black\")\np3 = plt.plot(models.q, models.lb, linestyle=\"dotted\", color=\"black\")\np4 = plt.plot(models.q, [ols[\"b\"]] * n, color=\"red\", label=\"OLS\")\np5 = plt.plot(models.q, [ols[\"lb\"]] * n, linestyle=\"dotted\", color=\"red\")\np6 = plt.plot(models.q, [ols[\"ub\"]] * n, linestyle=\"dotted\", color=\"red\")\nplt.ylabel(r\"$\\beta_{income}$\")\nplt.xlabel(\"Quantiles of the conditional food expenditure distribution\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_quantile_regression_12_0.png\" src=\"../../../_images/examples_notebooks_generated_quantile_regression_12_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Recursive Least Squares": [
        [
            "Recursive least squares is an expanding window version of ordinary least squares. In addition to availability of regression coefficients computed recursively, the recursively computed residuals the construction of statistics to investigate parameter instability.",
            "markdown"
        ],
        [
            "The RecursiveLS class allows computation of recursive residuals and computes CUSUM and CUSUM of squares statistics. Plotting these statistics along with reference lines denoting statistically significant deviations from the null hypothesis of stable parameters allows an easy visual indication of parameter stability.",
            "markdown"
        ],
        [
            "Finally, the RecursiveLS model allows imposing linear restrictions on the parameter vectors, and can be constructed using the formula interface.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom pandas_datareader.data import DataReader\n\nnp.set_printoptions(suppress=True)",
            "code"
        ],
        [
            "Example 3: Linear restrictions and formulas",
            "markdown"
        ]
    ],
    "Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper": [
        [
            "We first consider parameter stability in the copper dataset (description below).",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "print(sm.datasets.copper.DESCRLONG)\n\ndta = sm.datasets.copper.load_pandas().data\ndta.index = pd.date_range(\"1951-01-01\", \"1975-01-01\", freq=\"AS\")\nendog = dta[\"WORLDCONSUMPTION\"]\n\n# To the regressors in the dataset, we add a column of ones for an intercept\nexog = sm.add_constant(\n    dta[[\"COPPERPRICE\", \"INCOMEINDEX\", \"ALUMPRICE\", \"INVENTORYINDEX\"]]\n)",
            "code"
        ],
        [
            "This data describes the world copper market from 1951 through 1975.  In an\nexample, in Gill, the outcome variable (of a 2 stage estimation) is the world\nconsumption of copper for the 25 years.  The explanatory variables are the\nworld consumption of copper in 1000 metric tons, the constant dollar adjusted\nprice of copper, the price of a substitute, aluminum, an index of real per\ncapita income base 1970, an annual measure of manufacturer inventory change,\nand a time trend.",
            "code"
        ],
        [
            "First, construct and fit the model, and print a summary. Although the RLS model computes the regression parameters recursively, so there are as many estimates as there are datapoints, the summary table only presents the regression parameters estimated on the entire sample; except for small effects from initialization of the recursions, these estimates are equivalent to OLS estimates.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "mod = sm.RecursiveLS(endog, exog)\nres = mod.fit()\n\nprint(res.summary())",
            "code"
        ],
        [
            "Statespace Model Results\n==============================================================================\nDep. Variable:       WORLDCONSUMPTION   No. Observations:                   25\nModel:                    RecursiveLS   Log Likelihood                -154.720\nDate:                Wed, 02 Nov 2022   R-squared:                       0.965\nTime:                        17:06:36   AIC                            319.441\nSample:                    01-01-1951   BIC                            325.535\n                         - 01-01-1975   HQIC                           321.131\nCovariance Type:            nonrobust   Scale                       117717.127\n==================================================================================\n                     coef    std err          z      P|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst          -6562.3719   2378.939     -2.759      0.006   -1.12e+04   -1899.737\nCOPPERPRICE      -13.8132     15.041     -0.918      0.358     -43.292      15.666\nINCOMEINDEX      1.21e+04    763.401     15.853      0.000    1.06e+04    1.36e+04\nALUMPRICE         70.4146     32.678      2.155      0.031       6.367     134.462\nINVENTORYINDEX   311.7330   2130.084      0.146      0.884   -3863.155    4486.621\n===================================================================================\nLjung-Box (L1) (Q):                   2.17   Jarque-Bera (JB):                 1.70\nProb(Q):                              0.14   Prob(JB):                         0.43\nHeteroskedasticity (H):               3.38   Skew:                            -0.67\nProb(H) (two-sided):                  0.13   Kurtosis:                         2.53\n===================================================================================\n\nWarnings:\n[1] Parameters and covariance matrix estimates are RLS estimates conditional on the entire sample.",
            "code"
        ],
        [
            "The recursive coefficients are available in the recursive_coefficients attribute. Alternatively, plots can generated using the plot_recursive_coefficient method.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "print(res.recursive_coefficients.filtered[0])\nres.plot_recursive_coefficient(range(mod.k_exog), alpha=None, figsize=(10, 6))",
            "code"
        ],
        [
            "[     2.88890087      4.94795049   1558.41803044   1958.43326657\n -51474.95389825  -4168.95149564  -2252.61355052   -446.5591456\n  -5288.39795594  -6942.31935565  -7846.08902726  -6643.15121671\n  -6274.1101596   -7272.01696687  -6319.02648931  -5822.23929496\n  -6256.30903085  -6737.40446361  -6477.42841937  -5995.9074752\n  -6450.80678458  -6022.92167008  -5258.35153117  -5320.89136734\n  -6562.37193956]",
            "code"
        ],
        [
            "[4]:\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_recursive_ls_7_1.png\" src=\"../../../_images/examples_notebooks_generated_recursive_ls_7_1.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_recursive_ls_7_2.png\" src=\"../../../_images/examples_notebooks_generated_recursive_ls_7_2.png\"/>",
            "code"
        ],
        [
            "The CUSUM statistic is available in the cusum attribute, but usually it is more convenient to visually check for parameter stability using the plot_cusum method. In the plot below, the CUSUM statistic does not move outside of the 5% significance bands, so we fail to reject the null hypothesis of stable parameters at the 5% level.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "print(res.cusum)\nfig = res.plot_cusum()",
            "code"
        ],
        [
            "[ 0.69971507  0.65841239  1.2462967   2.05476027  2.39888914  3.17861975\n  2.67244668  2.01783211  2.46131743  2.05268634  0.95054332 -1.04505551\n -2.5546529  -2.29908156 -1.45289497 -1.95353998 -1.35046625  0.15789824\n  0.63286526 -1.4818459 ]\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_recursive_ls_9_1.png\" src=\"../../../_images/examples_notebooks_generated_recursive_ls_9_1.png\"/>",
            "code"
        ],
        [
            "Another related statistic is the CUSUM of squares. It is available in the cusum_squares attribute, but it is similarly more convenient to check it visually, using the plot_cusum_squares method. In the plot below, the CUSUM of squares statistic does not move outside of the 5% significance bands, so we fail to reject the null hypothesis of stable parameters at the 5% level.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "res.plot_cusum_squares()",
            "code"
        ],
        [
            "[6]:\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_recursive_ls_11_0.png\" src=\"../../../_images/examples_notebooks_generated_recursive_ls_11_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_recursive_ls_11_1.png\" src=\"../../../_images/examples_notebooks_generated_recursive_ls_11_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money": [
        [
            "The quantity theory of money suggests that \u201ca given change in the rate of change in the quantity of money induces \u2026 an equal change in the rate of price inflation\u201d (Lucas, 1980). Following Lucas, we examine the relationship between double-sided exponentially weighted moving averages of money growth and CPI inflation. Although Lucas found the relationship between these variables to be stable, more recently it appears that the relationship is unstable; see e.g.\u00a0Sargent and Surico (2010).",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "start = \"1959-12-01\"\nend = \"2015-01-01\"\nm2 = DataReader(\"M2SL\", \"fred\", start=start, end=end)\ncpi = DataReader(\"CPIAUCSL\", \"fred\", start=start, end=end)",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "def ewma(series, beta, n_window):\n    nobs = len(series)\n    scalar = (1 - beta) / (1 + beta)\n    ma = []\n    k = np.arange(n_window, 0, -1)\n    weights = np.r_[beta ** k, 1, beta ** k[::-1]]\n    for t in range(n_window, nobs - n_window):\n        window = series.iloc[t - n_window : t + n_window + 1].values\n        ma.append(scalar * np.sum(weights * window))\n    return pd.Series(ma, name=series.name, index=series.iloc[n_window:-n_window].index)\n\n\nm2_ewma = ewma(np.log(m2[\"M2SL\"].resample(\"QS\").mean()).diff().iloc[1:], 0.95, 10 * 4)\ncpi_ewma = ewma(\n    np.log(cpi[\"CPIAUCSL\"].resample(\"QS\").mean()).diff().iloc[1:], 0.95, 10 * 4\n)",
            "code"
        ],
        [
            "After constructing the moving averages using the \\(\\beta = 0.95\\) filter of Lucas (with a window of 10 years on either side), we plot each of the series below. Although they appear to move together prior for part of the sample, after 1990 they appear to diverge.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(13, 3))\n\nax.plot(m2_ewma, label=\"M2 Growth (EWMA)\")\nax.plot(cpi_ewma, label=\"CPI Inflation (EWMA)\")\nax.legend()",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f52c4b63f70\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_recursive_ls_16_1.png\" src=\"../../../_images/examples_notebooks_generated_recursive_ls_16_1.png\"/>",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "endog = cpi_ewma\nexog = sm.add_constant(m2_ewma)\nexog.columns = [\"const\", \"M2\"]\n\nmod = sm.RecursiveLS(endog, exog)\nres = mod.fit()\n\nprint(res.summary())",
            "code"
        ],
        [
            "Statespace Model Results\n==============================================================================\nDep. Variable:               CPIAUCSL   No. Observations:                  141\nModel:                    RecursiveLS   Log Likelihood                 692.884\nDate:                Wed, 02 Nov 2022   R-squared:                       0.813\nTime:                        17:06:38   AIC                          -1381.769\nSample:                    01-01-1970   BIC                          -1375.871\n                         - 01-01-2005   HQIC                         -1379.372\nCovariance Type:            nonrobust   Scale                            0.000\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0034      0.001     -6.014      0.000      -0.004      -0.002\nM2             0.9128      0.037     24.603      0.000       0.840       0.986\n===================================================================================\nLjung-Box (L1) (Q):                 138.24   Jarque-Bera (JB):                18.20\nProb(Q):                              0.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               5.30   Skew:                            -0.81\nProb(H) (two-sided):                  0.00   Kurtosis:                         2.27\n===================================================================================\n\nWarnings:\n[1] Parameters and covariance matrix estimates are RLS estimates conditional on the entire sample.",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "res.plot_recursive_coefficient(1, alpha=None)",
            "code"
        ],
        [
            "[11]:\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_recursive_ls_18_0.png\" src=\"../../../_images/examples_notebooks_generated_recursive_ls_18_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_recursive_ls_18_1.png\" src=\"../../../_images/examples_notebooks_generated_recursive_ls_18_1.png\"/>",
            "code"
        ],
        [
            "The CUSUM plot now shows substantial deviation at the 5% level, suggesting a rejection of the null hypothesis of parameter stability.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "res.plot_cusum()",
            "code"
        ],
        [
            "[12]:\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_recursive_ls_20_0.png\" src=\"../../../_images/examples_notebooks_generated_recursive_ls_20_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_recursive_ls_20_1.png\" src=\"../../../_images/examples_notebooks_generated_recursive_ls_20_1.png\"/>",
            "code"
        ],
        [
            "Similarly, the CUSUM of squares shows substantial deviation at the 5% level, also suggesting a rejection of the null hypothesis of parameter stability.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "res.plot_cusum_squares()",
            "code"
        ],
        [
            "[13]:\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_recursive_ls_22_0.png\" src=\"../../../_images/examples_notebooks_generated_recursive_ls_22_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_recursive_ls_22_1.png\" src=\"../../../_images/examples_notebooks_generated_recursive_ls_22_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Recursive Least Squares->Linear restrictions": [
        [
            "It is not hard to implement linear restrictions, using the constraints parameter in constructing the model.",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "endog = dta[\"WORLDCONSUMPTION\"]\nexog = sm.add_constant(\n    dta[[\"COPPERPRICE\", \"INCOMEINDEX\", \"ALUMPRICE\", \"INVENTORYINDEX\"]]\n)\n\nmod = sm.RecursiveLS(endog, exog, constraints=\"COPPERPRICE = ALUMPRICE\")\nres = mod.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "Statespace Model Results\n==============================================================================\nDep. Variable:       WORLDCONSUMPTION   No. Observations:                   25\nModel:                    RecursiveLS   Log Likelihood                -144.004\nDate:                Wed, 02 Nov 2022   R-squared:                       0.989\nTime:                        17:06:39   AIC                            296.008\nSample:                    01-01-1951   BIC                            300.883\n                         - 01-01-1975   HQIC                           297.360\nCovariance Type:            nonrobust   Scale                       137155.014\n==================================================================================\n                     coef    std err          z      P|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst          -4839.4893   2412.410     -2.006      0.045   -9567.726    -111.253\nCOPPERPRICE        5.9797     12.704      0.471      0.638     -18.921      30.880\nINCOMEINDEX     1.115e+04    666.308     16.738      0.000    9847.002    1.25e+04\nALUMPRICE          5.9797     12.704      0.471      0.638     -18.921      30.880\nINVENTORYINDEX   241.3447   2298.951      0.105      0.916   -4264.516    4747.205\n===================================================================================\nLjung-Box (L1) (Q):                   6.27   Jarque-Bera (JB):                 1.78\nProb(Q):                              0.01   Prob(JB):                         0.41\nHeteroskedasticity (H):               1.75   Skew:                            -0.63\nProb(H) (two-sided):                  0.48   Kurtosis:                         2.32\n===================================================================================\n\nWarnings:\n[1] Parameters and covariance matrix estimates are RLS estimates conditional on the entire sample.\n[2] Covariance matrix is singular or near-singular, with condition number 1.05e+17. Standard errors may be unstable.",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Recursive Least Squares->Formula": [
        [
            "One could fit the same model using the class method from_formula.",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "mod = sm.RecursiveLS.from_formula(\n    \"WORLDCONSUMPTION ~ COPPERPRICE + INCOMEINDEX + ALUMPRICE + INVENTORYINDEX\",\n    dta,\n    constraints=\"COPPERPRICE = ALUMPRICE\",\n)\nres = mod.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "Statespace Model Results\n==============================================================================\nDep. Variable:       WORLDCONSUMPTION   No. Observations:                   25\nModel:                    RecursiveLS   Log Likelihood                -144.004\nDate:                Wed, 02 Nov 2022   R-squared:                       0.989\nTime:                        17:06:39   AIC                            296.008\nSample:                    01-01-1951   BIC                            300.883\n                         - 01-01-1975   HQIC                           297.360\nCovariance Type:            nonrobust   Scale                       137155.014\n==================================================================================\n                     coef    std err          z      P|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept      -4839.4893   2412.410     -2.006      0.045   -9567.726    -111.253\nCOPPERPRICE        5.9797     12.704      0.471      0.638     -18.921      30.880\nINCOMEINDEX     1.115e+04    666.308     16.738      0.000    9847.002    1.25e+04\nALUMPRICE          5.9797     12.704      0.471      0.638     -18.921      30.880\nINVENTORYINDEX   241.3447   2298.951      0.105      0.916   -4264.516    4747.205\n===================================================================================\nLjung-Box (L1) (Q):                   6.27   Jarque-Bera (JB):                 1.78\nProb(Q):                              0.01   Prob(JB):                         0.41\nHeteroskedasticity (H):               1.75   Skew:                            -0.63\nProb(H) (two-sided):                  0.48   Kurtosis:                         2.32\n===================================================================================\n\nWarnings:\n[1] Parameters and covariance matrix estimates are RLS estimates conditional on the entire sample.\n[2] Covariance matrix is singular or near-singular, with condition number 1.05e+17. Standard errors may be unstable.",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency AS-JAN will be used.\n  self._init_dates(dates, freq)",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Rolling Least Squares": [
        [
            "Rolling OLS applies OLS across a fixed windows of observations and then rolls (moves or slides) the window across the data set. They key parameter is window which determines the number of observations used in each OLS regression. By default, RollingOLS drops missing values in the window and so will estimate the model using the available data points.",
            "markdown"
        ],
        [
            "Estimated values are aligned so that models estimated using data points \\(i+1, i+2, ... i+window\\) are stored in location \\(i+window\\).",
            "markdown"
        ],
        [
            "Start by importing the modules that are used in this notebook.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline",
            "code"
        ],
        [
            "pandas-datareader is used to download data from . The two data sets downloaded are the 3 Fama-French factors and the 10 industry portfolios. Data is available from 1926.",
            "markdown"
        ],
        [
            "The data are monthly returns for the factors or industry portfolios.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "factors = pdr.get_data_famafrench(\"F-F_Research_Data_Factors\", start=\"1-1-1926\")[0]\nfactors.head()",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "industries = pdr.get_data_famafrench(\"10_Industry_Portfolios\", start=\"1-1-1926\")[0]\nindustries.head()",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "The first model estimated is a rolling version of the CAPM that regresses the excess return of Technology sector firms on the excess return of the market.",
            "markdown"
        ],
        [
            "The window is 60 months, and so results are available after the first 60 (window) months. The first 59 (window - 1) estimates are all nan filled.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "endog = industries.HiTec - factors.RF.values\nexog = sm.add_constant(factors[\"Mkt-RF\"])\nrols = RollingOLS(endog, exog, window=60)\nrres = rols.fit()\nparams = rres.params.copy()\nparams.index = np.arange(1, params.shape[0] + 1)\nparams.head()",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "params.iloc[57:62]",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "params.tail()",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "We next plot the market loading along with a 95% point-wise confidence interval. The alpha=False omits the constant column, if present.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "fig = rres.plot_recursive_coefficient(variables=[\"Mkt-RF\"], figsize=(14, 6))\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_rolling_ls_10_0.png\" src=\"../../../_images/examples_notebooks_generated_rolling_ls_10_0.png\"/>",
            "code"
        ],
        [
            "Next, the model is expanded to include all three factors, the excess market, the size factor and the value factor.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "exog_vars = [\"Mkt-RF\", \"SMB\", \"HML\"]\nexog = sm.add_constant(factors[exog_vars])\nrols = RollingOLS(endog, exog, window=60)\nrres = rols.fit()\nfig = rres.plot_recursive_coefficient(variables=exog_vars, figsize=(14, 18))\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\" src=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Rolling Least Squares->Formulas": [
        [
            "RollingOLS and RollingWLS both support model specification using the formula interface. The example below is equivalent to the 3-factor model estimated previously. Note that one variable is renamed to have a valid Python variable name.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "joined = pd.concat([factors, industries], axis=1)\njoined[\"Mkt_RF\"] = joined[\"Mkt-RF\"]\nmod = RollingOLS.from_formula(\"HiTec ~ Mkt_RF + SMB + HML\", data=joined, window=60)\nrres = mod.fit()\nrres.params.tail()",
            "code"
        ],
        [
            "[9]:",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Rolling Least Squares->RollingWLS: Rolling Weighted Least Squares": [
        [
            "The rolling module also provides RollingWLS which takes an optional weights input to perform rolling weighted least squares. It produces results that match WLS when applied to rolling windows of data.",
            "markdown"
        ]
    ],
    "Examples->Linear Regression Models->Rolling Least Squares->Fit Options": [
        [
            "Fit accepts other optional keywords to set the covariance estimator. Only two estimators are supported, 'nonrobust' (the classic OLS estimator) and 'HC0' which is White\u2019s heteroskedasticity robust estimator.",
            "markdown"
        ],
        [
            "You can set params_only=True to only estimate the model parameters. This is substantially faster than computing the full set of values required to perform inference.",
            "markdown"
        ],
        [
            "Finally, the parameter reset can be set to a positive integer to control estimation error in very long samples. RollingOLS avoids the full matrix product when rolling by only adding the most recent observation and removing the dropped observation as it rolls through the sample. Setting reset uses the full inner product every reset periods. In most applications this parameter can be omitted.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "%timeit rols.fit()\n%timeit rols.fit(params_only=True)",
            "code"
        ],
        [
            "255 ms \u00b1 2.81 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n62.2 ms \u00b1 686 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Rolling Least Squares->Expanding Sample": [
        [
            "It is possible to expand the sample until sufficient observations are available for the full window length. In this example, we start once we have 12 observations available, and then increase the sample until we have 60 observations available. The first non-nan value is computed using 12 observations, the second 13, and so on. All other estimates are computed using 60 observations.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "res = RollingOLS(endog, exog, window=60, min_nobs=12, expanding=True).fit()\nres.params.iloc[10:15]",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "res.nobs[10:15]",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "Date\n1927-05     0\n1927-06    12\n1927-07    13\n1927-08    14\n1927-09    15\nFreq: M, dtype: int64",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Regression Diagnostics": [
        [
            "This example file shows how to use a few of the statsmodels regression diagnostic tests in a real-life context. You can learn about more tests and find out more information about the tests here on the",
            "markdown"
        ],
        [
            "Note that most of the tests described here only return a tuple of numbers, without any annotation. A full description of outputs is always included in the docstring and in the online statsmodels documentation. For presentation purposes, we use the zip(name,test) construct to pretty-print short descriptions in the examples below.",
            "markdown"
        ]
    ],
    "Examples->Linear Regression Models->Regression Diagnostics->Estimate a regression model": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "from statsmodels.compat import lzip\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport statsmodels.stats.api as sms\nimport matplotlib.pyplot as plt\n\n# Load data\nurl = \"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/HistData/Guerry.csv\"\ndat = pd.read_csv(url)\n\n# Fit regression model (using the natural log of one of the regressors)\nresults = smf.ols(\"Lottery ~ Literacy + np.log(Pop1831)\", data=dat).fit()\n\n# Inspect the results\nprint(results.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                Lottery   R-squared:                       0.348\nModel:                            OLS   Adj. R-squared:                  0.333\nMethod:                 Least Squares   F-statistic:                     22.20\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           1.90e-08\nTime:                        17:11:11   Log-Likelihood:                -379.82\nNo. Observations:                  86   AIC:                             765.6\nDf Residuals:                      83   BIC:                             773.0\nDf Model:                           2\nCovariance Type:            nonrobust\n===================================================================================\n                      coef    std err          t      P|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nIntercept         246.4341     35.233      6.995      0.000     176.358     316.510\nLiteracy           -0.4889      0.128     -3.832      0.000      -0.743      -0.235\nnp.log(Pop1831)   -31.3114      5.977     -5.239      0.000     -43.199     -19.424\n==============================================================================\nOmnibus:                        3.713   Durbin-Watson:                   2.019\nProb(Omnibus):                  0.156   Jarque-Bera (JB):                3.394\nSkew:                          -0.487   Prob(JB):                        0.183\nKurtosis:                       3.003   Cond. No.                         702.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Regression Diagnostics->Normality of the residuals": [
        [
            "Jarque-Bera test:",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "name = [\"Jarque-Bera\", \"Chi^2 two-tail prob.\", \"Skew\", \"Kurtosis\"]\ntest = sms.jarque_bera(results.resid)\nlzip(name, test)",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "[('Jarque-Bera', 3.393608024843173),\n ('Chi^2 two-tail prob.', 0.1832683123166331),\n ('Skew', -0.4865803431122342),\n ('Kurtosis', 3.0034177578816346)]",
            "code"
        ],
        [
            "Omni test:",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "name = [\"Chi^2\", \"Two-tail probability\"]\ntest = sms.omni_normtest(results.resid)\nlzip(name, test)",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "[('Chi^2', 3.713437811597192), ('Two-tail probability', 0.15618424580304746)]",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Regression Diagnostics->Influence tests": [
        [
            "Once created, an object of class OLSInfluence holds attributes and methods that allow users to assess the influence of each observation. For example, we can compute and extract the first few rows of DFbetas by:",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "from statsmodels.stats.outliers_influence import OLSInfluence\n\ntest_class = OLSInfluence(results)\ntest_class.dfbetas[:5, :]",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "array([[-0.00301154,  0.00290872,  0.00118179],\n       [-0.06425662,  0.04043093,  0.06281609],\n       [ 0.01554894, -0.03556038, -0.00905336],\n       [ 0.17899858,  0.04098207, -0.18062352],\n       [ 0.29679073,  0.21249207, -0.3213655 ]])",
            "code"
        ],
        [
            "Explore other options by typing dir(influence_test)",
            "markdown"
        ],
        [
            "Useful information on leverage can also be plotted:",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "from statsmodels.graphics.regressionplots import plot_leverage_resid2\n\nfig, ax = plt.subplots(figsize=(8, 6))\nfig = plot_leverage_resid2(results, ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_diagnostics_13_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_diagnostics_13_0.png\"/>",
            "code"
        ],
        [
            "Other plotting options can be found on the",
            "markdown"
        ]
    ],
    "Examples->Linear Regression Models->Regression Diagnostics->Multicollinearity": [
        [
            "Condition number:",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "np.linalg.cond(results.model.exog)",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "702.1792145490066",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Regression Diagnostics->Heteroskedasticity tests": [
        [
            "Breush-Pagan test:",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "name = [\"Lagrange multiplier statistic\", \"p-value\", \"f-value\", \"f p-value\"]\ntest = sms.het_breuschpagan(results.resid, results.model.exog)\nlzip(name, test)",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "[('Lagrange multiplier statistic', 4.893213374093985),\n ('p-value', 0.08658690502352087),\n ('f-value', 2.5037159462564538),\n ('f p-value', 0.08794028782672857)]",
            "code"
        ],
        [
            "Goldfeld-Quandt test",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "name = [\"F statistic\", \"p-value\"]\ntest = sms.het_goldfeldquandt(results.resid, results.model.exog)\nlzip(name, test)",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "[('F statistic', 1.100242243637814), ('p-value', 0.38202950686925286)]",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Regression Diagnostics->Linearity": [
        [
            "Harvey-Collier multiplier test for Null hypothesis that the linear specification is correct:",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "name = [\"t value\", \"p value\"]\ntest = sms.linear_harvey_collier(results)\nlzip(name, test)",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "[('t value', -1.0796490077761558), ('p value', 0.2834639247568444)]",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Weighted Least Squares": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.iolib.table import SimpleTable, default_txt_fmt\n\nnp.random.seed(1024)",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Weighted Least Squares->WLS Estimation->Artificial data: Heteroscedasticity 2 groups": [
        [
            "Model assumptions:",
            "markdown"
        ],
        [
            "Misspecification: true model is quadratic, estimate only linear",
            "markdown"
        ],
        [
            "Independent noise/error term",
            "markdown"
        ],
        [
            "Two groups for error variance, low and high variance groups",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "nsample = 50\nx = np.linspace(0, 20, nsample)\nX = np.column_stack((x, (x - 5) ** 2))\nX = sm.add_constant(X)\nbeta = [5.0, 0.5, -0.01]\nsig = 0.5\nw = np.ones(nsample)\nw[nsample * 6 // 10 :] = 3\ny_true = np.dot(X, beta)\ne = np.random.normal(size=nsample)\ny = y_true + sig * w * e\nX = X[:, [0, 1]]",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Weighted Least Squares->WLS Estimation->WLS knowing the true variance ratio of heteroscedasticity": [
        [
            "In this example, w is the standard deviation of the error. WLS requires that the weights are proportional to the inverse of the error variance.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "mod_wls = sm.WLS(y, X, weights=1.0 / (w ** 2))\nres_wls = mod_wls.fit()\nprint(res_wls.summary())",
            "code"
        ],
        [
            "WLS Regression Results\n==============================================================================\nDep. Variable:                      y   R-squared:                       0.927\nModel:                            WLS   Adj. R-squared:                  0.926\nMethod:                 Least Squares   F-statistic:                     613.2\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           5.44e-29\nTime:                        17:07:44   Log-Likelihood:                -51.136\nNo. Observations:                  50   AIC:                             106.3\nDf Residuals:                      48   BIC:                             110.1\nDf Model:                           1\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          5.2469      0.143     36.790      0.000       4.960       5.534\nx1             0.4466      0.018     24.764      0.000       0.410       0.483\n==============================================================================\nOmnibus:                        0.407   Durbin-Watson:                   2.317\nProb(Omnibus):                  0.816   Jarque-Bera (JB):                0.103\nSkew:                          -0.104   Prob(JB):                        0.950\nKurtosis:                       3.075   Cond. No.                         14.6\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Weighted Least Squares->OLS vs.\u00a0WLS": [
        [
            "Estimate an OLS model for comparison:",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "res_ols = sm.OLS(y, X).fit()\nprint(res_ols.params)\nprint(res_wls.params)",
            "code"
        ],
        [
            "[5.24256099 0.43486879]\n[5.24685499 0.44658241]",
            "code"
        ],
        [
            "Compare the WLS standard errors to heteroscedasticity corrected OLS standard errors:",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "se = np.vstack(\n    [\n        [res_wls.bse],\n        [res_ols.bse],\n        [res_ols.HC0_se],\n        [res_ols.HC1_se],\n        [res_ols.HC2_se],\n        [res_ols.HC3_se],\n    ]\n)\nse = np.round(se, 4)\ncolnames = [\"x1\", \"const\"]\nrownames = [\"WLS\", \"OLS\", \"OLS_HC0\", \"OLS_HC1\", \"OLS_HC3\", \"OLS_HC3\"]\ntabl = SimpleTable(se, colnames, rownames, txt_fmt=default_txt_fmt)\nprint(tabl)",
            "code"
        ],
        [
            "=====================\n          x1   const\n---------------------\nWLS     0.1426  0.018\nOLS     0.2707 0.0233\nOLS_HC0  0.194 0.0281\nOLS_HC1  0.198 0.0287\nOLS_HC3 0.2003  0.029\nOLS_HC3  0.207   0.03\n---------------------",
            "code"
        ],
        [
            "Calculate OLS prediction interval:",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "covb = res_ols.cov_params()\nprediction_var = res_ols.mse_resid + (X * np.dot(covb, X.T).T).sum(1)\nprediction_std = np.sqrt(prediction_var)\ntppf = stats.t.ppf(0.975, res_ols.df_resid)",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "pred_ols = res_ols.get_prediction()\niv_l_ols = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u_ols = pred_ols.summary_frame()[\"obs_ci_upper\"]",
            "code"
        ],
        [
            "Draw a plot to compare predicted values in WLS and OLS:",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "pred_wls = res_wls.get_prediction()\niv_l = pred_wls.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_wls.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot(x, y, \"o\", label=\"Data\")\nax.plot(x, y_true, \"b-\", label=\"True\")\n# OLS\nax.plot(x, res_ols.fittedvalues, \"r--\")\nax.plot(x, iv_u_ols, \"r--\", label=\"OLS\")\nax.plot(x, iv_l_ols, \"r--\")\n# WLS\nax.plot(x, res_wls.fittedvalues, \"g--.\")\nax.plot(x, iv_u, \"g--\", label=\"WLS\")\nax.plot(x, iv_l, \"g--\")\nax.legend(loc=\"best\")",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f4baeee04f0\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_wls_15_1.png\" src=\"../../../_images/examples_notebooks_generated_wls_15_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Weighted Least Squares->Feasible Weighted Least Squares (2-stage FWLS)": [
        [
            "Like w, w_est is proportional to the standard deviation, and so must be squared.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "resid1 = res_ols.resid[w == 1.0]\nvar1 = resid1.var(ddof=int(res_ols.df_model) + 1)\nresid2 = res_ols.resid[w != 1.0]\nvar2 = resid2.var(ddof=int(res_ols.df_model) + 1)\nw_est = w.copy()\nw_est[w != 1.0] = np.sqrt(var2) / np.sqrt(var1)\nres_fwls = sm.WLS(y, X, 1.0 / ((w_est ** 2))).fit()\nprint(res_fwls.summary())",
            "code"
        ],
        [
            "WLS Regression Results\n==============================================================================\nDep. Variable:                      y   R-squared:                       0.931\nModel:                            WLS   Adj. R-squared:                  0.929\nMethod:                 Least Squares   F-statistic:                     646.7\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           1.66e-29\nTime:                        17:07:44   Log-Likelihood:                -50.716\nNo. Observations:                  50   AIC:                             105.4\nDf Residuals:                      48   BIC:                             109.3\nDf Model:                           1\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          5.2363      0.135     38.720      0.000       4.964       5.508\nx1             0.4492      0.018     25.431      0.000       0.414       0.485\n==============================================================================\nOmnibus:                        0.247   Durbin-Watson:                   2.343\nProb(Omnibus):                  0.884   Jarque-Bera (JB):                0.179\nSkew:                          -0.136   Prob(JB):                        0.915\nKurtosis:                       2.893   Cond. No.                         14.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Linear Mixed-Effects": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning",
            "code"
        ],
        [
            "<strong>Note</strong>: The R code and the results in this notebook has been converted to markdown so that R is not required to build the documents. The R results in the notebook were computed using R 3.5.1 and lme4 1.1.",
            "markdown"
        ],
        [
            "%load_ext rpy2.ipython",
            "code"
        ],
        [
            "%R library(lme4)",
            "code"
        ],
        [
            "array(['lme4', 'Matrix', 'tools', 'stats', 'graphics', 'grDevices',\n       'utils', 'datasets', 'methods', 'base'], dtype='&lt;U9')",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Variance Component Analysis": [
        [
            "This notebook illustrates variance components analysis for two-level nested and crossed designs.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.regression.mixed_linear_model import VCSpec\nimport pandas as pd",
            "code"
        ],
        [
            "Make the notebook reproducible",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "np.random.seed(3123)",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Variance Component Analysis->Nested analysis": [
        [
            "In our discussion below, \u201cGroup 2\u201d is nested within \u201cGroup 1\u201d. As a concrete example, \u201cGroup 1\u201d might be school districts, with \u201cGroup 2\u201d being individual schools. The function below generates data from such a population. In a nested analysis, the group 2 labels that are nested within different group 1 labels are treated as independent groups, even if they have the same label. For example, two schools labeled \u201cschool 1\u201d that are in two different school districts are treated as independent\nschools, even though they have the same label.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "def generate_nested(\n    n_group1=200, n_group2=20, n_rep=10, group1_sd=2, group2_sd=3, unexplained_sd=4\n):\n\n    # Group 1 indicators\n    group1 = np.kron(np.arange(n_group1), np.ones(n_group2 * n_rep))\n\n    # Group 1 effects\n    u = group1_sd * np.random.normal(size=n_group1)\n    effects1 = np.kron(u, np.ones(n_group2 * n_rep))\n\n    # Group 2 indicators\n    group2 = np.kron(np.ones(n_group1), np.kron(np.arange(n_group2), np.ones(n_rep)))\n\n    # Group 2 effects\n    u = group2_sd * np.random.normal(size=n_group1 * n_group2)\n    effects2 = np.kron(u, np.ones(n_rep))\n\n    e = unexplained_sd * np.random.normal(size=n_group1 * n_group2 * n_rep)\n    y = effects1 + effects2 + e\n\n    df = pd.DataFrame({\"y\": y, \"group1\": group1, \"group2\": group2})\n\n    return df",
            "code"
        ],
        [
            "Generate a data set to analyze.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "df = generate_nested()",
            "code"
        ],
        [
            "Using all the default arguments for generate_nested, the population values of \u201cgroup 1 Var\u201d and \u201cgroup 2 Var\u201d are 2^2=4 and 3^2=9, respectively. The unexplained variance, listed as \u201cscale\u201d at the top of the summary table, has population value 4^2=16.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "model1 = sm.MixedLM.from_formula(\n    \"y ~ 1\",\n    re_formula=\"1\",\n    vc_formula={\"group2\": \"0 + C(group2)\"},\n    groups=\"group1\",\n    data=df,\n)\nresult1 = model1.fit()\nprint(result1.summary())",
            "code"
        ],
        [
            "Mixed Linear Model Regression Results\n==========================================================\nModel:            MixedLM Dependent Variable: y\nNo. Observations: 40000   Method:             REML\nNo. Groups:       200     Scale:              15.8825\nMin. group size:  200     Log-Likelihood:     -116022.3805\nMax. group size:  200     Converged:          Yes\nMean group size:  200.0\n-----------------------------------------------------------\n            Coef.   Std.Err.    z     P|z|  [0.025  0.975]\n-----------------------------------------------------------\nIntercept   -0.035     0.149  -0.232  0.817  -0.326   0.257\ngroup1 Var   3.917     0.112\ngroup2 Var   8.742     0.063\n==========================================================",
            "code"
        ],
        [
            "If we wish to avoid the formula interface, we can fit the same model by building the design matrices manually.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "def f(x):\n    n = x.shape[0]\n    g2 = x.group2\n    u = g2.unique()\n    u.sort()\n    uv = {v: k for k, v in enumerate(u)}\n    mat = np.zeros((n, len(u)))\n    for i in range(n):\n        mat[i, uv[g2.iloc[i]]] = 1\n    colnames = [\"%d\" % z for z in u]\n    return mat, colnames",
            "code"
        ],
        [
            "Then we set up the variance components using the VCSpec class.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "vcm = df.groupby(\"group1\").apply(f).to_list()\nmats = [x[0] for x in vcm]\ncolnames = [x[1] for x in vcm]\nnames = [\"group2\"]\nvcs = VCSpec(names, [colnames], [mats])",
            "code"
        ],
        [
            "Finally we fit the model. It can be seen that the results of the two fits are identical.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "code"
        ],
        [
            "Mixed Linear Model Regression Results\n==========================================================\nModel:            MixedLM Dependent Variable: y\nNo. Observations: 40000   Method:             REML\nNo. Groups:       200     Scale:              15.8825\nMin. group size:  200     Log-Likelihood:     -116022.3805\nMax. group size:  200     Converged:          Yes\nMean group size:  200.0\n-----------------------------------------------------------\n            Coef.   Std.Err.    z     P|z|  [0.025  0.975]\n-----------------------------------------------------------\nconst       -0.035     0.149  -0.232  0.817  -0.326   0.257\nx_re1 Var    3.917     0.112\ngroup2 Var   8.742     0.063\n==========================================================",
            "code"
        ]
    ],
    "Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis": [
        [
            "In a crossed analysis, the levels of one group can occur in any combination with the levels of the another group. The groups in Statsmodels MixedLM are always nested, but it is possible to fit a crossed model by having only one group, and specifying all random effects as variance components. Many, but not all crossed models can be fit in this way. The function below generates a crossed data set with two levels of random structure.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "def generate_crossed(\n    n_group1=100, n_group2=100, n_rep=4, group1_sd=2, group2_sd=3, unexplained_sd=4\n):\n\n    # Group 1 indicators\n    group1 = np.kron(\n        np.arange(n_group1, dtype=int), np.ones(n_group2 * n_rep, dtype=int)\n    )\n    group1 = group1[np.random.permutation(len(group1))]\n\n    # Group 1 effects\n    u = group1_sd * np.random.normal(size=n_group1)\n    effects1 = u[group1]\n\n    # Group 2 indicators\n    group2 = np.kron(\n        np.arange(n_group2, dtype=int), np.ones(n_group2 * n_rep, dtype=int)\n    )\n    group2 = group2[np.random.permutation(len(group2))]\n\n    # Group 2 effects\n    u = group2_sd * np.random.normal(size=n_group2)\n    effects2 = u[group2]\n\n    e = unexplained_sd * np.random.normal(size=n_group1 * n_group2 * n_rep)\n    y = effects1 + effects2 + e\n\n    df = pd.DataFrame({\"y\": y, \"group1\": group1, \"group2\": group2})\n\n    return df",
            "code"
        ],
        [
            "Generate a data set to analyze.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "df = generate_crossed()",
            "code"
        ],
        [
            "Next we fit the model, note that the groups vector is constant. Using the default parameters for generate_crossed, the level 1 variance should be 2^2=4, the level 2 variance should be 3^2=9, and the unexplained variance should be 4^2=16.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "vc = {\"g1\": \"0 + C(group1)\", \"g2\": \"0 + C(group2)\"}\noo = np.ones(df.shape[0])\nmodel3 = sm.MixedLM.from_formula(\"y ~ 1\", groups=oo, vc_formula=vc, data=df)\nresult3 = model3.fit()\nprint(result3.summary())",
            "code"
        ],
        [
            "Mixed Linear Model Regression Results\n==========================================================\nModel:            MixedLM Dependent Variable: y\nNo. Observations: 40000   Method:             REML\nNo. Groups:       1       Scale:              15.9824\nMin. group size:  40000   Log-Likelihood:     -112684.9688\nMax. group size:  40000   Converged:          Yes\nMean group size:  40000.0\n-----------------------------------------------------------\n            Coef.   Std.Err.    z     P|z|  [0.025  0.975]\n-----------------------------------------------------------\nIntercept   -0.251     0.353  -0.710  0.478  -0.943   0.442\ng1 Var       4.282     0.154\ng2 Var       8.150     0.291\n==========================================================",
            "code"
        ],
        [
            "If we wish to avoid the formula interface, we can fit the same model by building the design matrices manually.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "def f(g):\n    n = len(g)\n    u = g.unique()\n    u.sort()\n    uv = {v: k for k, v in enumerate(u)}\n    mat = np.zeros((n, len(u)))\n    for i in range(n):\n        mat[i, uv[g[i]]] = 1\n    colnames = [\"%d\" % z for z in u]\n    return [mat], [colnames]\n\n\nvcm = [f(df.group1), f(df.group2)]\nmats = [x[0] for x in vcm]\ncolnames = [x[1] for x in vcm]\nnames = [\"group1\", \"group2\"]\nvcs = VCSpec(names, colnames, mats)",
            "code"
        ],
        [
            "Here we fit the model without using formulas, it is simple to check that the results for models 3 and 4 are identical.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "oo = np.ones(df.shape[0])\nmodel4 = sm.MixedLM(df.y, oo[:, None], exog_re=None, groups=oo, exog_vc=vcs)\nresult4 = model4.fit()\nprint(result4.summary())",
            "code"
        ],
        [
            "Mixed Linear Model Regression Results\n==========================================================\nModel:            MixedLM Dependent Variable: y\nNo. Observations: 40000   Method:             REML\nNo. Groups:       1       Scale:              15.9824\nMin. group size:  40000   Log-Likelihood:     -112684.9688\nMax. group size:  40000   Converged:          Yes\nMean group size:  40000.0\n-----------------------------------------------------------\n            Coef.   Std.Err.    z     P|z|  [0.025  0.975]\n-----------------------------------------------------------\nconst       -0.251     0.353  -0.710  0.478  -0.943   0.442\ngroup1 Var   4.282     0.154\ngroup2 Var   8.150     0.291\n==========================================================",
            "code"
        ]
    ],
    "Examples->Plotting->Regression Plots": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "from statsmodels.compat import lzip\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "code"
        ]
    ],
    "Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Load the Data": [
        [
            "We can use a utility function to load any R dataset available from the great Rdatasets package.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "prestige = sm.datasets.get_rdataset(\"Duncan\", \"carData\", cache=True).data",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "prestige.head()",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "prestige_model = ols(\"prestige ~ income + education\", data=prestige).fit()",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "print(prestige_model.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:               prestige   R-squared:                       0.828\nModel:                            OLS   Adj. R-squared:                  0.820\nMethod:                 Least Squares   F-statistic:                     101.2\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           8.65e-17\nTime:                        17:07:08   Log-Likelihood:                -178.98\nNo. Observations:                  45   AIC:                             364.0\nDf Residuals:                      42   BIC:                             369.4\nDf Model:                           2\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -6.0647      4.272     -1.420      0.163     -14.686       2.556\nincome         0.5987      0.120      5.003      0.000       0.357       0.840\neducation      0.5458      0.098      5.555      0.000       0.348       0.744\n==============================================================================\nOmnibus:                        1.279   Durbin-Watson:                   1.458\nProb(Omnibus):                  0.528   Jarque-Bera (JB):                0.520\nSkew:                           0.155   Prob(JB):                        0.771\nKurtosis:                       3.426   Cond. No.                         163.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ]
    ],
    "Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Influence plots": [
        [
            "Influence plots show the (externally) studentized residuals vs.\u00a0the leverage of each observation as measured by the hat matrix.",
            "markdown"
        ],
        [
            "Externally studentized residuals are residuals that are scaled by their standard deviation where\n\n\\[var(\\hat{\\epsilon}_i)=\\hat{\\sigma}^2_i(1-h_{ii})\\]",
            "markdown"
        ],
        [
            "with\n\n\\[\\hat{\\sigma}^2_i=\\frac{1}{n - p - 1 \\;\\;}\\sum_{j}^{n}\\;\\;\\;\\forall \\;\\;\\; j \\neq i\\]",
            "markdown"
        ],
        [
            "\\(n\\) is the number of observations and \\(p\\) is the number of regressors. \\(h_{ii}\\) is the \\(i\\)-th diagonal element of the hat matrix\n\n\\[H=X(X^{\\;\\prime}X)^{-1}X^{\\;\\prime}\\]",
            "markdown"
        ],
        [
            "The influence of each point can be visualized by the criterion keyword argument. Options are Cook\u2019s distance and DFFITS, two measures of influence.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "fig = sm.graphics.influence_plot(prestige_model, criterion=\"cooks\")\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_12_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_12_0.png\"/>",
            "code"
        ],
        [
            "As you can see there are a few worrisome observations. Both contractor and reporter have low leverage but a large residual. RR.engineer has small residual and large leverage. Conductor and minister have both high leverage and large residuals, and, therefore, large influence.",
            "markdown"
        ]
    ],
    "Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)": [
        [
            "Since we are doing multivariate regressions, we cannot just look at individual bivariate plots to discern relationships. Instead, we want to look at the relationship of the dependent variable and independent variables conditional on the other independent variables. We can do this through using partial regression plots, otherwise known as added variable plots.",
            "markdown"
        ],
        [
            "In a partial regression plot, to discern the relationship between the response variable and the \\(k\\)-th variable, we compute the residuals by regressing the response variable versus the independent variables excluding \\(X_k\\). We can denote this by \\(X_{\\sim k}\\). We then compute the residuals by regressing \\(X_k\\) on \\(X_{\\sim k}\\). The partial regression plot is the plot of the former versus the latter residuals.",
            "markdown"
        ],
        [
            "The notable points of this plot are that the fitted line has slope \\(\\beta_k\\) and intercept zero. The residuals of this plot are the same as those of the least squares fit of the original model with full \\(X\\). You can discern the effects of the individual data values on the estimation of a coefficient easily. If obs_labels is True, then these points are annotated with their observation label. You can also see the violation of underlying assumptions such as homoskedasticity and\nlinearity.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "fig = sm.graphics.plot_partregress(\n    \"prestige\", \"income\", [\"income\", \"education\"], data=prestige\n)\nfig.tight_layout(pad=1.0)",
            "code"
        ],
        [
            "eval_env: 1\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_16_1.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_16_1.png\"/>",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "fig = sm.graphics.plot_partregress(\"prestige\", \"income\", [\"education\"], data=prestige)\nfig.tight_layout(pad=1.0)",
            "code"
        ],
        [
            "eval_env: 1\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_17_1.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_17_1.png\"/>",
            "code"
        ],
        [
            "As you can see the partial regression plot confirms the influence of conductor, minister, and RR.engineer on the partial relationship between income and prestige. The cases greatly decrease the effect of income on prestige. Dropping these cases confirms this.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "subset = ~prestige.index.isin([\"conductor\", \"RR.engineer\", \"minister\"])\nprestige_model2 = ols(\n    \"prestige ~ income + education\", data=prestige, subset=subset\n).fit()\nprint(prestige_model2.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:               prestige   R-squared:                       0.876\nModel:                            OLS   Adj. R-squared:                  0.870\nMethod:                 Least Squares   F-statistic:                     138.1\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           2.02e-18\nTime:                        17:07:10   Log-Likelihood:                -160.59\nNo. Observations:                  42   AIC:                             327.2\nDf Residuals:                      39   BIC:                             332.4\nDf Model:                           2\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -6.3174      3.680     -1.717      0.094     -13.760       1.125\nincome         0.9307      0.154      6.053      0.000       0.620       1.242\neducation      0.2846      0.121      2.345      0.024       0.039       0.530\n==============================================================================\nOmnibus:                        3.811   Durbin-Watson:                   1.468\nProb(Omnibus):                  0.149   Jarque-Bera (JB):                2.802\nSkew:                          -0.614   Prob(JB):                        0.246\nKurtosis:                       3.303   Cond. No.                         158.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "For a quick check of all the regressors, you can use plot_partregress_grid. These plots will not label the points, but you can use them to identify problems and then use plot_partregress to get more information.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "fig = sm.graphics.plot_partregress_grid(prestige_model)\nfig.tight_layout(pad=1.0)",
            "code"
        ],
        [
            "eval_env: 1\neval_env: 1\neval_env: 1\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_21_1.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_21_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Component-Component plus Residual (CCPR) Plots": [
        [
            "The CCPR plot provides a way to judge the effect of one regressor on the response variable by taking into account the effects of the other independent variables. The partial residuals plot is defined as \\(\\text{Residuals} + B_iX_i \\text{ }\\text{ }\\) versus \\(X_i\\). The component adds \\(B_iX_i\\) versus \\(X_i\\) to show where the fitted line would lie. Care should be taken if \\(X_i\\) is highly correlated with any of the other independent variables. If this is the case, the\nvariance evident in the plot will be an underestimate of the true variance.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "fig = sm.graphics.plot_ccpr(prestige_model, \"education\")\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_24_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_24_0.png\"/>",
            "code"
        ],
        [
            "As you can see the relationship between the variation in prestige explained by education conditional on income seems to be linear, though you can see there are some observations that are exerting considerable influence on the relationship. We can quickly look at more than one variable by using plot_ccpr_grid.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "fig = sm.graphics.plot_ccpr_grid(prestige_model)\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_26_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_26_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Single Variable Regression Diagnostics": [
        [
            "The plot_regress_exog function is a convenience function that gives a 2x2 plot containing the dependent variable and fitted values with confidence intervals vs.\u00a0the independent variable chosen, the residuals of the model vs.\u00a0the chosen independent variable, a partial regression plot, and a CCPR plot. This function can be used for quickly checking modeling assumptions with respect to a single regressor.",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "fig = sm.graphics.plot_regress_exog(prestige_model, \"education\")\nfig.tight_layout(pad=1.0)",
            "code"
        ],
        [
            "eval_env: 1\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_29_1.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_29_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Fit Plot": [
        [
            "The plot_fit function plots the fitted values versus a chosen independent variable. It includes prediction confidence intervals and optionally plots the true dependent variable.",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "fig = sm.graphics.plot_fit(prestige_model, \"education\")\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_32_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_32_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset": [
        [
            "Compare the following to",
            "markdown"
        ],
        [
            "Though the data here is not the same as in that example. You could run that example by uncommenting the necessary cells below.",
            "markdown"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "# dta = pd.read_csv(\"http://www.stat.ufl.edu/~aa/social/csv_files/statewide-crime-2.csv\")\n# dta = dta.set_index(\"State\", inplace=True).dropna()\n# dta.rename(columns={\"VR\" : \"crime\",\n#                    \"MR\" : \"murder\",\n#                    \"M\"  : \"pctmetro\",\n#                    \"W\"  : \"pctwhite\",\n#                    \"H\"  : \"pcths\",\n#                    \"P\"  : \"poverty\",\n#                    \"S\"  : \"single\"\n#                    }, inplace=True)\n#\n# crime_model = ols(\"murder ~ pctmetro + poverty + pcths + single\", data=dta).fit()",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "dta = sm.datasets.statecrime.load_pandas().data",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "crime_model = ols(\"murder ~ urban + poverty + hs_grad + single\", data=dta).fit()\nprint(crime_model.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                 murder   R-squared:                       0.813\nModel:                            OLS   Adj. R-squared:                  0.797\nMethod:                 Least Squares   F-statistic:                     50.08\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           3.42e-16\nTime:                        17:07:12   Log-Likelihood:                -95.050\nNo. Observations:                  51   AIC:                             200.1\nDf Residuals:                      46   BIC:                             209.8\nDf Model:                           4\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -44.1024     12.086     -3.649      0.001     -68.430     -19.774\nurban          0.0109      0.015      0.707      0.483      -0.020       0.042\npoverty        0.4121      0.140      2.939      0.005       0.130       0.694\nhs_grad        0.3059      0.117      2.611      0.012       0.070       0.542\nsingle         0.6374      0.070      9.065      0.000       0.496       0.779\n==============================================================================\nOmnibus:                        1.618   Durbin-Watson:                   2.507\nProb(Omnibus):                  0.445   Jarque-Bera (JB):                0.831\nSkew:                          -0.220   Prob(JB):                        0.660\nKurtosis:                       3.445   Cond. No.                     5.80e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.8e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
            "code"
        ]
    ],
    "Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)": [
        [
            "[19]:",
            "code"
        ],
        [
            "fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "code"
        ],
        [
            "eval_env: 1\neval_env: 1\neval_env: 1\neval_env: 1\neval_env: 1\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_39_1.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_39_1.png\"/>",
            "code"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "fig = sm.graphics.plot_partregress(\n    \"murder\", \"hs_grad\", [\"urban\", \"poverty\", \"single\"], data=dta\n)\nfig.tight_layout(pad=1.0)",
            "code"
        ],
        [
            "eval_env: 1\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_40_1.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_40_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Leverage-Resid2 Plot": [
        [
            "Closely related to the influence_plot is the leverage-resid2 plot.",
            "markdown"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "fig = sm.graphics.plot_leverage_resid2(crime_model)\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_43_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_43_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Influence Plot": [
        [
            "[22]:",
            "code"
        ],
        [
            "fig = sm.graphics.influence_plot(crime_model)\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_45_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_45_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers.": [
        [
            "Part of the problem here in recreating the Stata results is that M-estimators are not robust to leverage points. MM-estimators should do better with this examples.",
            "markdown"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "from statsmodels.formula.api import rlm",
            "code"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "rob_crime_model = rlm(\n    \"murder ~ urban + poverty + hs_grad + single\",\n    data=dta,\n    M=sm.robust.norms.TukeyBiweight(3),\n).fit(conv=\"weights\")\nprint(rob_crime_model.summary())",
            "code"
        ],
        [
            "Robust linear Model Regression Results\n==============================================================================\nDep. Variable:                 murder   No. Observations:                   51\nModel:                            RLM   Df Residuals:                       46\nMethod:                          IRLS   Df Model:                            4\nNorm:                   TukeyBiweight\nScale Est.:                       mad\nCov Type:                          H1\nDate:                Wed, 02 Nov 2022\nTime:                        17:07:14\nNo. Iterations:                    50\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -4.2986      9.494     -0.453      0.651     -22.907      14.310\nurban          0.0029      0.012      0.241      0.809      -0.021       0.027\npoverty        0.2753      0.110      2.499      0.012       0.059       0.491\nhs_grad       -0.0302      0.092     -0.328      0.743      -0.211       0.150\nsingle         0.2902      0.055      5.253      0.000       0.182       0.398\n==============================================================================\n\nIf the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .",
            "code"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "# rob_crime_model = rlm(\"murder ~ pctmetro + poverty + pcths + single\", data=dta, M=sm.robust.norms.TukeyBiweight()).fit(conv=\"weights\")\n# print(rob_crime_model.summary())",
            "code"
        ],
        [
            "There is not yet an influence diagnostics method as part of RLM, but we can recreate them. (This depends on the status of )",
            "markdown"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "weights = rob_crime_model.weights\nidx = weights  0\nX = rob_crime_model.model.exog[idx.values]\nww = weights[idx] / weights[idx].mean()\nhat_matrix_diag = ww * (X * np.linalg.pinv(X).T).sum(1)\nresid = rob_crime_model.resid\nresid2 = resid ** 2\nresid2 /= resid2.sum()\nnobs = int(idx.sum())\nhm = hat_matrix_diag.mean()\nrm = resid2.mean()",
            "code"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "from statsmodels.graphics import utils\n\nfig, ax = plt.subplots(figsize=(16, 8))\nax.plot(resid2[idx], hat_matrix_diag, \"o\")\nax = utils.annotate_axes(\n    range(nobs),\n    labels=rob_crime_model.model.data.row_labels[idx],\n    points=lzip(resid2[idx], hat_matrix_diag),\n    offset_points=[(-5, 5)] * nobs,\n    size=\"large\",\n    ax=ax,\n)\nax.set_xlabel(\"resid2\")\nax.set_ylabel(\"leverage\")\nylim = ax.get_ylim()\nax.vlines(rm, *ylim)\nxlim = ax.get_xlim()\nax.hlines(hm, *xlim)\nax.margins(0, 0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_53_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_53_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Plotting->Categorical Interactions": [
        [
            "In this example, we will visualize the interaction between categorical factors. First, we will create some categorical data. Then, we will plot it using the interaction_plot function, which internally re-codes the x-factor categories to integers.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom statsmodels.graphics.factorplots import interaction_plot",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "np.random.seed(12345)\nweight = pd.Series(np.repeat([\"low\", \"hi\", \"low\", \"hi\"], 15), name=\"weight\")\nnutrition = pd.Series(np.repeat([\"lo_carb\", \"hi_carb\"], 30), name=\"nutrition\")\ndays = np.log(np.random.randint(1, 30, size=60))",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(6, 6))\nfig = interaction_plot(\n    x=weight,\n    trace=nutrition,\n    response=days,\n    colors=[\"red\", \"blue\"],\n    markers=[\"D\", \"^\"],\n    ms=10,\n    ax=ax,\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_categorical_interaction_plot_4_0.png\" src=\"../../../_images/examples_notebooks_generated_categorical_interaction_plot_4_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Plotting->Box Plots": [
        [
            "The following illustrates some options for the boxplot in statsmodels. These include violin_plot and bean_plot.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm",
            "code"
        ]
    ],
    "Examples->Plotting->Box Plots->Bean Plots": [
        [
            "The following example is taken from the docstring of beanplot.",
            "markdown"
        ],
        [
            "We use the American National Election Survey 1996 dataset, which has Party Identification of respondents as independent variable and (among other data) age as dependent variable.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "data = sm.datasets.anes96.load_pandas()\nparty_ID = np.arange(7)\nlabels = [\n    \"Strong Democrat\",\n    \"Weak Democrat\",\n    \"Independent-Democrat\",\n    \"Independent-Independent\",\n    \"Independent-Republican\",\n    \"Weak Republican\",\n    \"Strong Republican\",\n]",
            "code"
        ],
        [
            "Group age by party ID, and create a violin plot with it:",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "plt.rcParams[\"figure.subplot.bottom\"] = 0.23  # keep labels visible\nplt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # make plot larger in notebook\nage = [data.exog[\"age\"][data.endog == id] for id in party_ID]\nfig = plt.figure()\nax = fig.add_subplot(111)\nplot_opts = {\n    \"cutoff_val\": 5,\n    \"cutoff_type\": \"abs\",\n    \"label_fontsize\": \"small\",\n    \"label_rotation\": 30,\n}\nsm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\n# plt.show()",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "Text(0, 0.5, 'Age')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_plots_boxplots_7_1.png\" src=\"../../../_images/examples_notebooks_generated_plots_boxplots_7_1.png\"/>",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "def beanplot(data, plot_opts={}, jitter=False):\n    \"\"\"helper function to try out different plot options\"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plot_opts_ = {\n        \"cutoff_val\": 5,\n        \"cutoff_type\": \"abs\",\n        \"label_fontsize\": \"small\",\n        \"label_rotation\": 30,\n    }\n    plot_opts_.update(plot_opts)\n    sm.graphics.beanplot(\n        data, ax=ax, labels=labels, jitter=jitter, plot_opts=plot_opts_\n    )\n    ax.set_xlabel(\"Party identification of respondent.\")\n    ax.set_ylabel(\"Age\")",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "fig = beanplot(age, jitter=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_plots_boxplots_9_0.png\" src=\"../../../_images/examples_notebooks_generated_plots_boxplots_9_0.png\"/>",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "fig = beanplot(age, plot_opts={\"violin_width\": 0.5, \"violin_fc\": \"#66c2a5\"})\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_plots_boxplots_10_0.png\" src=\"../../../_images/examples_notebooks_generated_plots_boxplots_10_0.png\"/>",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "fig = beanplot(age, plot_opts={\"violin_fc\": \"#66c2a5\"})\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_plots_boxplots_11_0.png\" src=\"../../../_images/examples_notebooks_generated_plots_boxplots_11_0.png\"/>",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "fig = beanplot(\n    age, plot_opts={\"bean_size\": 0.2, \"violin_width\": 0.75, \"violin_fc\": \"#66c2a5\"}\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_plots_boxplots_12_0.png\" src=\"../../../_images/examples_notebooks_generated_plots_boxplots_12_0.png\"/>",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "fig = beanplot(age, jitter=True, plot_opts={\"violin_fc\": \"#66c2a5\"})\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_plots_boxplots_13_0.png\" src=\"../../../_images/examples_notebooks_generated_plots_boxplots_13_0.png\"/>",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "fig = beanplot(\n    age, jitter=True, plot_opts={\"violin_width\": 0.5, \"violin_fc\": \"#66c2a5\"}\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_plots_boxplots_14_0.png\" src=\"../../../_images/examples_notebooks_generated_plots_boxplots_14_0.png\"/>",
            "code"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "",
            "code"
        ]
    ],
    "Examples->Plotting->Box Plots->Advanced Box Plots": [
        [
            "Based of example script example_enhanced_boxplots.py (by Ralf Gommers)",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\n\n\n# Necessary to make horizontal axis labels fit\nplt.rcParams[\"figure.subplot.bottom\"] = 0.23\n\ndata = sm.datasets.anes96.load_pandas()\nparty_ID = np.arange(7)\nlabels = [\n    \"Strong Democrat\",\n    \"Weak Democrat\",\n    \"Independent-Democrat\",\n    \"Independent-Independent\",\n    \"Independent-Republican\",\n    \"Weak Republican\",\n    \"Strong Republican\",\n]\n\n# Group age by party ID.\nage = [data.exog[\"age\"][data.endog == id] for id in party_ID]",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "# Create a violin plot.\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nsm.graphics.violinplot(\n    age,\n    ax=ax,\n    labels=labels,\n    plot_opts={\n        \"cutoff_val\": 5,\n        \"cutoff_type\": \"abs\",\n        \"label_fontsize\": \"small\",\n        \"label_rotation\": 30,\n    },\n)\n\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\nax.set_title(\"US national election '96 - Age & Party Identification\")",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "Text(0.5, 1.0, \"US national election '96 - Age & Party Identification\")\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_plots_boxplots_19_1.png\" src=\"../../../_images/examples_notebooks_generated_plots_boxplots_19_1.png\"/>",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "# Create a bean plot.\nfig2 = plt.figure()\nax = fig2.add_subplot(111)\n\nsm.graphics.beanplot(\n    age,\n    ax=ax,\n    labels=labels,\n    plot_opts={\n        \"cutoff_val\": 5,\n        \"cutoff_type\": \"abs\",\n        \"label_fontsize\": \"small\",\n        \"label_rotation\": 30,\n    },\n)\n\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\nax.set_title(\"US national election '96 - Age & Party Identification\")",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "Text(0.5, 1.0, \"US national election '96 - Age & Party Identification\")\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_plots_boxplots_20_1.png\" src=\"../../../_images/examples_notebooks_generated_plots_boxplots_20_1.png\"/>",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "# Create a jitter plot.\nfig3 = plt.figure()\nax = fig3.add_subplot(111)\n\nplot_opts = {\n    \"cutoff_val\": 5,\n    \"cutoff_type\": \"abs\",\n    \"label_fontsize\": \"small\",\n    \"label_rotation\": 30,\n    \"violin_fc\": (0.8, 0.8, 0.8),\n    \"jitter_marker\": \".\",\n    \"jitter_marker_size\": 3,\n    \"bean_color\": \"#FF6F00\",\n    \"bean_mean_color\": \"#009D91\",\n}\nsm.graphics.beanplot(age, ax=ax, labels=labels, jitter=True, plot_opts=plot_opts)\n\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\nax.set_title(\"US national election '96 - Age & Party Identification\")",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "Text(0.5, 1.0, \"US national election '96 - Age & Party Identification\")\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_plots_boxplots_21_1.png\" src=\"../../../_images/examples_notebooks_generated_plots_boxplots_21_1.png\"/>",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "# Create an asymmetrical jitter plot.\nix = data.exog[\"income\"] &lt; 16  # incomes &lt; $30k\nage = data.exog[\"age\"][ix]\nendog = data.endog[ix]\nage_lower_income = [age[endog == id] for id in party_ID]\n\nix = data.exog[\"income\"] = 20  # incomes  $50k\nage = data.exog[\"age\"][ix]\nendog = data.endog[ix]\nage_higher_income = [age[endog == id] for id in party_ID]\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nplot_opts[\"violin_fc\"] = (0.5, 0.5, 0.5)\nplot_opts[\"bean_show_mean\"] = False\nplot_opts[\"bean_show_median\"] = False\nplot_opts[\"bean_legend_text\"] = \"Income &lt; \\$30k\"\nplot_opts[\"cutoff_val\"] = 10\nsm.graphics.beanplot(\n    age_lower_income,\n    ax=ax,\n    labels=labels,\n    side=\"left\",\n    jitter=True,\n    plot_opts=plot_opts,\n)\nplot_opts[\"violin_fc\"] = (0.7, 0.7, 0.7)\nplot_opts[\"bean_color\"] = \"#009D91\"\nplot_opts[\"bean_legend_text\"] = \"Income  \\$50k\"\nsm.graphics.beanplot(\n    age_higher_income,\n    ax=ax,\n    labels=labels,\n    side=\"right\",\n    jitter=True,\n    plot_opts=plot_opts,\n)\n\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\nax.set_title(\"US national election '96 - Age & Party Identification\")\n\n\n# Show all plots.\n# plt.show()",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "Text(0.5, 1.0, \"US national election '96 - Age & Party Identification\")\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_plots_boxplots_22_1.png\" src=\"../../../_images/examples_notebooks_generated_plots_boxplots_22_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Getting Started": [
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np\nimport statsmodels.api as sm",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Getting Started->Data": [
        [
            "Load data from Spector and Mazzeo (1980). Examples follow Greene\u2019s Econometric Analysis Ch. 21 (5th Edition).",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "code"
        ],
        [
            "Inspect the data:",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "print(spector_data.exog.head())\nprint(spector_data.endog.head())",
            "code"
        ],
        [
            "GPA  TUCE  PSI  const\n0  2.66  20.0  0.0    1.0\n1  2.89  22.0  0.0    1.0\n2  3.28  24.0  0.0    1.0\n3  2.92  12.0  0.0    1.0\n4  4.00  21.0  0.0    1.0\n0    0.0\n1    0.0\n2    0.0\n3    0.0\n4    1.0\nName: GRADE, dtype: float64",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Getting Started->Linear Probability Model (OLS)": [
        [
            "[4]:",
            "code"
        ],
        [
            "lpm_mod = sm.OLS(spector_data.endog, spector_data.exog)\nlpm_res = lpm_mod.fit()\nprint(\"Parameters: \", lpm_res.params[:-1])",
            "code"
        ],
        [
            "Parameters:  GPA     0.463852\nTUCE    0.010495\nPSI     0.378555\ndtype: float64",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Getting Started->Logit Model": [
        [
            "[5]:",
            "code"
        ],
        [
            "logit_mod = sm.Logit(spector_data.endog, spector_data.exog)\nlogit_res = logit_mod.fit(disp=0)\nprint(\"Parameters: \", logit_res.params)",
            "code"
        ],
        [
            "Parameters:  GPA       2.826113\nTUCE      0.095158\nPSI       2.378688\nconst   -13.021347\ndtype: float64",
            "code"
        ],
        [
            "Marginal Effects",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "margeff = logit_res.get_margeff()\nprint(margeff.summary())",
            "code"
        ],
        [
            "Logit Marginal Effects\n=====================================\nDep. Variable:                  GRADE\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nGPA            0.3626      0.109      3.313      0.001       0.148       0.577\nTUCE           0.0122      0.018      0.686      0.493      -0.023       0.047\nPSI            0.3052      0.092      3.304      0.001       0.124       0.486\n==============================================================================",
            "code"
        ],
        [
            "As in all the discrete data models presented below, we can print a nice summary of results:",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "print(logit_res.summary())",
            "code"
        ],
        [
            "Logit Regression Results\n==============================================================================\nDep. Variable:                  GRADE   No. Observations:                   32\nModel:                          Logit   Df Residuals:                       28\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 02 Nov 2022   Pseudo R-squ.:                  0.3740\nTime:                        17:10:42   Log-Likelihood:                -12.890\nconverged:                       True   LL-Null:                       -20.592\nCovariance Type:            nonrobust   LLR p-value:                  0.001502\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nGPA            2.8261      1.263      2.238      0.025       0.351       5.301\nTUCE           0.0952      0.142      0.672      0.501      -0.182       0.373\nPSI            2.3787      1.065      2.234      0.025       0.292       4.465\nconst        -13.0213      4.931     -2.641      0.008     -22.687      -3.356\n==============================================================================",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Getting Started->Probit Model": [
        [
            "[8]:",
            "code"
        ],
        [
            "probit_mod = sm.Probit(spector_data.endog, spector_data.exog)\nprobit_res = probit_mod.fit()\nprobit_margeff = probit_res.get_margeff()\nprint(\"Parameters: \", probit_res.params)\nprint(\"Marginal effects: \")\nprint(probit_margeff.summary())",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.400588\n         Iterations 6\nParameters:  GPA      1.625810\nTUCE     0.051729\nPSI      1.426332\nconst   -7.452320\ndtype: float64\nMarginal effects:\n       Probit Marginal Effects\n=====================================\nDep. Variable:                  GRADE\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nGPA            0.3608      0.113      3.182      0.001       0.139       0.583\nTUCE           0.0115      0.018      0.624      0.533      -0.025       0.048\nPSI            0.3165      0.090      3.508      0.000       0.140       0.493\n==============================================================================",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Getting Started->Multinomial Logit": [
        [
            "Load data from the American National Election Studies:",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "anes_data = sm.datasets.anes96.load()\nanes_exog = anes_data.exog\nanes_exog = sm.add_constant(anes_exog)",
            "code"
        ],
        [
            "Inspect the data:",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "print(anes_data.exog.head())\nprint(anes_data.endog.head())",
            "code"
        ],
        [
            "logpopul  selfLR   age  educ  income\n0 -2.302585     7.0  36.0   3.0     1.0\n1  5.247550     3.0  20.0   4.0     1.0\n2  3.437208     2.0  24.0   6.0     1.0\n3  4.420045     3.0  28.0   6.0     1.0\n4  6.461624     5.0  68.0   6.0     1.0\n0    6.0\n1    1.0\n2    1.0\n3    1.0\n4    0.0\nName: PID, dtype: float64",
            "code"
        ],
        [
            "Fit MNL model:",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 1.548647\n         Iterations 7\n                 0         1         2         3         4          5\nconst    -0.373402 -2.250913 -3.665584 -7.613843 -7.060478 -12.105751\nlogpopul -0.011536 -0.088751 -0.105967 -0.091557 -0.093285  -0.140881\nselfLR    0.297714  0.391669  0.573451  1.278772  1.346962   2.070080\nage      -0.024945 -0.022898 -0.014851 -0.008681 -0.017904  -0.009433\neduc      0.082491  0.181043 -0.007152  0.199828  0.216939   0.321926\nincome    0.005197  0.047874  0.057575  0.084498  0.080958   0.108894",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Getting Started->Poisson": [
        [
            "Load the Rand data. Note that this example is similar to Cameron and Trivedi\u2019s Microeconometrics Table 20.5, but it is slightly different because of minor changes in the data.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "code"
        ],
        [
            "Fit Poisson model:",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "poisson_mod = sm.Poisson(rand_data.endog, rand_exog)\npoisson_res = poisson_mod.fit(method=\"newton\")\nprint(poisson_res.summary())",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 3.091609\n         Iterations 6\n                          Poisson Regression Results\n==============================================================================\nDep. Variable:                  mdvis   No. Observations:                20190\nModel:                        Poisson   Df Residuals:                    20180\nMethod:                           MLE   Df Model:                            9\nDate:                Wed, 02 Nov 2022   Pseudo R-squ.:                 0.06343\nTime:                        17:10:43   Log-Likelihood:                -62420.\nconverged:                       True   LL-Null:                       -66647.\nCovariance Type:            nonrobust   LLR p-value:                     0.000\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nlncoins       -0.0525      0.003    -18.216      0.000      -0.058      -0.047\nidp           -0.2471      0.011    -23.272      0.000      -0.268      -0.226\nlpi            0.0353      0.002     19.302      0.000       0.032       0.039\nfmde          -0.0346      0.002    -21.439      0.000      -0.038      -0.031\nphyslm         0.2717      0.012     22.200      0.000       0.248       0.296\ndisea          0.0339      0.001     60.098      0.000       0.033       0.035\nhlthg         -0.0126      0.009     -1.366      0.172      -0.031       0.005\nhlthf          0.0541      0.015      3.531      0.000       0.024       0.084\nhlthp          0.2061      0.026      7.843      0.000       0.155       0.258\nconst          0.7004      0.011     62.741      0.000       0.678       0.722\n==============================================================================",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Getting Started->Negative Binomial": [
        [
            "The negative binomial model gives slightly different results.",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "mod_nbin = sm.NegativeBinomial(rand_data.endog, rand_exog)\nres_nbin = mod_nbin.fit(disp=False)\nprint(res_nbin.summary())",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"",
            "code"
        ],
        [
            "NegativeBinomial Regression Results\n==============================================================================\nDep. Variable:                  mdvis   No. Observations:                20190\nModel:               NegativeBinomial   Df Residuals:                    20180\nMethod:                           MLE   Df Model:                            9\nDate:                Wed, 02 Nov 2022   Pseudo R-squ.:                 0.01845\nTime:                        17:10:43   Log-Likelihood:                -43384.\nconverged:                      False   LL-Null:                       -44199.\nCovariance Type:            nonrobust   LLR p-value:                     0.000\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nlncoins       -0.0579      0.006     -9.515      0.000      -0.070      -0.046\nidp           -0.2678      0.023    -11.802      0.000      -0.312      -0.223\nlpi            0.0412      0.004      9.938      0.000       0.033       0.049\nfmde          -0.0381      0.003    -11.216      0.000      -0.045      -0.031\nphyslm         0.2691      0.030      8.985      0.000       0.210       0.328\ndisea          0.0382      0.001     26.080      0.000       0.035       0.041\nhlthg         -0.0441      0.020     -2.201      0.028      -0.083      -0.005\nhlthf          0.0173      0.036      0.478      0.632      -0.054       0.088\nhlthp          0.1782      0.074      2.399      0.016       0.033       0.324\nconst          0.6635      0.025     26.786      0.000       0.615       0.712\nalpha          1.2930      0.019     69.477      0.000       1.256       1.329\n==============================================================================",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Getting Started->Alternative solvers": [
        [
            "The default method for fitting discrete data MLE models is Newton-Raphson. You can use other solvers by using the method argument:",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "mlogit_res = mlogit_mod.fit(method=\"bfgs\", maxiter=250)\nprint(mlogit_res.summary())",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 1.548647\n         Iterations: 111\n         Function evaluations: 117\n         Gradient evaluations: 117\n                          MNLogit Regression Results\n==============================================================================\nDep. Variable:                    PID   No. Observations:                  944\nModel:                        MNLogit   Df Residuals:                      908\nMethod:                           MLE   Df Model:                           30\nDate:                Wed, 02 Nov 2022   Pseudo R-squ.:                  0.1648\nTime:                        17:10:44   Log-Likelihood:                -1461.9\nconverged:                       True   LL-Null:                       -1750.3\nCovariance Type:            nonrobust   LLR p-value:                1.822e-102\n==============================================================================\n     PID=1       coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.3734      0.630     -0.593      0.553      -1.608       0.861\nlogpopul      -0.0115      0.034     -0.337      0.736      -0.079       0.056\nselfLR         0.2977      0.094      3.180      0.001       0.114       0.481\nage           -0.0249      0.007     -3.823      0.000      -0.038      -0.012\neduc           0.0825      0.074      1.121      0.262      -0.062       0.227\nincome         0.0052      0.018      0.295      0.768      -0.029       0.040\n------------------------------------------------------------------------------\n     PID=2       coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.2509      0.763     -2.949      0.003      -3.747      -0.755\nlogpopul      -0.0888      0.039     -2.266      0.023      -0.166      -0.012\nselfLR         0.3917      0.108      3.619      0.000       0.180       0.604\nage           -0.0229      0.008     -2.893      0.004      -0.038      -0.007\neduc           0.1810      0.085      2.123      0.034       0.014       0.348\nincome         0.0479      0.022      2.149      0.032       0.004       0.092\n------------------------------------------------------------------------------\n     PID=3       coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -3.6656      1.157     -3.169      0.002      -5.932      -1.399\nlogpopul      -0.1060      0.057     -1.858      0.063      -0.218       0.006\nselfLR         0.5734      0.159      3.617      0.000       0.263       0.884\nage           -0.0149      0.011     -1.311      0.190      -0.037       0.007\neduc          -0.0072      0.126     -0.057      0.955      -0.255       0.240\nincome         0.0576      0.034      1.713      0.087      -0.008       0.123\n------------------------------------------------------------------------------\n     PID=4       coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -7.6139      0.958     -7.951      0.000      -9.491      -5.737\nlogpopul      -0.0916      0.044     -2.091      0.037      -0.177      -0.006\nselfLR         1.2788      0.129      9.921      0.000       1.026       1.531\nage           -0.0087      0.008     -1.031      0.302      -0.025       0.008\neduc           0.1998      0.094      2.123      0.034       0.015       0.384\nincome         0.0845      0.026      3.226      0.001       0.033       0.136\n------------------------------------------------------------------------------\n     PID=5       coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -7.0605      0.844     -8.362      0.000      -8.715      -5.406\nlogpopul      -0.0933      0.039     -2.371      0.018      -0.170      -0.016\nselfLR         1.3470      0.117     11.494      0.000       1.117       1.577\nage           -0.0179      0.008     -2.352      0.019      -0.033      -0.003\neduc           0.2169      0.085      2.552      0.011       0.050       0.384\nincome         0.0810      0.023      3.524      0.000       0.036       0.126\n------------------------------------------------------------------------------\n     PID=6       coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -12.1058      1.060    -11.421      0.000     -14.183     -10.028\nlogpopul      -0.1409      0.042     -3.343      0.001      -0.223      -0.058\nselfLR         2.0701      0.143     14.435      0.000       1.789       2.351\nage           -0.0094      0.008     -1.160      0.246      -0.025       0.007\neduc           0.3219      0.091      3.534      0.000       0.143       0.500\nincome         0.1089      0.025      4.304      0.000       0.059       0.158\n==============================================================================",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data": [
        [
            "A survey of women only was conducted in 1974 by Redbook asking about extramarital affairs.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "print(sm.datasets.fair.SOURCE)",
            "code"
        ],
        [
            "Fair, Ray. 1978. \"A Theory of Extramarital Affairs,\" `Journal of Political\nEconomy`, February, 45-61.\n\nThe data is available at http://fairmodel.econ.yale.edu/rayfair/pdf/2011b.htm",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "print(sm.datasets.fair.NOTE)",
            "code"
        ],
        [
            "::\n\n    Number of observations: 6366\n    Number of variables: 9\n    Variable name definitions:\n\n        rate_marriage   : How rate marriage, 1 = very poor, 2 = poor, 3 = fair,\n                        4 = good, 5 = very good\n        age             : Age\n        yrs_married     : No. years married. Interval approximations. See\n                        original paper for detailed explanation.\n        children        : No. children\n        religious       : How relgious, 1 = not, 2 = mildly, 3 = fairly,\n                        4 = strongly\n        educ            : Level of education, 9 = grade school, 12 = high\n                        school, 14 = some college, 16 = college graduate,\n                        17 = some graduate school, 20 = advanced degree\n        occupation      : 1 = student, 2 = farming, agriculture; semi-skilled,\n                        or unskilled worker; 3 = white-colloar; 4 = teacher\n                        counselor social worker, nurse; artist, writers;\n                        technician, skilled worker, 5 = managerial,\n                        administrative, business, 6 = professional with\n                        advanced degree\n        occupation_husb : Husband's occupation. Same as occupation.\n        affairs         : measure of time spent in extramarital affairs\n\n    See the original paper for more details.",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "dta = sm.datasets.fair.load_pandas().data",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "dta[\"affair\"] = (dta[\"affairs\"]  0).astype(float)\nprint(dta.head(10))",
            "code"
        ],
        [
            "rate_marriage   age  yrs_married  children  religious  educ  occupation  \\\n0            3.0  32.0          9.0       3.0        3.0  17.0         2.0\n1            3.0  27.0         13.0       3.0        1.0  14.0         3.0\n2            4.0  22.0          2.5       0.0        1.0  16.0         3.0\n3            4.0  37.0         16.5       4.0        3.0  16.0         5.0\n4            5.0  27.0          9.0       1.0        1.0  14.0         3.0\n5            4.0  27.0          9.0       0.0        2.0  14.0         3.0\n6            5.0  37.0         23.0       5.5        2.0  12.0         5.0\n7            5.0  37.0         23.0       5.5        2.0  12.0         2.0\n8            3.0  22.0          2.5       0.0        2.0  12.0         3.0\n9            3.0  27.0          6.0       0.0        1.0  16.0         3.0\n\n   occupation_husb   affairs  affair\n0              5.0  0.111111     1.0\n1              4.0  3.230769     1.0\n2              5.0  1.400000     1.0\n3              5.0  0.727273     1.0\n4              4.0  4.666666     1.0\n5              4.0  4.666666     1.0\n6              4.0  0.852174     1.0\n7              3.0  1.826086     1.0\n8              3.0  4.799999     1.0\n9              5.0  1.333333     1.0",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "print(dta.describe())",
            "code"
        ],
        [
            "rate_marriage          age  yrs_married     children    religious  \\\ncount    6366.000000  6366.000000  6366.000000  6366.000000  6366.000000\nmean        4.109645    29.082862     9.009425     1.396874     2.426170\nstd         0.961430     6.847882     7.280120     1.433471     0.878369\nmin         1.000000    17.500000     0.500000     0.000000     1.000000\n25%         4.000000    22.000000     2.500000     0.000000     2.000000\n50%         4.000000    27.000000     6.000000     1.000000     2.000000\n75%         5.000000    32.000000    16.500000     2.000000     3.000000\nmax         5.000000    42.000000    23.000000     5.500000     4.000000\n\n              educ   occupation  occupation_husb      affairs       affair\ncount  6366.000000  6366.000000      6366.000000  6366.000000  6366.000000\nmean     14.209865     3.424128         3.850141     0.705374     0.322495\nstd       2.178003     0.942399         1.346435     2.203374     0.467468\nmin       9.000000     1.000000         1.000000     0.000000     0.000000\n25%      12.000000     3.000000         3.000000     0.000000     0.000000\n50%      14.000000     3.000000         4.000000     0.000000     0.000000\n75%      16.000000     4.000000         5.000000     0.484848     1.000000\nmax      20.000000     6.000000         6.000000    57.599991     1.000000",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "affair_mod = logit(\n    \"affair ~ occupation + educ + occupation_husb\"\n    \"+ rate_marriage + age + yrs_married + children\"\n    \" + religious\",\n    dta,\n).fit()",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.545314\n         Iterations 6",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "print(affair_mod.summary())",
            "code"
        ],
        [
            "Logit Regression Results\n==============================================================================\nDep. Variable:                 affair   No. Observations:                 6366\nModel:                          Logit   Df Residuals:                     6357\nMethod:                           MLE   Df Model:                            8\nDate:                Wed, 02 Nov 2022   Pseudo R-squ.:                  0.1327\nTime:                        17:10:29   Log-Likelihood:                -3471.5\nconverged:                       True   LL-Null:                       -4002.5\nCovariance Type:            nonrobust   LLR p-value:                5.807e-224\n===================================================================================\n                      coef    std err          z      P|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nIntercept           3.7257      0.299     12.470      0.000       3.140       4.311\noccupation          0.1602      0.034      4.717      0.000       0.094       0.227\neduc               -0.0392      0.015     -2.533      0.011      -0.070      -0.009\noccupation_husb     0.0124      0.023      0.541      0.589      -0.033       0.057\nrate_marriage      -0.7161      0.031    -22.784      0.000      -0.778      -0.655\nage                -0.0605      0.010     -5.885      0.000      -0.081      -0.040\nyrs_married         0.1100      0.011     10.054      0.000       0.089       0.131\nchildren           -0.0042      0.032     -0.134      0.893      -0.066       0.058\nreligious          -0.3752      0.035    -10.792      0.000      -0.443      -0.307\n===================================================================================",
            "code"
        ],
        [
            "How well are we predicting?",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "affair_mod.pred_table()",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "array([[3882.,  431.],\n       [1326.,  727.]])",
            "code"
        ],
        [
            "The coefficients of the discrete choice model do not tell us much. What we\u2019re after is marginal effects.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "mfx = affair_mod.get_margeff()\nprint(mfx.summary())",
            "code"
        ],
        [
            "Logit Marginal Effects\n=====================================\nDep. Variable:                 affair\nMethod:                          dydx\nAt:                           overall\n===================================================================================\n                     dy/dx    std err          z      P|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------\noccupation          0.0293      0.006      4.744      0.000       0.017       0.041\neduc               -0.0072      0.003     -2.538      0.011      -0.013      -0.002\noccupation_husb     0.0023      0.004      0.541      0.589      -0.006       0.010\nrate_marriage      -0.1308      0.005    -26.891      0.000      -0.140      -0.121\nage                -0.0110      0.002     -5.937      0.000      -0.015      -0.007\nyrs_married         0.0201      0.002     10.327      0.000       0.016       0.024\nchildren           -0.0008      0.006     -0.134      0.893      -0.012       0.011\nreligious          -0.0685      0.006    -11.119      0.000      -0.081      -0.056\n===================================================================================",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "respondent1000 = dta.iloc[1000]\nprint(respondent1000)",
            "code"
        ],
        [
            "rate_marriage       4.000000\nage                37.000000\nyrs_married        23.000000\nchildren            3.000000\nreligious           3.000000\neduc               12.000000\noccupation          3.000000\noccupation_husb     4.000000\naffairs             0.521739\naffair              1.000000\nName: 1000, dtype: float64",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "resp = dict(\n    zip(\n        range(1, 9),\n        respondent1000[\n            [\n                \"occupation\",\n                \"educ\",\n                \"occupation_husb\",\n                \"rate_marriage\",\n                \"age\",\n                \"yrs_married\",\n                \"children\",\n                \"religious\",\n            ]\n        ].tolist(),\n    )\n)\nresp.update({0: 1})\nprint(resp)",
            "code"
        ],
        [
            "{1: 3.0, 2: 12.0, 3: 4.0, 4: 4.0, 5: 37.0, 6: 23.0, 7: 3.0, 8: 3.0, 0: 1}",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "mfx = affair_mod.get_margeff(atexog=resp)\nprint(mfx.summary())",
            "code"
        ],
        [
            "Logit Marginal Effects\n=====================================\nDep. Variable:                 affair\nMethod:                          dydx\nAt:                           overall\n===================================================================================\n                     dy/dx    std err          z      P|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------\noccupation          0.0400      0.008      4.711      0.000       0.023       0.057\neduc               -0.0098      0.004     -2.537      0.011      -0.017      -0.002\noccupation_husb     0.0031      0.006      0.541      0.589      -0.008       0.014\nrate_marriage      -0.1788      0.008    -22.743      0.000      -0.194      -0.163\nage                -0.0151      0.003     -5.928      0.000      -0.020      -0.010\nyrs_married         0.0275      0.003     10.256      0.000       0.022       0.033\nchildren           -0.0011      0.008     -0.134      0.893      -0.017       0.014\nreligious          -0.0937      0.009    -10.722      0.000      -0.111      -0.077\n===================================================================================",
            "code"
        ],
        [
            "predict expects a DataFrame since patsy is used to select columns.",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "respondent1000 = dta.iloc[[1000]]\naffair_mod.predict(respondent1000)",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "1000    0.518782\ndtype: float64",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "affair_mod.fittedvalues[1000]",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "0.07516159285055479",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "affair_mod.model.cdf(affair_mod.fittedvalues[1000])",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "0.518781557212145",
            "code"
        ],
        [
            "The \u201ccorrect\u201d model here is likely the Tobit model. We have an work in progress branch \u201ctobit-model\u201d on github, if anyone is interested in censored regression models.",
            "markdown"
        ]
    ],
    "Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Exercise: Logit vs Probit": [
        [
            "[18]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nsupport = np.linspace(-6, 6, 1000)\nax.plot(support, stats.logistic.cdf(support), \"r-\", label=\"Logistic\")\nax.plot(support, stats.norm.cdf(support), label=\"Probit\")\nax.legend()",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f37e66a5ab0\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_discrete_choice_example_25_1.png\" src=\"../../../_images/examples_notebooks_generated_discrete_choice_example_25_1.png\"/>",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nsupport = np.linspace(-6, 6, 1000)\nax.plot(support, stats.logistic.pdf(support), \"r-\", label=\"Logistic\")\nax.plot(support, stats.norm.pdf(support), label=\"Probit\")\nax.legend()",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f37e45e0e20\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_discrete_choice_example_26_1.png\" src=\"../../../_images/examples_notebooks_generated_discrete_choice_example_26_1.png\"/>",
            "code"
        ],
        [
            "Compare the estimates of the Logit Fair model above to a Probit model. Does the prediction table look better? Much difference in marginal effects?",
            "markdown"
        ]
    ],
    "Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example": [
        [
            "[20]:",
            "code"
        ],
        [
            "print(sm.datasets.star98.SOURCE)",
            "code"
        ],
        [
            "Jeff Gill's `Generalized Linear Models: A Unified Approach`\n\nhttp://jgill.wustl.edu/research/books.html",
            "code"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "print(sm.datasets.star98.DESCRLONG)",
            "code"
        ],
        [
            "This data is on the California education policy and outcomes (STAR program\nresults for 1998.  The data measured standardized testing by the California\nDepartment of Education that required evaluation of 2nd - 11th grade students\nby the the Stanford 9 test on a variety of subjects.  This dataset is at\nthe level of the unified school district and consists of 303 cases.  The\nbinary response variable represents the number of 9th graders scoring\nover the national median value on the mathematics exam.\n\nThe data used in this example is only a subset of the original source.",
            "code"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "print(sm.datasets.star98.NOTE)",
            "code"
        ],
        [
            "::\n\n    Number of Observations - 303 (counties in California).\n\n    Number of Variables - 13 and 8 interaction terms.\n\n    Definition of variables names::\n\n        NABOVE   - Total number of students above the national median for the\n                   math section.\n        NBELOW   - Total number of students below the national median for the\n                   math section.\n        LOWINC   - Percentage of low income students\n        PERASIAN - Percentage of Asian student\n        PERBLACK - Percentage of black students\n        PERHISP  - Percentage of Hispanic students\n        PERMINTE - Percentage of minority teachers\n        AVYRSEXP - Sum of teachers' years in educational service divided by the\n                number of teachers.\n        AVSALK   - Total salary budget including benefits divided by the number\n                   of full-time teachers (in thousands)\n        PERSPENK - Per-pupil spending (in thousands)\n        PTRATIO  - Pupil-teacher ratio.\n        PCTAF    - Percentage of students taking UC/CSU prep courses\n        PCTCHRT  - Percentage of charter schools\n        PCTYRRND - Percentage of year-round schools\n\n        The below variables are interaction terms of the variables defined\n        above.\n\n        PERMINTE_AVYRSEXP\n        PEMINTE_AVSAL\n        AVYRSEXP_AVSAL\n        PERSPEN_PTRATIO\n        PERSPEN_PCTAF\n        PTRATIO_PCTAF\n        PERMINTE_AVTRSEXP_AVSAL\n        PERSPEN_PTRATIO_PCTAF",
            "code"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "dta = sm.datasets.star98.load_pandas().data\nprint(dta.columns)",
            "code"
        ],
        [
            "Index(['NABOVE', 'NBELOW', 'LOWINC', 'PERASIAN', 'PERBLACK', 'PERHISP',\n       'PERMINTE', 'AVYRSEXP', 'AVSALK', 'PERSPENK', 'PTRATIO', 'PCTAF',\n       'PCTCHRT', 'PCTYRRND', 'PERMINTE_AVYRSEXP', 'PERMINTE_AVSAL',\n       'AVYRSEXP_AVSAL', 'PERSPEN_PTRATIO', 'PERSPEN_PCTAF', 'PTRATIO_PCTAF',\n       'PERMINTE_AVYRSEXP_AVSAL', 'PERSPEN_PTRATIO_PCTAF'],\n      dtype='object')",
            "code"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "print(\n    dta[\n        [\"NABOVE\", \"NBELOW\", \"LOWINC\", \"PERASIAN\", \"PERBLACK\", \"PERHISP\", \"PERMINTE\"]\n    ].head(10)\n)",
            "code"
        ],
        [
            "NABOVE  NBELOW    LOWINC   PERASIAN   PERBLACK    PERHISP   PERMINTE\n0   452.0   355.0  34.39730  23.299300  14.235280  11.411120  15.918370\n1   144.0    40.0  17.36507  29.328380   8.234897   9.314884  13.636360\n2   337.0   234.0  32.64324   9.226386  42.406310  13.543720  28.834360\n3   395.0   178.0  11.90953  13.883090   3.796973  11.443110  11.111110\n4     8.0    57.0  36.88889  12.187500  76.875000   7.604167  43.589740\n5  1348.0   899.0  20.93149  28.023510   4.643221  13.808160  15.378490\n6   477.0   887.0  53.26898   8.447858  19.374830  37.905330  25.525530\n7   565.0   347.0  15.19009   3.665781   2.649680  13.092070   6.203008\n8   205.0   320.0  28.21582  10.430420   6.786374  32.334300  13.461540\n9   469.0   598.0  32.77897  17.178310  12.484930  28.323290  27.259890",
            "code"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "print(\n    dta[\n        [\"AVYRSEXP\", \"AVSALK\", \"PERSPENK\", \"PTRATIO\", \"PCTAF\", \"PCTCHRT\", \"PCTYRRND\"]\n    ].head(10)\n)",
            "code"
        ],
        [
            "AVYRSEXP    AVSALK  PERSPENK   PTRATIO     PCTAF  PCTCHRT   PCTYRRND\n0  14.70646  59.15732  4.445207  21.71025  57.03276      0.0  22.222220\n1  16.08324  59.50397  5.267598  20.44278  64.62264      0.0   0.000000\n2  14.59559  60.56992  5.482922  18.95419  53.94191      0.0   0.000000\n3  14.38939  58.33411  4.165093  21.63539  49.06103      0.0   7.142857\n4  13.90568  63.15364  4.324902  18.77984  52.38095      0.0   0.000000\n5  14.97755  66.97055  3.916104  24.51914  44.91578      0.0   2.380952\n6  14.67829  57.62195  4.270903  22.21278  32.28916      0.0  12.121210\n7  13.66197  63.44740  4.309734  24.59026  30.45267      0.0   0.000000\n8  16.41760  57.84564  4.527603  21.74138  22.64574      0.0   0.000000\n9  12.51864  57.80141  4.648917  20.26010  26.07099      0.0   0.000000",
            "code"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "formula = \"NABOVE + NBELOW ~ LOWINC + PERASIAN + PERBLACK + PERHISP + PCTCHRT \"\nformula += \"+ PCTYRRND + PERMINTE*AVYRSEXP*AVSALK + PERSPENK*PTRATIO*PCTAF\"",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution": [
        [
            "Toss a six-sided die 5 times, what\u2019s the probability of exactly 2 fours?",
            "markdown"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "stats.binom(5, 1.0 / 6).pmf(2)",
            "code"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "0.16075102880658423",
            "code"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "from scipy.special import comb\n\ncomb(5, 2) * (1 / 6.0) ** 2 * (5 / 6.0) ** 3",
            "code"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "0.1607510288065844",
            "code"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "from statsmodels.formula.api import glm\n\nglm_mod = glm(formula, dta, family=sm.families.Binomial()).fit()",
            "code"
        ],
        [
            "[30]:",
            "code"
        ],
        [
            "print(glm_mod.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n================================================================================\nDep. Variable:     ['NABOVE', 'NBELOW']   No. Observations:                  303\nModel:                              GLM   Df Residuals:                      282\nModel Family:                  Binomial   Df Model:                           20\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -2998.6\nDate:                  Wed, 02 Nov 2022   Deviance:                       4078.8\nTime:                          17:10:30   Pearson chi2:                 4.05e+03\nNo. Iterations:                       5   Pseudo R-squ. (CS):              1.000\nCovariance Type:              nonrobust\n============================================================================================\n                               coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------------\nIntercept                    2.9589      1.547      1.913      0.056      -0.073       5.990\nLOWINC                      -0.0168      0.000    -38.749      0.000      -0.018      -0.016\nPERASIAN                     0.0099      0.001     16.505      0.000       0.009       0.011\nPERBLACK                    -0.0187      0.001    -25.182      0.000      -0.020      -0.017\nPERHISP                     -0.0142      0.000    -32.818      0.000      -0.015      -0.013\nPCTCHRT                      0.0049      0.001      3.921      0.000       0.002       0.007\nPCTYRRND                    -0.0036      0.000    -15.878      0.000      -0.004      -0.003\nPERMINTE                     0.2545      0.030      8.498      0.000       0.196       0.313\nAVYRSEXP                     0.2407      0.057      4.212      0.000       0.129       0.353\nPERMINTE:AVYRSEXP           -0.0141      0.002     -7.391      0.000      -0.018      -0.010\nAVSALK                       0.0804      0.014      5.775      0.000       0.053       0.108\nPERMINTE:AVSALK             -0.0040      0.000     -8.450      0.000      -0.005      -0.003\nAVYRSEXP:AVSALK             -0.0039      0.001     -4.059      0.000      -0.006      -0.002\nPERMINTE:AVYRSEXP:AVSALK     0.0002   2.99e-05      7.428      0.000       0.000       0.000\nPERSPENK                    -1.9522      0.317     -6.162      0.000      -2.573      -1.331\nPTRATIO                     -0.3341      0.061     -5.453      0.000      -0.454      -0.214\nPERSPENK:PTRATIO             0.0917      0.015      6.321      0.000       0.063       0.120\nPCTAF                       -0.1690      0.033     -5.169      0.000      -0.233      -0.105\nPERSPENK:PCTAF               0.0490      0.007      6.574      0.000       0.034       0.064\nPTRATIO:PCTAF                0.0080      0.001      5.362      0.000       0.005       0.011\nPERSPENK:PTRATIO:PCTAF      -0.0022      0.000     -6.445      0.000      -0.003      -0.002\n============================================================================================",
            "code"
        ],
        [
            "The number of trials",
            "markdown"
        ],
        [
            "[31]:",
            "code"
        ],
        [
            "glm_mod.model.data.orig_endog.sum(1)",
            "code"
        ],
        [
            "[31]:",
            "code"
        ],
        [
            "0      807.0\n1      184.0\n2      571.0\n3      573.0\n4       65.0\n       ...\n298    342.0\n299    154.0\n300    595.0\n301    709.0\n302    156.0\nLength: 303, dtype: float64",
            "code"
        ],
        [
            "[32]:",
            "code"
        ],
        [
            "glm_mod.fittedvalues * glm_mod.model.data.orig_endog.sum(1)",
            "code"
        ],
        [
            "[32]:",
            "code"
        ],
        [
            "0      470.732584\n1      138.266178\n2      285.832629\n3      392.702917\n4       20.963146\n          ...\n298    111.464708\n299     61.037884\n300    235.517446\n301    290.952508\n302     53.312851\nLength: 303, dtype: float64",
            "code"
        ],
        [
            "First differences: We hold all explanatory variables constant at their means and manipulate the percentage of low income households to assess its impact on the response variables:",
            "markdown"
        ],
        [
            "[33]:",
            "code"
        ],
        [
            "exog = glm_mod.model.data.orig_exog  # get the dataframe",
            "code"
        ],
        [
            "[34]:",
            "code"
        ],
        [
            "means25 = exog.mean()\nprint(means25)",
            "code"
        ],
        [
            "Intercept                       1.000000\nLOWINC                         41.409877\nPERASIAN                        5.896335\nPERBLACK                        5.636808\nPERHISP                        34.398080\nPCTCHRT                         1.175909\nPCTYRRND                       11.611905\nPERMINTE                       14.694747\nAVYRSEXP                       14.253875\nPERMINTE:AVYRSEXP             209.018700\nAVSALK                         58.640258\nPERMINTE:AVSALK               879.979883\nAVYRSEXP:AVSALK               839.718173\nPERMINTE:AVYRSEXP:AVSALK    12585.266464\nPERSPENK                        4.320310\nPTRATIO                        22.464250\nPERSPENK:PTRATIO               96.295756\nPCTAF                          33.630593\nPERSPENK:PCTAF                147.235740\nPTRATIO:PCTAF                 747.445536\nPERSPENK:PTRATIO:PCTAF       3243.607568\ndtype: float64",
            "code"
        ],
        [
            "[35]:",
            "code"
        ],
        [
            "means25[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.25)\nprint(means25)",
            "code"
        ],
        [
            "Intercept                       1.000000\nLOWINC                         26.683040\nPERASIAN                        5.896335\nPERBLACK                        5.636808\nPERHISP                        34.398080\nPCTCHRT                         1.175909\nPCTYRRND                       11.611905\nPERMINTE                       14.694747\nAVYRSEXP                       14.253875\nPERMINTE:AVYRSEXP             209.018700\nAVSALK                         58.640258\nPERMINTE:AVSALK               879.979883\nAVYRSEXP:AVSALK               839.718173\nPERMINTE:AVYRSEXP:AVSALK    12585.266464\nPERSPENK                        4.320310\nPTRATIO                        22.464250\nPERSPENK:PTRATIO               96.295756\nPCTAF                          33.630593\nPERSPENK:PCTAF                147.235740\nPTRATIO:PCTAF                 747.445536\nPERSPENK:PTRATIO:PCTAF       3243.607568\ndtype: float64",
            "code"
        ],
        [
            "[36]:",
            "code"
        ],
        [
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "code"
        ],
        [
            "Intercept                       1.000000\nLOWINC                         55.460075\nPERASIAN                        5.896335\nPERBLACK                        5.636808\nPERHISP                        34.398080\nPCTCHRT                         1.175909\nPCTYRRND                       11.611905\nPERMINTE                       14.694747\nAVYRSEXP                       14.253875\nPERMINTE:AVYRSEXP             209.018700\nAVSALK                         58.640258\nPERMINTE:AVSALK               879.979883\nAVYRSEXP:AVSALK               839.718173\nPERMINTE:AVYRSEXP:AVSALK    12585.266464\nPERSPENK                        4.320310\nPTRATIO                        22.464250\nPERSPENK:PTRATIO               96.295756\nPCTAF                          33.630593\nPERSPENK:PCTAF                147.235740\nPTRATIO:PCTAF                 747.445536\nPERSPENK:PTRATIO:PCTAF       3243.607568\ndtype: float64",
            "code"
        ],
        [
            "Again, predict expects a DataFrame since patsy is used to select columns.",
            "markdown"
        ],
        [
            "[37]:",
            "code"
        ],
        [
            "resp25 = glm_mod.predict(pd.DataFrame(means25).T)\nresp75 = glm_mod.predict(pd.DataFrame(means75).T)\ndiff = resp75 - resp25",
            "code"
        ],
        [
            "The interquartile first difference for the percentage of low income households in a school district is:",
            "markdown"
        ],
        [
            "[38]:",
            "code"
        ],
        [
            "print(\"%2.4f%%\" % (diff[0] * 100))",
            "code"
        ],
        [
            "-11.8863%",
            "code"
        ],
        [
            "[39]:",
            "code"
        ],
        [
            "nobs = glm_mod.nobs\ny = glm_mod.model.endog\nyhat = glm_mod.mu",
            "code"
        ],
        [
            "[40]:",
            "code"
        ],
        [
            "from statsmodels.graphics.api import abline_plot\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, ylabel=\"Observed Values\", xlabel=\"Fitted Values\")\nax.scatter(yhat, y)\ny_vs_yhat = sm.OLS(y, sm.add_constant(yhat, prepend=True)).fit()\nfig = abline_plot(model_results=y_vs_yhat, ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_discrete_choice_example_55_0.png\" src=\"../../../_images/examples_notebooks_generated_discrete_choice_example_55_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Plot fitted values vs Pearson residuals": [
        [
            "Pearson residuals are defined to be\n\n\\[\\frac{(y - \\mu)}{\\sqrt{(var(\\mu))}}\\]",
            "markdown"
        ],
        [
            "where var is typically determined by the family. E.g., binomial variance is \\(np(1 - p)\\)",
            "markdown"
        ],
        [
            "[41]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(\n    111,\n    title=\"Residual Dependence Plot\",\n    xlabel=\"Fitted Values\",\n    ylabel=\"Pearson Residuals\",\n)\nax.scatter(yhat, stats.zscore(glm_mod.resid_pearson))\nax.axis(\"tight\")\nax.plot([0.0, 1.0], [0.0, 0.0], \"k-\")",
            "code"
        ],
        [
            "[41]:",
            "code"
        ],
        [
            "[&lt;matplotlib.lines.Line2D at 0x7f37e4372530]\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_discrete_choice_example_58_1.png\" src=\"../../../_images/examples_notebooks_generated_discrete_choice_example_58_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Histogram of standardized deviance residuals with Kernel Density Estimate overlaid": [
        [
            "The definition of the deviance residuals depends on the family. For the Binomial distribution this is\n\n\\[r_{dev} = sign\\left(Y-\\mu\\right)*\\sqrt{2n(Y\\log\\frac{Y}{\\mu}+(1-Y)\\log\\frac{(1-Y)}{(1-\\mu)}}\\]",
            "markdown"
        ],
        [
            "They can be used to detect ill-fitting covariates",
            "markdown"
        ],
        [
            "[42]:",
            "code"
        ],
        [
            "resid = glm_mod.resid_deviance\nresid_std = stats.zscore(resid)\nkde_resid = sm.nonparametric.KDEUnivariate(resid_std)\nkde_resid.fit()",
            "code"
        ],
        [
            "[42]:",
            "code"
        ],
        [
            "&lt;statsmodels.nonparametric.kde.KDEUnivariate at 0x7f37e43ca110",
            "code"
        ],
        [
            "[43]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, title=\"Standardized Deviance Residuals\")\nax.hist(resid_std, bins=25, density=True)\nax.plot(kde_resid.support, kde_resid.density, \"r\")",
            "code"
        ],
        [
            "[43]:",
            "code"
        ],
        [
            "[&lt;matplotlib.lines.Line2D at 0x7f37e44160b0]\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_discrete_choice_example_62_1.png\" src=\"../../../_images/examples_notebooks_generated_discrete_choice_example_62_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->QQ-plot of deviance residuals": [
        [
            "[44]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nfig = sm.graphics.qqplot(resid, line=\"r\", ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_discrete_choice_example_64_0.png\" src=\"../../../_images/examples_notebooks_generated_discrete_choice_example_64_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Ordinal Regression": [
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel",
            "code"
        ],
        [
            "Loading a stata data file from the UCLA website.This notebook is inspired by  which is a R notebook from UCLA.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "url = \"https://stats.idre.ucla.edu/stat/data/ologit.dta\"\ndata_student = pd.read_stata(url)",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "data_student.head(5)",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "data_student.dtypes",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "apply     category\npared         int8\npublic        int8\ngpa        float32\ndtype: object",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "data_student['apply'].dtype",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "CategoricalDtype(categories=['unlikely', 'somewhat likely', 'very likely'], ordered=True)",
            "code"
        ],
        [
            "This dataset is about the probability for undergraduate students to apply to graduate school given three exogenous variables: - their grade point average(gpa), a float between 0 and 4. - pared, a binary that indicates if at least one parent went to graduate school. - and public, a binary that indicates if the current undergraduate institution of the student is public or private.",
            "markdown"
        ],
        [
            "apply, the target variable is categorical with ordered categories: unlikely &lt; somewhat likely &lt; very likely. It is a pd.Serie of categorical type, this is preferred over NumPy arrays.",
            "markdown"
        ],
        [
            "The model is based on a numerical latent variable \\(y_{latent}\\) that we cannot observe but that we can compute thanks to exogenous variables. Moreover we can use this \\(y_{latent}\\) to define \\(y\\) that we can observe.",
            "markdown"
        ],
        [
            "For more details see the the Documentation of OrderedModel,  or this .",
            "markdown"
        ]
    ],
    "Examples->Discrete Choice Models->Ordinal Regression->Probit ordinal regression:": [
        [
            "[6]:",
            "code"
        ],
        [
            "mod_prob = OrderedModel(data_student['apply'],\n                        data_student[['pared', 'public', 'gpa']],\n                        distr='probit')\n\nres_prob = mod_prob.fit(method='bfgs')\nres_prob.summary()",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.896869\n         Iterations: 17\n         Function evaluations: 21\n         Gradient evaluations: 21",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "In our model, we have 3 exogenous variables(the \\(\\beta\\)s if we keep the documentation\u2019s notations) so we have 3 coefficients that need to be estimated.",
            "markdown"
        ],
        [
            "Those 3 estimations and their standard errors can be retrieved in the summary table.",
            "markdown"
        ],
        [
            "Since there are 3 categories in the target variable(unlikely, somewhat likely, very likely), we have two thresholds to estimate. As explained in the doc of the method OrderedModel.transform_threshold_params, the first estimated threshold is the actual value and all the other thresholds are in terms of cumulative exponentiated increments. Actual thresholds values can be computed as follows:",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "num_of_thresholds = 2\nmod_prob.transform_threshold_params(res_prob.params[-num_of_thresholds:])",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "array([      -inf, 1.29684541, 2.50285886,        inf])",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:": [
        [
            "[8]:",
            "code"
        ],
        [
            "mod_log = OrderedModel(data_student['apply'],\n                        data_student[['pared', 'public', 'gpa']],\n                        distr='logit')\n\nres_log = mod_log.fit(method='bfgs', disp=False)\nres_log.summary()",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/miscmodels/ordinal_model.py:419: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n  xb = xb[:, None]",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "array([[0.54884071, 0.35932276, 0.09183653],\n       [0.30558191, 0.47594216, 0.21847593],\n       [0.22938356, 0.47819057, 0.29242587],\n       ...,\n       [0.69380357, 0.25470075, 0.05149568],\n       [0.54884071, 0.35932276, 0.09183653],\n       [0.50896794, 0.38494062, 0.10609145]])",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "pred_choice = predicted.argmax(1)\nprint('Fraction of correct choice predictions')\nprint((np.asarray(data_student['apply'].values.codes) == pred_choice).mean())",
            "code"
        ],
        [
            "Fraction of correct choice predictions\n0.5775",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Ordinal Regression->Ordinal regression with a custom cumulative cLogLog distribution:": [
        [
            "In addition to logit and probit regression, any continuous distribution from SciPy.stats package can be used for the distr argument. Alternatively, one can define its own distribution simply creating a subclass from rv_continuous and implementing a few methods.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "# using a SciPy distribution\nres_exp = OrderedModel(data_student['apply'],\n                           data_student[['pared', 'public', 'gpa']],\n                           distr=stats.expon).fit(method='bfgs', disp=False)\nres_exp.summary()",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "# minimal definition of a custom scipy distribution.\nclass CLogLog(stats.rv_continuous):\n    def _ppf(self, q):\n        return np.log(-np.log(1 - q))\n\n    def _cdf(self, x):\n        return 1 - np.exp(-np.exp(x))\n\n\ncloglog = CLogLog()\n\n# definition of the model and fitting\nres_cloglog = OrderedModel(data_student['apply'],\n                           data_student[['pared', 'public', 'gpa']],\n                           distr=cloglog).fit(method='bfgs', disp=False)\nres_cloglog.summary()",
            "code"
        ],
        [
            "[12]:",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog": [
        [
            "Pandas\u2019 ordered categorical and numeric values are supported as dependent variable in formulas. Other types will raise a ValueError.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "modf_logit = OrderedModel.from_formula(\"apply ~ 0 + pared + public + gpa\", data_student,\n                                      distr='logit')\nresf_logit = modf_logit.fit(method='bfgs')\nresf_logit.summary()",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.896281\n         Iterations: 22\n         Function evaluations: 24\n         Gradient evaluations: 24",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "Using numerical codes for the dependent variable is supported but loses the names of the category levels. The levels and names correspond to the unique values of the dependent variable sorted in alphanumeric order as in the case without using formulas.",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "data_student[\"apply_codes\"] = data_student['apply'].cat.codes * 2 + 5\ndata_student[\"apply_codes\"].head()",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "0    9\n1    7\n2    5\n3    7\n4    7\nName: apply_codes, dtype: int8",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "OrderedModel.from_formula(\"apply_codes ~ 0 + pared + public + gpa\", data_student,\n                          distr='logit').fit().summary()",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.896281\n         Iterations: 421\n         Function evaluations: 663",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "resf_logit.predict(data_student.iloc[:5])",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "Using string values directly as dependent variable raises a ValueError.",
            "markdown"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "data_student[\"apply_str\"] = np.asarray(data_student[\"apply\"])\ndata_student[\"apply_str\"].head()",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "0        very likely\n1    somewhat likely\n2           unlikely\n3    somewhat likely\n4    somewhat likely\nName: apply_str, dtype: object",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "OrderedModel.from_formula(\"apply_str ~ 0 + pared + public + gpa\", data_student,\n                          distr='logit')",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "&lt;statsmodels.miscmodels.ordinal_model.OrderedModel at 0x7faa615c3f40",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model": [
        [
            "The parameterization of OrderedModel requires that there is <strong>no</strong> constant in the model, neither explicit nor implicit. The constant is equivalent to shifting all thresholds and is therefore not separately identified.",
            "markdown"
        ],
        [
            "Patsy\u2019s formula specification does not allow a design matrix without explicit or implicit constant if there are categorical variables (or maybe splines) among explanatory variables. As workaround, statsmodels removes an explicit intercept.",
            "markdown"
        ],
        [
            "Consequently, there are two valid cases to get a design matrix without intercept.",
            "markdown"
        ],
        [
            "specify a model without explicit and implicit intercept which is possible if there are only numerical variables in the model.",
            "markdown"
        ],
        [
            "specify a model with an explicit intercept which statsmodels will remove.",
            "markdown"
        ],
        [
            "Models with an implicit intercept will be overparameterized, the parameter estimates will not be fully identified, cov_params will not be invertible and standard errors might contain nans.",
            "markdown"
        ],
        [
            "In the following we look at an example with an additional categorical variable.",
            "markdown"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "nobs = len(data_student)\ndata_student[\"dummy\"] = (np.arange(nobs) &lt; (nobs / 2)).astype(float)",
            "code"
        ],
        [
            "<strong>explicit intercept</strong>, that will be removed:",
            "markdown"
        ],
        [
            "Note \u201c1 +\u201d is here redundant because it is patsy\u2019s default.",
            "markdown"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "modfd_logit = OrderedModel.from_formula(\"apply ~ 1 + pared + public + gpa + C(dummy)\", data_student,\n                                      distr='logit')\nresfd_logit = modfd_logit.fit(method='bfgs')\nprint(resfd_logit.summary())",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.896247\n         Iterations: 26\n         Function evaluations: 28\n         Gradient evaluations: 28\n                             OrderedModel Results\n==============================================================================\nDep. Variable:                  apply   Log-Likelihood:                -358.50\nModel:                   OrderedModel   AIC:                             729.0\nMethod:            Maximum Likelihood   BIC:                             752.9\nDate:                Wed, 02 Nov 2022\nTime:                        17:11:15\nNo. Observations:                 400\nDf Residuals:                     394\nDf Model:                           6\n===============================================================================================\n                                  coef    std err          z      P|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nC(dummy)[T.1.0]                 0.0326      0.198      0.164      0.869      -0.356       0.421\npared                           1.0489      0.266      3.945      0.000       0.528       1.570\npublic                         -0.0589      0.298     -0.198      0.843      -0.643       0.525\ngpa                             0.6153      0.261      2.360      0.018       0.104       1.126\nunlikely/somewhat likely        2.2183      0.785      2.826      0.005       0.680       3.757\nsomewhat likely/very likely     0.7398      0.080      9.237      0.000       0.583       0.897\n===============================================================================================",
            "code"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "modfd_logit.k_vars",
            "code"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "4",
            "code"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "modfd_logit.k_constant",
            "code"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "0",
            "code"
        ],
        [
            "<strong>implicit intercept</strong> creates overparameterized model",
            "markdown"
        ],
        [
            "Specifying \u201c0 +\u201d in the formula drops the explicit intercept. However, the categorical encoding is now changed to include an implicit intercept. In this example, the created dummy variables C(dummy)[0.0] and C(dummy)[1.0] sum to one.",
            "markdown"
        ],
        [
            "OrderedModel.from_formula(\"apply ~ 0 + pared + public + gpa + C(dummy)\", data_student, distr='logit')",
            "code"
        ],
        [
            "To see what would happen in the overparameterized case, we can avoid the constant check in the model by explicitly specifying whether a constant is present or not. We use hasconst=False, even though the model has an implicit constant.",
            "markdown"
        ],
        [
            "The parameters of the two dummy variable columns and the first threshold are not separately identified. Estimates for those parameters and availability of standard errors are arbitrary and depends on numerical details that differ across environments.",
            "markdown"
        ],
        [
            "Some summary measures like log-likelihood value are not affected by this, within convergence tolerance and numerical precision. Prediction should also be possible. However, inference is not available, or is not valid.",
            "markdown"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "modfd2_logit = OrderedModel.from_formula(\"apply ~ 0 + pared + public + gpa + C(dummy)\", data_student,\n                                         distr='logit', hasconst=False)\nresfd2_logit = modfd2_logit.fit(method='bfgs')\nprint(resfd2_logit.summary())",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.896247\n         Iterations: 24\n         Function evaluations: 26\n         Gradient evaluations: 26\n                             OrderedModel Results\n==============================================================================\nDep. Variable:                  apply   Log-Likelihood:                -358.50\nModel:                   OrderedModel   AIC:                             731.0\nMethod:            Maximum Likelihood   BIC:                             758.9\nDate:                Wed, 02 Nov 2022\nTime:                        17:11:16\nNo. Observations:                 400\nDf Residuals:                     393\nDf Model:                           7\n===============================================================================================\n                                  coef    std err          z      P|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nC(dummy)[0.0]                  -0.6834    254.944     -0.003      0.998    -500.365     498.998\nC(dummy)[1.0]                  -0.6508    254.944     -0.003      0.998    -500.333     499.031\npared                           1.0489      0.266      3.944      0.000       0.528       1.570\npublic                         -0.0588      0.298     -0.197      0.844      -0.643       0.525\ngpa                             0.6153      0.261      2.360      0.018       0.104       1.126\nunlikely/somewhat likely        1.5349    254.945      0.006      0.995    -498.148     501.218\nsomewhat likely/very likely     0.7398      0.080      9.237      0.000       0.583       0.897\n===============================================================================================",
            "code"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "resfd2_logit.predict(data_student.iloc[:5])",
            "code"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "resf_logit.predict()",
            "code"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "array([[0.54884071, 0.35932276, 0.09183653],\n       [0.30558191, 0.47594216, 0.21847593],\n       [0.22938356, 0.47819057, 0.29242587],\n       ...,\n       [0.69380357, 0.25470075, 0.05149568],\n       [0.54884071, 0.35932276, 0.09183653],\n       [0.50896793, 0.38494062, 0.10609145]])",
            "code"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "",
            "code"
        ]
    ],
    "Examples->Discrete Choice Models->Ordinal Regression->Binary Model compared to Logit": [
        [
            "If there are only two levels of the dependent ordered categorical variable, then the model can also be estimated by a Logit model.",
            "markdown"
        ],
        [
            "The models are (theoretically) identical in this case except for the parameterization of the constant. Logit as most other models requires in general an intercept. This corresponds to the threshold parameter in the OrderedModel, however, with opposite sign.",
            "markdown"
        ],
        [
            "The implementation differs and not all of the same results statistic and post-estimation features are available. Estimated parameters and other results statistic differ mainly based on convergence tolerance of the optimization.",
            "markdown"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "from statsmodels.discrete.discrete_model import Logit\nfrom statsmodels.tools.tools import add_constant",
            "code"
        ],
        [
            "We drop the middle category from the data and keep the two extreme categories.",
            "markdown"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "mask_drop = data_student['apply'] == \"somewhat likely\"\ndata2 = data_student.loc[~mask_drop, :]\n# we need to remove the category also from the Categorical Index\ndata2['apply'].cat.remove_categories(\"somewhat likely\", inplace=True)\ndata2.head()",
            "code"
        ],
        [
            "/tmp/ipykernel_6943/1247146098.py:4: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n  data2['apply'].cat.remove_categories(\"somewhat likely\", inplace=True)",
            "code"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "mod_log = OrderedModel(data2['apply'],\n                        data2[['pared', 'public', 'gpa']],\n                        distr='logit')\n\nres_log = mod_log.fit(method='bfgs', disp=False)\nres_log.summary()",
            "code"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "The Logit model does not have a constant by default, we have to add it to our explanatory variables.",
            "markdown"
        ],
        [
            "The results are essentially identical between Logit and ordered model up to numerical precision mainly resulting from convergence tolerance in the estimation.",
            "markdown"
        ],
        [
            "The only difference is in the sign of the constant, Logit and OrdereModel have opposite signs of he constant. This is a consequence of the parameterization in terms of cut points in OrderedModel instead of including and constant column in the design matrix.",
            "markdown"
        ],
        [
            "[30]:",
            "code"
        ],
        [
            "ex = add_constant(data2[['pared', 'public', 'gpa']], prepend=False)\nmod_logit = Logit(data2['apply'].cat.codes, ex)\n\nres_logit = mod_logit.fit(method='bfgs', disp=False)",
            "code"
        ],
        [
            "[31]:",
            "code"
        ],
        [
            "res_logit.summary()",
            "code"
        ],
        [
            "[31]:",
            "code"
        ],
        [
            "Robust standard errors are also available in OrderedModel in the same way as in discrete.Logit. As example we specify HAC covariance type even though we have cross-sectional data and autocorrelation is not appropriate.",
            "markdown"
        ],
        [
            "[32]:",
            "code"
        ],
        [
            "res_logit_hac = mod_logit.fit(method='bfgs', disp=False, cov_type=\"hac\", cov_kwds={\"maxlags\": 2})\nres_log_hac = mod_log.fit(method='bfgs', disp=False, cov_type=\"hac\", cov_kwds={\"maxlags\": 2})",
            "code"
        ],
        [
            "[33]:",
            "code"
        ],
        [
            "res_logit_hac.bse.values - res_log_hac.bse",
            "code"
        ],
        [
            "[33]:",
            "code"
        ],
        [
            "pared                   6.526362e-08\npublic                 -3.820145e-07\ngpa                     8.616114e-08\nunlikely/very likely    3.570613e-07\ndtype: float64",
            "code"
        ]
    ],
    "Examples->Nonparametric Statistics->Univariate Kernel Density Estimator": [
        [
            "Kernel density estimation is the process of estimating an unknown probability density function using a kernel function \\(K(u)\\). While a histogram counts the number of data points in somewhat arbitrary regions, a kernel density estimate is a function defined as the sum of a kernel function on every data point. The kernel function typically exhibits the following properties:",
            "markdown"
        ],
        [
            "Symmetry such that \\(K(u) = K(-u)\\).",
            "markdown"
        ],
        [
            "Normalization such that \\(\\int_{-\\infty}^{\\infty} K(u) \\ du = 1\\) .",
            "markdown"
        ],
        [
            "Monotonically decreasing such that \\(K'(u) &lt; 0\\) when \\(u > 0\\).",
            "markdown"
        ],
        [
            "Expected value equal to zero such that \\(\\mathrm{E}[K] = 0\\).",
            "markdown"
        ],
        [
            "For more information about kernel density estimation, see for instance .",
            "markdown"
        ],
        [
            "A univariate kernel density estimator is implemented in sm.nonparametric.KDEUnivariate. In this example we will show the following:",
            "markdown"
        ],
        [
            "Basic usage, how to fit the estimator.",
            "markdown"
        ],
        [
            "The effect of varying the bandwidth of the kernel using the bw argument.",
            "markdown"
        ],
        [
            "The various kernel functions available using the kernel argument.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom statsmodels.distributions.mixture_rvs import mixture_rvs",
            "code"
        ]
    ],
    "Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->A univariate example": [
        [
            "[2]:",
            "code"
        ],
        [
            "np.random.seed(12345)  # Seed the random number generator for reproducible results",
            "code"
        ],
        [
            "We create a bimodal distribution: a mixture of two normal distributions with locations at -1 and 1.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "# Location, scale and weight for the two distributions\ndist1_loc, dist1_scale, weight1 = -1, 0.5, 0.25\ndist2_loc, dist2_scale, weight2 = 1, 0.5, 0.75\n\n# Sample from a mixture of distributions\nobs_dist = mixture_rvs(\n    prob=[weight1, weight2],\n    size=250,\n    dist=[stats.norm, stats.norm],\n    kwargs=(\n        dict(loc=dist1_loc, scale=dist1_scale),\n        dict(loc=dist2_loc, scale=dist2_scale),\n    ),\n)",
            "code"
        ],
        [
            "The simplest non-parametric technique for density estimation is the histogram.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 5))\nax = fig.add_subplot(111)\n\n# Scatter plot of data samples and histogram\nax.scatter(\n    obs_dist,\n    np.abs(np.random.randn(obs_dist.size)),\n    zorder=15,\n    color=\"red\",\n    marker=\"x\",\n    alpha=0.5,\n    label=\"Samples\",\n)\nlines = ax.hist(obs_dist, bins=20, edgecolor=\"k\", label=\"Histogram\")\n\nax.legend(loc=\"best\")\nax.grid(True, zorder=-5)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_7_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_7_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Fitting with the default arguments": [
        [
            "The histogram above is discontinuous. To compute a continuous probability density function, we can use kernel density estimation.",
            "markdown"
        ],
        [
            "We initialize a univariate kernel density estimator using KDEUnivariate.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "kde = sm.nonparametric.KDEUnivariate(obs_dist)\nkde.fit()  # Estimate the densities",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "&lt;statsmodels.nonparametric.kde.KDEUnivariate at 0x7fd61a077bb0",
            "code"
        ],
        [
            "We present a figure of the fit, as well as the true distribution.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 5))\nax = fig.add_subplot(111)\n\n# Plot the histrogram\nax.hist(\n    obs_dist,\n    bins=20,\n    density=True,\n    label=\"Histogram from samples\",\n    zorder=5,\n    edgecolor=\"k\",\n    alpha=0.5,\n)\n\n# Plot the KDE as fitted using the default arguments\nax.plot(kde.support, kde.density, lw=3, label=\"KDE from samples\", zorder=10)\n\n# Plot the true distribution\ntrue_values = (\n    stats.norm.pdf(loc=dist1_loc, scale=dist1_scale, x=kde.support) * weight1\n    + stats.norm.pdf(loc=dist2_loc, scale=dist2_scale, x=kde.support) * weight2\n)\nax.plot(kde.support, true_values, lw=3, label=\"True distribution\", zorder=15)\n\n# Plot the samples\nax.scatter(\n    obs_dist,\n    np.abs(np.random.randn(obs_dist.size)) / 40,\n    marker=\"x\",\n    color=\"red\",\n    zorder=20,\n    label=\"Samples\",\n    alpha=0.5,\n)\n\nax.legend(loc=\"best\")\nax.grid(True, zorder=-5)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_12_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_12_0.png\"/>",
            "code"
        ],
        [
            "In the code above, default arguments were used. We can also vary the bandwidth of the kernel, as we will now see.",
            "markdown"
        ]
    ],
    "Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Varying the bandwidth using the bw argument": [
        [
            "The bandwidth of the kernel can be adjusted using the bw argument. In the following example, a bandwidth of bw=0.2 seems to fit the data well.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 5))\nax = fig.add_subplot(111)\n\n# Plot the histrogram\nax.hist(\n    obs_dist,\n    bins=25,\n    label=\"Histogram from samples\",\n    zorder=5,\n    edgecolor=\"k\",\n    density=True,\n    alpha=0.5,\n)\n\n# Plot the KDE for various bandwidths\nfor bandwidth in [0.1, 0.2, 0.4]:\n    kde.fit(bw=bandwidth)  # Estimate the densities\n    ax.plot(\n        kde.support,\n        kde.density,\n        \"--\",\n        lw=2,\n        color=\"k\",\n        zorder=10,\n        label=\"KDE from samples, bw = {}\".format(round(bandwidth, 2)),\n    )\n\n# Plot the true distribution\nax.plot(kde.support, true_values, lw=3, label=\"True distribution\", zorder=15)\n\n# Plot the samples\nax.scatter(\n    obs_dist,\n    np.abs(np.random.randn(obs_dist.size)) / 50,\n    marker=\"x\",\n    color=\"red\",\n    zorder=20,\n    label=\"Data samples\",\n    alpha=0.5,\n)\n\nax.legend(loc=\"best\")\nax.set_xlim([-3, 3])\nax.grid(True, zorder=-5)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_16_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_16_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Comparing kernel functions": [
        [
            "In the example above, a Gaussian kernel was used. Several other kernels are also available.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "from statsmodels.nonparametric.kde import kernel_switch\n\nlist(kernel_switch.keys())",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "['gau', 'epa', 'uni', 'tri', 'biw', 'triw', 'cos', 'cos2', 'tric']",
            "code"
        ]
    ],
    "Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Comparing kernel functions->The available kernel functions": [
        [
            "[9]:",
            "code"
        ],
        [
            "# Create a figure\nfig = plt.figure(figsize=(12, 5))\n\n# Enumerate every option for the kernel\nfor i, (ker_name, ker_class) in enumerate(kernel_switch.items()):\n\n    # Initialize the kernel object\n    kernel = ker_class()\n\n    # Sample from the domain\n    domain = kernel.domain or [-3, 3]\n    x_vals = np.linspace(*domain, num=2 ** 10)\n    y_vals = kernel(x_vals)\n\n    # Create a subplot, set the title\n    ax = fig.add_subplot(3, 3, i + 1)\n    ax.set_title('Kernel function \"{}\"'.format(ker_name))\n    ax.plot(x_vals, y_vals, lw=3, label=\"{}\".format(ker_name))\n    ax.scatter([0], [0], marker=\"x\", color=\"red\")\n    plt.grid(True, zorder=-5)\n    ax.set_xlim(domain)\n\nplt.tight_layout()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_21_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_21_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Comparing kernel functions->The available kernel functions on three data points": [
        [
            "We now examine how the kernel density estimate will fit to three equally spaced data points.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "# Create three equidistant points\ndata = np.linspace(-1, 1, 3)\nkde = sm.nonparametric.KDEUnivariate(data)\n\n# Create a figure\nfig = plt.figure(figsize=(12, 5))\n\n# Enumerate every option for the kernel\nfor i, kernel in enumerate(kernel_switch.keys()):\n\n    # Create a subplot, set the title\n    ax = fig.add_subplot(3, 3, i + 1)\n    ax.set_title('Kernel function \"{}\"'.format(kernel))\n\n    # Fit the model (estimate densities)\n    kde.fit(kernel=kernel, fft=False, gridsize=2 ** 10)\n\n    # Create the plot\n    ax.plot(kde.support, kde.density, lw=3, label=\"KDE from samples\", zorder=10)\n    ax.scatter(data, np.zeros_like(data), marker=\"x\", color=\"red\")\n    plt.grid(True, zorder=-5)\n    ax.set_xlim([-3, 3])\n\nplt.tight_layout()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->A more difficult case": [
        [
            "The fit is not always perfect. See the example below for a harder case.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "obs_dist = mixture_rvs(\n    [0.25, 0.75],\n    size=250,\n    dist=[stats.norm, stats.beta],\n    kwargs=(dict(loc=-1, scale=0.5), dict(loc=1, scale=1, args=(1, 0.5))),\n)",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "kde = sm.nonparametric.KDEUnivariate(obs_dist)\nkde.fit()",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "&lt;statsmodels.nonparametric.kde.KDEUnivariate at 0x7fd5dd01d810",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 5))\nax = fig.add_subplot(111)\nax.hist(obs_dist, bins=20, density=True, edgecolor=\"k\", zorder=4, alpha=0.5)\nax.plot(kde.support, kde.density, lw=3, zorder=7)\n# Plot the samples\nax.scatter(\n    obs_dist,\n    np.abs(np.random.randn(obs_dist.size)) / 50,\n    marker=\"x\",\n    color=\"red\",\n    zorder=20,\n    label=\"Data samples\",\n    alpha=0.5,\n)\nax.grid(True, zorder=-5)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_28_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_28_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution": [
        [
            "Since the KDE is a distribution, we can access attributes and methods such as:",
            "markdown"
        ],
        [
            "entropy",
            "markdown"
        ],
        [
            "evaluate",
            "markdown"
        ],
        [
            "cdf",
            "markdown"
        ],
        [
            "icdf",
            "markdown"
        ],
        [
            "sf",
            "markdown"
        ],
        [
            "cumhazard",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "obs_dist = mixture_rvs(\n    [0.25, 0.75],\n    size=1000,\n    dist=[stats.norm, stats.norm],\n    kwargs=(dict(loc=-1, scale=0.5), dict(loc=1, scale=0.5)),\n)\nkde = sm.nonparametric.KDEUnivariate(obs_dist)\nkde.fit(gridsize=2 ** 10)",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "&lt;statsmodels.nonparametric.kde.KDEUnivariate at 0x7fd5e03ac100",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "kde.entropy",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "1.314324140492138",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "kde.evaluate(-1)",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "array([0.18085886])",
            "code"
        ]
    ],
    "Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution->Cumulative distribution, it\u2019s inverse, and the survival function": [
        [
            "[17]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 5))\nax = fig.add_subplot(111)\n\nax.plot(kde.support, kde.cdf, lw=3, label=\"CDF\")\nax.plot(np.linspace(0, 1, num=kde.icdf.size), kde.icdf, lw=3, label=\"Inverse CDF\")\nax.plot(kde.support, kde.sf, lw=3, label=\"Survival function\")\nax.legend(loc=\"best\")\nax.grid(True, zorder=-5)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_34_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_34_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution->The Cumulative Hazard Function": [
        [
            "[18]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 5))\nax = fig.add_subplot(111)\nax.plot(kde.support, kde.cumhazard, lw=3, label=\"Cumulative Hazard Function\")\nax.legend(loc=\"best\")\nax.grid(True, zorder=-5)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_36_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_36_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Nonparametric Statistics->Lowess Regression": [
        [
            "This notebook introduces the LOWESS smoother in the nonparametric package. LOWESS performs weighted local linear fits.",
            "markdown"
        ],
        [
            "We generated some non-linear data and perform a LOWESS fit, then compute a 95% confidence interval around the LOWESS fit by performing bootstrap resampling.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "# Seed for consistency\nnp.random.seed(1)",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "# Generate data looking like cosine\nx = np.random.uniform(0, 4 * np.pi, size=200)\ny = np.cos(x) + np.random.random(size=len(x))\n\n# Compute a lowess smoothing of the data\nsmoothed = sm.nonparametric.lowess(exog=x, endog=y, frac=0.2)",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "# Plot the fit line\nfig, ax = pylab.subplots()\n\nax.scatter(x, y)\nax.plot(smoothed[:, 0], smoothed[:, 1], c=\"k\")\npylab.autoscale(enable=True, axis=\"x\", tight=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_lowess_4_0.png\" src=\"../../../_images/examples_notebooks_generated_lowess_4_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Nonparametric Statistics->Lowess Regression->Confidence interval": [
        [
            "Now that we have performed a fit, we may want to know how precise it is. Bootstrap resampling gives one way of estimating confidence intervals around a LOWESS fit by recomputing the LOWESS fit for a large number of random resamplings from our data.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "# Now create a bootstrap confidence interval around the a LOWESS fit\n\n\ndef lowess_with_confidence_bounds(\n    x, y, eval_x, N=200, conf_interval=0.95, lowess_kw=None\n):\n    \"\"\"\n    Perform Lowess regression and determine a confidence interval by bootstrap resampling\n    \"\"\"\n    # Lowess smoothing\n    smoothed = sm.nonparametric.lowess(exog=x, endog=y, xvals=eval_x, **lowess_kw)\n\n    # Perform bootstrap resamplings of the data\n    # and  evaluate the smoothing at a fixed set of points\n    smoothed_values = np.empty((N, len(eval_x)))\n    for i in range(N):\n        sample = np.random.choice(len(x), len(x), replace=True)\n        sampled_x = x[sample]\n        sampled_y = y[sample]\n\n        smoothed_values[i] = sm.nonparametric.lowess(\n            exog=sampled_x, endog=sampled_y, xvals=eval_x, **lowess_kw\n        )\n\n    # Get the confidence interval\n    sorted_values = np.sort(smoothed_values, axis=0)\n    bound = int(N * (1 - conf_interval) / 2)\n    bottom = sorted_values[bound - 1]\n    top = sorted_values[-bound]\n\n    return smoothed, bottom, top\n\n\n# Compute the 95% confidence interval\neval_x = np.linspace(0, 4 * np.pi, 31)\nsmoothed, bottom, top = lowess_with_confidence_bounds(\n    x, y, eval_x, lowess_kw={\"frac\": 0.1}\n)",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "# Plot the confidence interval and fit\nfig, ax = pylab.subplots()\nax.scatter(x, y)\nax.plot(eval_x, smoothed, c=\"k\")\nax.fill_between(eval_x, bottom, top, alpha=0.5, color=\"b\")\npylab.autoscale(enable=True, axis=\"x\", tight=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_lowess_7_0.png\" src=\"../../../_images/examples_notebooks_generated_lowess_7_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Generalized Linear Models Overview": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import numpy as np\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom matplotlib import pyplot as plt\n\nplt.rc(\"figure\", figsize=(16,8))\nplt.rc(\"font\", size=14)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Load Star98 data": [
        [
            "In this example, we use the Star98 dataset which was taken with permission from Jeff Gill (2000) Generalized linear models: A unified approach. Codebook information can be obtained by typing:",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "print(sm.datasets.star98.NOTE)",
            "code"
        ],
        [
            "::\n\n    Number of Observations - 303 (counties in California).\n\n    Number of Variables - 13 and 8 interaction terms.\n\n    Definition of variables names::\n\n        NABOVE   - Total number of students above the national median for the\n                   math section.\n        NBELOW   - Total number of students below the national median for the\n                   math section.\n        LOWINC   - Percentage of low income students\n        PERASIAN - Percentage of Asian student\n        PERBLACK - Percentage of black students\n        PERHISP  - Percentage of Hispanic students\n        PERMINTE - Percentage of minority teachers\n        AVYRSEXP - Sum of teachers' years in educational service divided by the\n                number of teachers.\n        AVSALK   - Total salary budget including benefits divided by the number\n                   of full-time teachers (in thousands)\n        PERSPENK - Per-pupil spending (in thousands)\n        PTRATIO  - Pupil-teacher ratio.\n        PCTAF    - Percentage of students taking UC/CSU prep courses\n        PCTCHRT  - Percentage of charter schools\n        PCTYRRND - Percentage of year-round schools\n\n        The below variables are interaction terms of the variables defined\n        above.\n\n        PERMINTE_AVYRSEXP\n        PEMINTE_AVSAL\n        AVYRSEXP_AVSAL\n        PERSPEN_PTRATIO\n        PERSPEN_PCTAF\n        PTRATIO_PCTAF\n        PERMINTE_AVTRSEXP_AVSAL\n        PERSPEN_PTRATIO_PCTAF",
            "code"
        ],
        [
            "Load the data and add a constant to the exogenous (independent) variables:",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "data = sm.datasets.star98.load()\ndata.exog = sm.add_constant(data.exog, prepend=False)",
            "code"
        ],
        [
            "The dependent variable is N by 2 (Success: NABOVE, Failure: NBELOW):",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "print(data.endog.head())",
            "code"
        ],
        [
            "NABOVE  NBELOW\n0   452.0   355.0\n1   144.0    40.0\n2   337.0   234.0\n3   395.0   178.0\n4     8.0    57.0",
            "code"
        ],
        [
            "The independent variables include all the other variables described above, as well as the interaction terms:",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "print(data.exog.head())",
            "code"
        ],
        [
            "LOWINC   PERASIAN   PERBLACK    PERHISP  PERMINTE  AVYRSEXP    AVSALK  \\\n0  34.39730  23.299300  14.235280  11.411120  15.91837  14.70646  59.15732\n1  17.36507  29.328380   8.234897   9.314884  13.63636  16.08324  59.50397\n2  32.64324   9.226386  42.406310  13.543720  28.83436  14.59559  60.56992\n3  11.90953  13.883090   3.796973  11.443110  11.11111  14.38939  58.33411\n4  36.88889  12.187500  76.875000   7.604167  43.58974  13.90568  63.15364\n\n   PERSPENK   PTRATIO     PCTAF  ...   PCTYRRND  PERMINTE_AVYRSEXP  \\\n0  4.445207  21.71025  57.03276  ...  22.222220         234.102872\n1  5.267598  20.44278  64.62264  ...   0.000000         219.316851\n2  5.482922  18.95419  53.94191  ...   0.000000         420.854496\n3  4.165093  21.63539  49.06103  ...   7.142857         159.882095\n4  4.324902  18.77984  52.38095  ...   0.000000         606.144976\n\n   PERMINTE_AVSAL  AVYRSEXP_AVSAL  PERSPEN_PTRATIO  PERSPEN_PCTAF  \\\n0       941.68811        869.9948         96.50656      253.52242\n1       811.41756        957.0166        107.68435      340.40609\n2      1746.49488        884.0537        103.92435      295.75929\n3       648.15671        839.3923         90.11341      204.34375\n4      2752.85075        878.1943         81.22097      226.54248\n\n   PTRATIO_PCTAF  PERMINTE_AVYRSEXP_AVSAL  PERSPEN_PTRATIO_PCTAF  const\n0      1238.1955               13848.8985              5504.0352    1.0\n1      1321.0664               13050.2233              6958.8468    1.0\n2      1022.4252               25491.1232              5605.8777    1.0\n3      1061.4545                9326.5797              4421.0568    1.0\n4       983.7059               38280.2616              4254.4314    1.0\n\n[5 rows x 21 columns]",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Fit and summary": [
        [
            "[7]:",
            "code"
        ],
        [
            "glm_binom = sm.GLM(data.endog, data.exog, family=sm.families.Binomial())\nres = glm_binom.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n================================================================================\nDep. Variable:     ['NABOVE', 'NBELOW']   No. Observations:                  303\nModel:                              GLM   Df Residuals:                      282\nModel Family:                  Binomial   Df Model:                           20\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -2998.6\nDate:                  Wed, 02 Nov 2022   Deviance:                       4078.8\nTime:                          17:05:01   Pearson chi2:                 4.05e+03\nNo. Iterations:                       5   Pseudo R-squ. (CS):              1.000\nCovariance Type:              nonrobust\n===========================================================================================\n                              coef    std err          z      P|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------------\nLOWINC                     -0.0168      0.000    -38.749      0.000      -0.018      -0.016\nPERASIAN                    0.0099      0.001     16.505      0.000       0.009       0.011\nPERBLACK                   -0.0187      0.001    -25.182      0.000      -0.020      -0.017\nPERHISP                    -0.0142      0.000    -32.818      0.000      -0.015      -0.013\nPERMINTE                    0.2545      0.030      8.498      0.000       0.196       0.313\nAVYRSEXP                    0.2407      0.057      4.212      0.000       0.129       0.353\nAVSALK                      0.0804      0.014      5.775      0.000       0.053       0.108\nPERSPENK                   -1.9522      0.317     -6.162      0.000      -2.573      -1.331\nPTRATIO                    -0.3341      0.061     -5.453      0.000      -0.454      -0.214\nPCTAF                      -0.1690      0.033     -5.169      0.000      -0.233      -0.105\nPCTCHRT                     0.0049      0.001      3.921      0.000       0.002       0.007\nPCTYRRND                   -0.0036      0.000    -15.878      0.000      -0.004      -0.003\nPERMINTE_AVYRSEXP          -0.0141      0.002     -7.391      0.000      -0.018      -0.010\nPERMINTE_AVSAL             -0.0040      0.000     -8.450      0.000      -0.005      -0.003\nAVYRSEXP_AVSAL             -0.0039      0.001     -4.059      0.000      -0.006      -0.002\nPERSPEN_PTRATIO             0.0917      0.015      6.321      0.000       0.063       0.120\nPERSPEN_PCTAF               0.0490      0.007      6.574      0.000       0.034       0.064\nPTRATIO_PCTAF               0.0080      0.001      5.362      0.000       0.005       0.011\nPERMINTE_AVYRSEXP_AVSAL     0.0002   2.99e-05      7.428      0.000       0.000       0.000\nPERSPEN_PTRATIO_PCTAF      -0.0022      0.000     -6.445      0.000      -0.003      -0.002\nconst                       2.9589      1.547      1.913      0.056      -0.073       5.990\n===========================================================================================",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Quantities of interest": [
        [
            "[8]:",
            "code"
        ],
        [
            "print('Total number of trials:',  data.endog.iloc[:, 0].sum())\nprint('Parameters: ', res.params)\nprint('T-values: ', res.tvalues)",
            "code"
        ],
        [
            "Total number of trials: 108418.0\nParameters:  LOWINC                    -0.016815\nPERASIAN                   0.009925\nPERBLACK                  -0.018724\nPERHISP                   -0.014239\nPERMINTE                   0.254487\nAVYRSEXP                   0.240694\nAVSALK                     0.080409\nPERSPENK                  -1.952161\nPTRATIO                   -0.334086\nPCTAF                     -0.169022\nPCTCHRT                    0.004917\nPCTYRRND                  -0.003580\nPERMINTE_AVYRSEXP         -0.014077\nPERMINTE_AVSAL            -0.004005\nAVYRSEXP_AVSAL            -0.003906\nPERSPEN_PTRATIO            0.091714\nPERSPEN_PCTAF              0.048990\nPTRATIO_PCTAF              0.008041\nPERMINTE_AVYRSEXP_AVSAL    0.000222\nPERSPEN_PTRATIO_PCTAF     -0.002249\nconst                      2.958878\ndtype: float64\nT-values:  LOWINC                    -38.749083\nPERASIAN                   16.504736\nPERBLACK                  -25.182189\nPERHISP                   -32.817913\nPERMINTE                    8.498271\nAVYRSEXP                    4.212479\nAVSALK                      5.774998\nPERSPENK                   -6.161911\nPTRATIO                    -5.453217\nPCTAF                      -5.168654\nPCTCHRT                     3.921200\nPCTYRRND                  -15.878260\nPERMINTE_AVYRSEXP          -7.390931\nPERMINTE_AVSAL             -8.449639\nAVYRSEXP_AVSAL             -4.059162\nPERSPEN_PTRATIO             6.321099\nPERSPEN_PCTAF               6.574347\nPTRATIO_PCTAF               5.362290\nPERMINTE_AVYRSEXP_AVSAL     7.428064\nPERSPEN_PTRATIO_PCTAF      -6.445137\nconst                       1.913012\ndtype: float64",
            "code"
        ],
        [
            "First differences: We hold all explanatory variables constant at their means and manipulate the percentage of low income households to assess its impact on the response variables:",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "means = data.exog.mean(axis=0)\nmeans25 = means.copy()\nmeans25.iloc[0] = stats.scoreatpercentile(data.exog.iloc[:,0], 25)\nmeans75 = means.copy()\nmeans75.iloc[0] = lowinc_75per = stats.scoreatpercentile(data.exog.iloc[:,0], 75)\nresp_25 = res.predict(means25)\nresp_75 = res.predict(means75)\ndiff = resp_75 - resp_25",
            "code"
        ],
        [
            "The interquartile first difference for the percentage of low income households in a school district is:",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "print(\"%2.4f%%\" % (diff*100))",
            "code"
        ],
        [
            "-11.8753%",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Plots": [
        [
            "We extract information that will be used to draw some interesting plots:",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "nobs = res.nobs\ny = data.endog.iloc[:,0]/data.endog.sum(1)\nyhat = res.mu",
            "code"
        ],
        [
            "Plot yhat vs y:",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "from statsmodels.graphics.api import abline_plot",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots()\nax.scatter(yhat, y)\nline_fit = sm.OLS(y, sm.add_constant(yhat, prepend=True)).fit()\nabline_plot(model_results=line_fit, ax=ax)\n\n\nax.set_title('Model Fit Plot')\nax.set_ylabel('Observed values')\nax.set_xlabel('Fitted values');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_glm_23_0.png\" src=\"../../../_images/examples_notebooks_generated_glm_23_0.png\"/>",
            "code"
        ],
        [
            "Plot yhat vs.\u00a0Pearson residuals:",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots()\n\nax.scatter(yhat, res.resid_pearson)\nax.hlines(0, 0, 1)\nax.set_xlim(0, 1)\nax.set_title('Residual Dependence Plot')\nax.set_ylabel('Pearson Residuals')\nax.set_xlabel('Fitted values')",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "Text(0.5, 0, 'Fitted values')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_glm_25_1.png\" src=\"../../../_images/examples_notebooks_generated_glm_25_1.png\"/>",
            "code"
        ],
        [
            "Histogram of standardized deviance residuals:",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "from scipy import stats\n\nfig, ax = plt.subplots()\n\nresid = res.resid_deviance.copy()\nresid_std = stats.zscore(resid)\nax.hist(resid_std, bins=25)\nax.set_title('Histogram of standardized deviance residuals');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_glm_27_0.png\" src=\"../../../_images/examples_notebooks_generated_glm_27_0.png\"/>",
            "code"
        ],
        [
            "QQ Plot of Deviance Residuals:",
            "markdown"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "from statsmodels import graphics\ngraphics.gofplots.qqplot(resid, line='r')",
            "code"
        ],
        [
            "[16]:\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_glm_29_0.png\" src=\"../../../_images/examples_notebooks_generated_glm_29_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_glm_29_1.png\" src=\"../../../_images/examples_notebooks_generated_glm_29_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Gamma for proportional count response->Load Scottish Parliament Voting data": [
        [
            "In the example above, we printed the NOTE attribute to learn about the Star98 dataset. statsmodels datasets ships with other useful information. For example:",
            "markdown"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "print(sm.datasets.scotland.DESCRLONG)",
            "code"
        ],
        [
            "This data is based on the example in Gill and describes the proportion of\nvoters who voted Yes to grant the Scottish Parliament taxation powers.\nThe data are divided into 32 council districts.  This example's explanatory\nvariables include the amount of council tax collected in pounds sterling as\nof April 1997 per two adults before adjustments, the female percentage of\ntotal claims for unemployment benefits as of January, 1998, the standardized\nmortality rate (UK is 100), the percentage of labor force participation,\nregional GDP, the percentage of children aged 5 to 15, and an interaction term\nbetween female unemployment and the council tax.\n\nThe original source files and variable information are included in\n/scotland/src/",
            "code"
        ],
        [
            "Load the data and add a constant to the exogenous variables:",
            "markdown"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "data2 = sm.datasets.scotland.load()\ndata2.exog = sm.add_constant(data2.exog, prepend=False)\nprint(data2.exog.head())\nprint(data2.endog.head())",
            "code"
        ],
        [
            "COUTAX  UNEMPF    MOR   ACT      GDP   AGE  COUTAX_FEMALEUNEMP  const\n0   712.0    21.0  105.0  82.4  13566.0  12.3             14952.0    1.0\n1   643.0    26.5   97.0  80.2  13566.0  15.3             17039.5    1.0\n2   679.0    28.3  113.0  86.3   9611.0  13.9             19215.7    1.0\n3   801.0    27.1  109.0  80.4   9483.0  13.6             21707.1    1.0\n4   753.0    22.0  115.0  64.7   9265.0  14.6             16566.0    1.0\n0    60.3\n1    52.3\n2    53.4\n3    57.0\n4    68.7\nName: YES, dtype: float64",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Gamma for proportional count response->Model Fit and summary": [
        [
            "[19]:",
            "code"
        ],
        [
            "glm_gamma = sm.GLM(data2.endog, data2.exog, family=sm.families.Gamma(sm.families.links.log()))\nglm_results = glm_gamma.fit()\nprint(glm_results.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:                    YES   No. Observations:                   32\nModel:                            GLM   Df Residuals:                       24\nModel Family:                   Gamma   Df Model:                            7\nLink Function:                    log   Scale:                       0.0035927\nMethod:                          IRLS   Log-Likelihood:                -83.110\nDate:                Wed, 02 Nov 2022   Deviance:                     0.087988\nTime:                        17:05:03   Pearson chi2:                   0.0862\nNo. Iterations:                     7   Pseudo R-squ. (CS):             0.9797\nCovariance Type:            nonrobust\n======================================================================================\n                         coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nCOUTAX                -0.0024      0.001     -2.466      0.014      -0.004      -0.000\nUNEMPF                -0.1005      0.031     -3.269      0.001      -0.161      -0.040\nMOR                    0.0048      0.002      2.946      0.003       0.002       0.008\nACT                   -0.0067      0.003     -2.534      0.011      -0.012      -0.002\nGDP                 8.173e-06   7.19e-06      1.136      0.256   -5.93e-06    2.23e-05\nAGE                    0.0298      0.015      2.009      0.045       0.001       0.059\nCOUTAX_FEMALEUNEMP     0.0001   4.33e-05      2.724      0.006    3.31e-05       0.000\nconst                  5.6581      0.680      8.318      0.000       4.325       6.991\n======================================================================================",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Gaussian distribution with a noncanonical link->Artificial data": [
        [
            "[20]:",
            "code"
        ],
        [
            "nobs2 = 100\nx = np.arange(nobs2)\nnp.random.seed(54321)\nX = np.column_stack((x,x**2))\nX = sm.add_constant(X, prepend=False)\nlny = np.exp(-(.03*x + .0001*x**2 - 1.0)) + .001 * np.random.rand(nobs2)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Gaussian distribution with a noncanonical link->Fit and summary (artificial data)": [
        [
            "[21]:",
            "code"
        ],
        [
            "gauss_log = sm.GLM(lny, X, family=sm.families.Gaussian(sm.families.links.log()))\ngauss_log_results = gauss_log.fit()\nprint(gauss_log_results.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                  100\nModel:                            GLM   Df Residuals:                       97\nModel Family:                Gaussian   Df Model:                            2\nLink Function:                    log   Scale:                      1.0531e-07\nMethod:                          IRLS   Log-Likelihood:                 662.92\nDate:                Wed, 02 Nov 2022   Deviance:                   1.0215e-05\nTime:                        17:05:03   Pearson chi2:                 1.02e-05\nNo. Iterations:                     7   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1            -0.0300    5.6e-06  -5361.316      0.000      -0.030      -0.030\nx2         -9.939e-05   1.05e-07   -951.091      0.000   -9.96e-05   -9.92e-05\nconst          1.0003   5.39e-05   1.86e+04      0.000       1.000       1.000\n==============================================================================",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Using Formulas with GLMs": [
        [
            "This notebook illustrates how you can use R-style formulas to fit Generalized Linear Models.",
            "markdown"
        ],
        [
            "To begin, we load the Star98 dataset and we construct a formula and pre-process the data:",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nstar98 = sm.datasets.star98.load_pandas().data\nformula = \"SUCCESS ~ LOWINC + PERASIAN + PERBLACK + PERHISP + PCTCHRT + \\\n           PCTYRRND + PERMINTE*AVYRSEXP*AVSALK + PERSPENK*PTRATIO*PCTAF\"\ndta = star98[\n    [\n        \"NABOVE\",\n        \"NBELOW\",\n        \"LOWINC\",\n        \"PERASIAN\",\n        \"PERBLACK\",\n        \"PERHISP\",\n        \"PCTCHRT\",\n        \"PCTYRRND\",\n        \"PERMINTE\",\n        \"AVYRSEXP\",\n        \"AVSALK\",\n        \"PERSPENK\",\n        \"PTRATIO\",\n        \"PCTAF\",\n    ]\n].copy()\nendog = dta[\"NABOVE\"] / (dta[\"NABOVE\"] + dta.pop(\"NBELOW\"))\ndel dta[\"NABOVE\"]\ndta[\"SUCCESS\"] = endog",
            "code"
        ],
        [
            "Then, we fit the GLM model:",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "mod1 = smf.glm(formula=formula, data=dta, family=sm.families.Binomial()).fit()\nprint(mod1.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:                SUCCESS   No. Observations:                  303\nModel:                            GLM   Df Residuals:                      282\nModel Family:                Binomial   Df Model:                           20\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -127.33\nDate:                Wed, 02 Nov 2022   Deviance:                       8.5477\nTime:                        17:02:03   Pearson chi2:                     8.48\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.1115\nCovariance Type:            nonrobust\n============================================================================================\n                               coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------------\nIntercept                    0.4037     25.036      0.016      0.987     -48.665      49.472\nLOWINC                      -0.0204      0.010     -1.982      0.048      -0.041      -0.000\nPERASIAN                     0.0159      0.017      0.910      0.363      -0.018       0.050\nPERBLACK                    -0.0198      0.020     -1.004      0.316      -0.058       0.019\nPERHISP                     -0.0096      0.010     -0.951      0.341      -0.029       0.010\nPCTCHRT                     -0.0022      0.022     -0.103      0.918      -0.045       0.040\nPCTYRRND                    -0.0022      0.006     -0.348      0.728      -0.014       0.010\nPERMINTE                     0.1068      0.787      0.136      0.892      -1.436       1.650\nAVYRSEXP                    -0.0411      1.176     -0.035      0.972      -2.346       2.264\nPERMINTE:AVYRSEXP           -0.0031      0.054     -0.057      0.954      -0.108       0.102\nAVSALK                       0.0131      0.295      0.044      0.965      -0.566       0.592\nPERMINTE:AVSALK             -0.0019      0.013     -0.145      0.885      -0.028       0.024\nAVYRSEXP:AVSALK              0.0008      0.020      0.038      0.970      -0.039       0.041\nPERMINTE:AVYRSEXP:AVSALK  5.978e-05      0.001      0.068      0.946      -0.002       0.002\nPERSPENK                    -0.3097      4.233     -0.073      0.942      -8.606       7.987\nPTRATIO                      0.0096      0.919      0.010      0.992      -1.792       1.811\nPERSPENK:PTRATIO             0.0066      0.206      0.032      0.974      -0.397       0.410\nPCTAF                       -0.0143      0.474     -0.030      0.976      -0.944       0.916\nPERSPENK:PCTAF               0.0105      0.098      0.107      0.915      -0.182       0.203\nPTRATIO:PCTAF               -0.0001      0.022     -0.005      0.996      -0.044       0.044\nPERSPENK:PTRATIO:PCTAF      -0.0002      0.005     -0.051      0.959      -0.010       0.009\n============================================================================================",
            "code"
        ],
        [
            "Finally, we define a function to operate customized data transformation using the formula framework:",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "def double_it(x):\n    return 2 * x\n\n\nformula = \"SUCCESS ~ double_it(LOWINC) + PERASIAN + PERBLACK + PERHISP + PCTCHRT + \\\n           PCTYRRND + PERMINTE*AVYRSEXP*AVSALK + PERSPENK*PTRATIO*PCTAF\"\nmod2 = smf.glm(formula=formula, data=dta, family=sm.families.Binomial()).fit()\nprint(mod2.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:                SUCCESS   No. Observations:                  303\nModel:                            GLM   Df Residuals:                      282\nModel Family:                Binomial   Df Model:                           20\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -127.33\nDate:                Wed, 02 Nov 2022   Deviance:                       8.5477\nTime:                        17:02:03   Pearson chi2:                     8.48\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.1115\nCovariance Type:            nonrobust\n============================================================================================\n                               coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------------\nIntercept                    0.4037     25.036      0.016      0.987     -48.665      49.472\ndouble_it(LOWINC)           -0.0102      0.005     -1.982      0.048      -0.020      -0.000\nPERASIAN                     0.0159      0.017      0.910      0.363      -0.018       0.050\nPERBLACK                    -0.0198      0.020     -1.004      0.316      -0.058       0.019\nPERHISP                     -0.0096      0.010     -0.951      0.341      -0.029       0.010\nPCTCHRT                     -0.0022      0.022     -0.103      0.918      -0.045       0.040\nPCTYRRND                    -0.0022      0.006     -0.348      0.728      -0.014       0.010\nPERMINTE                     0.1068      0.787      0.136      0.892      -1.436       1.650\nAVYRSEXP                    -0.0411      1.176     -0.035      0.972      -2.346       2.264\nPERMINTE:AVYRSEXP           -0.0031      0.054     -0.057      0.954      -0.108       0.102\nAVSALK                       0.0131      0.295      0.044      0.965      -0.566       0.592\nPERMINTE:AVSALK             -0.0019      0.013     -0.145      0.885      -0.028       0.024\nAVYRSEXP:AVSALK              0.0008      0.020      0.038      0.970      -0.039       0.041\nPERMINTE:AVYRSEXP:AVSALK  5.978e-05      0.001      0.068      0.946      -0.002       0.002\nPERSPENK                    -0.3097      4.233     -0.073      0.942      -8.606       7.987\nPTRATIO                      0.0096      0.919      0.010      0.992      -1.792       1.811\nPERSPENK:PTRATIO             0.0066      0.206      0.032      0.974      -0.397       0.410\nPCTAF                       -0.0143      0.474     -0.030      0.976      -0.944       0.916\nPERSPENK:PCTAF               0.0105      0.098      0.107      0.915      -0.182       0.203\nPTRATIO:PCTAF               -0.0001      0.022     -0.005      0.996      -0.044       0.044\nPERSPENK:PTRATIO:PCTAF      -0.0002      0.005     -0.051      0.959      -0.010       0.009\n============================================================================================",
            "code"
        ],
        [
            "As expected, the coefficient for double_it(LOWINC) in the second model is half the size of the LOWINC coefficient from the first model:",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "print(mod1.params[1])\nprint(mod2.params[1] * 2)",
            "code"
        ],
        [
            "-0.02039598715475585\n-0.020395987154756844",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs": [
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data": [
        [
            "In this example, we\u2019ll use the affair dataset using a handful of exogenous variables to predict the extra-marital affair rate.",
            "markdown"
        ],
        [
            "Weights will be generated to show that freq_weights are equivalent to repeating records of data. On the other hand, var_weights is equivalent to aggregating data.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "print(sm.datasets.fair.NOTE)",
            "code"
        ],
        [
            "::\n\n    Number of observations: 6366\n    Number of variables: 9\n    Variable name definitions:\n\n        rate_marriage   : How rate marriage, 1 = very poor, 2 = poor, 3 = fair,\n                        4 = good, 5 = very good\n        age             : Age\n        yrs_married     : No. years married. Interval approximations. See\n                        original paper for detailed explanation.\n        children        : No. children\n        religious       : How relgious, 1 = not, 2 = mildly, 3 = fairly,\n                        4 = strongly\n        educ            : Level of education, 9 = grade school, 12 = high\n                        school, 14 = some college, 16 = college graduate,\n                        17 = some graduate school, 20 = advanced degree\n        occupation      : 1 = student, 2 = farming, agriculture; semi-skilled,\n                        or unskilled worker; 3 = white-colloar; 4 = teacher\n                        counselor social worker, nurse; artist, writers;\n                        technician, skilled worker, 5 = managerial,\n                        administrative, business, 6 = professional with\n                        advanced degree\n        occupation_husb : Husband's occupation. Same as occupation.\n        affairs         : measure of time spent in extramarital affairs\n\n    See the original paper for more details.",
            "code"
        ],
        [
            "Load the data into a pandas dataframe.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "data = sm.datasets.fair.load_pandas().data",
            "code"
        ],
        [
            "The dependent (endogenous) variable is affairs",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "data.describe()",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "data[:3]",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "In the following we will work mostly with Poisson. While using decimal affairs works, we convert them to integers to have a count distribution.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "data[\"affairs\"] = np.ceil(data[\"affairs\"])\ndata[:3]",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "(data[\"affairs\"] == 0).mean()",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "0.6775054979579014",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "np.bincount(data[\"affairs\"].astype(int))",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "array([4313,  934,  488,  180,  130,  172,    7,   21,   67,    2,    0,\n          0,   17,    0,    0,    0,    3,   12,    8,    0,    0,    0,\n          0,    0,    2,    2,    2,    3,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    1,    1,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    1])",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations": [
        [
            "We have 6366 observations in our original dataset. When we consider only some selected variables, then we have fewer unique observations. In the following we combine observations in two ways, first we combine observations that have values for all variables identical, and secondly we combine observations that have the same explanatory variables.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations->Dataset with unique observations": [
        [
            "We use pandas\u2019s groupby to combine identical observations and create a new variable freq that count how many observation have the values in the corresponding row.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "data2 = data.copy()\ndata2[\"const\"] = 1\ndc = (\n    data2[\"affairs rate_marriage age yrs_married const\".split()]\n    .groupby(\"affairs rate_marriage age yrs_married\".split())\n    .count()\n)\ndc.reset_index(inplace=True)\ndc.rename(columns={\"const\": \"freq\"}, inplace=True)\nprint(dc.shape)\ndc.head()",
            "code"
        ],
        [
            "(476, 5)",
            "code"
        ],
        [
            "[9]:",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations->Dataset with unique explanatory variables (exog)": [
        [
            "For the next dataset we combine observations that have the same values of the explanatory variables. However, because the response variable can differ among combined observations, we compute the mean and the sum of the response variable for all combined observations.",
            "markdown"
        ],
        [
            "We use again pandas groupby to combine observations and to create the new variables. We also flatten the MultiIndex into a simple index.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "gr = data[\"affairs rate_marriage age yrs_married\".split()].groupby(\n    \"rate_marriage age yrs_married\".split()\n)\ndf_a = gr.agg([\"mean\", \"sum\", \"count\"])\n\n\ndef merge_tuple(tpl):\n    if isinstance(tpl, tuple) and len(tpl)  1:\n        return \"_\".join(map(str, tpl))\n    else:\n        return tpl\n\n\ndf_a.columns = df_a.columns.map(merge_tuple)\ndf_a.reset_index(inplace=True)\nprint(df_a.shape)\ndf_a.head()",
            "code"
        ],
        [
            "(130, 6)",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "After combining observations with have a dataframe dc with 467 unique observations, and a dataframe df_a with 130 observations with unique values of the explanatory variables.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "print(\"number of rows: \\noriginal, with unique observations, with unique exog\")\ndata.shape[0], dc.shape[0], df_a.shape[0]",
            "code"
        ],
        [
            "number of rows:\noriginal, with unique observations, with unique exog",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "(6366, 476, 130)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis": [
        [
            "In the following, we compare the GLM-Poisson results of the original data with models of the combined observations where the multiplicity or aggregation is given by weights or exposure.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->original data": [
        [
            "[12]:",
            "code"
        ],
        [
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + age + yrs_married\",\n    data=data,\n    family=sm.families.Poisson(),\n)\nres_o = glm.fit()\nprint(res_o.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:                affairs   No. Observations:                 6366\nModel:                            GLM   Df Residuals:                     6362\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -10351.\nDate:                Wed, 02 Nov 2022   Deviance:                       15375.\nTime:                        17:04:57   Pearson chi2:                 3.23e+04\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.2420\nCovariance Type:            nonrobust\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         2.7155      0.107     25.294      0.000       2.505       2.926\nrate_marriage    -0.4952      0.012    -41.702      0.000      -0.518      -0.472\nage              -0.0299      0.004     -6.691      0.000      -0.039      -0.021\nyrs_married      -0.0108      0.004     -2.507      0.012      -0.019      -0.002\n=================================================================================",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "res_o.pearson_chi2 / res_o.df_resid",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "5.078702313363233",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->condensed data (unique observations with frequencies)": [
        [
            "Combining identical observations and using frequency weights to take into account the multiplicity of observations produces exactly the same results. Some results attribute will differ when we want to have information about the observation and not about the aggregate of all identical observations. For example, residuals do not take freq_weights into account.",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + age + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f = glm.fit()\nprint(res_f.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:                affairs   No. Observations:                  476\nModel:                            GLM   Df Residuals:                     6362\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -10351.\nDate:                Wed, 02 Nov 2022   Deviance:                       15375.\nTime:                        17:04:57   Pearson chi2:                 3.23e+04\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.9754\nCovariance Type:            nonrobust\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         2.7155      0.107     25.294      0.000       2.505       2.926\nrate_marriage    -0.4952      0.012    -41.702      0.000      -0.518      -0.472\nage              -0.0299      0.004     -6.691      0.000      -0.039      -0.021\nyrs_married      -0.0108      0.004     -2.507      0.012      -0.019      -0.002\n=================================================================================",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "res_f.pearson_chi2 / res_f.df_resid",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "5.0787023133632",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->condensed using var_weights instead of freq_weights": [
        [
            "Next, we compare var_weights to freq_weights. It is a common practice to incorporate var_weights when the endogenous variable reflects averages and not identical observations. I do not see a theoretical reason why it produces the same results (in general).",
            "markdown"
        ],
        [
            "This produces the same results but df_resid differs the freq_weights example because var_weights do not change the number of effective observations.",
            "markdown"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + age + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    var_weights=np.asarray(dc[\"freq\"]),\n)\nres_fv = glm.fit()\nprint(res_fv.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:                affairs   No. Observations:                  476\nModel:                            GLM   Df Residuals:                      472\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -10351.\nDate:                Wed, 02 Nov 2022   Deviance:                       15375.\nTime:                        17:04:57   Pearson chi2:                 3.23e+04\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.9754\nCovariance Type:            nonrobust\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         2.7155      0.107     25.294      0.000       2.505       2.926\nrate_marriage    -0.4952      0.012    -41.702      0.000      -0.518      -0.472\nage              -0.0299      0.004     -6.691      0.000      -0.039      -0.021\nyrs_married      -0.0108      0.004     -2.507      0.012      -0.019      -0.002\n=================================================================================",
            "code"
        ],
        [
            "Dispersion computed from the results is incorrect because of wrong df_resid. It is correct if we use the original df_resid.",
            "markdown"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "res_fv.pearson_chi2 / res_fv.df_resid, res_f.pearson_chi2 / res_f.df_resid",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "(68.45488160512008, 5.0787023133632)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->aggregated or averaged data (unique values of explanatory variables)": [
        [
            "For these cases we combine observations that have the same values of the explanatory variables. The corresponding response variable is either a sum or an average.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->aggregated or averaged data (unique values of explanatory variables)->using exposure": [
        [
            "If our dependent variable is the sum of the responses of all combined observations, then under the Poisson assumption the distribution remains the same but we have varying exposure given by the number of individuals that are represented by one aggregated observation.",
            "markdown"
        ],
        [
            "The parameter estimates and covariance of parameters are the same with the original data, but log-likelihood, deviance and Pearson chi-squared differ",
            "markdown"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "glm = smf.glm(\n    \"affairs_sum ~ rate_marriage + age + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    exposure=np.asarray(df_a[\"affairs_count\"]),\n)\nres_e = glm.fit()\nprint(res_e.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:            affairs_sum   No. Observations:                  130\nModel:                            GLM   Df Residuals:                      126\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -740.75\nDate:                Wed, 02 Nov 2022   Deviance:                       967.46\nTime:                        17:04:57   Pearson chi2:                     926.\nNo. Iterations:                     6   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         2.7155      0.107     25.294      0.000       2.505       2.926\nrate_marriage    -0.4952      0.012    -41.702      0.000      -0.518      -0.472\nage              -0.0299      0.004     -6.691      0.000      -0.039      -0.021\nyrs_married      -0.0108      0.004     -2.507      0.012      -0.019      -0.002\n=================================================================================",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "res_e.pearson_chi2 / res_e.df_resid",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "7.35078910917956",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->aggregated or averaged data (unique values of explanatory variables)->using var_weights": [
        [
            "We can also use the mean of all combined values of the dependent variable. In this case the variance will be related to the inverse of the total exposure reflected by one combined observation.",
            "markdown"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "glm = smf.glm(\n    \"affairs_mean ~ rate_marriage + age + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    var_weights=np.asarray(df_a[\"affairs_count\"]),\n)\nres_a = glm.fit()\nprint(res_a.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:           affairs_mean   No. Observations:                  130\nModel:                            GLM   Df Residuals:                      126\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -5954.2\nDate:                Wed, 02 Nov 2022   Deviance:                       967.46\nTime:                        17:04:57   Pearson chi2:                     926.\nNo. Iterations:                     5   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         2.7155      0.107     25.294      0.000       2.505       2.926\nrate_marriage    -0.4952      0.012    -41.702      0.000      -0.518      -0.472\nage              -0.0299      0.004     -6.691      0.000      -0.039      -0.021\nyrs_married      -0.0108      0.004     -2.507      0.012      -0.019      -0.002\n=================================================================================",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison": [
        [
            "We saw in the summary prints above that params and cov_params with associated Wald inference agree across versions. We summarize this in the following comparing individual results attributes across versions.",
            "markdown"
        ],
        [
            "Parameter estimates params, standard errors of the parameters bse and pvalues of the parameters for the tests that the parameters are zeros all agree. However, the likelihood and goodness-of-fit statistics, llf, deviance and pearson_chi2 only partially agree. Specifically, the aggregated version do not agree with the results using the original data.",
            "markdown"
        ],
        [
            "<strong>Warning</strong>: The behavior of llf, deviance and pearson_chi2 might still change in future versions.",
            "markdown"
        ],
        [
            "Both the sum and average of the response variable for unique values of the explanatory variables have a proper likelihood interpretation. However, this interpretation is not reflected in these three statistics. Computationally this might be due to missing adjustments when aggregated data is used. However, theoretically we can think in these cases, especially for var_weights of the misspecified case when likelihood analysis is inappropriate and the results should be interpreted as\nquasi-likelihood estimates. There is an ambiguity in the definition of var_weights because they can be used for averages with correctly specified likelihood as well as for variance adjustments in the quasi-likelihood case. We are currently not trying to match the likelihood specification. However, in the next section we show that likelihood ratio type tests still produce the same result for all aggregation versions when we assume that the underlying model is correctly specified.",
            "markdown"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "results_all = [res_o, res_f, res_e, res_a]\nnames = \"res_o res_f res_e res_a\".split()",
            "code"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "pd.concat([r.params for r in results_all], axis=1, keys=names)",
            "code"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "pd.concat([r.bse for r in results_all], axis=1, keys=names)",
            "code"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "pd.concat([r.pvalues for r in results_all], axis=1, keys=names)",
            "code"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "pd.DataFrame(\n    np.column_stack([[r.llf, r.deviance, r.pearson_chi2] for r in results_all]),\n    columns=names,\n    index=[\"llf\", \"deviance\", \"pearson chi2\"],\n)",
            "code"
        ],
        [
            "[25]:",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests": [
        [
            "We saw above that likelihood and related statistics do not agree between the aggregated and original, individual data. We illustrate in the following that likelihood ratio test and difference in deviance agree across versions, however Pearson chi-squared does not.",
            "markdown"
        ],
        [
            "As before: This is not sufficiently clear yet and could change.",
            "markdown"
        ],
        [
            "As a test case we drop the age variable and compute the likelihood ratio type statistics as difference between reduced or constrained and full or unconstrained model.",
            "markdown"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights": [
        [
            "[26]:",
            "code"
        ],
        [
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\", data=data, family=sm.families.Poisson()\n)\nres_o2 = glm.fit()\n# print(res_f2.summary())\nres_o2.pearson_chi2 - res_o.pearson_chi2, res_o2.deviance - res_o.deviance, res_o2.llf - res_o.llf",
            "code"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "(52.913431618813775, 45.726693322507344, -22.863346661253672)",
            "code"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "code"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "(52.91343161867189, 45.726693322505525, -22.863346661251853)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->aggregated data: exposure and var_weights": [
        [
            "Note: LR test agrees with original observations, pearson_chi2 differs and has the wrong sign.",
            "markdown"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "glm = smf.glm(\n    \"affairs_sum ~ rate_marriage + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    exposure=np.asarray(df_a[\"affairs_count\"]),\n)\nres_e2 = glm.fit()\nres_e2.pearson_chi2 - res_e.pearson_chi2, res_e2.deviance - res_e.deviance, res_e2.llf - res_e.llf",
            "code"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "(-31.61852752510731, 45.72669332250655, -22.863346661253217)",
            "code"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "glm = smf.glm(\n    \"affairs_mean ~ rate_marriage + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    var_weights=np.asarray(df_a[\"affairs_count\"]),\n)\nres_a2 = glm.fit()\nres_a2.pearson_chi2 - res_a.pearson_chi2, res_a2.deviance - res_a.deviance, res_a2.llf - res_a.llf",
            "code"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "(-31.618527525103218, 45.72669332250655, -22.863346661253672)",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Investigating Pearson chi-square statistic": [
        [
            "First, we do some sanity checks that there are no basic bugs in the computation of pearson_chi2 and resid_pearson.",
            "markdown"
        ],
        [
            "[30]:",
            "code"
        ],
        [
            "res_e2.pearson_chi2, res_e.pearson_chi2, (res_e2.resid_pearson ** 2).sum(), (\n    res_e.resid_pearson ** 2\n).sum()",
            "code"
        ],
        [
            "[30]:",
            "code"
        ],
        [
            "(894.5809002315173, 926.1994277566246, 894.5809002315173, 926.1994277566246)",
            "code"
        ],
        [
            "[31]:",
            "code"
        ],
        [
            "res_e._results.resid_response.mean(), res_e.model.family.variance(res_e.mu)[\n    :5\n], res_e.mu[:5]",
            "code"
        ],
        [
            "[31]:",
            "code"
        ],
        [
            "(1.617851152192228e-14,\n array([ 5.42753476, 46.42940306, 19.98971769, 38.50138978, 11.18341883]),\n array([ 5.42753476, 46.42940306, 19.98971769, 38.50138978, 11.18341883]))",
            "code"
        ],
        [
            "[32]:",
            "code"
        ],
        [
            "(res_e._results.resid_response ** 2 / res_e.model.family.variance(res_e.mu)).sum()",
            "code"
        ],
        [
            "[32]:",
            "code"
        ],
        [
            "926.1994277566246",
            "code"
        ],
        [
            "[33]:",
            "code"
        ],
        [
            "res_e2._results.resid_response.mean(), res_e2.model.family.variance(res_e2.mu)[\n    :5\n], res_e2.mu[:5]",
            "code"
        ],
        [
            "[33]:",
            "code"
        ],
        [
            "(9.9257354472334e-14,\n array([ 4.77165474, 44.4026604 , 22.2013302 , 39.14749309, 10.54229538]),\n array([ 4.77165474, 44.4026604 , 22.2013302 , 39.14749309, 10.54229538]))",
            "code"
        ],
        [
            "[34]:",
            "code"
        ],
        [
            "(res_e2._results.resid_response ** 2 / res_e2.model.family.variance(res_e2.mu)).sum()",
            "code"
        ],
        [
            "[34]:",
            "code"
        ],
        [
            "894.5809002315173",
            "code"
        ],
        [
            "[35]:",
            "code"
        ],
        [
            "(res_e2._results.resid_response ** 2).sum(), (res_e._results.resid_response ** 2).sum()",
            "code"
        ],
        [
            "[35]:",
            "code"
        ],
        [
            "(51204.8573783234, 47104.647795959645)",
            "code"
        ],
        [
            "One possible reason for the incorrect sign is that we are subtracting quadratic terms that are divided by different denominators. In some related cases, the recommendation in the literature is to use a common denominator. We can compare pearson chi-squared statistic using the same variance assumption in the full and reduced model.",
            "markdown"
        ],
        [
            "In this case we obtain the same pearson chi2 scaled difference between reduced and full model across all versions. (Issue  is intended to track this further.)",
            "markdown"
        ],
        [
            "[36]:",
            "code"
        ],
        [
            "(\n    (res_e2._results.resid_response ** 2 - res_e._results.resid_response ** 2)\n    / res_e2.model.family.variance(res_e2.mu)\n).sum()",
            "code"
        ],
        [
            "[36]:",
            "code"
        ],
        [
            "44.4331417512193",
            "code"
        ],
        [
            "[37]:",
            "code"
        ],
        [
            "(\n    (res_a2._results.resid_response ** 2 - res_a._results.resid_response ** 2)\n    / res_a2.model.family.variance(res_a2.mu)\n    * res_a2.model.var_weights\n).sum()",
            "code"
        ],
        [
            "[37]:",
            "code"
        ],
        [
            "44.43314175121901",
            "code"
        ],
        [
            "[38]:",
            "code"
        ],
        [
            "(\n    (res_f2._results.resid_response ** 2 - res_f._results.resid_response ** 2)\n    / res_f2.model.family.variance(res_f2.mu)\n    * res_f2.model.freq_weights\n).sum()",
            "code"
        ],
        [
            "[38]:",
            "code"
        ],
        [
            "44.43314175121954",
            "code"
        ],
        [
            "[39]:",
            "code"
        ],
        [
            "(\n    (res_o2._results.resid_response ** 2 - res_o._results.resid_response ** 2)\n    / res_o2.model.family.variance(res_o2.mu)\n).sum()",
            "code"
        ],
        [
            "[39]:",
            "code"
        ],
        [
            "44.43314175121988",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Weighting Observations with GLMs->Remainder": [
        [
            "The remainder of the notebook just contains some additional checks and can be ignored.",
            "markdown"
        ],
        [
            "[40]:",
            "code"
        ],
        [
            "np.exp(res_e2.model.exposure)[:5], np.asarray(df_a[\"affairs_count\"])[:5]",
            "code"
        ],
        [
            "[40]:",
            "code"
        ],
        [
            "(array([ 1., 10.,  5., 10.,  3.]), array([ 1, 10,  5, 10,  3]))",
            "code"
        ],
        [
            "[41]:",
            "code"
        ],
        [
            "res_e2.resid_pearson.sum() - res_e.resid_pearson.sum()",
            "code"
        ],
        [
            "[41]:",
            "code"
        ],
        [
            "-9.664817945861138",
            "code"
        ],
        [
            "[42]:",
            "code"
        ],
        [
            "res_e2.mu[:5]",
            "code"
        ],
        [
            "[42]:",
            "code"
        ],
        [
            "array([ 4.77165474, 44.4026604 , 22.2013302 , 39.14749309, 10.54229538])",
            "code"
        ],
        [
            "[43]:",
            "code"
        ],
        [
            "res_a2.pearson_chi2, res_a.pearson_chi2, res_a2.resid_pearson.sum(), res_a.resid_pearson.sum()",
            "code"
        ],
        [
            "[43]:",
            "code"
        ],
        [
            "(894.5809002315159, 926.1994277566191, -42.347207135187446, -32.68238918932903)",
            "code"
        ],
        [
            "[44]:",
            "code"
        ],
        [
            "(\n    (res_a2._results.resid_response ** 2)\n    / res_a2.model.family.variance(res_a2.mu)\n    * res_a2.model.var_weights\n).sum()",
            "code"
        ],
        [
            "[44]:",
            "code"
        ],
        [
            "894.5809002315159",
            "code"
        ],
        [
            "[45]:",
            "code"
        ],
        [
            "(\n    (res_a._results.resid_response ** 2)\n    / res_a.model.family.variance(res_a.mu)\n    * res_a.model.var_weights\n).sum()",
            "code"
        ],
        [
            "[45]:",
            "code"
        ],
        [
            "926.1994277566191",
            "code"
        ],
        [
            "[46]:",
            "code"
        ],
        [
            "(\n    (res_a._results.resid_response ** 2)\n    / res_a.model.family.variance(res_a2.mu)\n    * res_a.model.var_weights\n).sum()",
            "code"
        ],
        [
            "[46]:",
            "code"
        ],
        [
            "850.1477584802968",
            "code"
        ],
        [
            "[47]:",
            "code"
        ],
        [
            "res_e.model.endog[:5], res_e2.model.endog[:5]",
            "code"
        ],
        [
            "[47]:",
            "code"
        ],
        [
            "(array([ 0., 39., 17.,  9.,  4.]), array([ 0., 39., 17.,  9.,  4.]))",
            "code"
        ],
        [
            "[48]:",
            "code"
        ],
        [
            "res_a.model.endog[:5], res_a2.model.endog[:5]",
            "code"
        ],
        [
            "[48]:",
            "code"
        ],
        [
            "(array([0.        , 3.9       , 3.4       , 0.9       , 1.33333333]),\n array([0.        , 3.9       , 3.4       , 0.9       , 1.33333333]))",
            "code"
        ],
        [
            "[49]:",
            "code"
        ],
        [
            "res_a2.model.endog[:5] * np.exp(res_e2.model.exposure)[:5]",
            "code"
        ],
        [
            "[49]:",
            "code"
        ],
        [
            "array([ 0., 39., 17.,  9.,  4.])",
            "code"
        ],
        [
            "[50]:",
            "code"
        ],
        [
            "res_a2.model.endog[:5] * res_a2.model.var_weights[:5]",
            "code"
        ],
        [
            "[50]:",
            "code"
        ],
        [
            "array([ 0., 39., 17.,  9.,  4.])",
            "code"
        ],
        [
            "[51]:",
            "code"
        ],
        [
            "from scipy import stats\n\nstats.chi2.sf(27.19530754604785, 1), stats.chi2.sf(29.083798806764687, 1)",
            "code"
        ],
        [
            "[51]:",
            "code"
        ],
        [
            "(1.8390448369994542e-07, 6.931421143170174e-08)",
            "code"
        ],
        [
            "[52]:",
            "code"
        ],
        [
            "res_o.pvalues",
            "code"
        ],
        [
            "[52]:",
            "code"
        ],
        [
            "Intercept        3.756282e-141\nrate_marriage     0.000000e+00\nage               2.221918e-11\nyrs_married       1.219200e-02\ndtype: float64",
            "code"
        ],
        [
            "[53]:",
            "code"
        ],
        [
            "print(res_e2.summary())\nprint(res_e.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:            affairs_sum   No. Observations:                  130\nModel:                            GLM   Df Residuals:                      127\nModel Family:                 Poisson   Df Model:                            2\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -763.61\nDate:                Wed, 02 Nov 2022   Deviance:                       1013.2\nTime:                        17:04:57   Pearson chi2:                     895.\nNo. Iterations:                     6   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         2.0754      0.050     41.512      0.000       1.977       2.173\nrate_marriage    -0.4947      0.012    -41.743      0.000      -0.518      -0.471\nyrs_married      -0.0360      0.002    -17.542      0.000      -0.040      -0.032\n=================================================================================\n                 Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:            affairs_sum   No. Observations:                  130\nModel:                            GLM   Df Residuals:                      126\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -740.75\nDate:                Wed, 02 Nov 2022   Deviance:                       967.46\nTime:                        17:04:57   Pearson chi2:                     926.\nNo. Iterations:                     6   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         2.7155      0.107     25.294      0.000       2.505       2.926\nrate_marriage    -0.4952      0.012    -41.702      0.000      -0.518      -0.472\nage              -0.0299      0.004     -6.691      0.000      -0.039      -0.021\nyrs_married      -0.0108      0.004     -2.507      0.012      -0.019      -0.002\n=================================================================================",
            "code"
        ],
        [
            "[54]:",
            "code"
        ],
        [
            "print(res_f2.summary())\nprint(res_f.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:                affairs   No. Observations:                  476\nModel:                            GLM   Df Residuals:                     6363\nModel Family:                 Poisson   Df Model:                            2\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -10374.\nDate:                Wed, 02 Nov 2022   Deviance:                       15420.\nTime:                        17:04:57   Pearson chi2:                 3.24e+04\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.9729\nCovariance Type:            nonrobust\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         2.0754      0.050     41.512      0.000       1.977       2.173\nrate_marriage    -0.4947      0.012    -41.743      0.000      -0.518      -0.471\nyrs_married      -0.0360      0.002    -17.542      0.000      -0.040      -0.032\n=================================================================================\n                 Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:                affairs   No. Observations:                  476\nModel:                            GLM   Df Residuals:                     6362\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -10351.\nDate:                Wed, 02 Nov 2022   Deviance:                       15375.\nTime:                        17:04:57   Pearson chi2:                 3.23e+04\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.9754\nCovariance Type:            nonrobust\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         2.7155      0.107     25.294      0.000       2.505       2.926\nrate_marriage    -0.4952      0.012    -41.702      0.000      -0.518      -0.472\nage              -0.0299      0.004     -6.691      0.000      -0.039      -0.021\nyrs_married      -0.0108      0.004     -2.507      0.012      -0.019      -0.002\n=================================================================================",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Influence Measures for GLMs": [
        [
            "Based on draft version for GLMInfluence, which will also apply to discrete Logit, Probit and Poisson, and eventually be extended to cover most models outside of time series analysis.",
            "markdown"
        ],
        [
            "The example for logistic regression was used by Pregibon (1981) \u201cLogistic Regression diagnostics\u201d and is based on data by Finney (1947).",
            "markdown"
        ],
        [
            "GLMInfluence includes the basic influence measures but still misses some measures described in Pregibon (1981), for example those related to deviance and effects on confidence intervals.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import os.path\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod import families\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import statsmodels.stats.tests.test_influence\n\ntest_module = statsmodels.stats.tests.test_influence.__file__\ncur_dir = cur_dir = os.path.abspath(os.path.dirname(test_module))\n\nfile_name = \"binary_constrict.csv\"\nfile_path = os.path.join(cur_dir, \"results\", file_name)\ndf = pd.read_csv(file_path, index_col=0)",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "res = GLM(\n    df[\"constrict\"],\n    df[[\"const\", \"log_rate\", \"log_volumne\"]],\n    family=families.Binomial(),\n).fit(attach_wls=True, atol=1e-10)\nprint(res.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:              constrict   No. Observations:                   39\nModel:                            GLM   Df Residuals:                       36\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -14.614\nDate:                Wed, 02 Nov 2022   Deviance:                       29.227\nTime:                        17:10:15   Pearson chi2:                     34.2\nNo. Iterations:                     7   Pseudo R-squ. (CS):             0.4707\nCovariance Type:            nonrobust\n===============================================================================\n                  coef    std err          z      P|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst          -2.8754      1.321     -2.177      0.029      -5.464      -0.287\nlog_rate        4.5617      1.838      2.482      0.013       0.959       8.164\nlog_volumne     5.1793      1.865      2.777      0.005       1.524       8.834\n===============================================================================",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures": [
        [
            "GLMResults has a get_influence method similar to OLSResults, that returns and instance of the GLMInfluence class. This class has methods and (cached) attributes to inspect influence and outlier measures.",
            "markdown"
        ],
        [
            "This measures are based on a one-step approximation to the the results for deleting one observation. One-step approximations are usually accurate for small changes but underestimate the magnitude of large changes. Event though large changes are underestimated, they still show clearly the effect of influential observations",
            "markdown"
        ],
        [
            "In this example observation 4 and 18 have a large standardized residual and large Cook\u2019s distance, but not a large leverage. Observation 13 has the largest leverage but only small Cook\u2019s distance and not a large studentized residual.",
            "markdown"
        ],
        [
            "Only the two observations 4 and 18 have a large impact on the parameter estimates.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "infl = res.get_influence(observed=False)",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "fig = infl.plot_influence()\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_influence_glm_logit_7_0.png\" src=\"../../../_images/examples_notebooks_generated_influence_glm_logit_7_0.png\"/>",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "fig = infl.plot_index(y_var=\"cooks\", threshold=2 * infl.cooks_distance[0].mean())\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_influence_glm_logit_8_0.png\" src=\"../../../_images/examples_notebooks_generated_influence_glm_logit_8_0.png\"/>",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "fig = infl.plot_index(y_var=\"resid\", threshold=1)\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_influence_glm_logit_9_0.png\" src=\"../../../_images/examples_notebooks_generated_influence_glm_logit_9_0.png\"/>",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "fig = infl.plot_index(y_var=\"dfbeta\", idx=1, threshold=0.5)\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_influence_glm_logit_10_0.png\" src=\"../../../_images/examples_notebooks_generated_influence_glm_logit_10_0.png\"/>",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "fig = infl.plot_index(y_var=\"dfbeta\", idx=2, threshold=0.5)\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_influence_glm_logit_11_0.png\" src=\"../../../_images/examples_notebooks_generated_influence_glm_logit_11_0.png\"/>",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "fig = infl.plot_index(y_var=\"dfbeta\", idx=0, threshold=0.5)\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_influence_glm_logit_12_0.png\" src=\"../../../_images/examples_notebooks_generated_influence_glm_logit_12_0.png\"/>",
            "code"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "",
            "code"
        ]
    ],
    "Examples->Generalized Linear Models->Quasi-binomial regression": [
        [
            "This notebook demonstrates using custom variance functions and non-binary data with the quasi-binomial GLM family to perform a regression analysis using a dependent variable that is a proportion.",
            "markdown"
        ],
        [
            "The notebook uses the barley leaf blotch data that has been discussed in several textbooks. See below for one reference:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO",
            "code"
        ],
        [
            "The raw data, expressed as percentages. We will divide by 100 to obtain proportions.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "raw = StringIO(\n    \"\"\"0.05,0.00,1.25,2.50,5.50,1.00,5.00,5.00,17.50\n0.00,0.05,1.25,0.50,1.00,5.00,0.10,10.00,25.00\n0.00,0.05,2.50,0.01,6.00,5.00,5.00,5.00,42.50\n0.10,0.30,16.60,3.00,1.10,5.00,5.00,5.00,50.00\n0.25,0.75,2.50,2.50,2.50,5.00,50.00,25.00,37.50\n0.05,0.30,2.50,0.01,8.00,5.00,10.00,75.00,95.00\n0.50,3.00,0.00,25.00,16.50,10.00,50.00,50.00,62.50\n1.30,7.50,20.00,55.00,29.50,5.00,25.00,75.00,95.00\n1.50,1.00,37.50,5.00,20.00,50.00,50.00,75.00,95.00\n1.50,12.70,26.25,40.00,43.50,75.00,75.00,75.00,95.00\"\"\"\n)",
            "code"
        ],
        [
            "The regression model is a two-way additive model with site and variety effects. The data are a full unreplicated design with 10 rows (sites) and 9 columns (varieties).",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "df = pd.read_csv(raw, header=None)\ndf = df.melt()\ndf[\"site\"] = 1 + np.floor(df.index / 10).astype(int)\ndf[\"variety\"] = 1 + (df.index % 10)\ndf = df.rename(columns={\"value\": \"blotch\"})\ndf = df.drop(\"variable\", axis=1)\ndf[\"blotch\"] /= 100",
            "code"
        ],
        [
            "Fit the quasi-binomial regression with the standard variance function.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "model1 = sm.GLM.from_formula(\n    \"blotch ~ 0 + C(variety) + C(site)\", family=sm.families.Binomial(), data=df\n)\nresult1 = model1.fit(scale=\"X2\")\nprint(result1.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:                 blotch   No. Observations:                   90\nModel:                            GLM   Df Residuals:                       72\nModel Family:                Binomial   Df Model:                           17\nLink Function:                  Logit   Scale:                        0.088778\nMethod:                          IRLS   Log-Likelihood:                -20.791\nDate:                Wed, 02 Nov 2022   Deviance:                       6.1260\nTime:                        17:08:12   Pearson chi2:                     6.39\nNo. Iterations:                    10   Pseudo R-squ. (CS):             0.3198\nCovariance Type:            nonrobust\n==================================================================================\n                     coef    std err          z      P|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nC(variety)[1]     -8.0546      1.422     -5.664      0.000     -10.842      -5.268\nC(variety)[2]     -7.9046      1.412     -5.599      0.000     -10.672      -5.138\nC(variety)[3]     -7.3652      1.384     -5.321      0.000     -10.078      -4.652\nC(variety)[4]     -7.0065      1.372     -5.109      0.000      -9.695      -4.318\nC(variety)[5]     -6.4399      1.357     -4.746      0.000      -9.100      -3.780\nC(variety)[6]     -5.6835      1.344     -4.230      0.000      -8.317      -3.050\nC(variety)[7]     -5.4841      1.341     -4.090      0.000      -8.112      -2.856\nC(variety)[8]     -4.7126      1.331     -3.539      0.000      -7.322      -2.103\nC(variety)[9]     -4.5546      1.330     -3.425      0.001      -7.161      -1.948\nC(variety)[10]    -3.8016      1.320     -2.881      0.004      -6.388      -1.215\nC(site)[T.2]       1.6391      1.443      1.136      0.256      -1.190       4.468\nC(site)[T.3]       3.3265      1.349      2.466      0.014       0.682       5.971\nC(site)[T.4]       3.5822      1.344      2.664      0.008       0.947       6.217\nC(site)[T.5]       3.5831      1.344      2.665      0.008       0.948       6.218\nC(site)[T.6]       3.8933      1.340      2.905      0.004       1.266       6.520\nC(site)[T.7]       4.7300      1.335      3.544      0.000       2.114       7.346\nC(site)[T.8]       5.5227      1.335      4.138      0.000       2.907       8.139\nC(site)[T.9]       6.7946      1.341      5.068      0.000       4.167       9.422\n==================================================================================",
            "code"
        ],
        [
            "The plot below shows that the default variance function is not capturing the variance structure very well. Also note that the scale parameter estimate is quite small.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "plt.clf()\nplt.grid(True)\nplt.plot(result1.predict(linear=True), result1.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "Text(0, 0.5, 'Residual')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_quasibinomial_9_1.png\" src=\"../../../_images/examples_notebooks_generated_quasibinomial_9_1.png\"/>",
            "code"
        ],
        [
            "An alternative variance function is mu^2 * (1 - mu)^2.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "class vf(sm.families.varfuncs.VarianceFunction):\n    def __call__(self, mu):\n        return mu ** 2 * (1 - mu) ** 2\n\n    def deriv(self, mu):\n        return 2 * mu - 6 * mu ** 2 + 4 * mu ** 3",
            "code"
        ],
        [
            "Fit the quasi-binomial regression with the alternative variance function.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "bin = sm.families.Binomial()\nbin.variance = vf()\nmodel2 = sm.GLM.from_formula(\"blotch ~ 0 + C(variety) + C(site)\", family=bin, data=df)\nresult2 = model2.fit(scale=\"X2\")\nprint(result2.summary())",
            "code"
        ],
        [
            "Generalized Linear Model Regression Results\n==============================================================================\nDep. Variable:                 blotch   No. Observations:                   90\nModel:                            GLM   Df Residuals:                       72\nModel Family:                Binomial   Df Model:                           17\nLink Function:                  Logit   Scale:                         0.98855\nMethod:                          IRLS   Log-Likelihood:                -21.335\nDate:                Wed, 02 Nov 2022   Deviance:                       7.2134\nTime:                        17:08:12   Pearson chi2:                     71.2\nNo. Iterations:                    25   Pseudo R-squ. (CS):             0.3115\nCovariance Type:            nonrobust\n==================================================================================\n                     coef    std err          z      P|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nC(variety)[1]     -7.9224      0.445    -17.817      0.000      -8.794      -7.051\nC(variety)[2]     -8.3897      0.445    -18.868      0.000      -9.261      -7.518\nC(variety)[3]     -7.8436      0.445    -17.640      0.000      -8.715      -6.972\nC(variety)[4]     -6.9683      0.445    -15.672      0.000      -7.840      -6.097\nC(variety)[5]     -6.5697      0.445    -14.775      0.000      -7.441      -5.698\nC(variety)[6]     -6.5938      0.445    -14.829      0.000      -7.465      -5.722\nC(variety)[7]     -5.5823      0.445    -12.555      0.000      -6.454      -4.711\nC(variety)[8]     -4.6598      0.445    -10.480      0.000      -5.531      -3.788\nC(variety)[9]     -4.7869      0.445    -10.766      0.000      -5.658      -3.915\nC(variety)[10]    -4.0351      0.445     -9.075      0.000      -4.907      -3.164\nC(site)[T.2]       1.3831      0.445      3.111      0.002       0.512       2.255\nC(site)[T.3]       3.8601      0.445      8.681      0.000       2.989       4.732\nC(site)[T.4]       3.5570      0.445      8.000      0.000       2.686       4.428\nC(site)[T.5]       4.1079      0.445      9.239      0.000       3.236       4.979\nC(site)[T.6]       4.3054      0.445      9.683      0.000       3.434       5.177\nC(site)[T.7]       4.9181      0.445     11.061      0.000       4.047       5.790\nC(site)[T.8]       5.6949      0.445     12.808      0.000       4.823       6.566\nC(site)[T.9]       7.0676      0.445     15.895      0.000       6.196       7.939\n==================================================================================",
            "code"
        ],
        [
            "With the alternative variance function, the mean/variance relationship seems to capture the data well, and the estimated scale parameter is close to 1.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "plt.clf()\nplt.grid(True)\nplt.plot(result2.predict(linear=True), result2.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "Text(0, 0.5, 'Residual')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_quasibinomial_15_1.png\" src=\"../../../_images/examples_notebooks_generated_quasibinomial_15_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Robust Regression->M-estimators for Robust Regression": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "from statsmodels.compat import lmap\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "code"
        ],
        [
            "An M-estimator minimizes the function\n\n\n\\[Q(e_i, \\rho) = \\sum_i~\\rho \\left (\\frac{e_i}{s}\\right )\\]",
            "markdown"
        ],
        [
            "where \\(\\rho\\) is a symmetric function of the residuals",
            "markdown"
        ],
        [
            "The effect of \\(\\rho\\) is to reduce the influence of outliers",
            "markdown"
        ],
        [
            "\\(s\\) is an estimate of scale.",
            "markdown"
        ],
        [
            "The robust estimates \\(\\hat{\\beta}\\) are computed by the iteratively re-weighted least squares algorithm",
            "markdown"
        ],
        [
            "We have several choices available for the weighting functions to be used",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "norms = sm.robust.norms",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax",
            "code"
        ]
    ],
    "Examples->Robust Regression->M-estimators for Robust Regression->Andrew\u2019s Wave": [
        [
            "[5]:",
            "code"
        ],
        [
            "help(norms.AndrewWave.weights)",
            "code"
        ],
        [
            "Help on function weights in module statsmodels.robust.norms:\n\nweights(self, z)\n    Andrew's wave weighting function for the IRLS algorithm\n\n    The psi function scaled by z\n\n    Parameters\n    ----------\n    z : array_like\n        1d array\n\n    Returns\n    -------\n    weights : ndarray\n        weights(z) = sin(z/a)/(z/a)     for \\|z\\| &lt;= a*pi\n\n        weights(z) = 0                  for \\|z\\|  a*pi",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "a = 1.339\nsupport = np.linspace(-np.pi * a, np.pi * a, 100)\nandrew = norms.AndrewWave(a=a)\nplot_weights(\n    support, andrew.weights, [\"$-\\pi*a$\", \"0\", \"$\\pi*a$\"], [-np.pi * a, 0, np.pi * a]\n)",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: \n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_9_1.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_9_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Robust Regression->M-estimators for Robust Regression->Hampel\u2019s 17A": [
        [
            "[7]:",
            "code"
        ],
        [
            "help(norms.Hampel.weights)",
            "code"
        ],
        [
            "Help on function weights in module statsmodels.robust.norms:\n\nweights(self, z)\n    Hampel weighting function for the IRLS algorithm\n\n    The psi function scaled by z\n\n    Parameters\n    ----------\n    z : array_like\n        1d array\n\n    Returns\n    -------\n    weights : ndarray\n        weights(z) = 1                            for \\|z\\| &lt;= a\n\n        weights(z) = a/\\|z\\|                        for a &lt; \\|z\\| &lt;= b\n\n        weights(z) = a*(c - \\|z\\|)/(\\|z\\|*(c-b))      for b &lt; \\|z\\| &lt;= c\n\n        weights(z) = 0                            for \\|z\\|  c",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "c = 8\nsupport = np.linspace(-3 * c, 3 * c, 1000)\nhampel = norms.Hampel(a=2.0, b=4.0, c=c)\nplot_weights(support, hampel.weights, [\"3*c\", \"0\", \"3*c\"], [-3 * c, 0, 3 * c])",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: \n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_12_1.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_12_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Robust Regression->M-estimators for Robust Regression->Huber\u2019s t": [
        [
            "[9]:",
            "code"
        ],
        [
            "help(norms.HuberT.weights)",
            "code"
        ],
        [
            "Help on function weights in module statsmodels.robust.norms:\n\nweights(self, z)\n    Huber's t weighting function for the IRLS algorithm\n\n    The psi function scaled by z\n\n    Parameters\n    ----------\n    z : array_like\n        1d array\n\n    Returns\n    -------\n    weights : ndarray\n        weights(z) = 1          for \\|z\\| &lt;= t\n\n        weights(z) = t/\\|z\\|      for \\|z\\|  t",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "t = 1.345\nsupport = np.linspace(-3 * t, 3 * t, 1000)\nhuber = norms.HuberT(t=t)\nplot_weights(support, huber.weights, [\"-3*t\", \"0\", \"3*t\"], [-3 * t, 0, 3 * t])",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: \n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_15_1.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_15_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Robust Regression->M-estimators for Robust Regression->Least Squares": [
        [
            "[11]:",
            "code"
        ],
        [
            "help(norms.LeastSquares.weights)",
            "code"
        ],
        [
            "Help on function weights in module statsmodels.robust.norms:\n\nweights(self, z)\n    The least squares estimator weighting function for the IRLS algorithm.\n\n    The psi function scaled by the input z\n\n    Parameters\n    ----------\n    z : array_like\n        1d array\n\n    Returns\n    -------\n    weights : ndarray\n        weights(z) = np.ones(z.shape)",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "support = np.linspace(-3, 3, 1000)\nlst_sq = norms.LeastSquares()\nplot_weights(support, lst_sq.weights, [\"-3\", \"0\", \"3\"], [-3, 0, 3])",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: \n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_18_1.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_18_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Robust Regression->M-estimators for Robust Regression->Ramsay\u2019s Ea": [
        [
            "[13]:",
            "code"
        ],
        [
            "help(norms.RamsayE.weights)",
            "code"
        ],
        [
            "Help on function weights in module statsmodels.robust.norms:\n\nweights(self, z)\n    Ramsay's Ea weighting function for the IRLS algorithm\n\n    The psi function scaled by z\n\n    Parameters\n    ----------\n    z : array_like\n        1d array\n\n    Returns\n    -------\n    weights : ndarray\n        weights(z) = exp(-a*\\|z\\|)",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "a = 0.3\nsupport = np.linspace(-3 * a, 3 * a, 1000)\nramsay = norms.RamsayE(a=a)\nplot_weights(support, ramsay.weights, [\"-3*a\", \"0\", \"3*a\"], [-3 * a, 0, 3 * a])",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: \n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_21_1.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_21_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Robust Regression->M-estimators for Robust Regression->Trimmed Mean": [
        [
            "[15]:",
            "code"
        ],
        [
            "help(norms.TrimmedMean.weights)",
            "code"
        ],
        [
            "Help on function weights in module statsmodels.robust.norms:\n\nweights(self, z)\n    Least trimmed mean weighting function for the IRLS algorithm\n\n    The psi function scaled by z\n\n    Parameters\n    ----------\n    z : array_like\n        1d array\n\n    Returns\n    -------\n    weights : ndarray\n        weights(z) = 1             for \\|z\\| &lt;= c\n\n        weights(z) = 0             for \\|z\\|  c",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "c = 2\nsupport = np.linspace(-3 * c, 3 * c, 1000)\ntrimmed = norms.TrimmedMean(c=c)\nplot_weights(support, trimmed.weights, [\"-3*c\", \"0\", \"3*c\"], [-3 * c, 0, 3 * c])",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: \n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_24_1.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_24_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Robust Regression->M-estimators for Robust Regression->Tukey\u2019s Biweight": [
        [
            "[17]:",
            "code"
        ],
        [
            "help(norms.TukeyBiweight.weights)",
            "code"
        ],
        [
            "Help on function weights in module statsmodels.robust.norms:\n\nweights(self, z)\n    Tukey's biweight weighting function for the IRLS algorithm\n\n    The psi function scaled by z\n\n    Parameters\n    ----------\n    z : array_like\n        1d array\n\n    Returns\n    -------\n    weights : ndarray\n        psi(z) = (1 - (z/c)**2)**2          for \\|z\\| &lt;= R\n\n        psi(z) = 0                          for \\|z\\|  R",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "c = 4.685\nsupport = np.linspace(-3 * c, 3 * c, 1000)\ntukey = norms.TukeyBiweight(c=c)\nplot_weights(support, tukey.weights, [\"-3*c\", \"0\", \"3*c\"], [-3 * c, 0, 3 * c])",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: \n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_27_1.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_27_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators": [
        [
            "Robust estimates of the location",
            "markdown"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "x = np.array([1, 2, 3, 4, 500])",
            "code"
        ],
        [
            "The mean is not a robust estimator of location",
            "markdown"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "x.mean()",
            "code"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "102.0",
            "code"
        ],
        [
            "The median, on the other hand, is a robust estimator with a breakdown point of 50%",
            "markdown"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "np.median(x)",
            "code"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "3.0",
            "code"
        ],
        [
            "Analogously for the scale",
            "markdown"
        ],
        [
            "The standard deviation is not robust",
            "markdown"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "x.std()",
            "code"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "199.00251254695254",
            "code"
        ],
        [
            "Median Absolute Deviation\n\n\\[median_i |X_i - median_j(X_j)|)\\]",
            "markdown"
        ],
        [
            "Standardized Median Absolute Deviation is a consistent estimator for \\(\\hat{\\sigma}\\)\n\n\\[\\hat{\\sigma}=K \\cdot MAD\\]",
            "markdown"
        ],
        [
            "where \\(K\\) depends on the distribution. For the normal distribution for example,\n\n\\[K = \\Phi^{-1}(.75)\\]",
            "markdown"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "stats.norm.ppf(0.75)",
            "code"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "0.6744897501960817",
            "code"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "print(x)",
            "code"
        ],
        [
            "[  1   2   3   4 500]",
            "code"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "sm.robust.scale.mad(x)",
            "code"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "1.482602218505602",
            "code"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "np.array([1, 2, 3, 4, 5.0]).std()",
            "code"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "1.4142135623730951",
            "code"
        ],
        [
            "Another robust estimator of scale is the Interquartile Range (IQR)\n\n\\[\\left(\\hat{X}_{0.75} - \\hat{X}_{0.25}\\right),\\]",
            "markdown"
        ],
        [
            "where \\(\\hat{X}_{p}\\) is the sample p-th quantile and \\(K\\) depends on the distribution.",
            "markdown"
        ],
        [
            "The standardized IQR, given by \\(K \\cdot \\text{IQR}\\) for\n\n\\[K = \\frac{1}{\\Phi^{-1}(.75) - \\Phi^{-1}(.25)} \\approx 0.74,\\]",
            "markdown"
        ],
        [
            "is a consistent estimator of the standard deviation for normal data.",
            "markdown"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "sm.robust.scale.iqr(x)",
            "code"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "array(1.48260222)",
            "code"
        ],
        [
            "The IQR is less robust than the MAD in the sense that it has a lower breakdown point: it can withstand 25% outlying observations before being completely ruined, whereas the MAD can withstand 50% outlying observations. However, the IQR is better suited for asymmetric distributions.",
            "markdown"
        ],
        [
            "Yet another robust estimator of scale is the \\(Q_n\\) estimator, introduced in Rousseeuw & Croux (1993), \u2018Alternatives to the Median Absolute Deviation\u2019. Then \\(Q_n\\) estimator is given by\n\n\\[Q_n = K \\left\\lbrace \\vert X_{i} - X_{j}\\vert : i&lt;j\\right\\rbrace_{(h)}\\]",
            "markdown"
        ],
        [
            "where \\(h\\approx (1/4){{n}\\choose{2}}\\) and \\(K\\) is a given constant. In words, the \\(Q_n\\) estimator is the normalized \\(h\\)-th order statistic of the absolute differences of the data. The normalizing constant \\(K\\) is usually chosen as 2.219144, to make the estimator consistent for the standard deviation in the case of normal data. The \\(Q_n\\) estimator has a 50% breakdown point and a 82% asymptotic efficiency at the normal distribution, much higher than the 37%\nefficiency of the MAD.",
            "markdown"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "sm.robust.scale.qn_scale(x)",
            "code"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "2.219144465985076",
            "code"
        ],
        [
            "The default for Robust Linear Models is MAD",
            "markdown"
        ],
        [
            "another popular choice is Huber\u2019s proposal 2",
            "markdown"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "np.random.seed(12345)\nfat_tails = stats.t(6).rvs(40)",
            "code"
        ],
        [
            "[30]:",
            "code"
        ],
        [
            "kde = sm.nonparametric.KDEUnivariate(fat_tails)\nkde.fit()\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(kde.support, kde.density)",
            "code"
        ],
        [
            "[30]:",
            "code"
        ],
        [
            "[&lt;matplotlib.lines.Line2D at 0x7f4736abe3b0]\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_51_1.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_51_1.png\"/>",
            "code"
        ],
        [
            "[31]:",
            "code"
        ],
        [
            "print(fat_tails.mean(), fat_tails.std())",
            "code"
        ],
        [
            "0.0688231044810875 1.3471633229698652",
            "code"
        ],
        [
            "[32]:",
            "code"
        ],
        [
            "print(stats.norm.fit(fat_tails))",
            "code"
        ],
        [
            "(0.0688231044810875, 1.3471633229698652)",
            "code"
        ],
        [
            "[33]:",
            "code"
        ],
        [
            "print(stats.t.fit(fat_tails, f0=6))",
            "code"
        ],
        [
            "(6, 0.03900835312789366, 1.0563837144431252)",
            "code"
        ],
        [
            "[34]:",
            "code"
        ],
        [
            "huber = sm.robust.scale.Huber()\nloc, scale = huber(fat_tails)\nprint(loc, scale)",
            "code"
        ],
        [
            "0.04048984333271795 1.1557140047569665",
            "code"
        ],
        [
            "[35]:",
            "code"
        ],
        [
            "sm.robust.mad(fat_tails)",
            "code"
        ],
        [
            "[35]:",
            "code"
        ],
        [
            "1.115335001165415",
            "code"
        ],
        [
            "[36]:",
            "code"
        ],
        [
            "sm.robust.mad(fat_tails, c=stats.t(6).ppf(0.75))",
            "code"
        ],
        [
            "[36]:",
            "code"
        ],
        [
            "1.0483916565928972",
            "code"
        ],
        [
            "[37]:",
            "code"
        ],
        [
            "sm.robust.scale.mad(fat_tails)",
            "code"
        ],
        [
            "[37]:",
            "code"
        ],
        [
            "1.115335001165415",
            "code"
        ]
    ],
    "Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers": [
        [
            "[38]:",
            "code"
        ],
        [
            "from statsmodels.graphics.api import abline_plot\nfrom statsmodels.formula.api import ols, rlm",
            "code"
        ],
        [
            "[39]:",
            "code"
        ],
        [
            "prestige = sm.datasets.get_rdataset(\"Duncan\", \"carData\", cache=True).data",
            "code"
        ],
        [
            "[40]:",
            "code"
        ],
        [
            "print(prestige.head(10))",
            "code"
        ],
        [
            "type  income  education  prestige\naccountant  prof      62         86        82\npilot       prof      72         76        83\narchitect   prof      75         92        90\nauthor      prof      55         90        76\nchemist     prof      64         86        90\nminister    prof      21         84        87\nprofessor   prof      64         93        93\ndentist     prof      80        100        90\nreporter      wc      67         87        52\nengineer    prof      72         86        88",
            "code"
        ],
        [
            "[41]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 12))\nax1 = fig.add_subplot(211, xlabel=\"Income\", ylabel=\"Prestige\")\nax1.scatter(prestige.income, prestige.prestige)\nxy_outlier = prestige.loc[\"minister\", [\"income\", \"prestige\"]]\nax1.annotate(\"Minister\", xy_outlier, xy_outlier + 1, fontsize=16)\nax2 = fig.add_subplot(212, xlabel=\"Education\", ylabel=\"Prestige\")\nax2.scatter(prestige.education, prestige.prestige)",
            "code"
        ],
        [
            "[41]:",
            "code"
        ],
        [
            "&lt;matplotlib.collections.PathCollection at 0x7f473699df00\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_63_1.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_63_1.png\"/>",
            "code"
        ],
        [
            "[42]:",
            "code"
        ],
        [
            "ols_model = ols(\"prestige ~ income + education\", prestige).fit()\nprint(ols_model.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:               prestige   R-squared:                       0.828\nModel:                            OLS   Adj. R-squared:                  0.820\nMethod:                 Least Squares   F-statistic:                     101.2\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           8.65e-17\nTime:                        17:06:22   Log-Likelihood:                -178.98\nNo. Observations:                  45   AIC:                             364.0\nDf Residuals:                      42   BIC:                             369.4\nDf Model:                           2\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -6.0647      4.272     -1.420      0.163     -14.686       2.556\nincome         0.5987      0.120      5.003      0.000       0.357       0.840\neducation      0.5458      0.098      5.555      0.000       0.348       0.744\n==============================================================================\nOmnibus:                        1.279   Durbin-Watson:                   1.458\nProb(Omnibus):                  0.528   Jarque-Bera (JB):                0.520\nSkew:                           0.155   Prob(JB):                        0.771\nKurtosis:                       3.426   Cond. No.                         163.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "[43]:",
            "code"
        ],
        [
            "infl = ols_model.get_influence()\nstudent = infl.summary_frame()[\"student_resid\"]\nprint(student)",
            "code"
        ],
        [
            "accountant            0.303900\npilot                 0.340920\narchitect             0.072256\nauthor                0.000711\nchemist               0.826578\nminister              3.134519\nprofessor             0.768277\ndentist              -0.498082\nreporter             -2.397022\nengineer              0.306225\nundertaker           -0.187339\nlawyer               -0.303082\nphysician             0.355687\nwelfare.worker       -0.411406\nteacher               0.050510\nconductor            -1.704032\ncontractor            2.043805\nfactory.owner         1.602429\nstore.manager         0.142425\nbanker                0.508388\nbookkeeper           -0.902388\nmail.carrier         -1.433249\ninsurance.agent      -1.930919\nstore.clerk          -1.760491\ncarpenter             1.068858\nelectrician           0.731949\nRR.engineer           0.808922\nmachinist             1.887047\nauto.repairman        0.522735\nplumber              -0.377954\ngas.stn.attendant    -0.666596\ncoal.miner            1.018527\nstreetcar.motorman   -1.104485\ntaxi.driver           0.023322\ntruck.driver         -0.129227\nmachine.operator      0.499922\nbarber                0.173805\nbartender            -0.902422\nshoe.shiner          -0.429357\ncook                  0.127207\nsoda.clerk           -0.883095\nwatchman             -0.513502\njanitor              -0.079890\npoliceman             0.078847\nwaiter               -0.475972\nName: student_resid, dtype: float64",
            "code"
        ],
        [
            "[44]:",
            "code"
        ],
        [
            "print(student.loc[np.abs(student)  2])",
            "code"
        ],
        [
            "minister      3.134519\nreporter     -2.397022\ncontractor    2.043805\nName: student_resid, dtype: float64",
            "code"
        ],
        [
            "[45]:",
            "code"
        ],
        [
            "print(infl.summary_frame().loc[\"minister\"])",
            "code"
        ],
        [
            "dfb_Intercept      0.144937\ndfb_income        -1.220939\ndfb_education      1.263019\ncooks_d            0.566380\nstandard_resid     2.849416\nhat_diag           0.173058\ndffits_internal    1.303510\nstudent_resid      3.134519\ndffits             1.433935\nName: minister, dtype: float64",
            "code"
        ],
        [
            "[46]:",
            "code"
        ],
        [
            "sidak = ols_model.outlier_test(\"sidak\")\nsidak.sort_values(\"unadj_p\", inplace=True)\nprint(sidak)",
            "code"
        ],
        [
            "student_resid   unadj_p  sidak(p)\nminister                 3.134519  0.003177  0.133421\nreporter                -2.397022  0.021170  0.618213\ncontractor               2.043805  0.047433  0.887721\ninsurance.agent         -1.930919  0.060428  0.939485\nmachinist                1.887047  0.066248  0.954247\nstore.clerk             -1.760491  0.085783  0.982331\nconductor               -1.704032  0.095944  0.989315\nfactory.owner            1.602429  0.116738  0.996250\nmail.carrier            -1.433249  0.159369  0.999595\nstreetcar.motorman      -1.104485  0.275823  1.000000\ncarpenter                1.068858  0.291386  1.000000\ncoal.miner               1.018527  0.314400  1.000000\nbartender               -0.902422  0.372104  1.000000\nbookkeeper              -0.902388  0.372122  1.000000\nsoda.clerk              -0.883095  0.382334  1.000000\nchemist                  0.826578  0.413261  1.000000\nRR.engineer              0.808922  0.423229  1.000000\nprofessor                0.768277  0.446725  1.000000\nelectrician              0.731949  0.468363  1.000000\ngas.stn.attendant       -0.666596  0.508764  1.000000\nauto.repairman           0.522735  0.603972  1.000000\nwatchman                -0.513502  0.610357  1.000000\nbanker                   0.508388  0.613906  1.000000\nmachine.operator         0.499922  0.619802  1.000000\ndentist                 -0.498082  0.621088  1.000000\nwaiter                  -0.475972  0.636621  1.000000\nshoe.shiner             -0.429357  0.669912  1.000000\nwelfare.worker          -0.411406  0.682918  1.000000\nplumber                 -0.377954  0.707414  1.000000\nphysician                0.355687  0.723898  1.000000\npilot                    0.340920  0.734905  1.000000\nengineer                 0.306225  0.760983  1.000000\naccountant               0.303900  0.762741  1.000000\nlawyer                  -0.303082  0.763360  1.000000\nundertaker              -0.187339  0.852319  1.000000\nbarber                   0.173805  0.862874  1.000000\nstore.manager            0.142425  0.887442  1.000000\ntruck.driver            -0.129227  0.897810  1.000000\ncook                     0.127207  0.899399  1.000000\njanitor                 -0.079890  0.936713  1.000000\npoliceman                0.078847  0.937538  1.000000\narchitect                0.072256  0.942750  1.000000\nteacher                  0.050510  0.959961  1.000000\ntaxi.driver              0.023322  0.981507  1.000000\nauthor                   0.000711  0.999436  1.000000",
            "code"
        ],
        [
            "[47]:",
            "code"
        ],
        [
            "fdr = ols_model.outlier_test(\"fdr_bh\")\nfdr.sort_values(\"unadj_p\", inplace=True)\nprint(fdr)",
            "code"
        ],
        [
            "student_resid   unadj_p  fdr_bh(p)\nminister                 3.134519  0.003177   0.142974\nreporter                -2.397022  0.021170   0.476332\ncontractor               2.043805  0.047433   0.596233\ninsurance.agent         -1.930919  0.060428   0.596233\nmachinist                1.887047  0.066248   0.596233\nstore.clerk             -1.760491  0.085783   0.616782\nconductor               -1.704032  0.095944   0.616782\nfactory.owner            1.602429  0.116738   0.656653\nmail.carrier            -1.433249  0.159369   0.796844\nstreetcar.motorman      -1.104485  0.275823   0.999436\ncarpenter                1.068858  0.291386   0.999436\ncoal.miner               1.018527  0.314400   0.999436\nbartender               -0.902422  0.372104   0.999436\nbookkeeper              -0.902388  0.372122   0.999436\nsoda.clerk              -0.883095  0.382334   0.999436\nchemist                  0.826578  0.413261   0.999436\nRR.engineer              0.808922  0.423229   0.999436\nprofessor                0.768277  0.446725   0.999436\nelectrician              0.731949  0.468363   0.999436\ngas.stn.attendant       -0.666596  0.508764   0.999436\nauto.repairman           0.522735  0.603972   0.999436\nwatchman                -0.513502  0.610357   0.999436\nbanker                   0.508388  0.613906   0.999436\nmachine.operator         0.499922  0.619802   0.999436\ndentist                 -0.498082  0.621088   0.999436\nwaiter                  -0.475972  0.636621   0.999436\nshoe.shiner             -0.429357  0.669912   0.999436\nwelfare.worker          -0.411406  0.682918   0.999436\nplumber                 -0.377954  0.707414   0.999436\nphysician                0.355687  0.723898   0.999436\npilot                    0.340920  0.734905   0.999436\nengineer                 0.306225  0.760983   0.999436\naccountant               0.303900  0.762741   0.999436\nlawyer                  -0.303082  0.763360   0.999436\nundertaker              -0.187339  0.852319   0.999436\nbarber                   0.173805  0.862874   0.999436\nstore.manager            0.142425  0.887442   0.999436\ntruck.driver            -0.129227  0.897810   0.999436\ncook                     0.127207  0.899399   0.999436\njanitor                 -0.079890  0.936713   0.999436\npoliceman                0.078847  0.937538   0.999436\narchitect                0.072256  0.942750   0.999436\nteacher                  0.050510  0.959961   0.999436\ntaxi.driver              0.023322  0.981507   0.999436\nauthor                   0.000711  0.999436   0.999436",
            "code"
        ],
        [
            "[48]:",
            "code"
        ],
        [
            "rlm_model = rlm(\"prestige ~ income + education\", prestige).fit()\nprint(rlm_model.summary())",
            "code"
        ],
        [
            "Robust linear Model Regression Results\n==============================================================================\nDep. Variable:               prestige   No. Observations:                   45\nModel:                            RLM   Df Residuals:                       42\nMethod:                          IRLS   Df Model:                            2\nNorm:                          HuberT\nScale Est.:                       mad\nCov Type:                          H1\nDate:                Wed, 02 Nov 2022\nTime:                        17:06:22\nNo. Iterations:                    18\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -7.1107      3.879     -1.833      0.067     -14.713       0.492\nincome         0.7015      0.109      6.456      0.000       0.489       0.914\neducation      0.4854      0.089      5.441      0.000       0.311       0.660\n==============================================================================\n\nIf the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .",
            "code"
        ],
        [
            "[49]:",
            "code"
        ],
        [
            "print(rlm_model.weights)",
            "code"
        ],
        [
            "accountant            1.000000\npilot                 1.000000\narchitect             1.000000\nauthor                1.000000\nchemist               1.000000\nminister              0.344596\nprofessor             1.000000\ndentist               1.000000\nreporter              0.441669\nengineer              1.000000\nundertaker            1.000000\nlawyer                1.000000\nphysician             1.000000\nwelfare.worker        1.000000\nteacher               1.000000\nconductor             0.538445\ncontractor            0.552262\nfactory.owner         0.706169\nstore.manager         1.000000\nbanker                1.000000\nbookkeeper            1.000000\nmail.carrier          0.690764\ninsurance.agent       0.533499\nstore.clerk           0.618656\ncarpenter             0.935848\nelectrician           1.000000\nRR.engineer           1.000000\nmachinist             0.570360\nauto.repairman        1.000000\nplumber               1.000000\ngas.stn.attendant     1.000000\ncoal.miner            0.963821\nstreetcar.motorman    0.832870\ntaxi.driver           1.000000\ntruck.driver          1.000000\nmachine.operator      1.000000\nbarber                1.000000\nbartender             1.000000\nshoe.shiner           1.000000\ncook                  1.000000\nsoda.clerk            1.000000\nwatchman              1.000000\njanitor               1.000000\npoliceman             1.000000\nwaiter                1.000000\ndtype: float64",
            "code"
        ]
    ],
    "Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points": [
        [
            "Data is on the luminosity and temperature of 47 stars in the direction of Cygnus.",
            "markdown"
        ],
        [
            "[50]:",
            "code"
        ],
        [
            "dta = sm.datasets.get_rdataset(\"starsCYG\", \"robustbase\", cache=True).data",
            "code"
        ],
        [
            "[51]:",
            "code"
        ],
        [
            "from matplotlib.patches import Ellipse\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(\n    111,\n    xlabel=\"log(Temp)\",\n    ylabel=\"log(Light)\",\n    title=\"Hertzsprung-Russell Diagram of Star Cluster CYG OB1\",\n)\nax.scatter(*dta.values.T)\n# highlight outliers\ne = Ellipse((3.5, 6), 0.2, 1, alpha=0.25, color=\"r\")\nax.add_patch(e)\nax.annotate(\n    \"Red giants\",\n    xy=(3.6, 6),\n    xytext=(3.8, 6),\n    arrowprops=dict(facecolor=\"black\", shrink=0.05, width=2),\n    horizontalalignment=\"left\",\n    verticalalignment=\"bottom\",\n    clip_on=True,  # clip to the axes bounding box\n    fontsize=16,\n)\n# annotate these with their index\nfor i, row in dta.loc[dta[\"log.Te\"] &lt; 3.8].iterrows():\n    ax.annotate(i, row, row + 0.01, fontsize=14)\nxlim, ylim = ax.get_xlim(), ax.get_ylim()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_75_0.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_75_0.png\"/>",
            "code"
        ],
        [
            "[52]:",
            "code"
        ],
        [
            "from IPython.display import Image\n\nImage(filename=\"star_diagram.png\")",
            "code"
        ],
        [
            "[52]:\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_76_0.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_76_0.png\"/>",
            "code"
        ],
        [
            "[53]:",
            "code"
        ],
        [
            "y = dta[\"log.light\"]\nX = sm.add_constant(dta[\"log.Te\"], prepend=True)\nols_model = sm.OLS(y, X).fit()\nabline_plot(model_results=ols_model, ax=ax)",
            "code"
        ],
        [
            "[53]:\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_77_0.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_77_0.png\"/>",
            "code"
        ],
        [
            "[54]:",
            "code"
        ],
        [
            "rlm_mod = sm.RLM(y, X, sm.robust.norms.TrimmedMean(0.5)).fit()\nabline_plot(model_results=rlm_mod, ax=ax, color=\"red\")",
            "code"
        ],
        [
            "[54]:\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_78_0.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_78_0.png\"/>",
            "code"
        ],
        [
            "Why? Because M-estimators are not robust to leverage points.",
            "markdown"
        ],
        [
            "[55]:",
            "code"
        ],
        [
            "infl = ols_model.get_influence()",
            "code"
        ],
        [
            "[56]:",
            "code"
        ],
        [
            "h_bar = 2 * (ols_model.df_model + 1) / ols_model.nobs\nhat_diag = infl.summary_frame()[\"hat_diag\"]\nhat_diag.loc[hat_diag  h_bar]",
            "code"
        ],
        [
            "[56]:",
            "code"
        ],
        [
            "10    0.194103\n19    0.194103\n29    0.198344\n33    0.194103\nName: hat_diag, dtype: float64",
            "code"
        ],
        [
            "[57]:",
            "code"
        ],
        [
            "sidak2 = ols_model.outlier_test(\"sidak\")\nsidak2.sort_values(\"unadj_p\", inplace=True)\nprint(sidak2)",
            "code"
        ],
        [
            "student_resid   unadj_p  sidak(p)\n16      -2.049393  0.046415  0.892872\n13      -2.035329  0.047868  0.900286\n33       1.905847  0.063216  0.953543\n18      -1.575505  0.122304  0.997826\n1        1.522185  0.135118  0.998911\n3        1.522185  0.135118  0.998911\n21      -1.450418  0.154034  0.999615\n17      -1.426675  0.160731  0.999735\n29       1.388520  0.171969  0.999859\n14      -1.374733  0.176175  0.999889\n35       1.346543  0.185023  0.999933\n34      -1.272159  0.209999  0.999985\n28      -1.186946  0.241618  0.999998\n20      -1.150621  0.256103  0.999999\n44       1.134779  0.262612  0.999999\n39       1.091886  0.280826  1.000000\n19       1.064878  0.292740  1.000000\n6       -1.026873  0.310093  1.000000\n30      -1.009096  0.318446  1.000000\n22      -0.979768  0.332557  1.000000\n8        0.961218  0.341695  1.000000\n5        0.913802  0.365801  1.000000\n11       0.871997  0.387943  1.000000\n12       0.856685  0.396261  1.000000\n46      -0.833923  0.408829  1.000000\n10       0.743920  0.460879  1.000000\n42       0.727179  0.470968  1.000000\n15      -0.689258  0.494280  1.000000\n43       0.688272  0.494895  1.000000\n7        0.655712  0.515424  1.000000\n40      -0.646396  0.521381  1.000000\n26      -0.640978  0.524862  1.000000\n25      -0.545561  0.588123  1.000000\n32       0.472819  0.638680  1.000000\n37       0.472819  0.638680  1.000000\n38       0.462187  0.646225  1.000000\n0        0.430686  0.668799  1.000000\n31       0.341726  0.734184  1.000000\n36       0.318911  0.751303  1.000000\n4        0.307890  0.759619  1.000000\n9        0.235114  0.815211  1.000000\n41       0.187732  0.851950  1.000000\n2       -0.182093  0.856346  1.000000\n23      -0.156014  0.876736  1.000000\n27      -0.147406  0.883485  1.000000\n24       0.065195  0.948314  1.000000\n45       0.045675  0.963776  1.000000",
            "code"
        ],
        [
            "[58]:",
            "code"
        ],
        [
            "fdr2 = ols_model.outlier_test(\"fdr_bh\")\nfdr2.sort_values(\"unadj_p\", inplace=True)\nprint(fdr2)",
            "code"
        ],
        [
            "student_resid   unadj_p  fdr_bh(p)\n16      -2.049393  0.046415   0.764747\n13      -2.035329  0.047868   0.764747\n33       1.905847  0.063216   0.764747\n18      -1.575505  0.122304   0.764747\n1        1.522185  0.135118   0.764747\n3        1.522185  0.135118   0.764747\n21      -1.450418  0.154034   0.764747\n17      -1.426675  0.160731   0.764747\n29       1.388520  0.171969   0.764747\n14      -1.374733  0.176175   0.764747\n35       1.346543  0.185023   0.764747\n34      -1.272159  0.209999   0.764747\n28      -1.186946  0.241618   0.764747\n20      -1.150621  0.256103   0.764747\n44       1.134779  0.262612   0.764747\n39       1.091886  0.280826   0.764747\n19       1.064878  0.292740   0.764747\n6       -1.026873  0.310093   0.764747\n30      -1.009096  0.318446   0.764747\n22      -0.979768  0.332557   0.764747\n8        0.961218  0.341695   0.764747\n5        0.913802  0.365801   0.768599\n11       0.871997  0.387943   0.768599\n12       0.856685  0.396261   0.768599\n46      -0.833923  0.408829   0.768599\n10       0.743920  0.460879   0.770890\n42       0.727179  0.470968   0.770890\n15      -0.689258  0.494280   0.770890\n43       0.688272  0.494895   0.770890\n7        0.655712  0.515424   0.770890\n40      -0.646396  0.521381   0.770890\n26      -0.640978  0.524862   0.770890\n25      -0.545561  0.588123   0.837630\n32       0.472819  0.638680   0.843682\n37       0.472819  0.638680   0.843682\n38       0.462187  0.646225   0.843682\n0        0.430686  0.668799   0.849556\n31       0.341726  0.734184   0.892552\n36       0.318911  0.751303   0.892552\n4        0.307890  0.759619   0.892552\n9        0.235114  0.815211   0.922751\n41       0.187732  0.851950   0.922751\n2       -0.182093  0.856346   0.922751\n23      -0.156014  0.876736   0.922751\n27      -0.147406  0.883485   0.922751\n24       0.065195  0.948314   0.963776\n45       0.045675  0.963776   0.963776",
            "code"
        ],
        [
            "Let\u2019s delete that line",
            "markdown"
        ],
        [
            "[59]:",
            "code"
        ],
        [
            "l = ax.lines[-1]\nl.remove()\ndel l",
            "code"
        ],
        [
            "[60]:",
            "code"
        ],
        [
            "weights = np.ones(len(X))\nweights[X[X[\"log.Te\"] &lt; 3.8].index.values - 1] = 0\nwls_model = sm.WLS(y, X, weights=weights).fit()\nabline_plot(model_results=wls_model, ax=ax, color=\"green\")",
            "code"
        ],
        [
            "[60]:\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_86_0.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_86_0.png\"/>",
            "code"
        ],
        [
            "MM estimators are good for this type of problem, unfortunately, we do not yet have these yet.",
            "markdown"
        ],
        [
            "It\u2019s being worked on, but it gives a good excuse to look at the R cell magics in the notebook.",
            "markdown"
        ],
        [
            "[61]:",
            "code"
        ],
        [
            "yy = y.values[:, None]\nxx = X[\"log.Te\"].values[:, None]",
            "code"
        ],
        [
            "<strong>Note</strong>: The R code and the results in this notebook has been converted to markdown so that R is not required to build the documents. The R results in the notebook were computed using R 3.5.1 and robustbase 0.93.",
            "markdown"
        ],
        [
            "%load_ext rpy2.ipython\n\n%R library(robustbase)\n%Rpush yy xx\n%R mod &lt;- lmrob(yy ~ xx);\n%R params &lt;- mod$coefficients;\n%Rpull params",
            "code"
        ],
        [
            "%R print(mod)",
            "code"
        ],
        [
            "Call:\nlmrob(formula = yy ~ xx)\n \\-- method = \"MM\"\nCoefficients:\n(Intercept)           xx\n     -4.969        2.253",
            "code"
        ],
        [
            "[62]:",
            "code"
        ],
        [
            "params = [-4.969387980288108, 2.2531613477892365]  # Computed using R\nprint(params[0], params[1])",
            "code"
        ],
        [
            "-4.969387980288108 2.2531613477892365",
            "code"
        ],
        [
            "[63]:",
            "code"
        ],
        [
            "abline_plot(intercept=params[0], slope=params[1], ax=ax, color=\"red\")",
            "code"
        ],
        [
            "[63]:\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_94_0.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_94_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator": [
        [
            "[64]:",
            "code"
        ],
        [
            "np.random.seed(12345)\nnobs = 200\nbeta_true = np.array([3, 1, 2.5, 3, -4])\nX = np.random.uniform(-20, 20, size=(nobs, len(beta_true) - 1))\n# stack a constant in front\nX = sm.add_constant(X, prepend=True)  # np.c_[np.ones(nobs), X]\nmc_iter = 500\ncontaminate = 0.25  # percentage of response variables to contaminate",
            "code"
        ],
        [
            "[65]:",
            "code"
        ],
        [
            "all_betas = []\nfor i in range(mc_iter):\n    y = np.dot(X, beta_true) + np.random.normal(size=200)\n    random_idx = np.random.randint(0, nobs, size=int(contaminate * nobs))\n    y[random_idx] = np.random.uniform(-750, 750)\n    beta_hat = sm.RLM(y, X).fit().params\n    all_betas.append(beta_hat)",
            "code"
        ],
        [
            "[66]:",
            "code"
        ],
        [
            "all_betas = np.asarray(all_betas)\nse_loss = lambda x: np.linalg.norm(x, ord=2) ** 2\nse_beta = lmap(se_loss, all_betas - beta_true)",
            "code"
        ]
    ],
    "Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator->Squared error loss": [
        [
            "[67]:",
            "code"
        ],
        [
            "np.array(se_beta).mean()",
            "code"
        ],
        [
            "[67]:",
            "code"
        ],
        [
            "0.4450294873068656",
            "code"
        ],
        [
            "[68]:",
            "code"
        ],
        [
            "all_betas.mean(0)",
            "code"
        ],
        [
            "[68]:",
            "code"
        ],
        [
            "array([ 2.99711706,  0.99898147,  2.49909344,  2.99712918, -3.99626521])",
            "code"
        ],
        [
            "[69]:",
            "code"
        ],
        [
            "beta_true",
            "code"
        ],
        [
            "[69]:",
            "code"
        ],
        [
            "array([ 3. ,  1. ,  2.5,  3. , -4. ])",
            "code"
        ],
        [
            "[70]:",
            "code"
        ],
        [
            "se_loss(all_betas.mean(0) - beta_true)",
            "code"
        ],
        [
            "[70]:",
            "code"
        ],
        [
            "3.236091328675582e-05",
            "code"
        ]
    ],
    "Examples->Robust Regression->Comparing OLS and RLM": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm",
            "code"
        ]
    ],
    "Examples->Robust Regression->Comparing OLS and RLM->Estimation": [
        [
            "Load data:",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "data = sm.datasets.stackloss.load()\ndata.exog = sm.add_constant(data.exog)",
            "code"
        ],
        [
            "Huber\u2019s T norm with the (default) median absolute deviation scaling",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "huber_t = sm.RLM(data.endog, data.exog, M=sm.robust.norms.HuberT())\nhub_results = huber_t.fit()\nprint(hub_results.params)\nprint(hub_results.bse)\nprint(\n    hub_results.summary(\n        yname=\"y\", xname=[\"var_%d\" % i for i in range(len(hub_results.params))]\n    )\n)",
            "code"
        ],
        [
            "const       -41.026498\nAIRFLOW       0.829384\nWATERTEMP     0.926066\nACIDCONC     -0.127847\ndtype: float64\nconst        9.791899\nAIRFLOW      0.111005\nWATERTEMP    0.302930\nACIDCONC     0.128650\ndtype: float64\n                    Robust linear Model Regression Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                   21\nModel:                            RLM   Df Residuals:                       17\nMethod:                          IRLS   Df Model:                            3\nNorm:                          HuberT\nScale Est.:                       mad\nCov Type:                          H1\nDate:                Wed, 02 Nov 2022\nTime:                        17:06:58\nNo. Iterations:                    19\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nvar_0        -41.0265      9.792     -4.190      0.000     -60.218     -21.835\nvar_1          0.8294      0.111      7.472      0.000       0.612       1.047\nvar_2          0.9261      0.303      3.057      0.002       0.332       1.520\nvar_3         -0.1278      0.129     -0.994      0.320      -0.380       0.124\n==============================================================================\n\nIf the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .",
            "code"
        ],
        [
            "Huber\u2019s T norm with \u2018H2\u2019 covariance matrix",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "hub_results2 = huber_t.fit(cov=\"H2\")\nprint(hub_results2.params)\nprint(hub_results2.bse)",
            "code"
        ],
        [
            "const       -41.026498\nAIRFLOW       0.829384\nWATERTEMP     0.926066\nACIDCONC     -0.127847\ndtype: float64\nconst        9.089504\nAIRFLOW      0.119460\nWATERTEMP    0.322355\nACIDCONC     0.117963\ndtype: float64",
            "code"
        ],
        [
            "Andrew\u2019s Wave norm with Huber\u2019s Proposal 2 scaling and \u2018H3\u2019 covariance matrix",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "andrew_mod = sm.RLM(data.endog, data.exog, M=sm.robust.norms.AndrewWave())\nandrew_results = andrew_mod.fit(scale_est=sm.robust.scale.HuberScale(), cov=\"H3\")\nprint(\"Parameters: \", andrew_results.params)",
            "code"
        ],
        [
            "Parameters:  const       -40.881796\nAIRFLOW       0.792761\nWATERTEMP     1.048576\nACIDCONC     -0.133609\ndtype: float64",
            "code"
        ],
        [
            "See help(sm.RLM.fit) for more options and module sm.robust.scale for scale options",
            "markdown"
        ]
    ],
    "Examples->Robust Regression->Comparing OLS and RLM->Comparing OLS and RLM": [
        [
            "Artificial data with outliers:",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "nsample = 50\nx1 = np.linspace(0, 20, nsample)\nX = np.column_stack((x1, (x1 - 5) ** 2))\nX = sm.add_constant(X)\nsig = 0.3  # smaller error variance makes OLS&lt;-RLM contrast bigger\nbeta = [5, 0.5, -0.0]\ny_true2 = np.dot(X, beta)\ny2 = y_true2 + sig * 1.0 * np.random.normal(size=nsample)\ny2[[39, 41, 43, 45, 48]] -= 5  # add some outliers (10% of nsample)",
            "code"
        ]
    ],
    "Examples->Robust Regression->Comparing OLS and RLM->Comparing OLS and RLM->Example 1: quadratic function with linear truth": [
        [
            "Note that the quadratic term in OLS regression will capture outlier effects.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "res = sm.OLS(y2, X).fit()\nprint(res.params)\nprint(res.bse)\nprint(res.predict())",
            "code"
        ],
        [
            "[ 4.78516573  0.55123012 -0.01444285]\n[0.44525161 0.06874084 0.0060825 ]\n[ 4.42409451  4.70563066  4.98235453  5.25426612  5.52136544  5.78365248\n  6.04112723  6.29378972  6.54163992  6.78467784  7.02290349  7.25631686\n  7.48491795  7.70870677  7.92768331  8.14184756  8.35119955  8.55573925\n  8.75546667  8.95038182  9.14048469  9.32577528  9.5062536   9.68191964\n  9.85277339 10.01881488 10.18004408 10.336461   10.48806565 10.63485802\n 10.77683811 10.91400593 11.04636146 11.17390472 11.2966357  11.41455441\n 11.52766083 11.63595498 11.73943685 11.83810644 11.93196376 12.02100879\n 12.10524155 12.18466203 12.25927024 12.32906616 12.39404981 12.45422118\n 12.50958027 12.56012709]",
            "code"
        ],
        [
            "Estimate RLM:",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "resrlm = sm.RLM(y2, X).fit()\nprint(resrlm.params)\nprint(resrlm.bse)",
            "code"
        ],
        [
            "[ 4.68933411e+00  5.38229820e-01 -3.74367086e-03]\n[0.11236366 0.01734743 0.00153498]",
            "code"
        ],
        [
            "Draw a plot to compare OLS estimates to the robust estimates:",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(x1, y2, \"o\", label=\"data\")\nax.plot(x1, y_true2, \"b-\", label=\"True\")\npred_ols = res.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nax.plot(x1, res.fittedvalues, \"r-\", label=\"OLS\")\nax.plot(x1, iv_u, \"r--\")\nax.plot(x1, iv_l, \"r--\")\nax.plot(x1, resrlm.fittedvalues, \"g.-\", label=\"RLM\")\nax.legend(loc=\"best\")",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f88c4387b20\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_0_18_1.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_0_18_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Robust Regression->Comparing OLS and RLM->Comparing OLS and RLM->Example 2: linear function with linear truth": [
        [
            "Fit a new OLS model using only the linear term and the constant:",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "X2 = X[:, [0, 1]]\nres2 = sm.OLS(y2, X2).fit()\nprint(res2.params)\nprint(res2.bse)",
            "code"
        ],
        [
            "[5.36730096 0.40680163]\n[0.38922146 0.03353689]",
            "code"
        ],
        [
            "Estimate RLM:",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "resrlm2 = sm.RLM(y2, X2).fit()\nprint(resrlm2.params)\nprint(resrlm2.bse)",
            "code"
        ],
        [
            "[4.82751717 0.50304602]\n[0.09082931 0.00782622]",
            "code"
        ],
        [
            "Draw a plot to compare OLS estimates to the robust estimates:",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "pred_ols = res2.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot(x1, y2, \"o\", label=\"data\")\nax.plot(x1, y_true2, \"b-\", label=\"True\")\nax.plot(x1, res2.fittedvalues, \"r-\", label=\"OLS\")\nax.plot(x1, iv_u, \"r--\")\nax.plot(x1, iv_l, \"r--\")\nax.plot(x1, resrlm2.fittedvalues, \"g.-\", label=\"RLM\")\nlegend = ax.legend(loc=\"best\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_0_24_0.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_0_24_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Generalized Estimating Equations->GEE Nested Covariance Structure": [
        [
            "This notebook is a simulation study that illustrates and evaluates the performance of the GEE nested covariance structure.",
            "markdown"
        ],
        [
            "A nested covariance structure is based on a nested sequence of groups, or \u201clevels\u201d. The top level in the hierarchy is defined by the groups argument to GEE. Subsequent levels are defined by the dep_data argument to GEE.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm",
            "code"
        ],
        [
            "Set the number of covariates.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "p = 5",
            "code"
        ],
        [
            "These parameters define the population variance for each level of grouping.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "groups_var = 1\nlevel1_var = 2\nlevel2_var = 3\nresid_var = 4",
            "code"
        ],
        [
            "Set the number of groups",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "n_groups = 100",
            "code"
        ],
        [
            "Set the number of observations at each level of grouping. Here, everything is balanced, i.e.\u00a0within a level every group has the same size.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "group_size = 20\nlevel1_size = 10\nlevel2_size = 5",
            "code"
        ],
        [
            "Calculate the total sample size.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "n = n_groups * group_size * level1_size * level2_size",
            "code"
        ],
        [
            "Construct the design matrix.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "xmat = np.random.normal(size=(n, p))",
            "code"
        ],
        [
            "Construct labels showing which group each observation belongs to at each level.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "groups_ix = np.kron(np.arange(n // group_size), np.ones(group_size)).astype(int)\nlevel1_ix = np.kron(np.arange(n // level1_size), np.ones(level1_size)).astype(int)\nlevel2_ix = np.kron(np.arange(n // level2_size), np.ones(level2_size)).astype(int)",
            "code"
        ],
        [
            "Simulate the random effects.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "groups_re = np.sqrt(groups_var) * np.random.normal(size=n // group_size)\nlevel1_re = np.sqrt(level1_var) * np.random.normal(size=n // level1_size)\nlevel2_re = np.sqrt(level2_var) * np.random.normal(size=n // level2_size)",
            "code"
        ],
        [
            "Simulate the response variable.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "y = groups_re[groups_ix] + level1_re[level1_ix] + level2_re[level2_ix]\ny += np.sqrt(resid_var) * np.random.normal(size=n)",
            "code"
        ],
        [
            "Put everything into a dataframe.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "df = pd.DataFrame(xmat, columns=[\"x%d\" % j for j in range(p)])\ndf[\"y\"] = y + xmat[:, 0] - xmat[:, 3]\ndf[\"groups_ix\"] = groups_ix\ndf[\"level1_ix\"] = level1_ix\ndf[\"level2_ix\"] = level2_ix",
            "code"
        ],
        [
            "Fit the model.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "cs = sm.cov_struct.Nested()\ndep_fml = \"0 + level1_ix + level2_ix\"\nm = sm.GEE.from_formula(\n    \"y ~ x0 + x1 + x2 + x3 + x4\",\n    cov_struct=cs,\n    dep_data=dep_fml,\n    groups=\"groups_ix\",\n    data=df,\n)\nr = m.fit()",
            "code"
        ],
        [
            "The estimated covariance parameters should be similar to groups_var, level1_var, etc. as defined above.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "r.cov_struct.summary()",
            "code"
        ],
        [
            "[13]:",
            "code"
        ]
    ],
    "Examples->Generalized Estimating Equations->GEE Score Tests": [
        [
            "This notebook uses simulation to demonstrate robust GEE score tests. These tests can be used in a GEE analysis to compare nested hypotheses about the mean structure. The tests are robust to miss-specification of the working correlation model, and to certain forms of misspecification of the variance structure (e.g.\u00a0as captured by the scale parameter in a quasi-Poisson analysis).",
            "markdown"
        ],
        [
            "The data are simulated as clusters, where there is dependence within but not between clusters. The cluster-wise dependence is induced using a copula approach. The data marginally follow a negative binomial (gamma/Poisson) mixture.",
            "markdown"
        ],
        [
            "The level and power of the tests are considered below to assess the performance of the tests.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import pandas as pd\nimport numpy as np\nfrom scipy.stats.distributions import norm, poisson\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "code"
        ],
        [
            "The function defined in the following cell uses a copula approach to simulate correlated random values that marginally follow a negative binomial distribution. The input parameter u is an array of values in (0, 1). The elements of u must be marginally uniformly distributed on (0, 1). Correlation in u will induce correlations in the returned negative binomial values. The array parameter mu gives the marginal means, and the scalar parameter scale defines the mean/variance\nrelationship (the variance is scale times the mean). The lengths of u and mu must be the same.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "def negbinom(u, mu, scale):\n    p = (scale - 1) / scale\n    r = mu * (1 - p) / p\n    x = np.random.gamma(r, p / (1 - p), len(u))\n    return poisson.ppf(u, mu=x)",
            "code"
        ],
        [
            "Below are some parameters that govern the data used in the simulation.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "# Sample size\nn = 1000\n\n# Number of covariates (including intercept) in the alternative hypothesis model\np = 5\n\n# Cluster size\nm = 10\n\n# Intraclass correlation (controls strength of clustering)\nr = 0.5\n\n# Group indicators\ngrp = np.kron(np.arange(n/m), np.ones(m))",
            "code"
        ],
        [
            "The simulation uses a fixed design matrix.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "# Build a design matrix for the alternative (more complex) model\nx = np.random.normal(size=(n, p))\nx[:, 0] = 1",
            "code"
        ],
        [
            "The null design matrix is nested in the alternative design matrix. It has rank two less than the alternative design matrix.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "x0 = x[:, 0:3]",
            "code"
        ],
        [
            "The GEE score test is robust to dependence and overdispersion. Here we set the overdispersion parameter. The variance of the negative binomial distribution for each observation is equal to scale times its mean value.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "# Scale parameter for negative binomial distribution\nscale = 10",
            "code"
        ],
        [
            "In the next cell, we set up the mean structures for the null and alternative models",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "# The coefficients used to define the linear predictors\ncoeff = [[4, 0.4, -0.2], [4, 0.4, -0.2, 0, -0.04]]\n\n# The linear predictors\nlp = [np.dot(x0, coeff[0]), np.dot(x, coeff[1])]\n\n# The mean values\nmu = [np.exp(lp[0]), np.exp(lp[1])]",
            "code"
        ],
        [
            "Below is a function that carries out the simulation.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "# hyp = 0 is the null hypothesis, hyp = 1 is the alternative hypothesis.\n# cov_struct is a statsmodels covariance structure\ndef dosim(hyp, cov_struct=None, mcrep=500):\n\n    # Storage for the simulation results\n    scales = [[], []]\n\n    # P-values from the score test\n    pv = []\n\n    # Monte Carlo loop\n    for k in range(mcrep):\n\n        # Generate random \"probability points\" u  that are uniformly\n        # distributed, and correlated within clusters\n        z = np.random.normal(size=n)\n        u = np.random.normal(size=n//m)\n        u = np.kron(u, np.ones(m))\n        z = r*z +np.sqrt(1-r**2)*u\n        u = norm.cdf(z)\n\n        # Generate the observed responses\n        y = negbinom(u, mu=mu[hyp], scale=scale)\n\n        # Fit the null model\n        m0 = sm.GEE(y, x0, groups=grp, cov_struct=cov_struct, family=sm.families.Poisson())\n        r0 = m0.fit(scale='X2')\n        scales[0].append(r0.scale)\n\n        # Fit the alternative model\n        m1 = sm.GEE(y, x, groups=grp, cov_struct=cov_struct, family=sm.families.Poisson())\n        r1 = m1.fit(scale='X2')\n        scales[1].append(r1.scale)\n\n        # Carry out the score test\n        st = m1.compare_score_test(r0)\n        pv.append(st[\"p-value\"])\n\n    pv = np.asarray(pv)\n    rslt = [np.mean(pv), np.mean(pv &lt; 0.1)]\n\n    return rslt, scales",
            "code"
        ],
        [
            "Run the simulation using the independence working covariance structure. We expect the mean to be around 0 under the null hypothesis, and much lower under the alternative hypothesis. Similarly, we expect that under the null hypothesis, around 10% of the p-values are less than 0.1, and a much greater fraction of the p-values are less than 0.1 under the alternative hypothesis.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "rslt, scales = [], []\n\nfor hyp in 0, 1:\n    s, t = dosim(hyp, sm.cov_struct.Independence())\n    rslt.append(s)\n    scales.append(t)\n\nrslt = pd.DataFrame(rslt, index=[\"H0\", \"H1\"], columns=[\"Mean\", \"Prop(p&lt;0.1)\"])\n\nprint(rslt)",
            "code"
        ],
        [
            "Mean  Prop(p&lt;0.1)\nH0  0.490046        0.110\nH1  0.055073        0.844",
            "code"
        ],
        [
            "Next we check to make sure that the scale parameter estimates are reasonable. We are assessing the robustness of the GEE score test to dependence and overdispersion, so here we are confirming that the overdispersion is present as expected.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "_ = plt.boxplot([scales[0][0], scales[0][1], scales[1][0], scales[1][1]])\nplt.ylabel(\"Estimated scale\")",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "Text(0, 0.5, 'Estimated scale')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_gee_score_test_simulation_19_1.png\" src=\"../../../_images/examples_notebooks_generated_gee_score_test_simulation_19_1.png\"/>",
            "code"
        ],
        [
            "Next we conduct the same analysis using an exchangeable working correlation model. Note that this will be slower than the example above using independent working correlation, so we use fewer Monte Carlo repetitions.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "rslt, scales = [], []\n\nfor hyp in 0, 1:\n    s, t = dosim(hyp, sm.cov_struct.Exchangeable(), mcrep=100)\n    rslt.append(s)\n    scales.append(t)\n\nrslt = pd.DataFrame(rslt, index=[\"H0\", \"H1\"], columns=[\"Mean\", \"Prop(p&lt;0.1)\"])\n\nprint(rslt)",
            "code"
        ],
        [
            "Mean  Prop(p&lt;0.1)\nH0  0.526380         0.09\nH1  0.046456         0.86",
            "code"
        ]
    ],
    "Examples->Statistics->ANOVA": [
        [
            "Note: This script is based heavily on Jonathan Taylor\u2019s class notes",
            "markdown"
        ],
        [
            "Download and format data:",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "from urllib.request import urlopen\nimport numpy as np\n\nnp.set_printoptions(precision=4, suppress=True)\n\nimport pandas as pd\n\npd.set_option(\"display.width\", 100)\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\nfrom statsmodels.graphics.api import interaction_plot, abline_plot\nfrom statsmodels.stats.anova import anova_lm\n\ntry:\n    salary_table = pd.read_csv(\"salary.table\")\nexcept:  # recent pandas can read URL without urlopen\n    url = \"http://stats191.stanford.edu/data/salary.table\"\n    fh = urlopen(url)\n    salary_table = pd.read_table(fh)\n    salary_table.to_csv(\"salary.table\")\n\nE = salary_table.E\nM = salary_table.M\nX = salary_table.X\nS = salary_table.S",
            "code"
        ],
        [
            "Take a look at the data:",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "plt.figure(figsize=(6, 6))\nsymbols = [\"D\", \"^\"]\ncolors = [\"r\", \"g\", \"blue\"]\nfactor_groups = salary_table.groupby([\"E\", \"M\"])\nfor values, group in factor_groups:\n    i, j = values\n    plt.scatter(group[\"X\"], group[\"S\"], marker=symbols[j], color=colors[i - 1], s=144)\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "Text(0, 0.5, 'Salary')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_5_1.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_5_1.png\"/>",
            "code"
        ],
        [
            "Fit a linear model:",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "formula = \"S ~ C(E) + C(M) + X\"\nlm = ols(formula, salary_table).fit()\nprint(lm.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                      S   R-squared:                       0.957\nModel:                            OLS   Adj. R-squared:                  0.953\nMethod:                 Least Squares   F-statistic:                     226.8\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           2.23e-27\nTime:                        17:11:04   Log-Likelihood:                -381.63\nNo. Observations:                  46   AIC:                             773.3\nDf Residuals:                      41   BIC:                             782.4\nDf Model:                           4\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   8035.5976    386.689     20.781      0.000    7254.663    8816.532\nC(E)[T.2]   3144.0352    361.968      8.686      0.000    2413.025    3875.045\nC(E)[T.3]   2996.2103    411.753      7.277      0.000    2164.659    3827.762\nC(M)[T.1]   6883.5310    313.919     21.928      0.000    6249.559    7517.503\nX            546.1840     30.519     17.896      0.000     484.549     607.819\n==============================================================================\nOmnibus:                        2.293   Durbin-Watson:                   2.237\nProb(Omnibus):                  0.318   Jarque-Bera (JB):                1.362\nSkew:                          -0.077   Prob(JB):                        0.506\nKurtosis:                       2.171   Cond. No.                         33.5\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "Have a look at the created design matrix:",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "lm.model.exog[:5]",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "array([[1., 0., 0., 1., 1.],\n       [1., 0., 1., 0., 1.],\n       [1., 0., 1., 1., 1.],\n       [1., 1., 0., 0., 1.],\n       [1., 0., 1., 0., 1.]])",
            "code"
        ],
        [
            "Or since we initially passed in a DataFrame, we have a DataFrame available in",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "lm.model.data.orig_exog[:5]",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "We keep a reference to the original untouched data in",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "lm.model.data.frame[:5]",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "Influence statistics",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "infl = lm.get_influence()\nprint(infl.summary_table())",
            "code"
        ],
        [
            "==================================================================================================\n       obs      endog     fitted     Cook's   student.   hat diag    dffits   ext.stud.     dffits\n                           value          d   residual              internal   residual\n--------------------------------------------------------------------------------------------------\n         0  13876.000  15465.313      0.104     -1.683      0.155     -0.722     -1.723     -0.739\n         1  11608.000  11577.992      0.000      0.031      0.130      0.012      0.031      0.012\n         2  18701.000  18461.523      0.001      0.247      0.109      0.086      0.244      0.085\n         3  11283.000  11725.817      0.005     -0.458      0.113     -0.163     -0.453     -0.162\n         4  11767.000  11577.992      0.001      0.197      0.130      0.076      0.195      0.075\n         5  20872.000  19155.532      0.092      1.787      0.126      0.678      1.838      0.698\n         6  11772.000  12272.001      0.006     -0.513      0.101     -0.172     -0.509     -0.170\n         7  10535.000   9127.966      0.056      1.457      0.116      0.529      1.478      0.537\n         8  12195.000  12124.176      0.000      0.074      0.123      0.028      0.073      0.027\n         9  12313.000  12818.185      0.005     -0.516      0.091     -0.163     -0.511     -0.161\n        10  14975.000  16557.681      0.084     -1.655      0.134     -0.650     -1.692     -0.664\n        11  21371.000  19701.716      0.078      1.728      0.116      0.624      1.772      0.640\n        12  19800.000  19553.891      0.001      0.252      0.096      0.082      0.249      0.081\n        13  11417.000  10220.334      0.033      1.227      0.098      0.405      1.234      0.408\n        14  20263.000  20100.075      0.001      0.166      0.093      0.053      0.165      0.053\n        15  13231.000  13216.544      0.000      0.015      0.114      0.005      0.015      0.005\n        16  12884.000  13364.369      0.004     -0.488      0.082     -0.146     -0.483     -0.145\n        17  13245.000  13910.553      0.007     -0.674      0.075     -0.192     -0.669     -0.191\n        18  13677.000  13762.728      0.000     -0.089      0.113     -0.032     -0.087     -0.031\n        19  15965.000  17650.049      0.082     -1.747      0.119     -0.642     -1.794     -0.659\n        20  12336.000  11312.702      0.021      1.043      0.087      0.323      1.044      0.323\n        21  21352.000  21192.443      0.001      0.163      0.091      0.052      0.161      0.051\n        22  13839.000  14456.737      0.006     -0.624      0.070     -0.171     -0.619     -0.170\n        23  22884.000  21340.268      0.052      1.579      0.095      0.511      1.610      0.521\n        24  16978.000  18742.417      0.083     -1.822      0.111     -0.644     -1.877     -0.664\n        25  14803.000  15549.105      0.008     -0.751      0.065     -0.199     -0.747     -0.198\n        26  17404.000  19288.601      0.093     -1.944      0.110     -0.684     -2.016     -0.709\n        27  22184.000  22284.811      0.000     -0.103      0.096     -0.034     -0.102     -0.033\n        28  13548.000  12405.070      0.025      1.162      0.083      0.350      1.167      0.352\n        29  14467.000  13497.438      0.018      0.987      0.086      0.304      0.987      0.304\n        30  15942.000  16641.473      0.007     -0.705      0.068     -0.190     -0.701     -0.189\n        31  23174.000  23377.179      0.001     -0.209      0.108     -0.073     -0.207     -0.072\n        32  23780.000  23525.004      0.001      0.260      0.092      0.083      0.257      0.082\n        33  25410.000  24071.188      0.040      1.370      0.096      0.446      1.386      0.451\n        34  14861.000  14043.622      0.014      0.834      0.091      0.263      0.831      0.262\n        35  16882.000  17733.841      0.012     -0.863      0.077     -0.249     -0.860     -0.249\n        36  24170.000  24469.547      0.003     -0.312      0.127     -0.119     -0.309     -0.118\n        37  15990.000  15135.990      0.018      0.878      0.104      0.300      0.876      0.299\n        38  26330.000  25163.556      0.035      1.202      0.109      0.420      1.209      0.422\n        39  17949.000  18826.209      0.017     -0.897      0.093     -0.288     -0.895     -0.287\n        40  25685.000  26108.099      0.008     -0.452      0.169     -0.204     -0.447     -0.202\n        41  27837.000  26802.108      0.039      1.087      0.141      0.440      1.089      0.441\n        42  18838.000  19918.577      0.033     -1.119      0.117     -0.407     -1.123     -0.408\n        43  17483.000  16774.542      0.018      0.743      0.138      0.297      0.739      0.295\n        44  19207.000  20464.761      0.052     -1.313      0.131     -0.511     -1.325     -0.515\n        45  19346.000  18959.278      0.009      0.423      0.208      0.216      0.419      0.214\n==================================================================================================",
            "code"
        ],
        [
            "or get a dataframe",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "df_infl = infl.summary_frame()",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "df_infl[:5]",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "Now plot the residuals within the groups separately:",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "Text(0, 0.5, 'Residuals')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_20_1.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_20_1.png\"/>",
            "code"
        ],
        [
            "Now we will test some interactions using anova or f_test",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "interX_lm = ols(\"S ~ C(E) * X + C(M)\", salary_table).fit()\nprint(interX_lm.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                      S   R-squared:                       0.961\nModel:                            OLS   Adj. R-squared:                  0.955\nMethod:                 Least Squares   F-statistic:                     158.6\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           8.23e-26\nTime:                        17:11:05   Log-Likelihood:                -379.47\nNo. Observations:                  46   AIC:                             772.9\nDf Residuals:                      39   BIC:                             785.7\nDf Model:                           6\nCovariance Type:            nonrobust\n===============================================================================\n                  coef    std err          t      P|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept    7256.2800    549.494     13.205      0.000    6144.824    8367.736\nC(E)[T.2]    4172.5045    674.966      6.182      0.000    2807.256    5537.753\nC(E)[T.3]    3946.3649    686.693      5.747      0.000    2557.396    5335.333\nC(M)[T.1]    7102.4539    333.442     21.300      0.000    6428.005    7776.903\nX             632.2878     53.185     11.888      0.000     524.710     739.865\nC(E)[T.2]:X  -125.5147     69.863     -1.797      0.080    -266.826      15.796\nC(E)[T.3]:X  -141.2741     89.281     -1.582      0.122    -321.861      39.313\n==============================================================================\nOmnibus:                        0.432   Durbin-Watson:                   2.179\nProb(Omnibus):                  0.806   Jarque-Bera (JB):                0.590\nSkew:                           0.144   Prob(JB):                        0.744\nKurtosis:                       2.526   Cond. No.                         69.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "Do an ANOVA check",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "from statsmodels.stats.api import anova_lm\n\ntable1 = anova_lm(lm, interX_lm)\nprint(table1)\n\ninterM_lm = ols(\"S ~ X + C(E)*C(M)\", data=salary_table).fit()\nprint(interM_lm.summary())\n\ntable2 = anova_lm(lm, interM_lm)\nprint(table2)",
            "code"
        ],
        [
            "df_resid           ssr  df_diff       ss_diff         F    Pr(F)\n0      41.0  4.328072e+07      0.0           NaN       NaN       NaN\n1      39.0  3.941068e+07      2.0  3.870040e+06  1.914856  0.160964\n                            OLS Regression Results\n==============================================================================\nDep. Variable:                      S   R-squared:                       0.999\nModel:                            OLS   Adj. R-squared:                  0.999\nMethod:                 Least Squares   F-statistic:                     5517.\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           1.67e-55\nTime:                        17:11:05   Log-Likelihood:                -298.74\nNo. Observations:                  46   AIC:                             611.5\nDf Residuals:                      39   BIC:                             624.3\nDf Model:                           6\nCovariance Type:            nonrobust\n=======================================================================================\n                          coef    std err          t      P|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept            9472.6854     80.344    117.902      0.000    9310.175    9635.196\nC(E)[T.2]            1381.6706     77.319     17.870      0.000    1225.279    1538.063\nC(E)[T.3]            1730.7483    105.334     16.431      0.000    1517.690    1943.806\nC(M)[T.1]            3981.3769    101.175     39.351      0.000    3776.732    4186.022\nC(E)[T.2]:C(M)[T.1]  4902.5231    131.359     37.322      0.000    4636.825    5168.222\nC(E)[T.3]:C(M)[T.1]  3066.0351    149.330     20.532      0.000    2763.986    3368.084\nX                     496.9870      5.566     89.283      0.000     485.728     508.246\n==============================================================================\nOmnibus:                       74.761   Durbin-Watson:                   2.244\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1037.873\nSkew:                          -4.103   Prob(JB):                    4.25e-226\nKurtosis:                      24.776   Cond. No.                         79.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n   df_resid           ssr  df_diff       ss_diff           F        Pr(F)\n0      41.0  4.328072e+07      0.0           NaN         NaN           NaN\n1      39.0  1.178168e+06      2.0  4.210255e+07  696.844466  3.025504e-31",
            "code"
        ],
        [
            "The design matrix as a DataFrame",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "interM_lm.model.data.orig_exog[:5]",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "The design matrix as an ndarray",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "interM_lm.model.exog\ninterM_lm.model.exog_names",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "['Intercept',\n 'C(E)[T.2]',\n 'C(E)[T.3]',\n 'C(M)[T.1]',\n 'C(E)[T.2]:C(M)[T.1]',\n 'C(E)[T.3]:C(M)[T.1]',\n 'X']",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "infl = interM_lm.get_influence()\nresid = infl.resid_studentized_internal\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        resid[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X\")\nplt.ylabel(\"standardized resids\")",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "Text(0, 0.5, 'standardized resids')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_29_1.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_29_1.png\"/>",
            "code"
        ],
        [
            "Looks like one observation is an outlier.",
            "markdown"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "drop_idx = abs(resid).argmax()\nprint(drop_idx)  # zero-based index\nidx = salary_table.index.drop(drop_idx)\n\nlm32 = ols(\"S ~ C(E) + X + C(M)\", data=salary_table, subset=idx).fit()\n\nprint(lm32.summary())\nprint(\"\\n\")\n\ninterX_lm32 = ols(\"S ~ C(E) * X + C(M)\", data=salary_table, subset=idx).fit()\n\nprint(interX_lm32.summary())\nprint(\"\\n\")\n\n\ntable3 = anova_lm(lm32, interX_lm32)\nprint(table3)\nprint(\"\\n\")\n\n\ninterM_lm32 = ols(\"S ~ X + C(E) * C(M)\", data=salary_table, subset=idx).fit()\n\ntable4 = anova_lm(lm32, interM_lm32)\nprint(table4)\nprint(\"\\n\")",
            "code"
        ],
        [
            "32\n                            OLS Regression Results\n==============================================================================\nDep. Variable:                      S   R-squared:                       0.955\nModel:                            OLS   Adj. R-squared:                  0.950\nMethod:                 Least Squares   F-statistic:                     211.7\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           2.45e-26\nTime:                        17:11:05   Log-Likelihood:                -373.79\nNo. Observations:                  45   AIC:                             757.6\nDf Residuals:                      40   BIC:                             766.6\nDf Model:                           4\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   8044.7518    392.781     20.482      0.000    7250.911    8838.592\nC(E)[T.2]   3129.5286    370.470      8.447      0.000    2380.780    3878.277\nC(E)[T.3]   2999.4451    416.712      7.198      0.000    2157.238    3841.652\nC(M)[T.1]   6866.9856    323.991     21.195      0.000    6212.175    7521.796\nX            545.7855     30.912     17.656      0.000     483.311     608.260\n==============================================================================\nOmnibus:                        2.511   Durbin-Watson:                   2.265\nProb(Omnibus):                  0.285   Jarque-Bera (JB):                1.400\nSkew:                          -0.044   Prob(JB):                        0.496\nKurtosis:                       2.140   Cond. No.                         33.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n                            OLS Regression Results\n==============================================================================\nDep. Variable:                      S   R-squared:                       0.959\nModel:                            OLS   Adj. R-squared:                  0.952\nMethod:                 Least Squares   F-statistic:                     147.7\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           8.97e-25\nTime:                        17:11:05   Log-Likelihood:                -371.70\nNo. Observations:                  45   AIC:                             757.4\nDf Residuals:                      38   BIC:                             770.0\nDf Model:                           6\nCovariance Type:            nonrobust\n===============================================================================\n                  coef    std err          t      P|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept    7266.0887    558.872     13.001      0.000    6134.711    8397.466\nC(E)[T.2]    4162.0846    685.728      6.070      0.000    2773.900    5550.269\nC(E)[T.3]    3940.4359    696.067      5.661      0.000    2531.322    5349.549\nC(M)[T.1]    7088.6387    345.587     20.512      0.000    6389.035    7788.243\nX             631.6892     53.950     11.709      0.000     522.473     740.905\nC(E)[T.2]:X  -125.5009     70.744     -1.774      0.084    -268.714      17.712\nC(E)[T.3]:X  -139.8410     90.728     -1.541      0.132    -323.511      43.829\n==============================================================================\nOmnibus:                        0.617   Durbin-Watson:                   2.194\nProb(Omnibus):                  0.734   Jarque-Bera (JB):                0.728\nSkew:                           0.162   Prob(JB):                        0.695\nKurtosis:                       2.468   Cond. No.                         68.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n   df_resid           ssr  df_diff       ss_diff         F    Pr(F)\n0      40.0  4.320910e+07      0.0           NaN       NaN       NaN\n1      38.0  3.937424e+07      2.0  3.834859e+06  1.850508  0.171042\n\n\n   df_resid           ssr  df_diff       ss_diff            F        Pr(F)\n0      40.0  4.320910e+07      0.0           NaN          NaN           NaN\n1      38.0  1.711881e+05      2.0  4.303791e+07  4776.734853  2.291239e-46",
            "code"
        ],
        [
            "Replot the residuals",
            "markdown"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "resid = interM_lm32.get_influence().summary_frame()[\"standard_resid\"]\n\nplt.figure(figsize=(6, 6))\nresid = resid.reindex(X.index)\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X.loc[idx],\n        resid.loc[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X[~[32]]\")\nplt.ylabel(\"standardized resids\")",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "Text(0, 0.5, 'standardized resids')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_33_1.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_33_1.png\"/>",
            "code"
        ],
        [
            "Plot the fitted values",
            "markdown"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "Text(0, 0.5, 'Salary')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_35_1.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_35_1.png\"/>",
            "code"
        ],
        [
            "From our first look at the data, the difference between Master\u2019s and PhD in the management group is different than in the non-management group. This is an interaction between the two qualitative variables management,M and education,E. We can visualize this by first removing the effect of experience, then plotting the means within each of the 6 groups using interaction.plot.",
            "markdown"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "U = S - X * interX_lm32.params[\"X\"]\n\nplt.figure(figsize=(6, 6))\ninteraction_plot(\n    E, M, U, colors=[\"red\", \"blue\"], markers=[\"^\", \"D\"], markersize=10, ax=plt.gca()\n)",
            "code"
        ],
        [
            "[20]:\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_37_0.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_37_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_37_1.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_37_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Statistics->ANOVA->Minority Employment Data": [
        [
            "[21]:",
            "code"
        ],
        [
            "try:\n    jobtest_table = pd.read_table(\"jobtest.table\")\nexcept:  # do not have data already\n    url = \"http://stats191.stanford.edu/data/jobtest.table\"\n    jobtest_table = pd.read_table(url)\n\nfactor_group = jobtest_table.groupby([\"MINORITY\"])\n\nfig, ax = plt.subplots(figsize=(6, 6))\ncolors = [\"purple\", \"green\"]\nmarkers = [\"o\", \"v\"]\nfor factor, group in factor_group:\n    ax.scatter(\n        group[\"TEST\"],\n        group[\"JPERF\"],\n        color=colors[factor],\n        marker=markers[factor],\n        s=12 ** 2,\n    )\nax.set_xlabel(\"TEST\")\nax.set_ylabel(\"JPERF\")",
            "code"
        ],
        [
            "/tmp/ipykernel_6876/3843909548.py:12: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n  for factor, group in factor_group:",
            "code"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "Text(0, 0.5, 'JPERF')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_39_2.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_39_2.png\"/>",
            "code"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "min_lm = ols(\"JPERF ~ TEST\", data=jobtest_table).fit()\nprint(min_lm.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                  JPERF   R-squared:                       0.517\nModel:                            OLS   Adj. R-squared:                  0.490\nMethod:                 Least Squares   F-statistic:                     19.25\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           0.000356\nTime:                        17:11:06   Log-Likelihood:                -36.614\nNo. Observations:                  20   AIC:                             77.23\nDf Residuals:                      18   BIC:                             79.22\nDf Model:                           1\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.0350      0.868      1.192      0.249      -0.789       2.859\nTEST           2.3605      0.538      4.387      0.000       1.230       3.491\n==============================================================================\nOmnibus:                        0.324   Durbin-Watson:                   2.896\nProb(Omnibus):                  0.850   Jarque-Bera (JB):                0.483\nSkew:                          -0.186   Prob(JB):                        0.785\nKurtosis:                       2.336   Cond. No.                         5.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(6, 6))\nfor factor, group in factor_group:\n    ax.scatter(\n        group[\"TEST\"],\n        group[\"JPERF\"],\n        color=colors[factor],\n        marker=markers[factor],\n        s=12 ** 2,\n    )\n\nax.set_xlabel(\"TEST\")\nax.set_ylabel(\"JPERF\")\nfig = abline_plot(model_results=min_lm, ax=ax)",
            "code"
        ],
        [
            "/tmp/ipykernel_6876/1345731516.py:2: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n  for factor, group in factor_group:\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_41_1.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_41_1.png\"/>",
            "code"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "min_lm2 = ols(\"JPERF ~ TEST + TEST:MINORITY\", data=jobtest_table).fit()\n\nprint(min_lm2.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                  JPERF   R-squared:                       0.632\nModel:                            OLS   Adj. R-squared:                  0.589\nMethod:                 Least Squares   F-statistic:                     14.59\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           0.000204\nTime:                        17:11:06   Log-Likelihood:                -33.891\nNo. Observations:                  20   AIC:                             73.78\nDf Residuals:                      17   BIC:                             76.77\nDf Model:                           2\nCovariance Type:            nonrobust\n=================================================================================\n                    coef    std err          t      P|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         1.1211      0.780      1.437      0.169      -0.525       2.768\nTEST              1.8276      0.536      3.412      0.003       0.698       2.958\nTEST:MINORITY     0.9161      0.397      2.306      0.034       0.078       1.754\n==============================================================================\nOmnibus:                        0.388   Durbin-Watson:                   3.008\nProb(Omnibus):                  0.823   Jarque-Bera (JB):                0.514\nSkew:                           0.050   Prob(JB):                        0.773\nKurtosis:                       2.221   Cond. No.                         5.96\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(6, 6))\nfor factor, group in factor_group:\n    ax.scatter(\n        group[\"TEST\"],\n        group[\"JPERF\"],\n        color=colors[factor],\n        marker=markers[factor],\n        s=12 ** 2,\n    )\n\nfig = abline_plot(\n    intercept=min_lm2.params[\"Intercept\"],\n    slope=min_lm2.params[\"TEST\"],\n    ax=ax,\n    color=\"purple\",\n)\nfig = abline_plot(\n    intercept=min_lm2.params[\"Intercept\"],\n    slope=min_lm2.params[\"TEST\"] + min_lm2.params[\"TEST:MINORITY\"],\n    ax=ax,\n    color=\"green\",\n)",
            "code"
        ],
        [
            "/tmp/ipykernel_6876/1032128782.py:2: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n  for factor, group in factor_group:\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_43_1.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_43_1.png\"/>",
            "code"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "min_lm3 = ols(\"JPERF ~ TEST + MINORITY\", data=jobtest_table).fit()\nprint(min_lm3.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                  JPERF   R-squared:                       0.572\nModel:                            OLS   Adj. R-squared:                  0.522\nMethod:                 Least Squares   F-statistic:                     11.38\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           0.000731\nTime:                        17:11:07   Log-Likelihood:                -35.390\nNo. Observations:                  20   AIC:                             76.78\nDf Residuals:                      17   BIC:                             79.77\nDf Model:                           2\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.6120      0.887      0.690      0.500      -1.260       2.483\nTEST           2.2988      0.522      4.400      0.000       1.197       3.401\nMINORITY       1.0276      0.691      1.487      0.155      -0.430       2.485\n==============================================================================\nOmnibus:                        0.251   Durbin-Watson:                   3.028\nProb(Omnibus):                  0.882   Jarque-Bera (JB):                0.437\nSkew:                          -0.059   Prob(JB):                        0.804\nKurtosis:                       2.286   Cond. No.                         5.72\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(6, 6))\nfor factor, group in factor_group:\n    ax.scatter(\n        group[\"TEST\"],\n        group[\"JPERF\"],\n        color=colors[factor],\n        marker=markers[factor],\n        s=12 ** 2,\n    )\n\nfig = abline_plot(\n    intercept=min_lm3.params[\"Intercept\"],\n    slope=min_lm3.params[\"TEST\"],\n    ax=ax,\n    color=\"purple\",\n)\nfig = abline_plot(\n    intercept=min_lm3.params[\"Intercept\"] + min_lm3.params[\"MINORITY\"],\n    slope=min_lm3.params[\"TEST\"],\n    ax=ax,\n    color=\"green\",\n)",
            "code"
        ],
        [
            "/tmp/ipykernel_6876/1594511031.py:2: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n  for factor, group in factor_group:\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_45_1.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_45_1.png\"/>",
            "code"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "min_lm4 = ols(\"JPERF ~ TEST * MINORITY\", data=jobtest_table).fit()\nprint(min_lm4.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                  JPERF   R-squared:                       0.664\nModel:                            OLS   Adj. R-squared:                  0.601\nMethod:                 Least Squares   F-statistic:                     10.55\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           0.000451\nTime:                        17:11:07   Log-Likelihood:                -32.971\nNo. Observations:                  20   AIC:                             73.94\nDf Residuals:                      16   BIC:                             77.92\nDf Model:                           3\nCovariance Type:            nonrobust\n=================================================================================\n                    coef    std err          t      P|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         2.0103      1.050      1.914      0.074      -0.216       4.236\nTEST              1.3134      0.670      1.959      0.068      -0.108       2.735\nMINORITY         -1.9132      1.540     -1.242      0.232      -5.179       1.352\nTEST:MINORITY     1.9975      0.954      2.093      0.053      -0.026       4.021\n==============================================================================\nOmnibus:                        3.377   Durbin-Watson:                   3.015\nProb(Omnibus):                  0.185   Jarque-Bera (JB):                1.330\nSkew:                           0.120   Prob(JB):                        0.514\nKurtosis:                       1.760   Cond. No.                         13.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(8, 6))\nfor factor, group in factor_group:\n    ax.scatter(\n        group[\"TEST\"],\n        group[\"JPERF\"],\n        color=colors[factor],\n        marker=markers[factor],\n        s=12 ** 2,\n    )\n\nfig = abline_plot(\n    intercept=min_lm4.params[\"Intercept\"],\n    slope=min_lm4.params[\"TEST\"],\n    ax=ax,\n    color=\"purple\",\n)\nfig = abline_plot(\n    intercept=min_lm4.params[\"Intercept\"] + min_lm4.params[\"MINORITY\"],\n    slope=min_lm4.params[\"TEST\"] + min_lm4.params[\"TEST:MINORITY\"],\n    ax=ax,\n    color=\"green\",\n)",
            "code"
        ],
        [
            "/tmp/ipykernel_6876/2552459825.py:2: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n  for factor, group in factor_group:\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_47_1.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_47_1.png\"/>",
            "code"
        ],
        [
            "[30]:",
            "code"
        ],
        [
            "# is there any effect of MINORITY on slope or intercept?\ntable5 = anova_lm(min_lm, min_lm4)\nprint(table5)",
            "code"
        ],
        [
            "df_resid        ssr  df_diff    ss_diff         F    Pr(F)\n0      18.0  45.568297      0.0        NaN       NaN       NaN\n1      16.0  31.655473      2.0  13.912824  3.516061  0.054236",
            "code"
        ],
        [
            "[31]:",
            "code"
        ],
        [
            "# is there any effect of MINORITY on intercept\ntable6 = anova_lm(min_lm, min_lm3)\nprint(table6)",
            "code"
        ],
        [
            "df_resid        ssr  df_diff   ss_diff         F    Pr(F)\n0      18.0  45.568297      0.0       NaN       NaN       NaN\n1      17.0  40.321546      1.0  5.246751  2.212087  0.155246",
            "code"
        ],
        [
            "[32]:",
            "code"
        ],
        [
            "# is there any effect of MINORITY on slope\ntable7 = anova_lm(min_lm, min_lm2)\nprint(table7)",
            "code"
        ],
        [
            "df_resid        ssr  df_diff    ss_diff         F    Pr(F)\n0      18.0  45.568297      0.0        NaN       NaN       NaN\n1      17.0  34.707653      1.0  10.860644  5.319603  0.033949",
            "code"
        ],
        [
            "[33]:",
            "code"
        ],
        [
            "# is it just the slope or both?\ntable8 = anova_lm(min_lm2, min_lm4)\nprint(table8)",
            "code"
        ],
        [
            "df_resid        ssr  df_diff  ss_diff         F    Pr(F)\n0      17.0  34.707653      0.0      NaN       NaN       NaN\n1      16.0  31.655473      1.0  3.05218  1.542699  0.232115",
            "code"
        ]
    ],
    "Examples->Statistics->ANOVA->One-way ANOVA": [
        [
            "[34]:",
            "code"
        ],
        [
            "try:\n    rehab_table = pd.read_csv(\"rehab.table\")\nexcept:\n    url = \"http://stats191.stanford.edu/data/rehab.csv\"\n    rehab_table = pd.read_table(url, delimiter=\",\")\n    rehab_table.to_csv(\"rehab.table\")\n\nfig, ax = plt.subplots(figsize=(8, 6))\nfig = rehab_table.boxplot(\"Time\", \"Fitness\", ax=ax, grid=False)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_53_0.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_53_0.png\"/>",
            "code"
        ],
        [
            "[35]:",
            "code"
        ],
        [
            "rehab_lm = ols(\"Time ~ C(Fitness)\", data=rehab_table).fit()\ntable9 = anova_lm(rehab_lm)\nprint(table9)\n\nprint(rehab_lm.model.data.orig_exog)",
            "code"
        ],
        [
            "df  sum_sq     mean_sq          F    PR(F)\nC(Fitness)   2.0   672.0  336.000000  16.961538  0.000041\nResidual    21.0   416.0   19.809524        NaN       NaN\n    Intercept  C(Fitness)[T.2]  C(Fitness)[T.3]\n0         1.0              0.0              0.0\n1         1.0              0.0              0.0\n2         1.0              0.0              0.0\n3         1.0              0.0              0.0\n4         1.0              0.0              0.0\n5         1.0              0.0              0.0\n6         1.0              0.0              0.0\n7         1.0              0.0              0.0\n8         1.0              1.0              0.0\n9         1.0              1.0              0.0\n10        1.0              1.0              0.0\n11        1.0              1.0              0.0\n12        1.0              1.0              0.0\n13        1.0              1.0              0.0\n14        1.0              1.0              0.0\n15        1.0              1.0              0.0\n16        1.0              1.0              0.0\n17        1.0              1.0              0.0\n18        1.0              0.0              1.0\n19        1.0              0.0              1.0\n20        1.0              0.0              1.0\n21        1.0              0.0              1.0\n22        1.0              0.0              1.0\n23        1.0              0.0              1.0",
            "code"
        ],
        [
            "[36]:",
            "code"
        ],
        [
            "print(rehab_lm.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                   Time   R-squared:                       0.618\nModel:                            OLS   Adj. R-squared:                  0.581\nMethod:                 Least Squares   F-statistic:                     16.96\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           4.13e-05\nTime:                        17:11:08   Log-Likelihood:                -68.286\nNo. Observations:                  24   AIC:                             142.6\nDf Residuals:                      21   BIC:                             146.1\nDf Model:                           2\nCovariance Type:            nonrobust\n===================================================================================\n                      coef    std err          t      P|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nIntercept          38.0000      1.574     24.149      0.000      34.728      41.272\nC(Fitness)[T.2]    -6.0000      2.111     -2.842      0.010     -10.390      -1.610\nC(Fitness)[T.3]   -14.0000      2.404     -5.824      0.000     -18.999      -9.001\n==============================================================================\nOmnibus:                        0.163   Durbin-Watson:                   2.209\nProb(Omnibus):                  0.922   Jarque-Bera (JB):                0.211\nSkew:                          -0.163   Prob(JB):                        0.900\nKurtosis:                       2.675   Cond. No.                         3.80\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ]
    ],
    "Examples->Statistics->ANOVA->Two-way ANOVA": [
        [
            "[37]:",
            "code"
        ],
        [
            "try:\n    kidney_table = pd.read_table(\"./kidney.table\")\nexcept:\n    url = \"http://stats191.stanford.edu/data/kidney.table\"\n    kidney_table = pd.read_csv(url, delim_whitespace=True)",
            "code"
        ],
        [
            "Explore the dataset",
            "markdown"
        ],
        [
            "[38]:",
            "code"
        ],
        [
            "kidney_table.head(10)",
            "code"
        ],
        [
            "[38]:",
            "code"
        ],
        [
            "Balanced panel",
            "markdown"
        ],
        [
            "[39]:",
            "code"
        ],
        [
            "kt = kidney_table\nplt.figure(figsize=(8, 6))\nfig = interaction_plot(\n    kt[\"Weight\"],\n    kt[\"Duration\"],\n    np.log(kt[\"Days\"] + 1),\n    colors=[\"red\", \"blue\"],\n    markers=[\"D\", \"^\"],\n    ms=10,\n    ax=plt.gca(),\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_61_0.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_61_0.png\"/>",
            "code"
        ],
        [
            "You have things available in the calling namespace available in the formula evaluation namespace",
            "markdown"
        ],
        [
            "[40]:",
            "code"
        ],
        [
            "kidney_lm = ols(\"np.log(Days+1) ~ C(Duration) * C(Weight)\", data=kt).fit()\n\ntable10 = anova_lm(kidney_lm)\n\nprint(\n    anova_lm(ols(\"np.log(Days+1) ~ C(Duration) + C(Weight)\", data=kt).fit(), kidney_lm)\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Duration)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Weight)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)",
            "code"
        ],
        [
            "df_resid        ssr  df_diff   ss_diff        F    Pr(F)\n0      56.0  29.624856      0.0       NaN      NaN       NaN\n1      54.0  28.989198      2.0  0.635658  0.59204  0.556748\n   df_resid        ssr  df_diff    ss_diff          F    Pr(F)\n0      58.0  46.596147      0.0        NaN        NaN       NaN\n1      56.0  29.624856      2.0  16.971291  16.040454  0.000003\n   df_resid        ssr  df_diff   ss_diff         F   Pr(F)\n0      57.0  31.964549      0.0       NaN       NaN      NaN\n1      56.0  29.624856      1.0  2.339693  4.422732  0.03997",
            "code"
        ]
    ],
    "Examples->Statistics->ANOVA->Sum of squares": [
        [
            "Illustrates the use of different types of sums of squares (I,II,II) and how the Sum contrast can be used to produce the same output between the 3.",
            "markdown"
        ],
        [
            "Types I and II are equivalent under a balanced design.",
            "markdown"
        ],
        [
            "Do not use Type III with non-orthogonal contrast - ie., Treatment",
            "markdown"
        ],
        [
            "[41]:",
            "code"
        ],
        [
            "sum_lm = ols(\"np.log(Days+1) ~ C(Duration, Sum) * C(Weight, Sum)\", data=kt).fit()\n\nprint(anova_lm(sum_lm))\nprint(anova_lm(sum_lm, typ=2))\nprint(anova_lm(sum_lm, typ=3))",
            "code"
        ],
        [
            "df     sum_sq   mean_sq          F    PR(F)\nC(Duration, Sum)                  1.0   2.339693  2.339693   4.358293  0.041562\nC(Weight, Sum)                    2.0  16.971291  8.485645  15.806745  0.000004\nC(Duration, Sum):C(Weight, Sum)   2.0   0.635658  0.317829   0.592040  0.556748\nResidual                         54.0  28.989198  0.536837        NaN       NaN\n                                    sum_sq    df          F    PR(F)\nC(Duration, Sum)                  2.339693   1.0   4.358293  0.041562\nC(Weight, Sum)                   16.971291   2.0  15.806745  0.000004\nC(Duration, Sum):C(Weight, Sum)   0.635658   2.0   0.592040  0.556748\nResidual                         28.989198  54.0        NaN       NaN\n                                     sum_sq    df           F        PR(F)\nIntercept                        156.301830   1.0  291.153237  2.077589e-23\nC(Duration, Sum)                   2.339693   1.0    4.358293  4.156170e-02\nC(Weight, Sum)                    16.971291   2.0   15.806745  3.944502e-06\nC(Duration, Sum):C(Weight, Sum)    0.635658   2.0    0.592040  5.567479e-01\nResidual                          28.989198  54.0         NaN           NaN",
            "code"
        ],
        [
            "[42]:",
            "code"
        ],
        [
            "nosum_lm = ols(\n    \"np.log(Days+1) ~ C(Duration, Treatment) * C(Weight, Treatment)\", data=kt\n).fit()\nprint(anova_lm(nosum_lm))\nprint(anova_lm(nosum_lm, typ=2))\nprint(anova_lm(nosum_lm, typ=3))",
            "code"
        ],
        [
            "df     sum_sq   mean_sq          F    PR(F)\nC(Duration, Treatment)                        1.0   2.339693  2.339693   4.358293  0.041562\nC(Weight, Treatment)                          2.0  16.971291  8.485645  15.806745  0.000004\nC(Duration, Treatment):C(Weight, Treatment)   2.0   0.635658  0.317829   0.592040  0.556748\nResidual                                     54.0  28.989198  0.536837        NaN       NaN\n                                                sum_sq    df          F    PR(F)\nC(Duration, Treatment)                        2.339693   1.0   4.358293  0.041562\nC(Weight, Treatment)                         16.971291   2.0  15.806745  0.000004\nC(Duration, Treatment):C(Weight, Treatment)   0.635658   2.0   0.592040  0.556748\nResidual                                     28.989198  54.0        NaN       NaN\n                                                sum_sq    df          F    PR(F)\nIntercept                                    10.427596   1.0  19.424139  0.000050\nC(Duration, Treatment)                        0.054293   1.0   0.101134  0.751699\nC(Weight, Treatment)                         11.703387   2.0  10.900317  0.000106\nC(Duration, Treatment):C(Weight, Treatment)   0.635658   2.0   0.592040  0.556748\nResidual                                     28.989198  54.0        NaN       NaN",
            "code"
        ]
    ],
    "Examples->Statistics->Meta-Analysis in statsmodels": [
        [
            "Statsmodels include basic methods for meta-analysis. This notebook illustrates the current usage.",
            "markdown"
        ],
        [
            "Status: The results have been verified against R meta and metafor packages. However, the API is still experimental and will still change. Some options for additional methods that are available in R meta and metafor are missing.",
            "markdown"
        ],
        [
            "The support for meta-analysis has 3 parts:",
            "markdown"
        ],
        [
            "effect size functions: this currently includes effectsize_smd computes effect size and their standard errors for standardized mean difference,\neffectsize_2proportions computes effect sizes for comparing two independent proportions using risk difference, (log) risk ratio, (log) odds-ratio or arcsine square root transformation",
            "markdown"
        ],
        [
            "The combine_effects computes fixed and random effects estimate for the overall mean or effect. The returned results instance includes a forest plot function.",
            "markdown"
        ],
        [
            "helper functions to estimate the random effect variance, tau-squared",
            "markdown"
        ],
        [
            "The estimate of the overall effect size in combine_effects can also be performed using WLS or GLM with var_weights.",
            "markdown"
        ],
        [
            "Finally, the meta-analysis functions currently do not include the Mantel-Hanszel method. However, the fixed effects results can be computed directly using StratifiedTable as illustrated below.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\nfrom scipy import stats, optimize\n\nfrom statsmodels.regression.linear_model import WLS\nfrom statsmodels.genmod.generalized_linear_model import GLM\n\nfrom statsmodels.stats.meta_analysis import (\n    effectsize_smd,\n    effectsize_2proportions,\n    combine_effects,\n    _fit_tau_iterative,\n    _fit_tau_mm,\n    _fit_tau_iter_mm,\n)\n\n# increase line length for pandas\npd.set_option(\"display.width\", 100)",
            "code"
        ]
    ],
    "Examples->Statistics->Meta-Analysis in statsmodels->Example": [
        [
            "[3]:",
            "code"
        ],
        [
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "['Carroll', 'Grant', 'Peck', 'Donat', 'Stewart', 'Young']",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "mean2, sd2, nobs2, mean1, sd1, nobs1 = np.asarray(\n    dframe1[[\"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]]\n).T\nrownames = dframe1[\"study\"]\nrownames.tolist()",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "['Carroll', 'Grant', 'Peck', 'Donat', 'Stewart', 'Young']",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "np.array(nobs1 + nobs2)",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "array([120, 130,  80, 400,  95, 170])",
            "code"
        ]
    ],
    "Examples->Statistics->Meta-Analysis in statsmodels->Example->estimate effect size standardized mean difference": [
        [
            "[6]:",
            "code"
        ],
        [
            "eff, var_eff = effectsize_smd(mean2, sd2, nobs2, mean1, sd1, nobs1)",
            "code"
        ]
    ],
    "Examples->Statistics->Meta-Analysis in statsmodels->Example->Using one-step chi2, DerSimonian-Laird estimate for random effects variance tau": [
        [
            "Method option for random effect method_re=\"chi2\" or method_re=\"dl\", both names are accepted. This is commonly referred to as the DerSimonian-Laird method, it is based on a moment estimator based on pearson chi2 from the fixed effects estimate.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "res3 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=True, row_names=rownames)\n# TODO: we still need better information about conf_int of individual samples\n# We don't have enough information in the model for individual confidence intervals\n# if those are not based on normal distribution.\nres3.conf_int_samples(nobs=np.array(nobs1 + nobs2))\nprint(res3.summary_frame())",
            "code"
        ],
        [
            "eff    sd_eff    ci_low    ci_upp      w_fe      w_re\nCarroll            0.094524  0.182680 -0.267199  0.456248  0.123885  0.157529\nGrant              0.277356  0.176279 -0.071416  0.626129  0.133045  0.162828\nPeck               0.366546  0.225573 -0.082446  0.815538  0.081250  0.126223\nDonat              0.664385  0.102748  0.462389  0.866381  0.391606  0.232734\nStewart            0.461808  0.208310  0.048203  0.875413  0.095275  0.137949\nYoung              0.185165  0.153729 -0.118312  0.488641  0.174939  0.182736\nfixed effect       0.414961  0.064298  0.249677  0.580245  1.000000       NaN\nrandom effect      0.358486  0.105462  0.087388  0.629583       NaN  1.000000\nfixed effect wls   0.414961  0.099237  0.159864  0.670058  1.000000       NaN\nrandom effect wls  0.358486  0.090328  0.126290  0.590682       NaN  1.000000",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "res3.cache_ci",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "{(0.05,\n  True): (array([-0.26719942, -0.07141628, -0.08244568,  0.46238908,  0.04820269,\n         -0.1183121 ]), array([0.45624817, 0.62612908, 0.81553838, 0.86638112, 0.87541326,\n         0.48864139]))}",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "res3.method_re",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "'chi2'",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "fig = res3.plot_forest()\nfig.set_figheight(6)\nfig.set_figwidth(6)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_metaanalysis1_13_0.png\" src=\"../../../_images/examples_notebooks_generated_metaanalysis1_13_0.png\"/>",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "res3 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False, row_names=rownames)\n# TODO: we still need better information about conf_int of individual samples\n# We don't have enough information in the model for individual confidence intervals\n# if those are not based on normal distribution.\nres3.conf_int_samples(nobs=np.array(nobs1 + nobs2))\nprint(res3.summary_frame())",
            "code"
        ],
        [
            "eff    sd_eff    ci_low    ci_upp      w_fe      w_re\nCarroll            0.094524  0.182680 -0.263521  0.452570  0.123885  0.157529\nGrant              0.277356  0.176279 -0.068144  0.622857  0.133045  0.162828\nPeck               0.366546  0.225573 -0.075569  0.808662  0.081250  0.126223\nDonat              0.664385  0.102748  0.463002  0.865768  0.391606  0.232734\nStewart            0.461808  0.208310  0.053527  0.870089  0.095275  0.137949\nYoung              0.185165  0.153729 -0.116139  0.486468  0.174939  0.182736\nfixed effect       0.414961  0.064298  0.288939  0.540984  1.000000       NaN\nrandom effect      0.358486  0.105462  0.151785  0.565187       NaN  1.000000\nfixed effect wls   0.414961  0.099237  0.220460  0.609462  1.000000       NaN\nrandom effect wls  0.358486  0.090328  0.181446  0.535526       NaN  1.000000",
            "code"
        ]
    ],
    "Examples->Statistics->Meta-Analysis in statsmodels->Example->Using iterated, Paule-Mandel estimate for random effects variance tau": [
        [
            "The method commonly referred to as Paule-Mandel estimate is a method of moment estimate for the random effects variance that iterates between mean and variance estimate until convergence.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "res4 = combine_effects(\n    eff, var_eff, method_re=\"iterated\", use_t=False, row_names=rownames\n)\nres4_df = res4.summary_frame()\nprint(\"method RE:\", res4.method_re)\nprint(res4.summary_frame())\nfig = res4.plot_forest()",
            "code"
        ],
        [
            "method RE: iterated\n                        eff    sd_eff    ci_low    ci_upp      w_fe      w_re\nCarroll            0.094524  0.182680 -0.263521  0.452570  0.123885  0.152619\nGrant              0.277356  0.176279 -0.068144  0.622857  0.133045  0.159157\nPeck               0.366546  0.225573 -0.075569  0.808662  0.081250  0.116228\nDonat              0.664385  0.102748  0.463002  0.865768  0.391606  0.257767\nStewart            0.461808  0.208310  0.053527  0.870089  0.095275  0.129428\nYoung              0.185165  0.153729 -0.116139  0.486468  0.174939  0.184799\nfixed effect       0.414961  0.064298  0.288939  0.540984  1.000000       NaN\nrandom effect      0.366419  0.092390  0.185338  0.547500       NaN  1.000000\nfixed effect wls   0.414961  0.099237  0.220460  0.609462  1.000000       NaN\nrandom effect wls  0.366419  0.092390  0.185338  0.547500       NaN  1.000000\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_metaanalysis1_16_1.png\" src=\"../../../_images/examples_notebooks_generated_metaanalysis1_16_1.png\"/>",
            "code"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "",
            "code"
        ]
    ],
    "Examples->Statistics->Meta-Analysis in statsmodels->Example Kacker interlaboratory mean": [
        [
            "In this example the effect size is the mean of measurements in a lab. We combine the estimates from several labs to estimate and overall average.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "eff = np.array([61.00, 61.40, 62.21, 62.30, 62.34, 62.60, 62.70, 62.84, 65.90])\nvar_eff = np.array(\n    [0.2025, 1.2100, 0.0900, 0.2025, 0.3844, 0.5625, 0.0676, 0.0225, 1.8225]\n)\nrownames = [\"PTB\", \"NMi\", \"NIMC\", \"KRISS\", \"LGC\", \"NRC\", \"IRMM\", \"NIST\", \"LNE\"]",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "res2_DL = combine_effects(eff, var_eff, method_re=\"dl\", use_t=True, row_names=rownames)\nprint(\"method RE:\", res2_DL.method_re)\nprint(res2_DL.summary_frame())\nfig = res2_DL.plot_forest()\nfig.set_figheight(6)\nfig.set_figwidth(6)",
            "code"
        ],
        [
            "method RE: dl\n                         eff    sd_eff     ci_low     ci_upp      w_fe      w_re\nPTB                61.000000  0.450000  60.118016  61.881984  0.057436  0.123113\nNMi                61.400000  1.100000  59.244040  63.555960  0.009612  0.040314\nNIMC               62.210000  0.300000  61.622011  62.797989  0.129230  0.159749\nKRISS              62.300000  0.450000  61.418016  63.181984  0.057436  0.123113\nLGC                62.340000  0.620000  61.124822  63.555178  0.030257  0.089810\nNRC                62.600000  0.750000  61.130027  64.069973  0.020677  0.071005\nIRMM               62.700000  0.260000  62.190409  63.209591  0.172052  0.169810\nNIST               62.840000  0.150000  62.546005  63.133995  0.516920  0.194471\nLNE                65.900000  1.350000  63.254049  68.545951  0.006382  0.028615\nfixed effect       62.583397  0.107846  62.334704  62.832090  1.000000       NaN\nrandom effect      62.390139  0.245750  61.823439  62.956838       NaN  1.000000\nfixed effect wls   62.583397  0.189889  62.145512  63.021282  1.000000       NaN\nrandom effect wls  62.390139  0.294776  61.710384  63.069893       NaN  1.000000",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/stats/meta_analysis.py:106: UserWarning: `use_t=True` requires `nobs` for each sample or `ci_func`. Using normal distribution for confidence interval of individual samples.\n  warnings.warn(msg)\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_metaanalysis1_20_2.png\" src=\"../../../_images/examples_notebooks_generated_metaanalysis1_20_2.png\"/>",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "res2_PM = combine_effects(eff, var_eff, method_re=\"pm\", use_t=True, row_names=rownames)\nprint(\"method RE:\", res2_PM.method_re)\nprint(res2_PM.summary_frame())\nfig = res2_PM.plot_forest()\nfig.set_figheight(6)\nfig.set_figwidth(6)",
            "code"
        ],
        [
            "method RE: pm\n                         eff    sd_eff     ci_low     ci_upp      w_fe      w_re\nPTB                61.000000  0.450000  60.118016  61.881984  0.057436  0.125857\nNMi                61.400000  1.100000  59.244040  63.555960  0.009612  0.059656\nNIMC               62.210000  0.300000  61.622011  62.797989  0.129230  0.143658\nKRISS              62.300000  0.450000  61.418016  63.181984  0.057436  0.125857\nLGC                62.340000  0.620000  61.124822  63.555178  0.030257  0.104850\nNRC                62.600000  0.750000  61.130027  64.069973  0.020677  0.090122\nIRMM               62.700000  0.260000  62.190409  63.209591  0.172052  0.147821\nNIST               62.840000  0.150000  62.546005  63.133995  0.516920  0.156980\nLNE                65.900000  1.350000  63.254049  68.545951  0.006382  0.045201\nfixed effect       62.583397  0.107846  62.334704  62.832090  1.000000       NaN\nrandom effect      62.407620  0.338030  61.628120  63.187119       NaN  1.000000\nfixed effect wls   62.583397  0.189889  62.145512  63.021282  1.000000       NaN\nrandom effect wls  62.407620  0.338030  61.628120  63.187120       NaN  1.000000",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/stats/meta_analysis.py:106: UserWarning: `use_t=True` requires `nobs` for each sample or `ci_func`. Using normal distribution for confidence interval of individual samples.\n  warnings.warn(msg)\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_metaanalysis1_21_2.png\" src=\"../../../_images/examples_notebooks_generated_metaanalysis1_21_2.png\"/>",
            "code"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "",
            "code"
        ]
    ],
    "Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions": [
        [
            "In the following example the random effect variance tau is estimated to be zero. I then change two counts in the data, so the second example has random effects variance greater than zero.",
            "markdown"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "import io",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "ss = \"\"\"\\\n    study,nei,nci,e1i,c1i,e2i,c2i,e3i,c3i,e4i,c4i\n    1,19,22,16.0,20.0,11,12,4.0,8.0,4,3\n    2,34,35,22.0,22.0,18,12,15.0,8.0,15,6\n    3,72,68,44.0,40.0,21,15,10.0,3.0,3,0\n    4,22,20,19.0,12.0,14,5,5.0,4.0,2,3\n    5,70,32,62.0,27.0,42,13,26.0,6.0,15,5\n    6,183,94,130.0,65.0,80,33,47.0,14.0,30,11\n    7,26,50,24.0,30.0,13,18,5.0,10.0,3,9\n    8,61,55,51.0,44.0,37,30,19.0,19.0,11,15\n    9,36,25,30.0,17.0,23,12,13.0,4.0,10,4\n    10,45,35,43.0,35.0,19,14,8.0,4.0,6,0\n    11,246,208,169.0,139.0,106,76,67.0,42.0,51,35\n    12,386,141,279.0,97.0,170,46,97.0,21.0,73,8\n    13,59,32,56.0,30.0,34,17,21.0,9.0,20,7\n    14,45,15,42.0,10.0,18,3,9.0,1.0,9,1\n    15,14,18,14.0,18.0,13,14,12.0,13.0,9,12\n    16,26,19,21.0,15.0,12,10,6.0,4.0,5,1\n    17,74,75,,,42,40,,,23,30\"\"\"\ndf3 = pd.read_csv(io.StringIO(ss))\ndf_12y = df3[[\"e2i\", \"nei\", \"c2i\", \"nci\"]]\n# TODO: currently 1 is reference, switch labels\ncount1, nobs1, count2, nobs2 = df_12y.values.T\ndta = df_12y.values.T",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "eff, var_eff = effectsize_2proportions(*dta, statistic=\"rd\")",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "eff, var_eff",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "(array([ 0.03349282,  0.18655462,  0.07107843,  0.38636364,  0.19375   ,\n         0.08609464,  0.14      ,  0.06110283,  0.15888889,  0.02222222,\n         0.06550969,  0.11417337,  0.04502119,  0.2       ,  0.15079365,\n        -0.06477733,  0.03423423]),\n array([0.02409958, 0.01376482, 0.00539777, 0.01989341, 0.01096641,\n        0.00376814, 0.01422338, 0.00842011, 0.01639261, 0.01227827,\n        0.00211165, 0.00219739, 0.01192067, 0.016     , 0.0143398 ,\n        0.02267994, 0.0066352 ]))",
            "code"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "res5 = combine_effects(\n    eff, var_eff, method_re=\"iterated\", use_t=False\n)  # , row_names=rownames)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(\"RE variance tau2:\", res5.tau2)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "code"
        ],
        [
            "method RE: iterated\nRE variance tau2: 0\n                        eff    sd_eff    ci_low    ci_upp      w_fe      w_re\n0                  0.033493  0.155240 -0.270773  0.337758  0.017454  0.017454\n1                  0.186555  0.117324 -0.043395  0.416505  0.030559  0.030559\n2                  0.071078  0.073470 -0.072919  0.215076  0.077928  0.077928\n3                  0.386364  0.141044  0.109922  0.662805  0.021145  0.021145\n4                  0.193750  0.104721 -0.011499  0.398999  0.038357  0.038357\n5                  0.086095  0.061385 -0.034218  0.206407  0.111630  0.111630\n6                  0.140000  0.119262 -0.093749  0.373749  0.029574  0.029574\n7                  0.061103  0.091761 -0.118746  0.240951  0.049956  0.049956\n8                  0.158889  0.128034 -0.092052  0.409830  0.025660  0.025660\n9                  0.022222  0.110807 -0.194956  0.239401  0.034259  0.034259\n10                 0.065510  0.045953 -0.024556  0.155575  0.199199  0.199199\n11                 0.114173  0.046876  0.022297  0.206049  0.191426  0.191426\n12                 0.045021  0.109182 -0.168971  0.259014  0.035286  0.035286\n13                 0.200000  0.126491 -0.047918  0.447918  0.026290  0.026290\n14                 0.150794  0.119749 -0.083910  0.385497  0.029334  0.029334\n15                -0.064777  0.150599 -0.359945  0.230390  0.018547  0.018547\n16                 0.034234  0.081457 -0.125418  0.193887  0.063395  0.063395\nfixed effect       0.096212  0.020509  0.056014  0.136410  1.000000       NaN\nrandom effect      0.096212  0.020509  0.056014  0.136410       NaN  1.000000\nfixed effect wls   0.096212  0.016521  0.063831  0.128593  1.000000       NaN\nrandom effect wls  0.096212  0.016521  0.063831  0.128593       NaN  1.000000\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_metaanalysis1_28_1.png\" src=\"../../../_images/examples_notebooks_generated_metaanalysis1_28_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance": [
        [
            "[21]:",
            "code"
        ],
        [
            "dta_c = dta.copy()\ndta_c.T[0, 0] = 18\ndta_c.T[1, 0] = 22\ndta_c.T",
            "code"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "array([[ 18,  19,  12,  22],\n       [ 22,  34,  12,  35],\n       [ 21,  72,  15,  68],\n       [ 14,  22,   5,  20],\n       [ 42,  70,  13,  32],\n       [ 80, 183,  33,  94],\n       [ 13,  26,  18,  50],\n       [ 37,  61,  30,  55],\n       [ 23,  36,  12,  25],\n       [ 19,  45,  14,  35],\n       [106, 246,  76, 208],\n       [170, 386,  46, 141],\n       [ 34,  59,  17,  32],\n       [ 18,  45,   3,  15],\n       [ 13,  14,  14,  18],\n       [ 12,  26,  10,  19],\n       [ 42,  74,  40,  75]])",
            "code"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "eff, var_eff = effectsize_2proportions(*dta_c, statistic=\"rd\")\nres5 = combine_effects(\n    eff, var_eff, method_re=\"iterated\", use_t=False\n)  # , row_names=rownames)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "code"
        ],
        [
            "method RE: iterated\n                        eff    sd_eff    ci_low    ci_upp      w_fe      w_re\n0                  0.401914  0.117873  0.170887  0.632940  0.029850  0.038415\n1                  0.304202  0.114692  0.079410  0.528993  0.031529  0.040258\n2                  0.071078  0.073470 -0.072919  0.215076  0.076834  0.081017\n3                  0.386364  0.141044  0.109922  0.662805  0.020848  0.028013\n4                  0.193750  0.104721 -0.011499  0.398999  0.037818  0.046915\n5                  0.086095  0.061385 -0.034218  0.206407  0.110063  0.102907\n6                  0.140000  0.119262 -0.093749  0.373749  0.029159  0.037647\n7                  0.061103  0.091761 -0.118746  0.240951  0.049255  0.058097\n8                  0.158889  0.128034 -0.092052  0.409830  0.025300  0.033270\n9                  0.022222  0.110807 -0.194956  0.239401  0.033778  0.042683\n10                 0.065510  0.045953 -0.024556  0.155575  0.196403  0.141871\n11                 0.114173  0.046876  0.022297  0.206049  0.188739  0.139144\n12                 0.045021  0.109182 -0.168971  0.259014  0.034791  0.043759\n13                 0.200000  0.126491 -0.047918  0.447918  0.025921  0.033985\n14                 0.150794  0.119749 -0.083910  0.385497  0.028922  0.037383\n15                -0.064777  0.150599 -0.359945  0.230390  0.018286  0.024884\n16                 0.034234  0.081457 -0.125418  0.193887  0.062505  0.069751\nfixed effect       0.110252  0.020365  0.070337  0.150167  1.000000       NaN\nrandom effect      0.117633  0.024913  0.068804  0.166463       NaN  1.000000\nfixed effect wls   0.110252  0.022289  0.066567  0.153937  1.000000       NaN\nrandom effect wls  0.117633  0.024913  0.068804  0.166463       NaN  1.000000\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_metaanalysis1_31_1.png\" src=\"../../../_images/examples_notebooks_generated_metaanalysis1_31_1.png\"/>",
            "code"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "code"
        ],
        [
            "method RE: chi2\n                        eff    sd_eff    ci_low    ci_upp      w_fe      w_re\n0                  0.401914  0.117873  0.170887  0.632940  0.029850  0.036114\n1                  0.304202  0.114692  0.079410  0.528993  0.031529  0.037940\n2                  0.071078  0.073470 -0.072919  0.215076  0.076834  0.080779\n3                  0.386364  0.141044  0.109922  0.662805  0.020848  0.025973\n4                  0.193750  0.104721 -0.011499  0.398999  0.037818  0.044614\n5                  0.086095  0.061385 -0.034218  0.206407  0.110063  0.105901\n6                  0.140000  0.119262 -0.093749  0.373749  0.029159  0.035356\n7                  0.061103  0.091761 -0.118746  0.240951  0.049255  0.056098\n8                  0.158889  0.128034 -0.092052  0.409830  0.025300  0.031063\n9                  0.022222  0.110807 -0.194956  0.239401  0.033778  0.040357\n10                 0.065510  0.045953 -0.024556  0.155575  0.196403  0.154854\n11                 0.114173  0.046876  0.022297  0.206049  0.188739  0.151236\n12                 0.045021  0.109182 -0.168971  0.259014  0.034791  0.041435\n13                 0.200000  0.126491 -0.047918  0.447918  0.025921  0.031761\n14                 0.150794  0.119749 -0.083910  0.385497  0.028922  0.035095\n15                -0.064777  0.150599 -0.359945  0.230390  0.018286  0.022976\n16                 0.034234  0.081457 -0.125418  0.193887  0.062505  0.068449\nfixed effect       0.110252  0.020365  0.070337  0.150167  1.000000       NaN\nrandom effect      0.115580  0.023557  0.069410  0.161751       NaN  1.000000\nfixed effect wls   0.110252  0.022289  0.066567  0.153937  1.000000       NaN\nrandom effect wls  0.115580  0.024241  0.068068  0.163093       NaN  1.000000\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_metaanalysis1_32_1.png\" src=\"../../../_images/examples_notebooks_generated_metaanalysis1_32_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Replicate fixed effect analysis using GLM with var_weights": [
        [
            "combine_effects computes weighted average estimates which can be replicated using GLM with var_weights or with WLS. The scale option in GLM.fit can be used to replicate fixed meta-analysis with fixed and with HKSJ/WLS scale",
            "markdown"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "from statsmodels.genmod.generalized_linear_model import GLM",
            "code"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "eff, var_eff = effectsize_2proportions(*dta_c, statistic=\"or\")\nres = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres_frame = res.summary_frame()\nprint(res_frame.iloc[-4:])",
            "code"
        ],
        [
            "eff    sd_eff    ci_low    ci_upp  w_fe  w_re\nfixed effect       0.428037  0.090287  0.251076  0.604997   1.0   NaN\nrandom effect      0.429520  0.091377  0.250425  0.608615   NaN   1.0\nfixed effect wls   0.428037  0.090798  0.250076  0.605997   1.0   NaN\nrandom effect wls  0.429520  0.091595  0.249997  0.609044   NaN   1.0",
            "code"
        ],
        [
            "We need to fix scale=1 in order to replicate standard errors for the usual meta-analysis.",
            "markdown"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "weights = 1 / var_eff\nmod_glm = GLM(eff, np.ones(len(eff)), var_weights=weights)\nres_glm = mod_glm.fit(scale=1.0)\nprint(res_glm.summary().tables[1])",
            "code"
        ],
        [
            "==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.4280      0.090      4.741      0.000       0.251       0.605\n==============================================================================",
            "code"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "# check results\nres_glm.scale, res_glm.conf_int() - res_frame.loc[\n    \"fixed effect\", [\"ci_low\", \"ci_upp\"]\n].values",
            "code"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "(array(1.), array([[-1.11022302e-16, -1.11022302e-16]]))",
            "code"
        ],
        [
            "Using HKSJ variance adjustment in meta-analysis is equivalent to estimating the scale using pearson chi2, which is also the default for the gaussian family.",
            "markdown"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "res_glm = mod_glm.fit(scale=\"x2\")\nprint(res_glm.summary().tables[1])",
            "code"
        ],
        [
            "==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.4280      0.091      4.714      0.000       0.250       0.606\n==============================================================================",
            "code"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "# check results\nres_glm.scale, res_glm.conf_int() - res_frame.loc[\n    \"fixed effect\", [\"ci_low\", \"ci_upp\"]\n].values",
            "code"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "(1.0113358914264383, array([[-0.00100017,  0.00100017]]))",
            "code"
        ]
    ],
    "Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables": [
        [
            "The fixed effect for the log-odds-ratio using the Mantel-Hanszel can be directly computed using StratifiedTable.",
            "markdown"
        ],
        [
            "We need to create a 2 x 2 x k contingency table to be used with StratifiedTable.",
            "markdown"
        ],
        [
            "[30]:",
            "code"
        ],
        [
            "t, nt, c, nc = dta_c\ncounts = np.column_stack([t, nt - t, c, nc - c])\nctables = counts.T.reshape(2, 2, -1)\nctables[:, :, 0]",
            "code"
        ],
        [
            "[30]:",
            "code"
        ],
        [
            "array([[18,  1],\n       [12, 10]])",
            "code"
        ],
        [
            "[31]:",
            "code"
        ],
        [
            "counts[0]",
            "code"
        ],
        [
            "[31]:",
            "code"
        ],
        [
            "array([18,  1, 12, 10])",
            "code"
        ],
        [
            "[32]:",
            "code"
        ],
        [
            "dta_c.T[0]",
            "code"
        ],
        [
            "[32]:",
            "code"
        ],
        [
            "array([18, 19, 12, 22])",
            "code"
        ],
        [
            "[33]:",
            "code"
        ],
        [
            "import statsmodels.stats.api as smstats",
            "code"
        ],
        [
            "[34]:",
            "code"
        ],
        [
            "st = smstats.StratifiedTable(ctables.astype(np.float64))",
            "code"
        ],
        [
            "compare pooled log-odds-ratio and standard error to R meta package",
            "markdown"
        ],
        [
            "[35]:",
            "code"
        ],
        [
            "st.logodds_pooled, st.logodds_pooled - 0.4428186730553189  # R meta",
            "code"
        ],
        [
            "[35]:",
            "code"
        ],
        [
            "(0.4428186730553187, -2.220446049250313e-16)",
            "code"
        ],
        [
            "[36]:",
            "code"
        ],
        [
            "st.logodds_pooled_se, st.logodds_pooled_se - 0.08928560091027186  # R meta",
            "code"
        ],
        [
            "[36]:",
            "code"
        ],
        [
            "(0.08928560091027186, 0.0)",
            "code"
        ],
        [
            "[37]:",
            "code"
        ],
        [
            "st.logodds_pooled_confint()",
            "code"
        ],
        [
            "[37]:",
            "code"
        ],
        [
            "(0.2678221109331691, 0.6178152351774683)",
            "code"
        ],
        [
            "[38]:",
            "code"
        ],
        [
            "print(st.test_equal_odds())",
            "code"
        ],
        [
            "pvalue      0.34496419319878724\nstatistic   17.64707987033203",
            "code"
        ],
        [
            "[39]:",
            "code"
        ],
        [
            "print(st.test_null_odds())",
            "code"
        ],
        [
            "pvalue      6.615053645964153e-07\nstatistic   24.724136624311814",
            "code"
        ],
        [
            "check conversion to stratified contingency table",
            "markdown"
        ],
        [
            "Row sums of each table are the sample sizes for treatment and control experiments",
            "markdown"
        ],
        [
            "[40]:",
            "code"
        ],
        [
            "ctables.sum(1)",
            "code"
        ],
        [
            "[40]:",
            "code"
        ],
        [
            "array([[ 19,  34,  72,  22,  70, 183,  26,  61,  36,  45, 246, 386,  59,\n         45,  14,  26,  74],\n       [ 22,  35,  68,  20,  32,  94,  50,  55,  25,  35, 208, 141,  32,\n         15,  18,  19,  75]])",
            "code"
        ],
        [
            "[41]:",
            "code"
        ],
        [
            "nt, nc",
            "code"
        ],
        [
            "[41]:",
            "code"
        ],
        [
            "(array([ 19,  34,  72,  22,  70, 183,  26,  61,  36,  45, 246, 386,  59,\n         45,  14,  26,  74]),\n array([ 22,  35,  68,  20,  32,  94,  50,  55,  25,  35, 208, 141,  32,\n         15,  18,  19,  75]))",
            "code"
        ],
        [
            "<strong>Results from R meta package</strong>",
            "markdown"
        ],
        [
            "res_mb_hk = metabin(e2i, nei, c2i, nci, data=dat2, sm=\"OR\", Q.Cochrane=FALSE, method=\"MH\", method.tau=\"DL\", hakn=FALSE, backtransf=FALSE)\n res_mb_hk\n     logOR            95%-CI %W(fixed) %W(random)\n1   2.7081 [ 0.5265; 4.8896]       0.3        0.7\n2   1.2567 [ 0.2658; 2.2476]       2.1        3.2\n3   0.3749 [-0.3911; 1.1410]       5.4        5.4\n4   1.6582 [ 0.3245; 2.9920]       0.9        1.8\n5   0.7850 [-0.0673; 1.6372]       3.5        4.4\n6   0.3617 [-0.1528; 0.8762]      12.1       11.8\n7   0.5754 [-0.3861; 1.5368]       3.0        3.4\n8   0.2505 [-0.4881; 0.9892]       6.1        5.8\n9   0.6506 [-0.3877; 1.6889]       2.5        3.0\n10  0.0918 [-0.8067; 0.9903]       4.5        3.9\n11  0.2739 [-0.1047; 0.6525]      23.1       21.4\n12  0.4858 [ 0.0804; 0.8911]      18.6       18.8\n13  0.1823 [-0.6830; 1.0476]       4.6        4.2\n14  0.9808 [-0.4178; 2.3795]       1.3        1.6\n15  1.3122 [-1.0055; 3.6299]       0.4        0.6\n16 -0.2595 [-1.4450; 0.9260]       3.1        2.3\n17  0.1384 [-0.5076; 0.7844]       8.5        7.6\n\nNumber of studies combined: k = 17\n\n                      logOR           95%-CI    z  p-value\nFixed effect model   0.4428 [0.2678; 0.6178] 4.96 &lt; 0.0001\nRandom effects model 0.4295 [0.2504; 0.6086] 4.70 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0017 [0.0000; 0.4589]; tau = 0.0410 [0.0000; 0.6774];\n I^2 = 1.1% [0.0%; 51.6%]; H = 1.01 [1.00; 1.44]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 16.18   16  0.4404\n\nDetails on meta-analytical method:\n- Mantel-Haenszel method\n- DerSimonian-Laird estimator for tau^2\n- Jackson method for confidence interval of tau^2 and tau\n\n res_mb_hk$TE.fixed\n[1] 0.4428186730553189\n res_mb_hk$seTE.fixed\n[1] 0.08928560091027186\n c(res_mb_hk$lower.fixed, res_mb_hk$upper.fixed)\n[1] 0.2678221109331694 0.6178152351774684",
            "code"
        ],
        [
            "[42]:",
            "code"
        ],
        [
            "print(st.summary())",
            "code"
        ],
        [
            "Estimate   LCB    UCB\n-----------------------------------------\nPooled odds           1.557   1.307 1.855\nPooled log odds       0.443   0.268 0.618\nPooled risk ratio     1.270\n\n                 Statistic P-value\n-----------------------------------\nTest of OR=1        24.724   0.000\nTest constant OR    17.647   0.345\n\n-----------------------\nNumber of tables   17\nMin n              32\nMax n             527\nAvg n             139\nTotal n          2362\n-----------------------",
            "code"
        ]
    ],
    "Examples->Statistics->Mediation analysis with duration data": [
        [
            "This notebook demonstrates mediation analysis when the mediator and outcome are duration variables, modeled using proportional hazards regression. These examples are based on simulated data.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.stats.mediation import Mediation",
            "code"
        ],
        [
            "Make the notebook reproducible.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "np.random.seed(3424)",
            "code"
        ],
        [
            "Specify a sample size.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "n = 1000",
            "code"
        ],
        [
            "Generate an exposure variable.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "exp = np.random.normal(size=n)",
            "code"
        ],
        [
            "Generate a mediator variable.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "def gen_mediator():\n    mn = np.exp(exp)\n    mtime0 = -mn * np.log(np.random.uniform(size=n))\n    ctime = -2 * mn * np.log(np.random.uniform(size=n))\n    mstatus = (ctime = mtime0).astype(int)\n    mtime = np.where(mtime0 &lt;= ctime, mtime0, ctime)\n    return mtime0, mtime, mstatus",
            "code"
        ],
        [
            "Generate an outcome variable.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "def gen_outcome(otype, mtime0):\n    if otype == \"full\":\n        lp = 0.5 * mtime0\n    elif otype == \"no\":\n        lp = exp\n    else:\n        lp = exp + mtime0\n    mn = np.exp(-lp)\n    ytime0 = -mn * np.log(np.random.uniform(size=n))\n    ctime = -2 * mn * np.log(np.random.uniform(size=n))\n    ystatus = (ctime = ytime0).astype(int)\n    ytime = np.where(ytime0 &lt;= ctime, ytime0, ctime)\n    return ytime, ystatus",
            "code"
        ],
        [
            "Build a dataframe containing all the relevant variables.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "def build_df(ytime, ystatus, mtime0, mtime, mstatus):\n    df = pd.DataFrame(\n        {\n            \"ytime\": ytime,\n            \"ystatus\": ystatus,\n            \"mtime\": mtime,\n            \"mstatus\": mstatus,\n            \"exp\": exp,\n        }\n    )\n    return df",
            "code"
        ],
        [
            "Run the full simulation and analysis, under a particular population structure of mediation.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "def run(otype):\n\n    mtime0, mtime, mstatus = gen_mediator()\n    ytime, ystatus = gen_outcome(otype, mtime0)\n    df = build_df(ytime, ystatus, mtime0, mtime, mstatus)\n\n    outcome_model = sm.PHReg.from_formula(\n        \"ytime ~ exp + mtime\", status=\"ystatus\", data=df\n    )\n    mediator_model = sm.PHReg.from_formula(\"mtime ~ exp\", status=\"mstatus\", data=df)\n\n    med = Mediation(\n        outcome_model,\n        mediator_model,\n        \"exp\",\n        \"mtime\",\n        outcome_predict_kwargs={\"pred_only\": True},\n    )\n    med_result = med.fit(n_rep=20)\n    print(med_result.summary())",
            "code"
        ],
        [
            "Run the example with full mediation",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "run(\"full\")",
            "code"
        ],
        [
            "Estimate  Lower CI bound  Upper CI bound  P-value\nACME (control)            0.742427        0.643339        0.862745      0.0\nACME (treated)            0.742427        0.643339        0.862745      0.0\nADE (control)             0.073017       -0.016189        0.155321      0.1\nADE (treated)             0.073017       -0.016189        0.155321      0.1\nTotal effect              0.815444        0.675214        0.919580      0.0\nProp. mediated (control)  0.912695        0.814965        1.025747      0.0\nProp. mediated (treated)  0.912695        0.814965        1.025747      0.0\nACME (average)            0.742427        0.643339        0.862745      0.0\nADE (average)             0.073017       -0.016189        0.155321      0.1\nProp. mediated (average)  0.912695        0.814965        1.025747      0.0",
            "code"
        ],
        [
            "Run the example with partial mediation",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "run(\"partial\")",
            "code"
        ],
        [
            "Estimate  Lower CI bound  Upper CI bound  P-value\nACME (control)            0.987067        0.801560        1.192019      0.0\nACME (treated)            0.987067        0.801560        1.192019      0.0\nADE (control)             1.071734        0.964214        1.150352      0.0\nADE (treated)             1.071734        0.964214        1.150352      0.0\nTotal effect              2.058801        1.862231        2.288170      0.0\nProp. mediated (control)  0.481807        0.417501        0.533773      0.0\nProp. mediated (treated)  0.481807        0.417501        0.533773      0.0\nACME (average)            0.987067        0.801560        1.192019      0.0\nADE (average)             1.071734        0.964214        1.150352      0.0\nProp. mediated (average)  0.481807        0.417501        0.533773      0.0",
            "code"
        ],
        [
            "Run the example with no mediation",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "run(\"no\")",
            "code"
        ],
        [
            "Estimate  Lower CI bound  Upper CI bound  P-value\nACME (control)            0.010200       -0.039434        0.065176      1.0\nACME (treated)            0.010200       -0.039434        0.065176      1.0\nADE (control)             0.902295        0.824526        0.984934      0.0\nADE (treated)             0.902295        0.824526        0.984934      0.0\nTotal effect              0.912495        0.834728        1.009958      0.0\nProp. mediated (control)  0.003763       -0.044186        0.065520      1.0\nProp. mediated (treated)  0.003763       -0.044186        0.065520      1.0\nACME (average)            0.010200       -0.039434        0.065176      1.0\nADE (average)             0.902295        0.824526        0.984934      0.0\nProp. mediated (average)  0.003763       -0.044186        0.065520      1.0",
            "code"
        ]
    ],
    "Examples->Statistics->Copulas": [
        [
            "[1]:",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set_style(\"darkgrid\")\nsns.mpl.rc(\"figure\", figsize=(8, 8))",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n\n\n\n\n\n\n\n\n<script type=\"text/javascript\">\nvar element = document.currentScript.previousSibling.previousSibling;\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n\n</script>",
            "code"
        ],
        [
            "When modeling a system, there are often cases where multiple parameters are involved. Each of these parameters could be described with a given Probability Density Function (PDF). If would like to be able to generate a new set of parameter values, we need to be able to sample from these distributions-also called marginals. There are mainly two cases: (i) PDFs are independent; (ii) there is a dependency. One way to model the dependency it to use a <strong>copula</strong>.",
            "markdown"
        ]
    ],
    "Examples->Statistics->Copulas->Sampling from a copula": [
        [
            "Let\u2019s use a bi-variate example and assume first that we have a prior and know how to model the dependence between our 2 variables.",
            "markdown"
        ],
        [
            "In this case, we are using the Gumbel copula and fix its hyperparameter theta=2. We can visualize it\u2019s 2-dimensional PDF.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "from statsmodels.distributions.copula.api import (\n    CopulaDistribution, GumbelCopula, IndependenceCopula)\n\ncopula = GumbelCopula(theta=2)\n_ = copula.plot_pdf()  # returns a matplotlib figure\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_copula_5_0.png\" src=\"../../../_images/examples_notebooks_generated_copula_5_0.png\"/>",
            "code"
        ],
        [
            "And we can sample the PDF.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "sample = copula.rvs(10000)\nh = sns.jointplot(x=sample[:, 0], y=sample[:, 1], kind=\"hex\")\n_ = h.set_axis_labels(\"X1\", \"X2\", fontsize=16)",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/tools/rng_qrng.py:54: FutureWarning: Passing `None` as the seed currently return the NumPy singleton RandomState\n(np.random.mtrand._rand). After release 0.13 this will change to using the\ndefault generator provided by NumPy (np.random.default_rng()). If you need\nreproducible draws, you should pass a seeded np.random.Generator, e.g.,\n\nimport numpy as np\nseed = 32839283923801\nrng = np.random.default_rng(seed)\"\n\n  warnings.warn(_future_warn, FutureWarning)\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_copula_7_1.png\" src=\"../../../_images/examples_notebooks_generated_copula_7_1.png\"/>",
            "code"
        ],
        [
            "Let\u2019s come back to our 2 variables for a second. In this case we consider them to be gamma and normally distributed. If they would be independent from each other, we could sample from each PDF individually. Here we use a convenient class to do the same operation.",
            "markdown"
        ]
    ],
    "Examples->Statistics->Copulas->Sampling from a copula->Reproducibility": [
        [
            "Generating reproducible random values from copulas required explicitly setting the seed argument. seed accepts either an initialized NumPy Generator or RandomState, or any argument acceptable to np.random.default_rng, e.g., an integer or a sequence of integers. This example uses an integer.",
            "markdown"
        ],
        [
            "The singleton RandomState that is directly exposed in the np.random distributions is not used, and setting np.random.seed has no effect on the values generated.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "marginals = [stats.gamma(2), stats.norm]\njoint_dist = CopulaDistribution(copula=IndependenceCopula(), marginals=marginals)\nsample = joint_dist.rvs(512, random_state=20210801)\nh = sns.jointplot(x=sample[:, 0], y=sample[:, 1], kind=\"scatter\")\n_ = h.set_axis_labels(\"X1\", \"X2\", fontsize=16)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_copula_9_0.png\" src=\"../../../_images/examples_notebooks_generated_copula_9_0.png\"/>",
            "code"
        ],
        [
            "Now, above we have expressed the dependency between our variables using a copula, we can use this copula to sample a new set of observation with the same convenient class.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "joint_dist = CopulaDistribution(copula, marginals)\n# Use an initialized Generator object\nrng = np.random.default_rng([2, 0, 2, 1, 0, 8, 0, 1])\nsample = joint_dist.rvs(512, random_state=rng)\nh = sns.jointplot(x=sample[:, 0], y=sample[:, 1], kind=\"scatter\")\n_ = h.set_axis_labels(\"X1\", \"X2\", fontsize=16)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_copula_11_0.png\" src=\"../../../_images/examples_notebooks_generated_copula_11_0.png\"/>",
            "code"
        ],
        [
            "There are two things to note here. (i) as in the independent case, the marginals are correctly showing a gamma and normal distribution; (ii) the dependence is visible between the two variables.",
            "markdown"
        ]
    ],
    "Examples->Statistics->Copulas->Estimating copula parameters": [
        [
            "Now, imagine we already have experimental data and we know that there is a dependency that can be expressed using a Gumbel copula. But we don\u2019t know what is the hyperparameter value for our copula. In this case, we can estimate the value.",
            "markdown"
        ],
        [
            "We are going to use the sample we just generated as we already know the value of the hyperparameter we should get: theta=2.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "copula = GumbelCopula()\ntheta = copula.fit_corr_param(sample)\nprint(theta)",
            "code"
        ],
        [
            "2.049379621506455",
            "code"
        ],
        [
            "We can see that the estimated hyperparameter value is close to the value set previously.",
            "markdown"
        ]
    ],
    "Examples->Time Series Analysis->Autoregressions": [
        [
            "This notebook introduces autoregression modeling using the AutoReg model. It also covers aspects of ar_select_order assists in selecting models that minimize an information criteria such as the AIC. An autoregressive model has dynamics given by\n\n\\[y_t = \\delta + \\phi_1 y_{t-1} + \\ldots + \\phi_p y_{t-p} + \\epsilon_t.\\]",
            "markdown"
        ],
        [
            "AutoReg also permits models with:",
            "markdown"
        ],
        [
            "Deterministic terms (trend)",
            "markdown"
        ],
        [
            "n: No deterministic term",
            "markdown"
        ],
        [
            "c: Constant (default)",
            "markdown"
        ],
        [
            "ct: Constant and time trend",
            "markdown"
        ],
        [
            "t: Time trend only",
            "markdown"
        ],
        [
            "Seasonal dummies (seasonal)",
            "markdown"
        ],
        [
            "True includes \\(s-1\\) dummies where \\(s\\) is the period of the time series (e.g., 12 for monthly)",
            "markdown"
        ],
        [
            "Custom deterministic terms (deterministic)",
            "markdown"
        ],
        [
            "Accepts a DeterministicProcess",
            "markdown"
        ],
        [
            "Exogenous variables (exog)",
            "markdown"
        ],
        [
            "A DataFrame or array of exogenous variables to include in the model",
            "markdown"
        ],
        [
            "Omission of selected lags (lags)",
            "markdown"
        ],
        [
            "If lags is an iterable of integers, then only these are included in the model.",
            "markdown"
        ],
        [
            "The complete specification is\n\n\\[y_t = \\delta_0 + \\delta_1 t + \\phi_1 y_{t-1} + \\ldots + \\phi_p y_{t-p} + \\sum_{i=1}^{s-1} \\gamma_i d_i + \\sum_{j=1}^{m} \\kappa_j x_{t,j} + \\epsilon_t.\\]",
            "markdown"
        ],
        [
            "where:",
            "markdown"
        ],
        [
            "\\(d_i\\) is a seasonal dummy that is 1 if \\(mod(t, period) = i\\). Period 0 is excluded if the model contains a constant (c is in trend).",
            "markdown"
        ],
        [
            "\\(t\\) is a time trend (\\(1,2,\\ldots\\)) that starts with 1 in the first observation.",
            "markdown"
        ],
        [
            "\\(x_{t,j}\\) are exogenous regressors. <strong>Note</strong> these are time-aligned to the left-hand-side variable when defining a model.",
            "markdown"
        ],
        [
            "\\(\\epsilon_t\\) is assumed to be a white noise process.",
            "markdown"
        ],
        [
            "This first cell imports standard packages and sets plots to appear inline.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn as sns\nfrom statsmodels.tsa.api import acf, graphics, pacf\nfrom statsmodels.tsa.ar_model import AutoReg, ar_select_order",
            "code"
        ],
        [
            "This cell sets the plotting style, registers pandas date converters for matplotlib, and sets the default figure size.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "sns.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n# Default figure size\nsns.mpl.rc(\"figure\", figsize=(16, 6))\nsns.mpl.rc(\"font\", size=14)",
            "code"
        ],
        [
            "The first set of examples uses the month-over-month growth rate in U.S. Housing starts that has not been seasonally adjusted. The seasonality is evident by the regular pattern of peaks and troughs. We set the frequency for the time series to \u201cMS\u201d (month-start) to avoid warnings when using AutoReg.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>",
            "code"
        ],
        [
            "We can start with an AR(3). While this is not a good model for this data, it demonstrates the basic use of the API.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "AutoReg Model Results\n==============================================================================\nDep. Variable:               HOUSTNSA   No. Observations:                  725\nModel:                     AutoReg(3)   Log Likelihood               -2993.442\nMethod:               Conditional MLE   S.D. of innovations             15.289\nDate:                Wed, 02 Nov 2022   AIC                           5996.884\nTime:                        17:02:14   BIC                           6019.794\nSample:                    05-01-1959   HQIC                          6005.727\n                         - 06-01-2019\n===============================================================================\n                  coef    std err          z      P|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst           1.1228      0.573      1.961      0.050       0.000       2.245\nHOUSTNSA.L1     0.1910      0.036      5.235      0.000       0.120       0.263\nHOUSTNSA.L2     0.0058      0.037      0.155      0.877      -0.067       0.079\nHOUSTNSA.L3    -0.1939      0.036     -5.319      0.000      -0.265      -0.122\n                                    Roots\n=============================================================================\n                  Real          Imaginary           Modulus         Frequency\n-----------------------------------------------------------------------------\nAR.1            0.9680           -1.3298j            1.6448           -0.1499\nAR.2            0.9680           +1.3298j            1.6448            0.1499\nAR.3           -1.9064           -0.0000j            1.9064           -0.5000\n-----------------------------------------------------------------------------",
            "code"
        ],
        [
            "AutoReg supports the same covariance estimators as OLS. Below, we use cov_type=\"HC0\", which is White\u2019s covariance estimator. While the parameter estimates are the same, all of the quantities that depend on the standard error change.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "res = mod.fit(cov_type=\"HC0\")\nprint(res.summary())",
            "code"
        ],
        [
            "AutoReg Model Results\n==============================================================================\nDep. Variable:               HOUSTNSA   No. Observations:                  725\nModel:                     AutoReg(3)   Log Likelihood               -2993.442\nMethod:               Conditional MLE   S.D. of innovations             15.289\nDate:                Wed, 02 Nov 2022   AIC                           5996.884\nTime:                        17:02:14   BIC                           6019.794\nSample:                    05-01-1959   HQIC                          6005.727\n                         - 06-01-2019\n===============================================================================\n                  coef    std err          z      P|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst           1.1228      0.601      1.869      0.062      -0.055       2.300\nHOUSTNSA.L1     0.1910      0.035      5.499      0.000       0.123       0.259\nHOUSTNSA.L2     0.0058      0.039      0.150      0.881      -0.070       0.081\nHOUSTNSA.L3    -0.1939      0.036     -5.448      0.000      -0.264      -0.124\n                                    Roots\n=============================================================================\n                  Real          Imaginary           Modulus         Frequency\n-----------------------------------------------------------------------------\nAR.1            0.9680           -1.3298j            1.6448           -0.1499\nAR.2            0.9680           +1.3298j            1.6448            0.1499\nAR.3           -1.9064           -0.0000j            1.9064           -0.5000\n-----------------------------------------------------------------------------",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "sel = ar_select_order(housing, 13, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "AutoReg Model Results\n==============================================================================\nDep. Variable:               HOUSTNSA   No. Observations:                  725\nModel:                    AutoReg(13)   Log Likelihood               -2676.157\nMethod:               Conditional MLE   S.D. of innovations             10.378\nDate:                Wed, 02 Nov 2022   AIC                           5382.314\nTime:                        17:02:14   BIC                           5450.835\nSample:                    03-01-1960   HQIC                          5408.781\n                         - 06-01-2019\n================================================================================\n                   coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst            1.3615      0.458      2.970      0.003       0.463       2.260\nHOUSTNSA.L1     -0.2900      0.036     -8.161      0.000      -0.360      -0.220\nHOUSTNSA.L2     -0.0828      0.031     -2.652      0.008      -0.144      -0.022\nHOUSTNSA.L3     -0.0654      0.031     -2.106      0.035      -0.126      -0.005\nHOUSTNSA.L4     -0.1596      0.031     -5.166      0.000      -0.220      -0.099\nHOUSTNSA.L5     -0.0434      0.031     -1.387      0.165      -0.105       0.018\nHOUSTNSA.L6     -0.0884      0.031     -2.867      0.004      -0.149      -0.028\nHOUSTNSA.L7     -0.0556      0.031     -1.797      0.072      -0.116       0.005\nHOUSTNSA.L8     -0.1482      0.031     -4.803      0.000      -0.209      -0.088\nHOUSTNSA.L9     -0.0926      0.031     -2.960      0.003      -0.154      -0.031\nHOUSTNSA.L10    -0.1133      0.031     -3.665      0.000      -0.174      -0.053\nHOUSTNSA.L11     0.1151      0.031      3.699      0.000       0.054       0.176\nHOUSTNSA.L12     0.5352      0.031     17.133      0.000       0.474       0.596\nHOUSTNSA.L13     0.3178      0.036      8.937      0.000       0.248       0.388\n                                    Roots\n==============================================================================\n                   Real          Imaginary           Modulus         Frequency\n------------------------------------------------------------------------------\nAR.1             1.0913           -0.0000j            1.0913           -0.0000\nAR.2             0.8743           -0.5018j            1.0080           -0.0829\nAR.3             0.8743           +0.5018j            1.0080            0.0829\nAR.4             0.5041           -0.8765j            1.0111           -0.1669\nAR.5             0.5041           +0.8765j            1.0111            0.1669\nAR.6             0.0056           -1.0530j            1.0530           -0.2491\nAR.7             0.0056           +1.0530j            1.0530            0.2491\nAR.8            -0.5263           -0.9335j            1.0716           -0.3317\nAR.9            -0.5263           +0.9335j            1.0716            0.3317\nAR.10           -0.9525           -0.5880j            1.1194           -0.4120\nAR.11           -0.9525           +0.5880j            1.1194            0.4120\nAR.12           -1.2928           -0.2608j            1.3189           -0.4683\nAR.13           -1.2928           +0.2608j            1.3189            0.4683\n------------------------------------------------------------------------------",
            "code"
        ],
        [
            "plot_predict visualizes forecasts. Here we produce a large number of forecasts which show the string seasonality captured by the model.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "fig = res.plot_predict(720, 840)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_13_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_13_0.png\"/>",
            "code"
        ],
        [
            "plot_diagnositcs indicates that the model captures the key features in the data.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(16, 9))\nfig = res.plot_diagnostics(fig=fig, lags=30)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_15_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_15_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Autoregressions->Seasonal Dummies": [
        [
            "AutoReg supports seasonal dummies which are an alternative way to model seasonality. Including the dummies shortens the dynamics to only an AR(2).",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "sel = ar_select_order(housing, 13, seasonal=True, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "AutoReg Model Results\n==============================================================================\nDep. Variable:               HOUSTNSA   No. Observations:                  725\nModel:               Seas. AutoReg(2)   Log Likelihood               -2652.556\nMethod:               Conditional MLE   S.D. of innovations              9.487\nDate:                Wed, 02 Nov 2022   AIC                           5335.112\nTime:                        17:02:15   BIC                           5403.863\nSample:                    04-01-1959   HQIC                          5361.648\n                         - 06-01-2019\n===============================================================================\n                  coef    std err          z      P|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst           1.2726      1.373      0.927      0.354      -1.418       3.963\ns(2,12)        32.6477      1.824     17.901      0.000      29.073      36.222\ns(3,12)        23.0685      2.435      9.472      0.000      18.295      27.842\ns(4,12)        10.7267      2.693      3.983      0.000       5.449      16.005\ns(5,12)         1.6792      2.100      0.799      0.424      -2.437       5.796\ns(6,12)        -4.4229      1.896     -2.333      0.020      -8.138      -0.707\ns(7,12)        -4.2113      1.824     -2.309      0.021      -7.786      -0.636\ns(8,12)        -6.4124      1.791     -3.581      0.000      -9.922      -2.902\ns(9,12)         0.1095      1.800      0.061      0.952      -3.419       3.638\ns(10,12)      -16.7511      1.814     -9.234      0.000     -20.307     -13.196\ns(11,12)      -20.7023      1.862    -11.117      0.000     -24.352     -17.053\ns(12,12)      -11.9554      1.778     -6.724      0.000     -15.440      -8.470\nHOUSTNSA.L1    -0.2953      0.037     -7.994      0.000      -0.368      -0.223\nHOUSTNSA.L2    -0.1148      0.037     -3.107      0.002      -0.187      -0.042\n                                    Roots\n=============================================================================\n                  Real          Imaginary           Modulus         Frequency\n-----------------------------------------------------------------------------\nAR.1           -1.2862           -2.6564j            2.9514           -0.3218\nAR.2           -1.2862           +2.6564j            2.9514            0.3218\n-----------------------------------------------------------------------------",
            "code"
        ],
        [
            "The seasonal dummies are obvious in the forecasts which has a non-trivial seasonal component in all periods 10 years in to the future.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "fig = res.plot_predict(720, 840)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_20_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_20_0.png\"/>",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(16, 9))\nfig = res.plot_diagnostics(lags=30, fig=fig)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_21_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_21_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Autoregressions->Seasonal Dynamics": [
        [
            "While AutoReg does not directly support Seasonal components since it uses OLS to estimate parameters, it is possible to capture seasonal dynamics using an over-parametrized Seasonal AR that does not impose the restrictions in the Seasonal AR.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "yoy_housing = data.HOUSTNSA.pct_change(12).resample(\"MS\").last().dropna()\n_, ax = plt.subplots()\nax = yoy_housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_24_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_24_0.png\"/>",
            "code"
        ],
        [
            "We start by selecting a model using the simple method that only chooses the maximum lag. All lower lags are automatically included. The maximum lag to check is set to 13 since this allows the model to next a Seasonal AR that has both a short-run AR(1) component and a Seasonal AR(1) component, so that\n\n\\[(1-\\phi_s L^{12})(1-\\phi_1 L)y_t = \\epsilon_t\\]",
            "markdown"
        ],
        [
            "which becomes\n\n\\[y_t = \\phi_1 y_{t-1} +\\phi_s Y_{t-12} - \\phi_1\\phi_s Y_{t-13} + \\epsilon_t\\]",
            "markdown"
        ],
        [
            "when expanded. AutoReg does not enforce the structure, but can estimate the nesting model\n\n\\[y_t = \\phi_1 y_{t-1} +\\phi_{12} Y_{t-12} - \\phi_{13} Y_{t-13} + \\epsilon_t.\\]",
            "markdown"
        ],
        [
            "We see that all 13 lags are selected.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "sel = ar_select_order(yoy_housing, 13, old_names=False)\nsel.ar_lags",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]",
            "code"
        ],
        [
            "It seems unlikely that all 13 lags are required. We can set glob=True to search all \\(2^{13}\\) models that include up to 13 lags.",
            "markdown"
        ],
        [
            "Here we see that the first three are selected, as is the 7th, and finally the 12th and 13th are selected. This is superficially similar to the structure described above.",
            "markdown"
        ],
        [
            "After fitting the model, we take a look at the diagnostic plots that indicate that this specification appears to be adequate to capture the dynamics in the data.",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "sel = ar_select_order(yoy_housing, 13, glob=True, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "AutoReg Model Results\n==============================================================================\nDep. Variable:               HOUSTNSA   No. Observations:                  714\nModel:             Restr. AutoReg(13)   Log Likelihood                 589.177\nMethod:               Conditional MLE   S.D. of innovations              0.104\nDate:                Wed, 02 Nov 2022   AIC                          -1162.353\nTime:                        17:02:21   BIC                          -1125.933\nSample:                    02-01-1961   HQIC                         -1148.276\n                         - 06-01-2019\n================================================================================\n                   coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst            0.0035      0.004      0.875      0.382      -0.004       0.011\nHOUSTNSA.L1      0.5640      0.035     16.167      0.000       0.496       0.632\nHOUSTNSA.L2      0.2347      0.038      6.238      0.000       0.161       0.308\nHOUSTNSA.L3      0.2051      0.037      5.560      0.000       0.133       0.277\nHOUSTNSA.L7     -0.0903      0.030     -2.976      0.003      -0.150      -0.031\nHOUSTNSA.L12    -0.3791      0.034    -11.075      0.000      -0.446      -0.312\nHOUSTNSA.L13     0.3354      0.033     10.254      0.000       0.271       0.400\n                                    Roots\n==============================================================================\n                   Real          Imaginary           Modulus         Frequency\n------------------------------------------------------------------------------\nAR.1            -1.0309           -0.2682j            1.0652           -0.4595\nAR.2            -1.0309           +0.2682j            1.0652            0.4595\nAR.3            -0.7454           -0.7417j            1.0515           -0.3754\nAR.4            -0.7454           +0.7417j            1.0515            0.3754\nAR.5            -0.3172           -1.0221j            1.0702           -0.2979\nAR.6            -0.3172           +1.0221j            1.0702            0.2979\nAR.7             0.2419           -1.0573j            1.0846           -0.2142\nAR.8             0.2419           +1.0573j            1.0846            0.2142\nAR.9             0.7840           -0.8303j            1.1420           -0.1296\nAR.10            0.7840           +0.8303j            1.1420            0.1296\nAR.11            1.0730           -0.2386j            1.0992           -0.0348\nAR.12            1.0730           +0.2386j            1.0992            0.0348\nAR.13            1.1193           -0.0000j            1.1193           -0.0000\n------------------------------------------------------------------------------",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(16, 9))\nfig = res.plot_diagnostics(fig=fig, lags=30)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_29_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_29_0.png\"/>",
            "code"
        ],
        [
            "We can also include seasonal dummies. These are all insignificant since the model is using year-over-year changes.",
            "markdown"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "sel = ar_select_order(yoy_housing, 13, glob=True, seasonal=True, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "AutoReg Model Results\n====================================================================================\nDep. Variable:                     HOUSTNSA   No. Observations:                  714\nModel:             Restr. Seas. AutoReg(13)   Log Likelihood                 590.875\nMethod:                     Conditional MLE   S.D. of innovations              0.104\nDate:                      Wed, 02 Nov 2022   AIC                          -1143.751\nTime:                              17:02:36   BIC                          -1057.253\nSample:                          02-01-1961   HQIC                         -1110.317\n                               - 06-01-2019\n================================================================================\n                   coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst            0.0167      0.014      1.215      0.224      -0.010       0.044\ns(2,12)         -0.0179      0.019     -0.931      0.352      -0.056       0.020\ns(3,12)         -0.0121      0.019     -0.630      0.528      -0.050       0.026\ns(4,12)         -0.0210      0.019     -1.089      0.276      -0.059       0.017\ns(5,12)         -0.0223      0.019     -1.157      0.247      -0.060       0.015\ns(6,12)         -0.0224      0.019     -1.160      0.246      -0.060       0.015\ns(7,12)         -0.0212      0.019     -1.096      0.273      -0.059       0.017\ns(8,12)         -0.0101      0.019     -0.520      0.603      -0.048       0.028\ns(9,12)         -0.0095      0.019     -0.491      0.623      -0.047       0.028\ns(10,12)        -0.0049      0.019     -0.252      0.801      -0.043       0.033\ns(11,12)        -0.0084      0.019     -0.435      0.664      -0.046       0.030\ns(12,12)        -0.0077      0.019     -0.400      0.689      -0.046       0.030\nHOUSTNSA.L1      0.5630      0.035     16.160      0.000       0.495       0.631\nHOUSTNSA.L2      0.2347      0.038      6.248      0.000       0.161       0.308\nHOUSTNSA.L3      0.2075      0.037      5.634      0.000       0.135       0.280\nHOUSTNSA.L7     -0.0916      0.030     -3.013      0.003      -0.151      -0.032\nHOUSTNSA.L12    -0.3810      0.034    -11.149      0.000      -0.448      -0.314\nHOUSTNSA.L13     0.3373      0.033     10.327      0.000       0.273       0.401\n                                    Roots\n==============================================================================\n                   Real          Imaginary           Modulus         Frequency\n------------------------------------------------------------------------------\nAR.1            -1.0305           -0.2681j            1.0648           -0.4595\nAR.2            -1.0305           +0.2681j            1.0648            0.4595\nAR.3            -0.7447           -0.7414j            1.0509           -0.3754\nAR.4            -0.7447           +0.7414j            1.0509            0.3754\nAR.5            -0.3172           -1.0215j            1.0696           -0.2979\nAR.6            -0.3172           +1.0215j            1.0696            0.2979\nAR.7             0.2416           -1.0568j            1.0841           -0.2142\nAR.8             0.2416           +1.0568j            1.0841            0.2142\nAR.9             0.7837           -0.8304j            1.1418           -0.1296\nAR.10            0.7837           +0.8304j            1.1418            0.1296\nAR.11            1.0724           -0.2383j            1.0986           -0.0348\nAR.12            1.0724           +0.2383j            1.0986            0.0348\nAR.13            1.1192           -0.0000j            1.1192           -0.0000\n------------------------------------------------------------------------------",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Autoregressions->Industrial Production": [
        [
            "We will use the industrial production index data to examine forecasting.",
            "markdown"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "data = pdr.get_data_fred(\"INDPRO\", \"1959-01-01\", \"2019-06-01\")\nind_prod = data.INDPRO.pct_change(12).dropna().asfreq(\"MS\")\n_, ax = plt.subplots(figsize=(16, 9))\nind_prod.plot(ax=ax)",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: xlabel='DATE'\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_33_1.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_33_1.png\"/>",
            "code"
        ],
        [
            "We will start by selecting a model using up to 12 lags. An AR(13) minimizes the BIC criteria even though many coefficients are insignificant.",
            "markdown"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "sel = ar_select_order(ind_prod, 13, \"bic\", old_names=False)\nres = sel.model.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "AutoReg Model Results\n==============================================================================\nDep. Variable:                 INDPRO   No. Observations:                  714\nModel:                    AutoReg(13)   Log Likelihood                2322.270\nMethod:               Conditional MLE   S.D. of innovations              0.009\nDate:                Wed, 02 Nov 2022   AIC                          -4614.540\nTime:                        17:02:38   BIC                          -4546.252\nSample:                    02-01-1961   HQIC                         -4588.144\n                         - 06-01-2019\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0012      0.000      2.779      0.005       0.000       0.002\nINDPRO.L1      1.1582      0.035     33.196      0.000       1.090       1.227\nINDPRO.L2     -0.0824      0.053     -1.546      0.122      -0.187       0.022\nINDPRO.L3     -0.0015      0.053     -0.028      0.977      -0.105       0.102\nINDPRO.L4      0.0102      0.053      0.194      0.846      -0.093       0.114\nINDPRO.L5     -0.1339      0.053     -2.548      0.011      -0.237      -0.031\nINDPRO.L6     -0.0084      0.052     -0.161      0.872      -0.111       0.094\nINDPRO.L7      0.0556      0.052      1.065      0.287      -0.047       0.158\nINDPRO.L8     -0.0303      0.052     -0.582      0.561      -0.132       0.072\nINDPRO.L9      0.0939      0.052      1.807      0.071      -0.008       0.196\nINDPRO.L10    -0.0834      0.052     -1.604      0.109      -0.185       0.019\nINDPRO.L11     0.0019      0.052      0.037      0.971      -0.100       0.104\nINDPRO.L12    -0.3827      0.052     -7.381      0.000      -0.484      -0.281\nINDPRO.L13     0.3615      0.033     11.006      0.000       0.297       0.426\n                                    Roots\n==============================================================================\n                   Real          Imaginary           Modulus         Frequency\n------------------------------------------------------------------------------\nAR.1            -1.0400           -0.2913j            1.0801           -0.4565\nAR.2            -1.0400           +0.2913j            1.0801            0.4565\nAR.3            -0.7802           -0.8045j            1.1207           -0.3726\nAR.4            -0.7802           +0.8045j            1.1207            0.3726\nAR.5            -0.2726           -1.0538j            1.0885           -0.2903\nAR.6            -0.2726           +1.0538j            1.0885            0.2903\nAR.7             0.2715           -1.0506j            1.0851           -0.2097\nAR.8             0.2715           +1.0506j            1.0851            0.2097\nAR.9             0.8010           -0.7286j            1.0828           -0.1175\nAR.10            0.8010           +0.7286j            1.0828            0.1175\nAR.11            1.0218           -0.2219j            1.0456           -0.0340\nAR.12            1.0218           +0.2219j            1.0456            0.0340\nAR.13            1.0558           -0.0000j            1.0558           -0.0000\n------------------------------------------------------------------------------",
            "code"
        ],
        [
            "We can also use a global search which allows longer lags to enter if needed without requiring the shorter lags. Here we see many lags dropped. The model indicates there may be some seasonality in the data.",
            "markdown"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "sel = ar_select_order(ind_prod, 13, \"bic\", glob=True, old_names=False)\nsel.ar_lags\nres_glob = sel.model.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "AutoReg Model Results\n==============================================================================\nDep. Variable:                 INDPRO   No. Observations:                  714\nModel:                    AutoReg(13)   Log Likelihood                2322.270\nMethod:               Conditional MLE   S.D. of innovations              0.009\nDate:                Wed, 02 Nov 2022   AIC                          -4614.540\nTime:                        17:02:42   BIC                          -4546.252\nSample:                    02-01-1961   HQIC                         -4588.144\n                         - 06-01-2019\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0012      0.000      2.779      0.005       0.000       0.002\nINDPRO.L1      1.1582      0.035     33.196      0.000       1.090       1.227\nINDPRO.L2     -0.0824      0.053     -1.546      0.122      -0.187       0.022\nINDPRO.L3     -0.0015      0.053     -0.028      0.977      -0.105       0.102\nINDPRO.L4      0.0102      0.053      0.194      0.846      -0.093       0.114\nINDPRO.L5     -0.1339      0.053     -2.548      0.011      -0.237      -0.031\nINDPRO.L6     -0.0084      0.052     -0.161      0.872      -0.111       0.094\nINDPRO.L7      0.0556      0.052      1.065      0.287      -0.047       0.158\nINDPRO.L8     -0.0303      0.052     -0.582      0.561      -0.132       0.072\nINDPRO.L9      0.0939      0.052      1.807      0.071      -0.008       0.196\nINDPRO.L10    -0.0834      0.052     -1.604      0.109      -0.185       0.019\nINDPRO.L11     0.0019      0.052      0.037      0.971      -0.100       0.104\nINDPRO.L12    -0.3827      0.052     -7.381      0.000      -0.484      -0.281\nINDPRO.L13     0.3615      0.033     11.006      0.000       0.297       0.426\n                                    Roots\n==============================================================================\n                   Real          Imaginary           Modulus         Frequency\n------------------------------------------------------------------------------\nAR.1            -1.0400           -0.2913j            1.0801           -0.4565\nAR.2            -1.0400           +0.2913j            1.0801            0.4565\nAR.3            -0.7802           -0.8045j            1.1207           -0.3726\nAR.4            -0.7802           +0.8045j            1.1207            0.3726\nAR.5            -0.2726           -1.0538j            1.0885           -0.2903\nAR.6            -0.2726           +1.0538j            1.0885            0.2903\nAR.7             0.2715           -1.0506j            1.0851           -0.2097\nAR.8             0.2715           +1.0506j            1.0851            0.2097\nAR.9             0.8010           -0.7286j            1.0828           -0.1175\nAR.10            0.8010           +0.7286j            1.0828            0.1175\nAR.11            1.0218           -0.2219j            1.0456           -0.0340\nAR.12            1.0218           +0.2219j            1.0456            0.0340\nAR.13            1.0558           -0.0000j            1.0558           -0.0000\n------------------------------------------------------------------------------",
            "code"
        ],
        [
            "plot_predict can be used to produce forecast plots along with confidence intervals. Here we produce forecasts starting at the last observation and continuing for 18 months.",
            "markdown"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "ind_prod.shape",
            "code"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "(714,)",
            "code"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "fig = res_glob.plot_predict(start=714, end=732)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\"/>",
            "code"
        ],
        [
            "The forecasts from the full model and the restricted model are very similar. I also include an AR(5) which has very different dynamics",
            "markdown"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "res_ar5 = AutoReg(ind_prod, 5, old_names=False).fit()\npredictions = pd.DataFrame(\n    {\n        \"AR(5)\": res_ar5.predict(start=714, end=726),\n        \"AR(13)\": res.predict(start=714, end=726),\n        \"Restr. AR(13)\": res_glob.predict(start=714, end=726),\n    }\n)\n_, ax = plt.subplots()\nax = predictions.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\"/>",
            "code"
        ],
        [
            "The diagnostics indicate the model captures most of the the dynamics in the data. The ACF shows a patters at the seasonal frequency and so a more complete seasonal model (SARIMAX) may be needed.",
            "markdown"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(16, 9))\nfig = res_glob.plot_diagnostics(fig=fig, lags=30)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_44_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_44_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Deterministic Terms": [
        [
            "[1]:",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nplt.rc(\"figure\", figsize=(16, 9))\nplt.rc(\"font\", size=16)",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Deterministic Terms->Basic Use": [
        [
            "Basic configurations can be directly constructed through DeterministicProcess. These can include a constant, a time trend of any order, and either a seasonal or a Fourier component.",
            "markdown"
        ],
        [
            "The process requires an index, which is the index of the full-sample (or in-sample).",
            "markdown"
        ],
        [
            "First, we initialize a deterministic process with a constant, a linear time trend, and a 5-period seasonal term. The in_sample method returns the full set of values that match the index.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "from statsmodels.tsa.deterministic import DeterministicProcess\n\nindex = pd.RangeIndex(0, 100)\ndet_proc = DeterministicProcess(index, constant=True, order=1, seasonal=True, period=5)\ndet_proc.in_sample()",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "100 rows \u00d7 6 columns",
            "markdown"
        ],
        [
            "The out_of_sample returns the next steps values after the end of the in-sample.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "det_proc.out_of_sample(15)",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "range(start, stop) can also be used to produce the deterministic terms over any range including in- and out-of-sample.",
            "markdown"
        ]
    ],
    "Examples->Time Series Analysis->Deterministic Terms->Basic Use->Notes": [
        [
            "When the index is a pandas DatetimeIndex or a PeriodIndex, then start and stop can be date-like (strings, e.g., \u201c2020-06-01\u201d, or Timestamp) or integers.",
            "markdown"
        ],
        [
            "stop is always included in the range. While this is not very Pythonic, it is needed since both statsmodels and Pandas include stop when working with date-like slices.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "det_proc.range(190, 210)",
            "code"
        ],
        [
            "[4]:",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Deterministic Terms->Using a Date-like Index": [
        [
            "Next, we show the same steps using a PeriodIndex.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "index = pd.period_range(\"2020-03-01\", freq=\"M\", periods=60)\ndet_proc = DeterministicProcess(index, constant=True, fourier=2)\ndet_proc.in_sample().head(12)",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "det_proc.out_of_sample(12)",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "range accepts date-like arguments, which are usually given as strings.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "det_proc.range(\"2025-01\", \"2026-01\")",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "This is equivalent to using the integer values 58 and 70.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "det_proc.range(58, 70)",
            "code"
        ],
        [
            "[8]:",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Deterministic Terms->Advanced Construction": [
        [
            "Deterministic processes with features not supported directly through the constructor can be created using additional_terms which accepts a list of DetermisticTerm. Here we create a deterministic process with two seasonal components: day-of-week with a 5 day period and an annual captured through a Fourier component with a period of 365.25 days.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "from statsmodels.tsa.deterministic import Fourier, Seasonality, TimeTrend\n\nindex = pd.period_range(\"2020-03-01\", freq=\"D\", periods=2 * 365)\ntt = TimeTrend(constant=True)\nfour = Fourier(period=365.25, order=2)\nseas = Seasonality(period=7)\ndet_proc = DeterministicProcess(index, additional_terms=[tt, seas, four])\ndet_proc.in_sample().head(28)",
            "code"
        ],
        [
            "[9]:",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Deterministic Terms->Custom Deterministic Terms": [
        [
            "The DetermisticTerm Abstract Base Class is designed to be subclassed to help users write custom deterministic terms. We next show two examples. The first is a broken time trend that allows a break after a fixed number of periods. The second is a \u201ctrick\u201d deterministic term that allows exogenous data, which is not really a deterministic process, to be treated as if was deterministic. This lets use simplify gathering the terms needed for forecasting.",
            "markdown"
        ],
        [
            "These are intended to demonstrate the construction of custom terms. They can definitely be improved in terms of input validation.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "from statsmodels.tsa.deterministic import DeterministicTerm\n\n\nclass BrokenTimeTrend(DeterministicTerm):\n    def __init__(self, break_period: int):\n        self._break_period = break_period\n\n    def __str__(self):\n        return \"Broken Time Trend\"\n\n    def _eq_attr(self):\n        return (self._break_period,)\n\n    def in_sample(self, index: pd.Index):\n        nobs = index.shape[0]\n        terms = np.zeros((nobs, 2))\n        terms[self._break_period :, 0] = 1\n        terms[self._break_period :, 1] = np.arange(self._break_period + 1, nobs + 1)\n        return pd.DataFrame(terms, columns=[\"const_break\", \"trend_break\"], index=index)\n\n    def out_of_sample(\n        self, steps: int, index: pd.Index, forecast_index: pd.Index = None\n    ):\n        # Always call extend index first\n        fcast_index = self._extend_index(index, steps, forecast_index)\n        nobs = index.shape[0]\n        terms = np.zeros((steps, 2))\n        # Assume break period is in-sample\n        terms[:, 0] = 1\n        terms[:, 1] = np.arange(nobs + 1, nobs + steps + 1)\n        return pd.DataFrame(\n            terms, columns=[\"const_break\", \"trend_break\"], index=fcast_index\n        )",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "btt = BrokenTimeTrend(60)\ntt = TimeTrend(constant=True, order=1)\nindex = pd.RangeIndex(100)\ndet_proc = DeterministicProcess(index, additional_terms=[tt, btt])\ndet_proc.range(55, 65)",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "Next, we write a simple \u201cwrapper\u201d for some actual exogenous data that simplifies constructing out-of-sample exogenous arrays for forecasting.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "class ExogenousProcess(DeterministicTerm):\n    def __init__(self, data):\n        self._data = data\n\n    def __str__(self):\n        return \"Custom Exog Process\"\n\n    def _eq_attr(self):\n        return (id(self._data),)\n\n    def in_sample(self, index: pd.Index):\n        return self._data.loc[index]\n\n    def out_of_sample(\n        self, steps: int, index: pd.Index, forecast_index: pd.Index = None\n    ):\n        forecast_index = self._extend_index(index, steps, forecast_index)\n        return self._data.loc[forecast_index]",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "import numpy as np\n\ngen = np.random.default_rng(98765432101234567890)\nexog = pd.DataFrame(gen.integers(100, size=(300, 2)), columns=[\"exog1\", \"exog2\"])\nexog.head()",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "ep = ExogenousProcess(exog)\ntt = TimeTrend(constant=True, order=1)\n# The in-sample index\nidx = exog.index[:200]\ndet_proc = DeterministicProcess(idx, additional_terms=[tt, ep])",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "det_proc.in_sample().head()",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "det_proc.out_of_sample(10)",
            "code"
        ],
        [
            "[16]:",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Deterministic Terms->Model Support": [
        [
            "The only model that directly supports DeterministicProcess is AutoReg. A custom term can be set using the deterministic keyword argument.",
            "markdown"
        ],
        [
            "<strong>Note</strong>: Using a custom term requires that trend=\"n\" and seasonal=False so that all deterministic components must come from the custom deterministic term.",
            "markdown"
        ]
    ],
    "Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data": [
        [
            "Here we simulate some data that has an weekly seasonality captured by a Fourier series.",
            "markdown"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "gen = np.random.default_rng(98765432101234567890)\nidx = pd.RangeIndex(200)\ndet_proc = DeterministicProcess(idx, constant=True, period=52, fourier=2)\ndet_terms = det_proc.in_sample().to_numpy()\nparams = np.array([1.0, 3, -1, 4, -2])\nexog = det_terms @ params\ny = np.empty(200)\ny[0] = det_terms[0] @ params + gen.standard_normal()\nfor i in range(1, 200):\n    y[i] = 0.9 * y[i - 1] + det_terms[i] @ params + gen.standard_normal()\ny = pd.Series(y, index=idx)\nax = y.plot()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_deterministics_28_0.png\" src=\"../../../_images/examples_notebooks_generated_deterministics_28_0.png\"/>",
            "code"
        ],
        [
            "The model is then fit using the deterministic keyword argument. seasonal defaults to False but trend defaults to \"c\" so this needs to be changed.",
            "markdown"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "from statsmodels.tsa.api import AutoReg\n\nmod = AutoReg(y, 1, trend=\"n\", deterministic=det_proc)\nres = mod.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "AutoReg Model Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                  200\nModel:                     AutoReg(1)   Log Likelihood                -270.964\nMethod:               Conditional MLE   S.D. of innovations              0.944\nDate:                Wed, 02 Nov 2022   AIC                            555.927\nTime:                        17:07:20   BIC                            578.980\nSample:                             1   HQIC                           565.258\n                                  200\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.8436      0.172      4.916      0.000       0.507       1.180\nsin(1,52)      2.9738      0.160     18.587      0.000       2.660       3.287\ncos(1,52)     -0.6771      0.284     -2.380      0.017      -1.235      -0.120\nsin(2,52)      3.9951      0.099     40.336      0.000       3.801       4.189\ncos(2,52)     -1.7206      0.264     -6.519      0.000      -2.238      -1.203\ny.L1           0.9116      0.014     63.264      0.000       0.883       0.940\n                                    Roots\n=============================================================================\n                  Real          Imaginary           Modulus         Frequency\n-----------------------------------------------------------------------------\nAR.1            1.0970           +0.0000j            1.0970            0.0000\n-----------------------------------------------------------------------------",
            "code"
        ],
        [
            "We can use the plot_predict to show the predicted values and their prediction interval. The out-of-sample deterministic values are automatically produced by the deterministic process passed to AutoReg.",
            "markdown"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "fig = res.plot_predict(200, 200 + 2 * 52, True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_deterministics_32_0.png\" src=\"../../../_images/examples_notebooks_generated_deterministics_32_0.png\"/>",
            "code"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "auto_reg_forecast = res.predict(200, 211)\nauto_reg_forecast",
            "code"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "200    -3.253482\n201    -8.555660\n202   -13.607557\n203   -18.152622\n204   -21.950370\n205   -24.790116\n206   -26.503171\n207   -26.972781\n208   -26.141244\n209   -24.013773\n210   -20.658891\n211   -16.205310\ndtype: float64",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Deterministic Terms->Using with other models": [
        [
            "Other models do not support DeterministicProcess directly. We can instead manually pass any deterministic terms as exog to model that support exogenous values.",
            "markdown"
        ],
        [
            "Note that SARIMAX with exogenous variables is OLS with SARIMA errors so that the model is\n\n\\[\\begin{split}\\begin{align*}\n\\nu_t & = y_t - x_t \\beta  \\\\\n(1-\\phi(L))\\nu_t & = (1+\\theta(L))\\epsilon_t.\n\\end{align*}\\end{split}\\]",
            "markdown"
        ],
        [
            "The parameters on deterministic terms are not directly comparable to AutoReg which evolves according to the equation\n\n\\[(1-\\phi(L)) y_t = x_t \\beta + \\epsilon_t.\\]",
            "markdown"
        ],
        [
            "When \\(x_t\\) contains only deterministic terms, these two representation are equivalent (assuming \\(\\theta(L)=0\\) so that there is no MA).",
            "markdown"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "from statsmodels.tsa.api import SARIMAX\n\ndet_proc = DeterministicProcess(idx, period=52, fourier=2)\ndet_terms = det_proc.in_sample()\n\nmod = SARIMAX(y, order=(1, 0, 0), trend=\"c\", exog=det_terms)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "code"
        ],
        [
            "SARIMAX Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                  200\nModel:               SARIMAX(1, 0, 0)   Log Likelihood                -293.381\nDate:                Wed, 02 Nov 2022   AIC                            600.763\nTime:                        17:07:21   BIC                            623.851\nSample:                             0   HQIC                           610.106\n                                - 200\nCovariance Type:                  opg\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0797      0.140      0.567      0.570      -0.196       0.355\nsin(1,52)      9.1916      0.876     10.492      0.000       7.475      10.909\ncos(1,52)    -17.4348      0.891    -19.576      0.000     -19.180     -15.689\nsin(2,52)      1.2512      0.466      2.683      0.007       0.337       2.165\ncos(2,52)    -17.1863      0.434    -39.583      0.000     -18.037     -16.335\nar.L1          0.9957      0.007    150.762      0.000       0.983       1.009\nsigma2         1.0748      0.119      9.068      0.000       0.842       1.307\n===================================================================================\nLjung-Box (L1) (Q):                   2.16   Jarque-Bera (JB):                 1.03\nProb(Q):                              0.14   Prob(JB):                         0.60\nHeteroskedasticity (H):               0.71   Skew:                            -0.14\nProb(H) (two-sided):                  0.16   Kurtosis:                         2.78\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "The forecasts are similar but differ since the parameters of the SARIMAX are estimated using MLE while AutoReg uses OLS.",
            "markdown"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "sarimax_forecast = res.forecast(12, exog=det_proc.out_of_sample(12))\ndf = pd.concat([auto_reg_forecast, sarimax_forecast], axis=1)\ndf.columns = columns = [\"AutoReg\", \"SARIMAX\"]\ndf",
            "code"
        ],
        [
            "[22]:",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->ARMA: Sunspots Data": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.tsa.arima.model import ARIMA",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "from statsmodels.graphics.api import qqplot",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data": [
        [
            "[4]:",
            "code"
        ],
        [
            "print(sm.datasets.sunspots.NOTE)",
            "code"
        ],
        [
            "::\n\n    Number of Observations - 309 (Annual 1700 - 2008)\n    Number of Variables - 1\n    Variable name definitions::\n\n        SUNACTIVITY - Number of sunspots for each year\n\n    The data file contains a 'YEAR' variable that is not returned by load.",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "dta = sm.datasets.sunspots.load_pandas().data",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "dta.index = pd.Index(sm.tsa.datetools.dates_from_range(\"1700\", \"2008\"))\ndta.index.freq = dta.index.inferred_freq\ndel dta[\"YEAR\"]",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "dta.plot(figsize=(12, 8))",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: \n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_arma_0_8_1.png\" src=\"../../../_images/examples_notebooks_generated_tsa_arma_0_8_1.png\"/>",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(dta.values.squeeze(), lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(dta, lags=40, ax=ax2)",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/graphics/tsaplots.py:348: FutureWarning: The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n  warnings.warn(\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_arma_0_9_1.png\" src=\"../../../_images/examples_notebooks_generated_tsa_arma_0_9_1.png\"/>",
            "code"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "arma_mod20 = ARIMA(dta, order=(2, 0, 0)).fit()\nprint(arma_mod20.params)",
            "code"
        ],
        [
            "const      49.746198\nar.L1       1.390633\nar.L2      -0.688573\nsigma2    274.727182\ndtype: float64",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "arma_mod30 = ARIMA(dta, order=(3, 0, 0)).fit()",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "print(arma_mod20.aic, arma_mod20.bic, arma_mod20.hqic)",
            "code"
        ],
        [
            "2622.637093301418 2637.570458409009 2628.607481146664",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "print(arma_mod30.params)",
            "code"
        ],
        [
            "const      49.751911\nar.L1       1.300818\nar.L2      -0.508102\nar.L3      -0.129644\nsigma2    270.101139\ndtype: float64",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "print(arma_mod30.aic, arma_mod30.bic, arma_mod30.hqic)",
            "code"
        ],
        [
            "2619.4036292456526 2638.0703356301415 2626.86661405221",
            "code"
        ],
        [
            "Does our model obey the theory?",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "sm.stats.durbin_watson(arma_mod30.resid.values)",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "1.9564953612078884",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax = arma_mod30.resid.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_arma_0_18_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_arma_0_18_0.png\"/>",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "resid = arma_mod30.resid",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "stats.normaltest(resid)",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "NormaltestResult(statistic=49.84393223006768, pvalue=1.501507957159393e-11)",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nfig = qqplot(resid, line=\"q\", ax=ax, fit=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_arma_0_21_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_arma_0_21_0.png\"/>",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(resid.values.squeeze(), lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(resid, lags=40, ax=ax2)",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/graphics/tsaplots.py:348: FutureWarning: The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n  warnings.warn(\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_arma_0_22_1.png\" src=\"../../../_images/examples_notebooks_generated_tsa_arma_0_22_1.png\"/>",
            "code"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "r, q, p = sm.tsa.acf(resid.values.squeeze(), fft=True, qstat=True)\ndata = np.c_[np.arange(1, 25), r[1:], q, p]",
            "code"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "table = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "code"
        ],
        [
            "AC          Q      Prob(Q)\nlag\n1.0   0.009170   0.026239  8.713184e-01\n2.0   0.041793   0.572982  7.508939e-01\n3.0  -0.001338   0.573544  9.024612e-01\n4.0   0.136086   6.408642  1.706385e-01\n5.0   0.092465   9.111351  1.047043e-01\n6.0   0.091947  11.792661  6.675737e-02\n7.0   0.068747  13.296552  6.520425e-02\n8.0  -0.015022  13.368601  9.978086e-02\n9.0   0.187590  24.641072  3.394963e-03\n10.0  0.213715  39.320758  2.230588e-05\n11.0  0.201079  52.359565  2.346490e-07\n12.0  0.117180  56.802479  8.580351e-08\n13.0 -0.014057  56.866630  1.895209e-07\n14.0  0.015398  56.943864  4.000370e-07\n15.0 -0.024969  57.147642  7.746546e-07\n16.0  0.080916  59.295052  6.876728e-07\n17.0  0.041138  59.852008  1.111674e-06\n18.0 -0.052022  60.745723  1.549418e-06\n19.0  0.062496  62.040010  1.832778e-06\n20.0 -0.010303  62.075305  3.383285e-06\n21.0  0.074453  63.924941  3.195540e-06\n22.0  0.124954  69.152954  8.984238e-07\n23.0  0.093162  72.069214  5.803579e-07\n24.0 -0.082152  74.344911  4.716006e-07",
            "code"
        ],
        [
            "This indicates a lack of fit.",
            "markdown"
        ],
        [
            "In-sample dynamic prediction. How good does our model do?",
            "markdown"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "predict_sunspots = arma_mod30.predict(\"1990\", \"2012\", dynamic=True)\nprint(predict_sunspots)",
            "code"
        ],
        [
            "1990-12-31    167.048337\n1991-12-31    140.995022\n1992-12-31     94.862115\n1993-12-31     46.864439\n1994-12-31     11.246106\n1995-12-31     -4.718265\n1996-12-31     -1.164628\n1997-12-31     16.187246\n1998-12-31     39.022948\n1999-12-31     59.450799\n2000-12-31     72.171269\n2001-12-31     75.378329\n2002-12-31     70.438480\n2003-12-31     60.733987\n2004-12-31     50.204383\n2005-12-31     42.078584\n2006-12-31     38.116648\n2007-12-31     38.456730\n2008-12-31     41.965644\n2009-12-31     46.870948\n2010-12-31     51.424877\n2011-12-31     54.401403\n2012-12-31     55.323515\nFreq: A-DEC, Name: predicted_mean, dtype: float64",
            "code"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "def mean_forecast_err(y, yhat):\n    return y.sub(yhat).mean()",
            "code"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "mean_forecast_err(dta.SUNACTIVITY, predict_sunspots)",
            "code"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "5.634833001498669",
            "code"
        ],
        [
            "Exercise: Can you obtain a better fit for the Sunspots model? (Hint: sm.tsa.AR has a method select_order)",
            "markdown"
        ]
    ],
    "Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult": [
        [
            "[25]:",
            "code"
        ],
        [
            "from statsmodels.tsa.arima_process import ArmaProcess",
            "code"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "np.random.seed(1234)\n# include zero-th lag\narparams = np.array([1, 0.75, -0.65, -0.55, 0.9])\nmaparams = np.array([1, 0.65])",
            "code"
        ],
        [
            "Let\u2019s make sure this model is estimable.",
            "markdown"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "arma_t = ArmaProcess(arparams, maparams)",
            "code"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "arma_t.isinvertible",
            "code"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "True",
            "code"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "arma_t.isstationary",
            "code"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "False",
            "code"
        ],
        [
            "What does this mean?",
            "markdown"
        ],
        [
            "[30]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))",
            "code"
        ],
        [
            "[30]:",
            "code"
        ],
        [
            "[&lt;matplotlib.lines.Line2D at 0x7f1ffd1452a0]\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_arma_0_39_1.png\" src=\"../../../_images/examples_notebooks_generated_tsa_arma_0_39_1.png\"/>",
            "code"
        ],
        [
            "[31]:",
            "code"
        ],
        [
            "arparams = np.array([1, 0.35, -0.15, 0.55, 0.1])\nmaparams = np.array([1, 0.65])\narma_t = ArmaProcess(arparams, maparams)\narma_t.isstationary",
            "code"
        ],
        [
            "[31]:",
            "code"
        ],
        [
            "True",
            "code"
        ],
        [
            "[32]:",
            "code"
        ],
        [
            "arma_rvs = arma_t.generate_sample(nsample=500, burnin=250, scale=2.5)",
            "code"
        ],
        [
            "[33]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(arma_rvs, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(arma_rvs, lags=40, ax=ax2)",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/graphics/tsaplots.py:348: FutureWarning: The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n  warnings.warn(\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_arma_0_42_1.png\" src=\"../../../_images/examples_notebooks_generated_tsa_arma_0_42_1.png\"/>",
            "code"
        ],
        [
            "For mixed ARMA processes the Autocorrelation function is a mixture of exponentials and damped sine waves after (q-p) lags.",
            "markdown"
        ],
        [
            "The partial autocorrelation function is a mixture of exponentials and dampened sine waves after (p-q) lags.",
            "markdown"
        ],
        [
            "[34]:",
            "code"
        ],
        [
            "lags = int(10 * np.log10(arma_rvs.shape[0]))\narma11 = ARIMA(arma_rvs, order=(1, 0, 1)).fit()\nresid = arma11.resid\nr, q, p = sm.tsa.acf(resid, nlags=lags, fft=True, qstat=True)\ndata = np.c_[range(1, lags + 1), r[1:], q, p]\ntable = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "code"
        ],
        [
            "AC           Q      Prob(Q)\nlag\n1.0  -0.001244    0.000778  9.777436e-01\n2.0   0.052350    1.382049  5.010626e-01\n3.0  -0.522181  139.090106  5.938063e-30\n4.0   0.146506  149.951983  2.084573e-31\n5.0  -0.091171  154.166872  1.731083e-31\n6.0   0.337059  211.891306  5.568290e-43\n7.0  -0.160920  225.075262  5.519054e-45\n8.0   0.116132  231.955610  1.142179e-45\n9.0  -0.195352  251.464207  4.895752e-49\n10.0  0.166410  265.649428  2.760836e-51\n11.0 -0.126465  273.858717  2.767679e-52\n12.0  0.115015  280.662675  5.334651e-53\n13.0 -0.159302  293.742050  4.899046e-55\n14.0  0.095846  298.486519  2.444596e-55\n15.0 -0.062853  300.531001  4.335557e-55\n16.0  0.159244  313.681886  3.718133e-57\n17.0 -0.089423  317.837389  2.317190e-57\n18.0  0.002504  317.840655  1.018533e-56\n19.0 -0.124735  325.959706  9.297882e-58\n20.0  0.093960  330.576238  4.414194e-58\n21.0 -0.016212  330.713971  1.708363e-57\n22.0  0.054804  332.291098  3.279536e-57\n23.0 -0.110592  338.726892  6.325402e-58\n24.0  0.022742  338.999620  2.166837e-57\n25.0  0.029459  339.458217  6.665490e-57\n26.0  0.095294  344.266902  2.658405e-57",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n  warn('Non-stationary starting autoregressive parameters'\n/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'",
            "code"
        ],
        [
            "[35]:",
            "code"
        ],
        [
            "arma41 = ARIMA(arma_rvs, order=(4, 0, 1)).fit()\nresid = arma41.resid\nr, q, p = sm.tsa.acf(resid, nlags=lags, fft=True, qstat=True)\ndata = np.c_[range(1, lags + 1), r[1:], q, p]\ntable = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "code"
        ],
        [
            "AC          Q  Prob(Q)\nlag\n1.0  -0.007899   0.031383  0.859389\n2.0   0.004128   0.039972  0.980212\n3.0   0.018095   0.205341  0.976722\n4.0  -0.006766   0.228509  0.993949\n5.0   0.018123   0.395044  0.995465\n6.0   0.050690   1.700565  0.945078\n7.0   0.010253   1.754087  0.972191\n8.0  -0.011208   1.818176  0.986088\n9.0   0.020292   2.028663  0.991006\n10.0  0.001028   2.029204  0.996111\n11.0 -0.014033   2.130285  0.997983\n12.0 -0.023858   2.423052  0.998426\n13.0 -0.002108   2.425342  0.999339\n14.0 -0.018784   2.607562  0.999589\n15.0  0.011317   2.673844  0.999805\n16.0  0.042158   3.595554  0.999443\n17.0  0.007943   3.628344  0.999734\n18.0 -0.074312   6.504019  0.993685\n19.0 -0.023378   6.789205  0.995255\n20.0  0.002398   6.792213  0.997313\n21.0  0.000488   6.792338  0.998515\n22.0  0.017953   6.961578  0.999024\n23.0 -0.038576   7.744617  0.998744\n24.0 -0.029817   8.213410  0.998859\n25.0  0.077850  11.415980  0.990674\n26.0  0.040407  12.280577  0.989478",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Exercise: How good of in-sample prediction can you do for another series, say, CPI": [
        [
            "[36]:",
            "code"
        ],
        [
            "macrodta = sm.datasets.macrodata.load_pandas().data\nmacrodta.index = pd.Index(sm.tsa.datetools.dates_from_range(\"1959Q1\", \"2009Q3\"))\ncpi = macrodta[\"cpi\"]",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Exercise: How good of in-sample prediction can you do for another series, say, CPI->Hint:": [
        [
            "[37]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax = cpi.plot(ax=ax)\nax.legend()",
            "code"
        ],
        [
            "[37]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f1ffd0e2b30\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_arma_0_49_1.png\" src=\"../../../_images/examples_notebooks_generated_tsa_arma_0_49_1.png\"/>",
            "code"
        ],
        [
            "P-value of the unit-root test, resoundingly rejects the null of a unit-root.",
            "markdown"
        ],
        [
            "[38]:",
            "code"
        ],
        [
            "print(sm.tsa.adfuller(cpi)[1])",
            "code"
        ],
        [
            "0.9904328188337422",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->ARMA: Artificial Data": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)",
            "code"
        ],
        [
            "Generate some data from an ARMA process:",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "arparams = np.array([0.75, -0.25])\nmaparams = np.array([0.65, 0.35])",
            "code"
        ],
        [
            "The conventions of the arma_generate function require that we specify a 1 for the zero-lag of the AR and MA parameters and that the AR parameters be negated.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "arparams = np.r_[1, -arparams]\nmaparams = np.r_[1, maparams]\nnobs = 250\ny = arma_generate_sample(arparams, maparams, nobs)",
            "code"
        ],
        [
            "Now, optionally, we can add some dates information. For this example, we\u2019ll use a pandas time series.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "dates = pd.date_range(\"1980-1-1\", freq=\"M\", periods=nobs)\ny = pd.Series(y, index=dates)\narma_mod = ARIMA(y, order=(2, 0, 2), trend=\"n\")\narma_res = arma_mod.fit()",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "print(arma_res.summary())",
            "code"
        ],
        [
            "SARIMAX Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                  250\nModel:                 ARIMA(2, 0, 2)   Log Likelihood                -353.445\nDate:                Wed, 02 Nov 2022   AIC                            716.891\nTime:                        17:07:18   BIC                            734.498\nSample:                    01-31-1980   HQIC                           723.977\n                         - 10-31-2000\nCovariance Type:                  opg\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1          0.7905      0.142      5.566      0.000       0.512       1.069\nar.L2         -0.2314      0.124     -1.859      0.063      -0.475       0.013\nma.L1          0.7007      0.131      5.344      0.000       0.444       0.958\nma.L2          0.4061      0.097      4.177      0.000       0.216       0.597\nsigma2         0.9801      0.093     10.514      0.000       0.797       1.163\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):                 0.29\nProb(Q):                              0.96   Prob(JB):                         0.86\nHeteroskedasticity (H):               0.92   Skew:                             0.02\nProb(H) (two-sided):                  0.69   Kurtosis:                         2.84\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "y.tail()",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "2000-06-30    0.173211\n2000-07-31   -0.048325\n2000-08-31   -0.415804\n2000-09-30    0.338725\n2000-10-31    0.360838\nFreq: M, dtype: float64",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10, 8))\nfig = plot_predict(arma_res, start=\"1999-06-30\", end=\"2001-05-31\", ax=ax)\nlegend = ax.legend(loc=\"upper left\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_arma_1_11_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_arma_1_11_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Time Series Filters": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "dta = sm.datasets.macrodata.load_pandas().data",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "index = pd.Index(sm.tsa.datetools.dates_from_range(\"1959Q1\", \"2009Q3\"))\nprint(index)",
            "code"
        ],
        [
            "DatetimeIndex(['1959-03-31', '1959-06-30', '1959-09-30', '1959-12-31',\n               '1960-03-31', '1960-06-30', '1960-09-30', '1960-12-31',\n               '1961-03-31', '1961-06-30',\n               ...\n               '2007-06-30', '2007-09-30', '2007-12-31', '2008-03-31',\n               '2008-06-30', '2008-09-30', '2008-12-31', '2009-03-31',\n               '2009-06-30', '2009-09-30'],\n              dtype='datetime64[ns]', length=203, freq=None)",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "dta.index = index\ndel dta[\"year\"]\ndel dta[\"quarter\"]",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "print(sm.datasets.macrodata.NOTE)",
            "code"
        ],
        [
            "::\n    Number of Observations - 203\n\n    Number of Variables - 14\n\n    Variable name definitions::\n\n        year      - 1959q1 - 2009q3\n        quarter   - 1-4\n        realgdp   - Real gross domestic product (Bil. of chained 2005 US$,\n                    seasonally adjusted annual rate)\n        realcons  - Real personal consumption expenditures (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realinv   - Real gross private domestic investment (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realgovt  - Real federal consumption expenditures & gross investment\n                    (Bil. of chained 2005 US$, seasonally adjusted annual rate)\n        realdpi   - Real private disposable income (Bil. of chained 2005\n                    US$, seasonally adjusted annual rate)\n        cpi       - End of the quarter consumer price index for all urban\n                    consumers: all items (1982-84 = 100, seasonally adjusted).\n        m1        - End of the quarter M1 nominal money stock (Seasonally\n                    adjusted)\n        tbilrate  - Quarterly monthly average of the monthly 3-month\n                    treasury bill: secondary market rate\n        unemp     - Seasonally adjusted unemployment rate (%)\n        pop       - End of the quarter total population: all ages incl. armed\n                    forces over seas\n        infl      - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400)\n        realint   - Real interest rate (tbilrate - infl)",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "print(dta.head(10))",
            "code"
        ],
        [
            "realgdp  realcons  realinv  realgovt  realdpi    cpi     m1  \\\n1959-03-31  2710.349    1707.4  286.898   470.045   1886.9  28.98  139.7\n1959-06-30  2778.801    1733.7  310.859   481.301   1919.7  29.15  141.7\n1959-09-30  2775.488    1751.8  289.226   491.260   1916.4  29.35  140.5\n1959-12-31  2785.204    1753.7  299.356   484.052   1931.3  29.37  140.0\n1960-03-31  2847.699    1770.5  331.722   462.199   1955.5  29.54  139.6\n1960-06-30  2834.390    1792.9  298.152   460.400   1966.1  29.55  140.2\n1960-09-30  2839.022    1785.8  296.375   474.676   1967.8  29.75  140.9\n1960-12-31  2802.616    1788.2  259.764   476.434   1966.6  29.84  141.1\n1961-03-31  2819.264    1787.7  266.405   475.854   1984.5  29.81  142.1\n1961-06-30  2872.005    1814.3  286.246   480.328   2014.4  29.92  142.9\n\n            tbilrate  unemp      pop  infl  realint\n1959-03-31      2.82    5.8  177.146  0.00     0.00\n1959-06-30      3.08    5.1  177.830  2.34     0.74\n1959-09-30      3.82    5.3  178.657  2.74     1.09\n1959-12-31      4.33    5.6  179.386  0.27     4.06\n1960-03-31      3.50    5.2  180.007  2.31     1.19\n1960-06-30      2.68    5.2  180.671  0.14     2.55\n1960-09-30      2.36    5.6  181.528  2.70    -0.34\n1960-12-31      2.29    6.3  182.287  1.21     1.08\n1961-03-31      2.37    6.8  182.992 -0.40     2.77\n1961-06-30      2.29    7.0  183.691  1.47     0.81",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\ndta.realgdp.plot(ax=ax)\nlegend = ax.legend(loc=\"upper left\")\nlegend.prop.set_size(20)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_filters_8_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_filters_8_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Time Series Filters->Hodrick-Prescott Filter": [
        [
            "The Hodrick-Prescott filter separates a time-series \\(y_t\\) into a trend \\(\\tau_t\\) and a cyclical component \\(\\zeta_t\\)\n\n\\[y_t = \\tau_t + \\zeta_t\\]",
            "markdown"
        ],
        [
            "The components are determined by minimizing the following quadratic loss function\n\n\\[\\begin{split}\\min_{\\\\{ \\tau_{t}\\\\} }\\sum_{t}^{T}\\zeta_{t}^{2}+\\lambda\\sum_{t=1}^{T}\\left[\\left(\\tau_{t}-\\tau_{t-1}\\right)-\\left(\\tau_{t-1}-\\tau_{t-2}\\right)\\right]^{2}\\end{split}\\]",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "gdp_cycle, gdp_trend = sm.tsa.filters.hpfilter(dta.realgdp)",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "gdp_decomp = dta[[\"realgdp\"]].copy()\ngdp_decomp[\"cycle\"] = gdp_cycle\ngdp_decomp[\"trend\"] = gdp_trend",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\ngdp_decomp[[\"realgdp\", \"trend\"]][\"2000-03-31\":].plot(ax=ax, fontsize=16)\nlegend = ax.get_legend()\nlegend.prop.set_size(20)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_filters_13_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_filters_13_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical.": [
        [
            "The Baxter-King filter is intended to explicitly deal with the periodicity of the business cycle. By applying their band-pass filter to a series, they produce a new series that does not contain fluctuations at higher or lower than those of the business cycle. Specifically, the BK filter takes the form of a symmetric moving average\n\n\\[y_{t}^{*}=\\sum_{k=-K}^{k=K}a_ky_{t-k}\\]",
            "markdown"
        ],
        [
            "where \\(a_{-k}=a_k\\) and \\(\\sum_{k=-k}^{K}a_k=0\\) to eliminate any trend in the series and render it stationary if the series is I(1) or I(2).",
            "markdown"
        ],
        [
            "For completeness, the filter weights are determined as follows\n\n\\[a_{j} = B_{j}+\\theta\\text{ for }j=0,\\pm1,\\pm2,\\dots,\\pm K\\]\n\n\\[B_{0} = \\frac{\\left(\\omega_{2}-\\omega_{1}\\right)}{\\pi}\\]\n\n\\[B_{j} = \\frac{1}{\\pi j}\\left(\\sin\\left(\\omega_{2}j\\right)-\\sin\\left(\\omega_{1}j\\right)\\right)\\text{ for }j=0,\\pm1,\\pm2,\\dots,\\pm K\\]",
            "markdown"
        ],
        [
            "where \\(\\theta\\) is a normalizing constant such that the weights sum to zero.\n\n\\[\\theta=\\frac{-\\sum_{j=-K^{K}b_{j}}}{2K+1}\\]\n\n\\[\\omega_{1}=\\frac{2\\pi}{P_{H}}\\]\n\n\\[\\omega_{2}=\\frac{2\\pi}{P_{L}}\\]",
            "markdown"
        ],
        [
            "\\(P_L\\) and \\(P_H\\) are the periodicity of the low and high cut-off frequencies. Following Burns and Mitchell\u2019s work on US business cycles which suggests cycles last from 1.5 to 8 years, we use \\(P_L=6\\) and \\(P_H=32\\) by default.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "bk_cycles = sm.tsa.filters.bkfilter(dta[[\"infl\", \"unemp\"]])",
            "code"
        ],
        [
            "We lose K observations on both ends. It is suggested to use K=12 for quarterly data.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111)\nbk_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: \n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_filters_19_1.png\" src=\"../../../_images/examples_notebooks_generated_tsa_filters_19_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment": [
        [
            "The Christiano-Fitzgerald filter is a generalization of BK and can thus also be seen as weighted moving average. However, the CF filter is asymmetric about \\(t\\) as well as using the entire series. The implementation of their filter involves the calculations of the weights in\n\n\\[y_{t}^{*}=B_{0}y_{t}+B_{1}y_{t+1}+\\dots+B_{T-1-t}y_{T-1}+\\tilde B_{T-t}y_{T}+B_{1}y_{t-1}+\\dots+B_{t-2}y_{2}+\\tilde B_{t-1}y_{1}\\]",
            "markdown"
        ],
        [
            "for \\(t=3,4,...,T-2\\), where\n\n\\[B_{j} = \\frac{\\sin(jb)-\\sin(ja)}{\\pi j},j\\geq1\\]\n\n\\[B_{0} = \\frac{b-a}{\\pi},a=\\frac{2\\pi}{P_{u}},b=\\frac{2\\pi}{P_{L}}\\]",
            "markdown"
        ],
        [
            "\\(\\tilde B_{T-t}\\) and \\(\\tilde B_{t-1}\\) are linear functions of the \\(B_{j}\\)\u2019s, and the values for \\(t=1,2,T-1,\\) and \\(T\\) are also calculated in much the same way. \\(P_{U}\\) and \\(P_{L}\\) are as described above with the same interpretation.",
            "markdown"
        ],
        [
            "The CF filter is appropriate for series that may follow a random walk.",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "print(sm.tsa.stattools.adfuller(dta[\"unemp\"])[:3])",
            "code"
        ],
        [
            "(-2.5364584673346373, 0.10685366457233453, 9)",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "print(sm.tsa.stattools.adfuller(dta[\"infl\"])[:3])",
            "code"
        ],
        [
            "(-3.054514496257237, 0.030107620863485812, 2)",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "cf_cycles, cf_trend = sm.tsa.filters.cffilter(dta[[\"infl\", \"unemp\"]])\nprint(cf_cycles.head(10))",
            "code"
        ],
        [
            "infl_cycle  unemp_cycle\n1959-03-31    0.237927    -0.216867\n1959-06-30    0.770007    -0.343779\n1959-09-30    1.177736    -0.511024\n1959-12-31    1.256754    -0.686967\n1960-03-31    0.972128    -0.770793\n1960-06-30    0.491889    -0.640601\n1960-09-30    0.070189    -0.249741\n1960-12-31   -0.130432     0.301545\n1961-03-31   -0.134155     0.788992\n1961-06-30   -0.092073     0.985356",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(111)\ncf_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: \n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_filters_26_1.png\" src=\"../../../_images/examples_notebooks_generated_tsa_filters_26_1.png\"/>",
            "code"
        ],
        [
            "Filtering assumes a priori that business cycles exist. Due to this assumption, many macroeconomic models seek to create models that match the shape of impulse response functions rather than replicating properties of filtered series. See VAR notebook.",
            "markdown"
        ]
    ],
    "Examples->Time Series Analysis->Markov switching dynamic regression": [
        [
            "This notebook provides an example of the use of Markov switching models in statsmodels to estimate dynamic regression models with changes in regime. It follows the examples in the Stata Markov switching documentation, which can be found at .",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# NBER recessions\nfrom pandas_datareader.data import DataReader\nfrom datetime import datetime\n\nusrec = DataReader(\n    \"USREC\", \"fred\", start=datetime(1947, 1, 1), end=datetime(2013, 4, 1)\n)",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept": [
        [
            "The first example models the federal funds rate as noise around a constant intercept, but where the intercept changes during different regimes. The model is simply:\n\n\\[r_t = \\mu_{S_t} + \\varepsilon_t \\qquad \\varepsilon_t \\sim N(0, \\sigma^2)\\]",
            "markdown"
        ],
        [
            "where \\(S_t \\in \\{0, 1\\}\\), and the regime transitions according to\n\n\\[\\begin{split} P(S_t = s_t | S_{t-1} = s_{t-1}) =\n\\begin{bmatrix}\np_{00} & p_{10} \\\\\n1 - p_{00} & 1 - p_{10}\n\\end{bmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "We will estimate the parameters of this model by maximum likelihood: \\(p_{00}, p_{10}, \\mu_0, \\mu_1, \\sigma^2\\).",
            "markdown"
        ],
        [
            "The data used in this example can be found at .",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "# Get the federal funds rate data\nfrom statsmodels.tsa.regime_switching.tests.test_markov_regression import fedfunds\n\ndta_fedfunds = pd.Series(\n    fedfunds, index=pd.date_range(\"1954-07-01\", \"2010-10-01\", freq=\"QS\")\n)\n\n# Plot the data\ndta_fedfunds.plot(title=\"Federal funds rate\", figsize=(12, 3))\n\n# Fit the model\n# (a switching mean is the default of the MarkovRegession model)\nmod_fedfunds = sm.tsa.MarkovRegression(dta_fedfunds, k_regimes=2)\nres_fedfunds = mod_fedfunds.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_regression_4_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_regression_4_0.png\"/>",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "res_fedfunds.summary()",
            "code"
        ],
        [
            "[3]:\n\n\n\n\n\n\n\n<br/><br/>Warnings:<br/>[1] Covariance matrix calculated using numerical (complex-step) differentiation.",
            "code"
        ],
        [
            "From the summary output, the mean federal funds rate in the first regime (the \u201clow regime\u201d) is estimated to be \\(3.7\\) whereas in the \u201chigh regime\u201d it is \\(9.6\\). Below we plot the smoothed probabilities of being in the high regime. The model suggests that the 1980\u2019s was a time-period in which a high federal funds rate existed.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "res_fedfunds.smoothed_marginal_probabilities[1].plot(\n    title=\"Probability of being in the high regime\", figsize=(12, 3)\n)",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: title={'center': 'Probability of being in the high regime'}\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_regression_7_1.png\" src=\"../../../_images/examples_notebooks_generated_markov_regression_7_1.png\"/>",
            "code"
        ],
        [
            "From the estimated transition matrix we can calculate the expected duration of a low regime versus a high regime.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "print(res_fedfunds.expected_durations)",
            "code"
        ],
        [
            "[55.85400626 19.85506546]",
            "code"
        ],
        [
            "A low regime is expected to persist for about fourteen years, whereas the high regime is expected to persist for only about five years.",
            "markdown"
        ]
    ],
    "Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept and lagged dependent variable": [
        [
            "The second example augments the previous model to include the lagged value of the federal funds rate.\n\n\\[r_t = \\mu_{S_t} + r_{t-1} \\beta_{S_t} + \\varepsilon_t \\qquad \\varepsilon_t \\sim N(0, \\sigma^2)\\]",
            "markdown"
        ],
        [
            "where \\(S_t \\in \\{0, 1\\}\\), and the regime transitions according to\n\n\\[\\begin{split} P(S_t = s_t | S_{t-1} = s_{t-1}) =\n\\begin{bmatrix}\np_{00} & p_{10} \\\\\n1 - p_{00} & 1 - p_{10}\n\\end{bmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "We will estimate the parameters of this model by maximum likelihood: \\(p_{00}, p_{10}, \\mu_0, \\mu_1, \\beta_0, \\beta_1, \\sigma^2\\).",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "# Fit the model\nmod_fedfunds2 = sm.tsa.MarkovRegression(\n    dta_fedfunds.iloc[1:], k_regimes=2, exog=dta_fedfunds.iloc[:-1]\n)\nres_fedfunds2 = mod_fedfunds2.fit()",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "res_fedfunds2.summary()",
            "code"
        ],
        [
            "[7]:\n\n\n\n\n\n\n\n<br/><br/>Warnings:<br/>[1] Covariance matrix calculated using numerical (complex-step) differentiation.",
            "code"
        ],
        [
            "There are several things to notice from the summary output:",
            "markdown"
        ],
        [
            "The information criteria have decreased substantially, indicating that this model has a better fit than the previous model.",
            "markdown"
        ],
        [
            "The interpretation of the regimes, in terms of the intercept, have switched. Now the first regime has the higher intercept and the second regime has a lower intercept.",
            "markdown"
        ],
        [
            "Examining the smoothed probabilities of the high regime state, we now see quite a bit more variability.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "res_fedfunds2.smoothed_marginal_probabilities[0].plot(\n    title=\"Probability of being in the high regime\", figsize=(12, 3)\n)",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: title={'center': 'Probability of being in the high regime'}\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_regression_15_1.png\" src=\"../../../_images/examples_notebooks_generated_markov_regression_15_1.png\"/>",
            "code"
        ],
        [
            "Finally, the expected durations of each regime have decreased quite a bit.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "print(res_fedfunds2.expected_durations)",
            "code"
        ],
        [
            "[2.76105188 7.65529154]",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Markov switching dynamic regression->Taylor rule with 2 or 3 regimes": [
        [
            "We now include two additional exogenous variables - a measure of the output gap and a measure of inflation - to estimate a switching Taylor-type rule with both 2 and 3 regimes to see which fits the data better.",
            "markdown"
        ],
        [
            "Because the models can be often difficult to estimate, for the 3-regime model we employ a search over starting parameters to improve results, specifying 20 random search repetitions.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "# Get the additional data\nfrom statsmodels.tsa.regime_switching.tests.test_markov_regression import ogap, inf\n\ndta_ogap = pd.Series(ogap, index=pd.date_range(\"1954-07-01\", \"2010-10-01\", freq=\"QS\"))\ndta_inf = pd.Series(inf, index=pd.date_range(\"1954-07-01\", \"2010-10-01\", freq=\"QS\"))\n\nexog = pd.concat((dta_fedfunds.shift(), dta_ogap, dta_inf), axis=1).iloc[4:]\n\n# Fit the 2-regime model\nmod_fedfunds3 = sm.tsa.MarkovRegression(dta_fedfunds.iloc[4:], k_regimes=2, exog=exog)\nres_fedfunds3 = mod_fedfunds3.fit()\n\n# Fit the 3-regime model\nnp.random.seed(12345)\nmod_fedfunds4 = sm.tsa.MarkovRegression(dta_fedfunds.iloc[4:], k_regimes=3, exog=exog)\nres_fedfunds4 = mod_fedfunds4.fit(search_reps=20)",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "res_fedfunds3.summary()",
            "code"
        ],
        [
            "[11]:\n\n\n\n\n\n\n\n<br/><br/>Warnings:<br/>[1] Covariance matrix calculated using numerical (complex-step) differentiation.",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "res_fedfunds4.summary()",
            "code"
        ],
        [
            "[12]:\n\n\n\n\n\n\n\n\n<br/><br/>Warnings:<br/>[1] Covariance matrix calculated using numerical (complex-step) differentiation.",
            "code"
        ],
        [
            "Due to lower information criteria, we might prefer the 3-state model, with an interpretation of low-, medium-, and high-interest rate regimes. The smoothed probabilities of each regime are plotted below.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "fig, axes = plt.subplots(3, figsize=(10, 7))\n\nax = axes[0]\nax.plot(res_fedfunds4.smoothed_marginal_probabilities[0])\nax.set(title=\"Smoothed probability of a low-interest rate regime\")\n\nax = axes[1]\nax.plot(res_fedfunds4.smoothed_marginal_probabilities[1])\nax.set(title=\"Smoothed probability of a medium-interest rate regime\")\n\nax = axes[2]\nax.plot(res_fedfunds4.smoothed_marginal_probabilities[2])\nax.set(title=\"Smoothed probability of a high-interest rate regime\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_regression_23_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_regression_23_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Markov switching dynamic regression->Switching variances": [
        [
            "We can also accommodate switching variances. In particular, we consider the model\n\n\\[y_t = \\mu_{S_t} + y_{t-1} \\beta_{S_t} + \\varepsilon_t \\quad \\varepsilon_t \\sim N(0, \\sigma_{S_t}^2)\\]",
            "markdown"
        ],
        [
            "We use maximum likelihood to estimate the parameters of this model: \\(p_{00}, p_{10}, \\mu_0, \\mu_1, \\beta_0, \\beta_1, \\sigma_0^2, \\sigma_1^2\\).",
            "markdown"
        ],
        [
            "The application is to absolute returns on stocks, where the data can be found at .",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "# Get the federal funds rate data\nfrom statsmodels.tsa.regime_switching.tests.test_markov_regression import areturns\n\ndta_areturns = pd.Series(\n    areturns, index=pd.date_range(\"2004-05-04\", \"2014-5-03\", freq=\"W\")\n)\n\n# Plot the data\ndta_areturns.plot(title=\"Absolute returns, S&P500\", figsize=(12, 3))\n\n# Fit the model\nmod_areturns = sm.tsa.MarkovRegression(\n    dta_areturns.iloc[1:],\n    k_regimes=2,\n    exog=dta_areturns.iloc[:-1],\n    switching_variance=True,\n)\nres_areturns = mod_areturns.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_regression_25_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_regression_25_0.png\"/>",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "res_areturns.summary()",
            "code"
        ],
        [
            "[15]:\n\n\n\n\n\n\n<br/><br/>Warnings:<br/>[1] Covariance matrix calculated using numerical (complex-step) differentiation.",
            "code"
        ],
        [
            "The first regime is a low-variance regime and the second regime is a high-variance regime. Below we plot the probabilities of being in the low-variance regime. Between 2008 and 2012 there does not appear to be a clear indication of one regime guiding the economy.",
            "markdown"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "res_areturns.smoothed_marginal_probabilities[0].plot(\n    title=\"Probability of being in a low-variance regime\", figsize=(12, 3)\n)",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: title={'center': 'Probability of being in a low-variance regime'}\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_regression_28_1.png\" src=\"../../../_images/examples_notebooks_generated_markov_regression_28_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Markov switching autoregression": [
        [
            "This notebook provides an example of the use of Markov switching models in statsmodels to replicate a number of results presented in Kim and Nelson (1999). It applies the Hamilton (1989) filter the Kim (1994) smoother.",
            "markdown"
        ],
        [
            "This is tested against the Markov-switching models from E-views 8, which can be found at  or the Markov-switching models of Stata 14 which can be found at .",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nfrom datetime import datetime\nfrom io import BytesIO\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport requests\nimport statsmodels.api as sm\n\n# NBER recessions\nfrom pandas_datareader.data import DataReader\n\nusrec = DataReader(\n    \"USREC\", \"fred\", start=datetime(1947, 1, 1), end=datetime(2013, 4, 1)\n)",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Markov switching autoregression->Hamilton (1989) switching model of GNP": [
        [
            "This replicates Hamilton\u2019s (1989) seminal paper introducing Markov-switching models. The model is an autoregressive model of order 4 in which the mean of the process switches between two regimes. It can be written:\n\n\\[y_t = \\mu_{S_t} + \\phi_1 (y_{t-1} - \\mu_{S_{t-1}}) + \\phi_2 (y_{t-2} - \\mu_{S_{t-2}}) + \\phi_3 (y_{t-3} - \\mu_{S_{t-3}}) + \\phi_4 (y_{t-4} - \\mu_{S_{t-4}}) + \\varepsilon_t\\]",
            "markdown"
        ],
        [
            "Each period, the regime transitions according to the following matrix of transition probabilities:\n\n\\[\\begin{split} P(S_t = s_t | S_{t-1} = s_{t-1}) =\n\\begin{bmatrix}\np_{00} & p_{10} \\\\\np_{01} & p_{11}\n\\end{bmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(p_{ij}\\) is the probability of transitioning from regime \\(i\\), to regime \\(j\\).",
            "markdown"
        ],
        [
            "The model class is MarkovAutoregression in the time-series part of statsmodels. In order to create the model, we must specify the number of regimes with k_regimes=2, and the order of the autoregression with order=4. The default model also includes switching autoregressive coefficients, so here we also need to specify switching_ar=False to avoid that.",
            "markdown"
        ],
        [
            "After creation, the model is fit via maximum likelihood estimation. Under the hood, good starting parameters are found using a number of steps of the expectation maximization (EM) algorithm, and a quasi-Newton (BFGS) algorithm is applied to quickly find the maximum.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "# Get the RGNP data to replicate Hamilton\ndta = pd.read_stata(\"https://www.stata-press.com/data/r14/rgnp.dta\").iloc[1:]\ndta.index = pd.DatetimeIndex(dta.date, freq=\"QS\")\ndta_hamilton = dta.rgnp\n\n# Plot the data\ndta_hamilton.plot(title=\"Growth rate of Real GNP\", figsize=(12, 3))\n\n# Fit the model\nmod_hamilton = sm.tsa.MarkovAutoregression(\n    dta_hamilton, k_regimes=2, order=4, switching_ar=False\n)\nres_hamilton = mod_hamilton.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\"/>",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "res_hamilton.summary()",
            "code"
        ],
        [
            "[3]:\n\n\n\n\n\n\n\n<br/><br/>Warnings:<br/>[1] Covariance matrix calculated using numerical (complex-step) differentiation.",
            "code"
        ],
        [
            "We plot the filtered and smoothed probabilities of a recession. Filtered refers to an estimate of the probability at time \\(t\\) based on data up to and including time \\(t\\) (but excluding time \\(t+1, ..., T\\)). Smoothed refers to an estimate of the probability at time \\(t\\) using all the data in the sample.",
            "markdown"
        ],
        [
            "For reference, the shaded periods represent the NBER recessions.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "fig, axes = plt.subplots(2, figsize=(7, 7))\nax = axes[0]\nax.plot(res_hamilton.filtered_marginal_probabilities[0])\nax.fill_between(usrec.index, 0, 1, where=usrec[\"USREC\"].values, color=\"k\", alpha=0.1)\nax.set_xlim(dta_hamilton.index[4], dta_hamilton.index[-1])\nax.set(title=\"Filtered probability of recession\")\n\nax = axes[1]\nax.plot(res_hamilton.smoothed_marginal_probabilities[0])\nax.fill_between(usrec.index, 0, 1, where=usrec[\"USREC\"].values, color=\"k\", alpha=0.1)\nax.set_xlim(dta_hamilton.index[4], dta_hamilton.index[-1])\nax.set(title=\"Smoothed probability of recession\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_7_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_7_0.png\"/>",
            "code"
        ],
        [
            "From the estimated transition matrix we can calculate the expected duration of a recession versus an expansion.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "print(res_hamilton.expected_durations)",
            "code"
        ],
        [
            "[ 4.07604746 10.42589383]",
            "code"
        ],
        [
            "In this case, it is expected that a recession will last about one year (4 quarters) and an expansion about two and a half years.",
            "markdown"
        ]
    ],
    "Examples->Time Series Analysis->Markov switching autoregression->Kim, Nelson, and Startz (1998) Three-state Variance Switching": [
        [
            "This model demonstrates estimation with regime heteroskedasticity (switching of variances) and no mean effect. The dataset can be reached at .",
            "markdown"
        ],
        [
            "The model in question is:\n\n\\[\\begin{split}\\begin{align}\ny_t & = \\varepsilon_t \\\\\n\\varepsilon_t & \\sim N(0, \\sigma_{S_t}^2)\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "Since there is no autoregressive component, this model can be fit using the MarkovRegression class. Since there is no mean effect, we specify trend='n'. There are hypothesized to be three regimes for the switching variances, so we specify k_regimes=3 and switching_variance=True (by default, the variance is assumed to be the same across regimes).",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "# Get the dataset\new_excs = requests.get(\"http://econ.korea.ac.kr/~cjkim/MARKOV/data/ew_excs.prn\").content\nraw = pd.read_table(BytesIO(ew_excs), header=None, skipfooter=1, engine=\"python\")\nraw.index = pd.date_range(\"1926-01-01\", \"1995-12-01\", freq=\"MS\")\n\ndta_kns = raw.loc[:\"1986\"] - raw.loc[:\"1986\"].mean()\n\n# Plot the dataset\ndta_kns[0].plot(title=\"Excess returns\", figsize=(12, 3))\n\n# Fit the model\nmod_kns = sm.tsa.MarkovRegression(\n    dta_kns, k_regimes=3, trend=\"n\", switching_variance=True\n)\nres_kns = mod_kns.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\"/>",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "res_kns.summary()",
            "code"
        ],
        [
            "[7]:\n\n\n\n\n\n\n\n<br/><br/>Warnings:<br/>[1] Covariance matrix calculated using numerical (complex-step) differentiation.",
            "code"
        ],
        [
            "Below we plot the probabilities of being in each of the regimes; only in a few periods is a high-variance regime probable.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "fig, axes = plt.subplots(3, figsize=(10, 7))\n\nax = axes[0]\nax.plot(res_kns.smoothed_marginal_probabilities[0])\nax.set(title=\"Smoothed probability of a low-variance regime for stock returns\")\n\nax = axes[1]\nax.plot(res_kns.smoothed_marginal_probabilities[1])\nax.set(title=\"Smoothed probability of a medium-variance regime for stock returns\")\n\nax = axes[2]\nax.plot(res_kns.smoothed_marginal_probabilities[2])\nax.set(title=\"Smoothed probability of a high-variance regime for stock returns\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_15_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_15_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Markov switching autoregression->Filardo (1994) Time-Varying Transition Probabilities": [
        [
            "This model demonstrates estimation with time-varying transition probabilities. The dataset can be reached at .",
            "markdown"
        ],
        [
            "In the above models we have assumed that the transition probabilities are constant across time. Here we allow the probabilities to change with the state of the economy. Otherwise, the model is the same Markov autoregression of Hamilton (1989).",
            "markdown"
        ],
        [
            "Each period, the regime now transitions according to the following matrix of time-varying transition probabilities:\n\n\\[\\begin{split} P(S_t = s_t | S_{t-1} = s_{t-1}) =\n\\begin{bmatrix}\np_{00,t} & p_{10,t} \\\\\np_{01,t} & p_{11,t}\n\\end{bmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(p_{ij,t}\\) is the probability of transitioning from regime \\(i\\), to regime \\(j\\) in period \\(t\\), and is defined to be:\n\n\\[p_{ij,t} = \\frac{\\exp\\{ x_{t-1}' \\beta_{ij} \\}}{1 + \\exp\\{ x_{t-1}' \\beta_{ij} \\}}\\]",
            "markdown"
        ],
        [
            "Instead of estimating the transition probabilities as part of maximum likelihood, the regression coefficients \\(\\beta_{ij}\\) are estimated. These coefficients relate the transition probabilities to a vector of pre-determined or exogenous regressors \\(x_{t-1}\\).",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "# Get the dataset\nfilardo = requests.get(\"http://econ.korea.ac.kr/~cjkim/MARKOV/data/filardo.prn\").content\ndta_filardo = pd.read_table(\n    BytesIO(filardo), sep=\" +\", header=None, skipfooter=1, engine=\"python\"\n)\ndta_filardo.columns = [\"month\", \"ip\", \"leading\"]\ndta_filardo.index = pd.date_range(\"1948-01-01\", \"1991-04-01\", freq=\"MS\")\n\ndta_filardo[\"dlip\"] = np.log(dta_filardo[\"ip\"]).diff() * 100\n# Deflated pre-1960 observations by ratio of std. devs.\n# See hmt_tvp.opt or Filardo (1994) p. 302\nstd_ratio = (\n    dta_filardo[\"dlip\"][\"1960-01-01\":].std() / dta_filardo[\"dlip\"][:\"1959-12-01\"].std()\n)\ndta_filardo[\"dlip\"][:\"1959-12-01\"] = dta_filardo[\"dlip\"][:\"1959-12-01\"] * std_ratio\n\ndta_filardo[\"dlleading\"] = np.log(dta_filardo[\"leading\"]).diff() * 100\ndta_filardo[\"dmdlleading\"] = dta_filardo[\"dlleading\"] - dta_filardo[\"dlleading\"].mean()\n\n# Plot the data\ndta_filardo[\"dlip\"].plot(\n    title=\"Standardized growth rate of industrial production\", figsize=(13, 3)\n)\nplt.figure()\ndta_filardo[\"dmdlleading\"].plot(title=\"Leading indicator\", figsize=(13, 3))",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: title={'center': 'Leading indicator'}\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_17_1.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_17_1.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_17_2.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_17_2.png\"/>",
            "code"
        ],
        [
            "The time-varying transition probabilities are specified by the exog_tvtp parameter.",
            "markdown"
        ],
        [
            "Here we demonstrate another feature of model fitting - the use of a random search for MLE starting parameters. Because Markov switching models are often characterized by many local maxima of the likelihood function, performing an initial optimization step can be helpful to find the best parameters.",
            "markdown"
        ],
        [
            "Below, we specify that 20 random perturbations from the starting parameter vector are examined and the best one used as the actual starting parameters. Because of the random nature of the search, we seed the random number generator beforehand to allow replication of the result.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "mod_filardo = sm.tsa.MarkovAutoregression(\n    dta_filardo.iloc[2:][\"dlip\"],\n    k_regimes=2,\n    order=4,\n    switching_ar=False,\n    exog_tvtp=sm.add_constant(dta_filardo.iloc[1:-1][\"dmdlleading\"]),\n)\n\nnp.random.seed(12345)\nres_filardo = mod_filardo.fit(search_reps=20)",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "res_filardo.summary()",
            "code"
        ],
        [
            "[11]:\n\n\n\n\n\n\n\n<br/><br/>Warnings:<br/>[1] Covariance matrix calculated using numerical (complex-step) differentiation.",
            "code"
        ],
        [
            "Below we plot the smoothed probability of the economy operating in a low-production state, and again include the NBER recessions for comparison.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(12, 3))\n\nax.plot(res_filardo.smoothed_marginal_probabilities[0])\nax.fill_between(usrec.index, 0, 1, where=usrec[\"USREC\"].values, color=\"gray\", alpha=0.2)\nax.set_xlim(dta_filardo.index[6], dta_filardo.index[-1])\nax.set(title=\"Smoothed probability of a low-production state\")",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "[Text(0.5, 1.0, 'Smoothed probability of a low-production state')]\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_22_1.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_22_1.png\"/>",
            "code"
        ],
        [
            "Using the time-varying transition probabilities, we can see how the expected duration of a low-production state changes over time:",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "res_filardo.expected_durations[0].plot(\n    title=\"Expected duration of a low-production state\", figsize=(12, 3)\n)",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: title={'center': 'Expected duration of a low-production state'}\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_24_1.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_24_1.png\"/>",
            "code"
        ],
        [
            "During recessions, the expected duration of a low-production state is much higher than in an expansion.",
            "markdown"
        ]
    ],
    "Examples->Time Series Analysis->Exponential Smoothing": [
        [
            "Let us consider chapter 7 of the excellent treatise on the subject of Exponential Smoothing By Hyndman and Athanasopoulos [1]. We will work through all the examples in the chapter as they unfold.",
            "markdown"
        ],
        [
            "[1]",
            "markdown"
        ]
    ],
    "Examples->Time Series Analysis->Exponential Smoothing->Loading data": [
        [
            "First we load some data. We have included the R data in the notebook for expedience.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\n%matplotlib inline\n\ndata = [\n    446.6565,\n    454.4733,\n    455.663,\n    423.6322,\n    456.2713,\n    440.5881,\n    425.3325,\n    485.1494,\n    506.0482,\n    526.792,\n    514.2689,\n    494.211,\n]\nindex = pd.date_range(start=\"1996\", end=\"2008\", freq=\"A\")\noildata = pd.Series(data, index)\n\ndata = [\n    17.5534,\n    21.86,\n    23.8866,\n    26.9293,\n    26.8885,\n    28.8314,\n    30.0751,\n    30.9535,\n    30.1857,\n    31.5797,\n    32.5776,\n    33.4774,\n    39.0216,\n    41.3864,\n    41.5966,\n]\nindex = pd.date_range(start=\"1990\", end=\"2005\", freq=\"A\")\nair = pd.Series(data, index)\n\ndata = [\n    263.9177,\n    268.3072,\n    260.6626,\n    266.6394,\n    277.5158,\n    283.834,\n    290.309,\n    292.4742,\n    300.8307,\n    309.2867,\n    318.3311,\n    329.3724,\n    338.884,\n    339.2441,\n    328.6006,\n    314.2554,\n    314.4597,\n    321.4138,\n    329.7893,\n    346.3852,\n    352.2979,\n    348.3705,\n    417.5629,\n    417.1236,\n    417.7495,\n    412.2339,\n    411.9468,\n    394.6971,\n    401.4993,\n    408.2705,\n    414.2428,\n]\nindex = pd.date_range(start=\"1970\", end=\"2001\", freq=\"A\")\nlivestock2 = pd.Series(data, index)\n\ndata = [407.9979, 403.4608, 413.8249, 428.105, 445.3387, 452.9942, 455.7402]\nindex = pd.date_range(start=\"2001\", end=\"2008\", freq=\"A\")\nlivestock3 = pd.Series(data, index)\n\ndata = [\n    41.7275,\n    24.0418,\n    32.3281,\n    37.3287,\n    46.2132,\n    29.3463,\n    36.4829,\n    42.9777,\n    48.9015,\n    31.1802,\n    37.7179,\n    40.4202,\n    51.2069,\n    31.8872,\n    40.9783,\n    43.7725,\n    55.5586,\n    33.8509,\n    42.0764,\n    45.6423,\n    59.7668,\n    35.1919,\n    44.3197,\n    47.9137,\n]\nindex = pd.date_range(start=\"2005\", end=\"2010-Q4\", freq=\"QS-OCT\")\naust = pd.Series(data, index)",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing": [
        [
            "Lets use Simple Exponential Smoothing to forecast the below oil data.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "ax = oildata.plot()\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Oil (millions of tonnes)\")\nprint(\"Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\")",
            "code"
        ],
        [
            "Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_4_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_4_1.png\"/>",
            "code"
        ],
        [
            "Here we run three variants of simple exponential smoothing: 1. In fit1 we do not use the auto optimization but instead choose to explicitly provide the model with the \\(\\alpha=0.2\\) parameter 2. In fit2 as above we choose an \\(\\alpha=0.6\\) 3. In fit3 we allow statsmodels to automatically find an optimized \\(\\alpha\\) value for us. This is the recommended approach.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f0461656350\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_6_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_6_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method": [
        [
            "Lets take a look at another example. This time we use air pollution data and the Holt\u2019s Method. We will fit three examples again. 1. In fit1 we again choose not to use the optimizer and provide explicit values for \\(\\alpha=0.8\\) and \\(\\beta=0.2\\) 2. In fit2 we do the same as in fit1 but choose to use an exponential model rather than a Holt\u2019s additive model. 3. In fit3 we used a damped versions of the Holt\u2019s additive model but allow the dampening parameter \\(\\phi\\) to\nbe optimized while fixing the values for \\(\\alpha=0.8\\) and \\(\\beta=0.2\\)",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "fit1 = Holt(air, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast1 = fit1.forecast(5).rename(\"Holt's linear trend\")\nfit2 = Holt(air, exponential=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast2 = fit2.forecast(5).rename(\"Exponential trend\")\nfit3 = Holt(air, damped_trend=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2\n)\nfcast3 = fit3.forecast(5).rename(\"Additive damped trend\")\n\nplt.figure(figsize=(12, 8))\nplt.plot(air, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f0461056c80\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_8_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_8_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Seasonally adjusted data": [
        [
            "Lets look at some seasonally adjusted livestock data. We fit five Holt\u2019s models. The below table allows us to compare results when we use exponential versus additive and damped versus non-damped.",
            "markdown"
        ],
        [
            "Note: fit4 does not allow the parameter \\(\\phi\\) to be optimized by providing a fixed value of \\(\\phi=0.98\\)",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "fit1 = SimpleExpSmoothing(livestock2, initialization_method=\"estimated\").fit()\nfit2 = Holt(livestock2, initialization_method=\"estimated\").fit()\nfit3 = Holt(livestock2, exponential=True, initialization_method=\"estimated\").fit()\nfit4 = Holt(livestock2, damped_trend=True, initialization_method=\"estimated\").fit(\n    damping_trend=0.98\n)\nfit5 = Holt(\n    livestock2, exponential=True, damped_trend=True, initialization_method=\"estimated\"\n).fit()\nparams = [\n    \"smoothing_level\",\n    \"smoothing_trend\",\n    \"damping_trend\",\n    \"initial_level\",\n    \"initial_trend\",\n]\nresults = pd.DataFrame(\n    index=[r\"$\\alpha$\", r\"$\\beta$\", r\"$\\phi$\", r\"$l_0$\", \"$b_0$\", \"SSE\"],\n    columns=[\"SES\", \"Holt's\", \"Exponential\", \"Additive\", \"Multiplicative\"],\n)\nresults[\"SES\"] = [fit1.params[p] for p in params] + [fit1.sse]\nresults[\"Holt's\"] = [fit2.params[p] for p in params] + [fit2.sse]\nresults[\"Exponential\"] = [fit3.params[p] for p in params] + [fit3.sse]\nresults[\"Additive\"] = [fit4.params[p] for p in params] + [fit4.sse]\nresults[\"Multiplicative\"] = [fit5.params[p] for p in params] + [fit5.sse]\nresults",
            "code"
        ],
        [
            "[5]:",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data": [
        [
            "The following plots allow us to evaluate the level and slope/trend components of the above table\u2019s fits.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "code"
        ],
        [
            "Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Exponential Smoothing->Comparison": [
        [
            "Here we plot a comparison Simple Exponential Smoothing and Holt\u2019s Methods for various additive, exponential and damped combinations. All of the models parameters will be optimized by statsmodels.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "fit1 = SimpleExpSmoothing(livestock2, initialization_method=\"estimated\").fit()\nfcast1 = fit1.forecast(9).rename(\"SES\")\nfit2 = Holt(livestock2, initialization_method=\"estimated\").fit()\nfcast2 = fit2.forecast(9).rename(\"Holt's\")\nfit3 = Holt(livestock2, exponential=True, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(9).rename(\"Exponential\")\nfit4 = Holt(livestock2, damped_trend=True, initialization_method=\"estimated\").fit(\n    damping_trend=0.98\n)\nfcast4 = fit4.forecast(9).rename(\"Additive Damped\")\nfit5 = Holt(\n    livestock2, exponential=True, damped_trend=True, initialization_method=\"estimated\"\n).fit()\nfcast5 = fit5.forecast(9).rename(\"Multiplicative Damped\")\n\nax = livestock2.plot(color=\"black\", marker=\"o\", figsize=(12, 8))\nlivestock3.plot(ax=ax, color=\"black\", marker=\"o\", legend=False)\nfcast1.plot(ax=ax, color=\"red\", legend=True)\nfcast2.plot(ax=ax, color=\"green\", legend=True)\nfcast3.plot(ax=ax, color=\"blue\", legend=True)\nfcast4.plot(ax=ax, color=\"cyan\", legend=True)\nfcast5.plot(ax=ax, color=\"magenta\", legend=True)\nax.set_ylabel(\"Livestock, sheep in Asia (millions)\")\nplt.show()\nprint(\n    \"Figure 7.5: Forecasting livestock, sheep in Asia: comparing forecasting performance of non-seasonal methods.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_14_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_14_0.png\"/>",
            "code"
        ],
        [
            "Figure 7.5: Forecasting livestock, sheep in Asia: comparing forecasting performance of non-seasonal methods.",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal": [
        [
            "Finally we are able to run full Holt\u2019s Winters Seasonal Exponential Smoothing including a trend component and a seasonal component. statsmodels allows for all the combinations including as shown in the examples below: 1. fit1 additive trend, additive seasonal of period season_length=4 and the use of a Box-Cox transformation. 1. fit2 additive trend, multiplicative seasonal of period season_length=4 and the use of a Box-Cox transformation.. 1. fit3 additive damped trend,\nadditive seasonal of period season_length=4 and the use of a Box-Cox transformation. 1. fit4 additive damped trend, multiplicative seasonal of period season_length=4 and the use of a Box-Cox transformation.",
            "markdown"
        ],
        [
            "The plot shows the results and forecast for fit1 and fit2. The table allows us to compare the results and parameterizations.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "fit1 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"add\",\n    use_boxcox=True,\n    initialization_method=\"estimated\",\n).fit()\nfit2 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"mul\",\n    use_boxcox=True,\n    initialization_method=\"estimated\",\n).fit()\nfit3 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    use_boxcox=True,\n    initialization_method=\"estimated\",\n).fit()\nfit4 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"mul\",\n    damped_trend=True,\n    use_boxcox=True,\n    initialization_method=\"estimated\",\n).fit()\nresults = pd.DataFrame(\n    index=[r\"$\\alpha$\", r\"$\\beta$\", r\"$\\phi$\", r\"$\\gamma$\", r\"$l_0$\", \"$b_0$\", \"SSE\"]\n)\nparams = [\n    \"smoothing_level\",\n    \"smoothing_trend\",\n    \"damping_trend\",\n    \"smoothing_seasonal\",\n    \"initial_level\",\n    \"initial_trend\",\n]\nresults[\"Additive\"] = [fit1.params[p] for p in params] + [fit1.sse]\nresults[\"Multiplicative\"] = [fit2.params[p] for p in params] + [fit2.sse]\nresults[\"Additive Dam\"] = [fit3.params[p] for p in params] + [fit3.sse]\nresults[\"Multiplica Dam\"] = [fit4.params[p] for p in params] + [fit4.sse]\n\nax = aust.plot(\n    figsize=(10, 6),\n    marker=\"o\",\n    color=\"black\",\n    title=\"Forecasts from Holt-Winters' multiplicative method\",\n)\nax.set_ylabel(\"International visitor night in Australia (millions)\")\nax.set_xlabel(\"Year\")\nfit1.fittedvalues.plot(ax=ax, style=\"--\", color=\"red\")\nfit2.fittedvalues.plot(ax=ax, style=\"--\", color=\"green\")\n\nfit1.forecast(8).rename(\"Holt-Winters (add-add-seasonal)\").plot(\n    ax=ax, style=\"--\", marker=\"o\", color=\"red\", legend=True\n)\nfit2.forecast(8).rename(\"Holt-Winters (add-mul-seasonal)\").plot(\n    ax=ax, style=\"--\", marker=\"o\", color=\"green\", legend=True\n)\n\nplt.show()\nprint(\n    \"Figure 7.6: Forecasting international visitor nights in Australia using Holt-Winters method with both additive and multiplicative seasonality.\"\n)\n\nresults\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_16_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_16_0.png\"/>",
            "code"
        ],
        [
            "Figure 7.6: Forecasting international visitor nights in Australia using Holt-Winters method with both additive and multiplicative seasonality.",
            "code"
        ],
        [
            "[8]:",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals": [
        [
            "It is possible to get at the internals of the Exponential Smoothing models.",
            "markdown"
        ],
        [
            "Here we show some tables that allow you to view side by side the original values \\(y_t\\), the level \\(l_t\\), the trend \\(b_t\\), the season \\(s_t\\) and the fitted values \\(\\hat{y}_t\\). Note that these values only have meaningful values in the space of your original data if the fit is performed without a Box-Cox transformation.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "fit1 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"add\",\n    initialization_method=\"estimated\",\n).fit()\nfit2 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"mul\",\n    initialization_method=\"estimated\",\n).fit()",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "df = pd.DataFrame(\n    np.c_[aust, fit1.level, fit1.trend, fit1.season, fit1.fittedvalues],\n    columns=[r\"$y_t$\", r\"$l_t$\", r\"$b_t$\", r\"$s_t$\", r\"$\\hat{y}_t$\"],\n    index=aust.index,\n)\ndf.append(fit1.forecast(8).rename(r\"$\\hat{y}_t$\").to_frame(), sort=True)",
            "code"
        ],
        [
            "/tmp/ipykernel_5863/776371549.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df.append(fit1.forecast(8).rename(r\"$\\hat{y}_t$\").to_frame(), sort=True)",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "df = pd.DataFrame(\n    np.c_[aust, fit2.level, fit2.trend, fit2.season, fit2.fittedvalues],\n    columns=[r\"$y_t$\", r\"$l_t$\", r\"$b_t$\", r\"$s_t$\", r\"$\\hat{y}_t$\"],\n    index=aust.index,\n)\ndf.append(fit2.forecast(8).rename(r\"$\\hat{y}_t$\").to_frame(), sort=True)",
            "code"
        ],
        [
            "/tmp/ipykernel_5863/3145826131.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df.append(fit2.forecast(8).rename(r\"$\\hat{y}_t$\").to_frame(), sort=True)",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "Finally lets look at the levels, slopes/trends and seasonal components of the models.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "states1 = pd.DataFrame(\n    np.c_[fit1.level, fit1.trend, fit1.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nstates2 = pd.DataFrame(\n    np.c_[fit2.level, fit2.trend, fit2.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nfig, [[ax1, ax4], [ax2, ax5], [ax3, ax6]] = plt.subplots(3, 2, figsize=(12, 8))\nstates1[[\"level\"]].plot(ax=ax1)\nstates1[[\"slope\"]].plot(ax=ax2)\nstates1[[\"seasonal\"]].plot(ax=ax3)\nstates2[[\"level\"]].plot(ax=ax4)\nstates2[[\"slope\"]].plot(ax=ax5)\nstates2[[\"seasonal\"]].plot(ax=ax6)\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Seasonal Decomposition": [
        [
            "This note book illustrates the use of STL to decompose a time series into three components: trend, season(al) and residual. STL uses LOESS (locally estimated scatterplot smoothing) to extract smooths estimates of the three components. The key inputs into STL are:",
            "markdown"
        ],
        [
            "season - The length of the seasonal smoother. Must be odd.",
            "markdown"
        ],
        [
            "trend - The length of the trend smoother, usually around 150% of season. Must be odd and larger than season.",
            "markdown"
        ],
        [
            "low_pass - The length of the low-pass estimation window, usually the smallest odd number larger than the periodicity of the data.",
            "markdown"
        ],
        [
            "First we import the required packages, prepare the graphics environment, and prepare the data.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "plt.rc(\"figure\", figsize=(16, 12))\nplt.rc(\"font\", size=13)",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Seasonal Decomposition->Atmospheric CO2": [
        [
            "The example in Cleveland, Cleveland, McRae, and Terpenning (1990) uses CO2 data, which is in the list below. This monthly data (January 1959 to December 1987) has a clear trend and seasonality across the sample.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "co2 = [\n    315.58,\n    316.39,\n    316.79,\n    317.82,\n    318.39,\n    318.22,\n    316.68,\n    315.01,\n    314.02,\n    313.55,\n    315.02,\n    315.75,\n    316.52,\n    317.10,\n    317.79,\n    319.22,\n    320.08,\n    319.70,\n    318.27,\n    315.99,\n    314.24,\n    314.05,\n    315.05,\n    316.23,\n    316.92,\n    317.76,\n    318.54,\n    319.49,\n    320.64,\n    319.85,\n    318.70,\n    316.96,\n    315.17,\n    315.47,\n    316.19,\n    317.17,\n    318.12,\n    318.72,\n    319.79,\n    320.68,\n    321.28,\n    320.89,\n    319.79,\n    317.56,\n    316.46,\n    315.59,\n    316.85,\n    317.87,\n    318.87,\n    319.25,\n    320.13,\n    321.49,\n    322.34,\n    321.62,\n    319.85,\n    317.87,\n    316.36,\n    316.24,\n    317.13,\n    318.46,\n    319.57,\n    320.23,\n    320.89,\n    321.54,\n    322.20,\n    321.90,\n    320.42,\n    318.60,\n    316.73,\n    317.15,\n    317.94,\n    318.91,\n    319.73,\n    320.78,\n    321.23,\n    322.49,\n    322.59,\n    322.35,\n    321.61,\n    319.24,\n    318.23,\n    317.76,\n    319.36,\n    319.50,\n    320.35,\n    321.40,\n    322.22,\n    323.45,\n    323.80,\n    323.50,\n    322.16,\n    320.09,\n    318.26,\n    317.66,\n    319.47,\n    320.70,\n    322.06,\n    322.23,\n    322.78,\n    324.10,\n    324.63,\n    323.79,\n    322.34,\n    320.73,\n    319.00,\n    318.99,\n    320.41,\n    321.68,\n    322.30,\n    322.89,\n    323.59,\n    324.65,\n    325.30,\n    325.15,\n    323.88,\n    321.80,\n    319.99,\n    319.86,\n    320.88,\n    322.36,\n    323.59,\n    324.23,\n    325.34,\n    326.33,\n    327.03,\n    326.24,\n    325.39,\n    323.16,\n    321.87,\n    321.31,\n    322.34,\n    323.74,\n    324.61,\n    325.58,\n    326.55,\n    327.81,\n    327.82,\n    327.53,\n    326.29,\n    324.66,\n    323.12,\n    323.09,\n    324.01,\n    325.10,\n    326.12,\n    326.62,\n    327.16,\n    327.94,\n    329.15,\n    328.79,\n    327.53,\n    325.65,\n    323.60,\n    323.78,\n    325.13,\n    326.26,\n    326.93,\n    327.84,\n    327.96,\n    329.93,\n    330.25,\n    329.24,\n    328.13,\n    326.42,\n    324.97,\n    325.29,\n    326.56,\n    327.73,\n    328.73,\n    329.70,\n    330.46,\n    331.70,\n    332.66,\n    332.22,\n    331.02,\n    329.39,\n    327.58,\n    327.27,\n    328.30,\n    328.81,\n    329.44,\n    330.89,\n    331.62,\n    332.85,\n    333.29,\n    332.44,\n    331.35,\n    329.58,\n    327.58,\n    327.55,\n    328.56,\n    329.73,\n    330.45,\n    330.98,\n    331.63,\n    332.88,\n    333.63,\n    333.53,\n    331.90,\n    330.08,\n    328.59,\n    328.31,\n    329.44,\n    330.64,\n    331.62,\n    332.45,\n    333.36,\n    334.46,\n    334.84,\n    334.29,\n    333.04,\n    330.88,\n    329.23,\n    328.83,\n    330.18,\n    331.50,\n    332.80,\n    333.22,\n    334.54,\n    335.82,\n    336.45,\n    335.97,\n    334.65,\n    332.40,\n    331.28,\n    330.73,\n    332.05,\n    333.54,\n    334.65,\n    335.06,\n    336.32,\n    337.39,\n    337.66,\n    337.56,\n    336.24,\n    334.39,\n    332.43,\n    332.22,\n    333.61,\n    334.78,\n    335.88,\n    336.43,\n    337.61,\n    338.53,\n    339.06,\n    338.92,\n    337.39,\n    335.72,\n    333.64,\n    333.65,\n    335.07,\n    336.53,\n    337.82,\n    338.19,\n    339.89,\n    340.56,\n    341.22,\n    340.92,\n    339.26,\n    337.27,\n    335.66,\n    335.54,\n    336.71,\n    337.79,\n    338.79,\n    340.06,\n    340.93,\n    342.02,\n    342.65,\n    341.80,\n    340.01,\n    337.94,\n    336.17,\n    336.28,\n    337.76,\n    339.05,\n    340.18,\n    341.04,\n    342.16,\n    343.01,\n    343.64,\n    342.91,\n    341.72,\n    339.52,\n    337.75,\n    337.68,\n    339.14,\n    340.37,\n    341.32,\n    342.45,\n    343.05,\n    344.91,\n    345.77,\n    345.30,\n    343.98,\n    342.41,\n    339.89,\n    340.03,\n    341.19,\n    342.87,\n    343.74,\n    344.55,\n    345.28,\n    347.00,\n    347.37,\n    346.74,\n    345.36,\n    343.19,\n    340.97,\n    341.20,\n    342.76,\n    343.96,\n    344.82,\n    345.82,\n    347.24,\n    348.09,\n    348.66,\n    347.90,\n    346.27,\n    344.21,\n    342.88,\n    342.58,\n    343.99,\n    345.31,\n    345.98,\n    346.72,\n    347.63,\n    349.24,\n    349.83,\n    349.10,\n    347.52,\n    345.43,\n    344.48,\n    343.89,\n    345.29,\n    346.54,\n    347.66,\n    348.07,\n    349.12,\n    350.55,\n    351.34,\n    350.80,\n    349.10,\n    347.54,\n    346.20,\n    346.20,\n    347.44,\n    348.67,\n]\nco2 = pd.Series(\n    co2, index=pd.date_range(\"1-1-1959\", periods=len(co2), freq=\"M\"), name=\"CO2\"\n)\nco2.describe()",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "count    348.000000\nmean     330.123879\nstd       10.059747\nmin      313.550000\n25%      321.302500\n50%      328.820000\n75%      338.002500\nmax      351.340000\nName: CO2, dtype: float64",
            "code"
        ],
        [
            "The decomposition requires 1 input, the data series. If the data series does not have a frequency, then you must also specify period. The default value for seasonal is 7, and so should also be changed in most applications.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "from statsmodels.tsa.seasonal import STL\n\nstl = STL(co2, seasonal=13)\nres = stl.fit()\nfig = res.plot()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_6_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_6_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Seasonal Decomposition->Robust Fitting": [
        [
            "Setting robust uses a data-dependent weighting function that re-weights data when estimating the LOESS (and so is using LOWESS). Using robust estimation allows the model to tolerate larger errors that are visible on the bottom plot.",
            "markdown"
        ],
        [
            "Here we use a series the measures the production of electrical equipment in the EU.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "from statsmodels.datasets import elec_equip as ds\n\nelec_equip = ds.load().data",
            "code"
        ],
        [
            "Next, we estimate the model with and without robust weighting. The difference is minor and is most pronounced during the financial crisis of 2008. The non-robust estimate places equal weights on all observations and so produces smaller errors, on average. The weights vary between 0 and 1.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "def add_stl_plot(fig, res, legend):\n    \"\"\"Add 3 plots from a second STL fit\"\"\"\n    axs = fig.get_axes()\n    comps = [\"trend\", \"seasonal\", \"resid\"]\n    for ax, comp in zip(axs[1:], comps):\n        series = getattr(res, comp)\n        if comp == \"resid\":\n            ax.plot(series, marker=\"o\", linestyle=\"none\")\n        else:\n            ax.plot(series)\n            if comp == \"trend\":\n                ax.legend(legend, frameon=False)\n\n\nstl = STL(elec_equip, period=12, robust=True)\nres_robust = stl.fit()\nfig = res_robust.plot()\nres_non_robust = STL(elec_equip, period=12, robust=False).fit()\nadd_stl_plot(fig, res_non_robust, [\"Robust\", \"Non-robust\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\"/>",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(16, 5))\nlines = plt.plot(res_robust.weights, marker=\"o\", linestyle=\"none\")\nax = plt.gca()\nxlim = ax.set_xlim(elec_equip.index[0], elec_equip.index[-1])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_11_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_11_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Seasonal Decomposition->LOESS degree": [
        [
            "The default configuration estimates the LOESS model with both a constant and a trend. This can be changed to only include a constant by setting COMPONENT_deg to 0. Here the degree makes little difference except in the trend around the financial crisis of 2008.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "stl = STL(\n    elec_equip, period=12, seasonal_deg=0, trend_deg=0, low_pass_deg=0, robust=True\n)\nres_deg_0 = stl.fit()\nfig = res_robust.plot()\nadd_stl_plot(fig, res_deg_0, [\"Degree 1\", \"Degree 0\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_13_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_13_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Seasonal Decomposition->Performance": [
        [
            "Three options can be used to reduce the computational cost of the STL decomposition:",
            "markdown"
        ],
        [
            "seasonal_jump",
            "markdown"
        ],
        [
            "trend_jump",
            "markdown"
        ],
        [
            "low_pass_jump",
            "markdown"
        ],
        [
            "When these are non-zero, the LOESS for component COMPONENT is only estimated ever COMPONENT_jump observations, and linear interpolation is used between points. These values should not normally be more than 10-20% of the size of seasonal, trend or low_pass, respectively.",
            "markdown"
        ],
        [
            "The example below shows how these can reduce the computational cost by a factor of 15 using simulated data with both a low-frequency cosinusoidal trend and a sinusoidal seasonal pattern.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "import numpy as np\n\nrs = np.random.RandomState(0xA4FD94BC)\ntau = 2000\nt = np.arange(tau)\nperiod = int(0.05 * tau)\nseasonal = period + ((period % 2) == 0)  # Ensure odd\ne = 0.25 * rs.standard_normal(tau)\ny = np.cos(t / tau * 2 * np.pi) + 0.25 * np.sin(t / period * 2 * np.pi) + e\nplt.plot(y)\nplt.title(\"Simulated Data\")\nxlim = plt.gca().set_xlim(0, tau)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_15_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_15_0.png\"/>",
            "code"
        ],
        [
            "First, the base line model is estimated with all jumps equal to 1.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "mod = STL(y, period=period, seasonal=seasonal)\n%timeit mod.fit()\nres = mod.fit()\nfig = res.plot(observed=False, resid=False)",
            "code"
        ],
        [
            "224 ms \u00b1 36.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_17_1.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_17_1.png\"/>",
            "code"
        ],
        [
            "The jumps are all set to 15% of their window length. Limited linear interpolation makes little difference to the fit of the model.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "low_pass_jump = seasonal_jump = int(0.15 * (period + 1))\ntrend_jump = int(0.15 * 1.5 * (period + 1))\nmod = STL(\n    y,\n    period=period,\n    seasonal=seasonal,\n    seasonal_jump=seasonal_jump,\n    trend_jump=trend_jump,\n    low_pass_jump=low_pass_jump,\n)\n%timeit mod.fit()\nres = mod.fit()\nfig = res.plot(observed=False, resid=False)",
            "code"
        ],
        [
            "19.5 ms \u00b1 4.89 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_19_1.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_19_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Seasonal Decomposition->Forecasting with STL": [
        [
            "STLForecast simplifies the process of using STL to remove seasonalities and then using a standard time-series model to forecast the trend and cyclical components.",
            "markdown"
        ],
        [
            "Here we use STL to handle the seasonality and then an ARIMA(1,1,0) to model the deseasonalized data. The seasonal component is forecast from the find full cycle where\n\n\\[E[S_{T+h}|\\mathcal{F}_T]=\\hat{S}_{T-k}\\]",
            "markdown"
        ],
        [
            "where \\(k= m - h + m \\lfloor \\frac{h-1}{m} \\rfloor\\). The forecast automatically adds the seasonal component forecast to the ARIMA forecast.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "from statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.forecasting.stl import STLForecast\n\nelec_equip.index.freq = elec_equip.index.inferred_freq\nstlf = STLForecast(elec_equip, ARIMA, model_kwargs=dict(order=(1, 1, 0), trend=\"t\"))\nstlf_res = stlf.fit()\n\nforecast = stlf_res.forecast(24)\nplt.plot(elec_equip)\nplt.plot(forecast)\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_21_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_21_0.png\"/>",
            "code"
        ],
        [
            "summary contains information about both the time-series model and the STL decomposition.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "print(stlf_res.summary())",
            "code"
        ],
        [
            "STL Decomposition and SARIMAX Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                  257\nModel:                 ARIMA(1, 1, 0)   Log Likelihood                -522.434\nDate:                Wed, 02 Nov 2022   AIC                           1050.868\nTime:                        17:08:08   BIC                           1061.504\nSample:                    01-01-1995   HQIC                          1055.146\n                         - 05-01-2016\nCovariance Type:                  opg\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             0.1171      0.118      0.995      0.320      -0.113       0.348\nar.L1         -0.0435      0.049     -0.880      0.379      -0.140       0.053\nsigma2         3.4682      0.188     18.406      0.000       3.099       3.837\n===================================================================================\nLjung-Box (L1) (Q):                   0.01   Jarque-Bera (JB):               223.01\nProb(Q):                              0.92   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.33   Skew:                            -0.26\nProb(H) (two-sided):                  0.00   Kurtosis:                         7.54\n                                STL Configuration\n=================================================================================\nPeriod:                            12       Trend Length:                      23\nSeasonal:                           7       Trend deg:                          1\nSeasonal deg:                       1       Trend jump:                         1\nSeasonal jump:                      1       Low pass:                          13\nRobust:                         False       Low pass deg:                       1\n---------------------------------------------------------------------------------\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Stationarity and detrending (ADF/KPSS)": [
        [
            "Stationarity means that the statistical properties of a time series i.e.\u00a0mean, variance and covariance do not change over time. Many statistical models require the series to be stationary to make effective and precise predictions.",
            "markdown"
        ],
        [
            "Two statistical tests would be used to check the stationarity of a time series \u2013 Augmented Dickey Fuller (\u201cADF\u201d) test and Kwiatkowski-Phillips-Schmidt-Shin (\u201cKPSS\u201d) test. A method to convert a non-stationary time series into stationary series shall also be used.",
            "markdown"
        ],
        [
            "This first cell imports standard packages and sets plots to appear inline.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm",
            "code"
        ],
        [
            "Sunspots dataset is used. It contains yearly (1700-2008) data on sunspots from the National Geophysical Data Center.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "sunspots = sm.datasets.sunspots.load_pandas().data",
            "code"
        ],
        [
            "Some preprocessing is carried out on the data. The \u201cYEAR\u201d column is used in creating index.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "sunspots.index = pd.Index(sm.tsa.datetools.dates_from_range(\"1700\", \"2008\"))\ndel sunspots[\"YEAR\"]",
            "code"
        ],
        [
            "The data is plotted now.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "sunspots.plot(figsize=(12, 8))",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: \n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stationarity_detrending_adf_kpss_8_1.png\" src=\"../../../_images/examples_notebooks_generated_stationarity_detrending_adf_kpss_8_1.png\"/>",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Stationarity and detrending (ADF/KPSS)->ADF test": [
        [
            "ADF test is used to determine the presence of unit root in the series, and hence helps in understand if the series is stationary or not. The null and alternate hypothesis of this test are:",
            "markdown"
        ],
        [
            "Null Hypothesis: The series has a unit root.",
            "markdown"
        ],
        [
            "Alternate Hypothesis: The series has no unit root.",
            "markdown"
        ],
        [
            "If the null hypothesis in failed to be rejected, this test may provide evidence that the series is non-stationary.",
            "markdown"
        ],
        [
            "A function is created to carry out the ADF test on a time series.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "from statsmodels.tsa.stattools import adfuller\n\n\ndef adf_test(timeseries):\n    print(\"Results of Dickey-Fuller Test:\")\n    dftest = adfuller(timeseries, autolag=\"AIC\")\n    dfoutput = pd.Series(\n        dftest[0:4],\n        index=[\n            \"Test Statistic\",\n            \"p-value\",\n            \"#Lags Used\",\n            \"Number of Observations Used\",\n        ],\n    )\n    for key, value in dftest[4].items():\n        dfoutput[\"Critical Value (%s)\" % key] = value\n    print(dfoutput)",
            "code"
        ]
    ],
    "Examples->Time Series Analysis->Stationarity and detrending (ADF/KPSS)->KPSS test": [
        [
            "KPSS is another test for checking the stationarity of a time series. The null and alternate hypothesis for the KPSS test are opposite that of the ADF test.",
            "markdown"
        ],
        [
            "Null Hypothesis: The process is trend stationary.",
            "markdown"
        ],
        [
            "Alternate Hypothesis: The series has a unit root (series is not stationary).",
            "markdown"
        ],
        [
            "A function is created to carry out the KPSS test on a time series.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "from statsmodels.tsa.stattools import kpss\n\n\ndef kpss_test(timeseries):\n    print(\"Results of KPSS Test:\")\n    kpsstest = kpss(timeseries, regression=\"c\", nlags=\"auto\")\n    kpss_output = pd.Series(\n        kpsstest[0:3], index=[\"Test Statistic\", \"p-value\", \"Lags Used\"]\n    )\n    for key, value in kpsstest[3].items():\n        kpss_output[\"Critical Value (%s)\" % key] = value\n    print(kpss_output)",
            "code"
        ],
        [
            "The ADF tests gives the following results \u2013 test statistic, p value and the critical value at 1%, 5% , and 10% confidence intervals.",
            "markdown"
        ],
        [
            "ADF test is now applied on the data.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "adf_test(sunspots[\"SUNACTIVITY\"])",
            "code"
        ],
        [
            "Results of Dickey-Fuller Test:\nTest Statistic                  -2.837781\np-value                          0.053076\n#Lags Used                       8.000000\nNumber of Observations Used    300.000000\nCritical Value (1%)             -3.452337\nCritical Value (5%)             -2.871223\nCritical Value (10%)            -2.571929\ndtype: float64",
            "code"
        ],
        [
            "Based upon the significance level of 0.05 and the p-value of ADF test, the null hypothesis can not be rejected. Hence, the series is non-stationary.",
            "markdown"
        ],
        [
            "The KPSS tests gives the following results \u2013 test statistic, p value and the critical value at 1%, 5% , and 10% confidence intervals.",
            "markdown"
        ],
        [
            "KPSS test is now applied on the data.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "kpss_test(sunspots[\"SUNACTIVITY\"])",
            "code"
        ],
        [
            "Results of KPSS Test:\nTest Statistic           0.669866\np-value                  0.016285\nLags Used                7.000000\nCritical Value (10%)     0.347000\nCritical Value (5%)      0.463000\nCritical Value (2.5%)    0.574000\nCritical Value (1%)      0.739000\ndtype: float64",
            "code"
        ],
        [
            "Based upon the significance level of 0.05 and the p-value of KPSS test, there is evidence for rejecting the null hypothesis in favor of the alternative. Hence, the series is non-stationary as per the KPSS test.",
            "markdown"
        ],
        [
            "It is always better to apply both the tests, so that it can be ensured that the series is truly stationary. Possible outcomes of applying these stationary tests are as follows:\n\nCase 1: Both tests conclude that the series is not stationary - The series is not stationary\nCase 2: Both tests conclude that the series is stationary - The series is stationary\nCase 3: KPSS indicates stationarity and ADF indicates non-stationarity - The series is trend stationary. Trend needs to be removed to make series strict stationary. The detrended series is checked for stationarity.\nCase 4: KPSS indicates non-stationarity and ADF indicates stationarity - The series is difference stationary. Differencing is to be used to make series stationary. The differenced series is checked for stationarity.",
            "markdown"
        ],
        [
            "Here, due to the difference in the results from ADF test and KPSS test, it can be inferred that the series is trend stationary and not strict stationary. The series can be detrended by differencing or by model fitting.",
            "markdown"
        ]
    ],
    "Examples->Time Series Analysis->Stationarity and detrending (ADF/KPSS)->Detrending by Differencing": [
        [
            "It is one of the simplest methods for detrending a time series. A new series is constructed where the value at the current time step is calculated as the difference between the original observation and the observation at the previous time step.",
            "markdown"
        ],
        [
            "Differencing is applied on the data and the result is plotted.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "sunspots[\"SUNACTIVITY_diff\"] = sunspots[\"SUNACTIVITY\"] - sunspots[\"SUNACTIVITY\"].shift(\n    1\n)\nsunspots[\"SUNACTIVITY_diff\"].dropna().plot(figsize=(12, 8))",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: \n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stationarity_detrending_adf_kpss_20_1.png\" src=\"../../../_images/examples_notebooks_generated_stationarity_detrending_adf_kpss_20_1.png\"/>",
            "code"
        ],
        [
            "ADF test is now applied on these detrended values and stationarity is checked.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "adf_test(sunspots[\"SUNACTIVITY_diff\"].dropna())",
            "code"
        ],
        [
            "Results of Dickey-Fuller Test:\nTest Statistic                -1.486166e+01\np-value                        1.715552e-27\n#Lags Used                     7.000000e+00\nNumber of Observations Used    3.000000e+02\nCritical Value (1%)           -3.452337e+00\nCritical Value (5%)           -2.871223e+00\nCritical Value (10%)          -2.571929e+00\ndtype: float64",
            "code"
        ],
        [
            "Based upon the p-value of ADF test, there is evidence for rejecting the null hypothesis in favor of the alternative. Hence, the series is strict stationary now.",
            "markdown"
        ],
        [
            "KPSS test is now applied on these detrended values and stationarity is checked.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "kpss_test(sunspots[\"SUNACTIVITY_diff\"].dropna())",
            "code"
        ],
        [
            "Results of KPSS Test:\nTest Statistic           0.021193\np-value                  0.100000\nLags Used                0.000000\nCritical Value (10%)     0.347000\nCritical Value (5%)      0.463000\nCritical Value (2.5%)    0.574000\nCritical Value (1%)      0.739000\ndtype: float64",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/tsa/stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\n  warnings.warn(",
            "code"
        ],
        [
            "Based upon the p-value of KPSS test, the null hypothesis can not be rejected. Hence, the series is stationary.",
            "markdown"
        ]
    ],
    "Examples->Time Series Analysis->Stationarity and detrending (ADF/KPSS)->Conclusion": [
        [
            "Two tests for checking the stationarity of a time series are used, namely ADF test and KPSS test. Detrending is carried out by using differencing. Trend stationary time series is converted into strict stationary time series. Requisite forecasting model can now be applied on a stationary time series data.",
            "markdown"
        ]
    ],
    "Examples->State space models->SARIMAX: Introduction": [
        [
            "This notebook replicates examples from the Stata ARIMA time series estimation and postestimation documentation.",
            "markdown"
        ],
        [
            "First, we replicate the four estimation examples :\n\nARIMA(1,1,1) model on the U.S. Wholesale Price Index (WPI) dataset.\nVariation of example 1 which adds an MA(4) term to the ARIMA(1,1,1) specification to allow for an additive seasonal effect.\nARIMA(2,1,0) x (1,1,0,12) model of monthly airline data. This example allows a multiplicative seasonal effect.\nARMA(1,1) model with exogenous regressors; describes consumption as an autoregressive process on which also the money supply is assumed to be an explanatory variable.",
            "markdown"
        ],
        [
            "Second, we demonstrate postestimation capabilitites to replicate . The model from example 4 is used to demonstrate:\n\nOne-step-ahead in-sample prediction\nn-step-ahead out-of-sample forecasting\nn-step-ahead in-sample dynamic prediction\n\n\n\n\n\n\nIn\u00a0[1]:",
            "markdown"
        ],
        [
            "%matplotlib inline\n\n\n\n\n\n\n\nIn\u00a0[2]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport requests\nfrom io import BytesIO",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX: Introduction->ARIMA Example 1: Arima": [
        [
            "As can be seen in the graphs from Example 2, the Wholesale price index (WPI) is growing over time (i.e. is not stationary). Therefore an ARMA model is not a good specification. In this first example, we consider a model where the original time series is assumed to be integrated of order 1, so that the difference is assumed to be stationary, and fit a model with one autoregressive lag and one moving average lag, as well as an intercept term.",
            "markdown"
        ],
        [
            "The postulated data process is then:\n$$\n\\Delta y_t = c + \\phi_1 \\Delta y_{t-1} + \\theta_1 \\epsilon_{t-1} + \\epsilon_{t}\n$$",
            "markdown"
        ],
        [
            "where $c$ is the intercept of the ARMA model, $\\Delta$ is the first-difference operator, and we assume $\\epsilon_{t} \\sim N(0, \\sigma^2)$. This can be rewritten to emphasize lag polynomials as (this will be useful in example 2, below):\n$$\n(1 - \\phi_1 L ) \\Delta y_t = c + (1 + \\theta_1 L) \\epsilon_{t}\n$$",
            "markdown"
        ],
        [
            "where $L$ is the lag operator.",
            "markdown"
        ],
        [
            "Notice that one difference between the Stata output and the output below is that Stata estimates the following model:\n$$\n(\\Delta y_t - \\beta_0) = \\phi_1 ( \\Delta y_{t-1} - \\beta_0) + \\theta_1 \\epsilon_{t-1} + \\epsilon_{t}\n$$",
            "markdown"
        ],
        [
            "where $\\beta_0$ is the mean of the process $y_t$. This model is equivalent to the one estimated in the Statsmodels SARIMAX class, but the interpretation is different. To see the equivalence, note that:\n$$\n(\\Delta y_t - \\beta_0) = \\phi_1 ( \\Delta y_{t-1} - \\beta_0) + \\theta_1 \\epsilon_{t-1} + \\epsilon_{t} \\\\\n\\Delta y_t = (1 - \\phi_1) \\beta_0 + \\phi_1 \\Delta y_{t-1} + \\theta_1 \\epsilon_{t-1} + \\epsilon_{t}\n$$",
            "markdown"
        ],
        [
            "so that $c = (1 - \\phi_1) \\beta_0$.\n\n\n\n\n\nIn\u00a0[3]:",
            "markdown"
        ],
        [
            "# Dataset\nwpi1 = requests.get('https://www.stata-press.com/data/r12/wpi1.dta').content\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\n\n# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "code"
        ],
        [
            "Statespace Model Results                           \n==============================================================================\nDep. Variable:                    wpi   No. Observations:                  124\nModel:               SARIMAX(1, 1, 1)   Log Likelihood                -135.351\nDate:                Sun, 24 Nov 2019   AIC                            278.703\nTime:                        07:49:34   BIC                            289.951\nSample:                    01-01-1960   HQIC                           283.272\n                         - 10-01-1990                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0943      0.068      1.389      0.165      -0.039       0.227\nar.L1          0.8742      0.055     16.028      0.000       0.767       0.981\nma.L1         -0.4120      0.100     -4.119      0.000      -0.608      -0.216\nsigma2         0.5257      0.053      9.849      0.000       0.421       0.630\n===================================================================================\nLjung-Box (Q):                       37.12   Jarque-Bera (JB):                 9.78\nProb(Q):                              0.60   Prob(JB):                         0.01\nHeteroskedasticity (H):              15.93   Skew:                             0.28\nProb(H) (two-sided):                  0.00   Kurtosis:                         4.26\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "/home/travis/build/statsmodels/statsmodels/statsmodels/tsa/base/tsa_model.py:165: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.\n  % freq, ValueWarning)",
            "code"
        ],
        [
            "Thus the maximum likelihood estimates imply that for the process above, we have:\n$$\n\\Delta y_t = 0.1050 + 0.8740 \\Delta y_{t-1} - 0.4206 \\epsilon_{t-1} + \\epsilon_{t}\n$$",
            "markdown"
        ],
        [
            "where $\\epsilon_{t} \\sim N(0, 0.5226)$. Finally, recall that $c = (1 - \\phi_1) \\beta_0$, and here $c = 0.1050$ and $\\phi_1 = 0.8740$. To compare with the output from Stata, we could calculate the mean:\n$$\\beta_0 = \\frac{c}{1 - \\phi_1} = \\frac{0.1050}{1 - 0.8740} = 0.83$$",
            "markdown"
        ],
        [
            "<strong>Note</strong>: these values are slightly different from the values in the Stata documentation because the optimizer in Statsmodels has found parameters here that yield a higher likelihood. Nonetheless, they are very close.",
            "markdown"
        ]
    ],
    "Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects": [
        [
            "This model is an extension of that from example 1. Here the data is assumed to follow the process:\n$$\n\\Delta y_t = c + \\phi_1 \\Delta y_{t-1} + \\theta_1 \\epsilon_{t-1} + \\theta_4 \\epsilon_{t-4} + \\epsilon_{t}\n$$",
            "markdown"
        ],
        [
            "The new part of this model is that there is allowed to be a annual seasonal effect (it is annual even though the periodicity is 4 because the dataset is quarterly). The second difference is that this model uses the log of the data rather than the level.",
            "markdown"
        ],
        [
            "Before estimating the dataset, graphs showing:\n\nThe time series (in logs)\nThe first difference of the time series (in logs)\nThe autocorrelation function\nThe partial autocorrelation function.",
            "markdown"
        ],
        [
            "From the first two graphs, we note that the original time series does not appear to be stationary, whereas the first-difference does. This supports either estimating an ARMA model on the first-difference of the data, or estimating an ARIMA model with 1 order of integration (recall that we are taking the latter approach). The last two graphs support the use of an ARMA(1,1,1) model.\n\n\n\n\n\nIn\u00a0[4]:",
            "markdown"
        ],
        [
            "# Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:",
            "code"
        ],
        [
            "# Graph data\nfig, axes = plt.subplots(1, 2, figsize=(15,4))\n\n# Levels\naxes[0].plot(data.index._mpl_repr(), data['wpi'], '-')\naxes[0].set(title='US Wholesale Price Index')\n\n# Log difference\naxes[1].plot(data.index._mpl_repr(), data['D.ln_wpi'], '-')\naxes[1].hlines(0, data.index[0], data.index[-1], 'r')\naxes[1].set(title='US Wholesale Price Index - difference of logs');",
            "code"
        ],
        [
            "/home/travis/miniconda/envs/statsmodels-test/lib/python3.7/site-packages/pandas/plotting/_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n\nTo register the converters:\n\t from pandas.plotting import register_matplotlib_converters\n\t register_matplotlib_converters()\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>\n\n\n\n\n\n\n\nIn\u00a0[6]:",
            "code"
        ],
        [
            "# Graph data\nfig, axes = plt.subplots(1, 2, figsize=(15,4))\n\nfig = sm.graphics.tsa.plot_acf(data.iloc[1:]['D.ln_wpi'], lags=40, ax=axes[0])\nfig = sm.graphics.tsa.plot_pacf(data.iloc[1:]['D.ln_wpi'], lags=40, ax=axes[1])\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "code"
        ],
        [
            "To understand how to specify this model in Statsmodels, first recall that from example 1 we used the following code to specify the ARIMA(1,1,1) model:",
            "markdown"
        ],
        [
            "mod = sm.tsa.statespace.SARIMAX(data['wpi'], trend='c', order=(1,1,1))",
            "code"
        ],
        [
            "The order argument is a tuple of the form (AR specification, Integration order, MA specification). The integration order must be an integer (for example, here we assumed one order of integration, so it was specified as 1. In a pure ARMA model where the underlying data is already stationary, it would be 0).",
            "markdown"
        ],
        [
            "For the AR specification and MA specification components, there are two possiblities. The first is to specify the <strong>maximum degree</strong> of the corresponding lag polynomial, in which case the component is an integer. For example, if we wanted to specify an ARIMA(1,1,4) process, we would use:",
            "markdown"
        ],
        [
            "mod = sm.tsa.statespace.SARIMAX(data['wpi'], trend='c', order=(1,1,4))",
            "code"
        ],
        [
            "and the corresponding data process would be:\n$$\ny_t = c + \\phi_1 y_{t-1} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\theta_3 \\epsilon_{t-3} + \\theta_4 \\epsilon_{t-4} + \\epsilon_{t}\n$$",
            "markdown"
        ],
        [
            "or\n$$\n(1 - \\phi_1 L)\\Delta y_t = c + (1 + \\theta_1 L + \\theta_2 L^2 + \\theta_3 L^3 + \\theta_4 L^4) \\epsilon_{t}\n$$",
            "markdown"
        ],
        [
            "When the specification parameter is given as a maximum degree of the lag polynomial, it implies that all polynomial terms up to that degree are included. Notice that this is not the model we want to use, because it would include terms for $\\epsilon_{t-2}$ and $\\epsilon_{t-3}$, which we don't want here.",
            "markdown"
        ],
        [
            "What we want is a polynomial that has terms for the 1st and 4th degrees, but leaves out the 2nd and 3rd terms. To do that, we need to provide a tuple for the specifiation parameter, where the tuple describes <strong>the lag polynomial itself</strong>. In particular, here we would want to use:",
            "markdown"
        ],
        [
            "ar = 1          # this is the maximum degree specification\nma = (1,0,0,1)  # this is the lag polynomial specification\nmod = sm.tsa.statespace.SARIMAX(data['wpi'], trend='c', order=(ar,1,ma)))",
            "code"
        ],
        [
            "This gives the following form for the process of the data:\n$$\n\\Delta y_t = c + \\phi_1 \\Delta y_{t-1} + \\theta_1 \\epsilon_{t-1} + \\theta_4 \\epsilon_{t-4} + \\epsilon_{t} \\\\\n(1 - \\phi_1 L)\\Delta y_t = c + (1 + \\theta_1 L + \\theta_4 L^4) \\epsilon_{t}\n$$",
            "markdown"
        ],
        [
            "which is what we want.\n\n\n\n\n\nIn\u00a0[7]:",
            "markdown"
        ],
        [
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "code"
        ],
        [
            "/home/travis/build/statsmodels/statsmodels/statsmodels/tsa/base/tsa_model.py:165: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.\n  % freq, ValueWarning)",
            "code"
        ],
        [
            "Statespace Model Results                           \n==============================================================================\nDep. Variable:                 ln_wpi   No. Observations:                  124\nModel:               SARIMAX(1, 1, 1)   Log Likelihood                 382.427\nDate:                Sun, 24 Nov 2019   AIC                           -756.854\nTime:                        07:49:35   BIC                           -745.605\nSample:                    01-01-1960   HQIC                          -752.285\n                         - 10-01-1990                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0013      0.001      1.327      0.184      -0.001       0.003\nar.L1          0.8825      0.043     20.550      0.000       0.798       0.967\nma.L1         -0.4765      0.092     -5.176      0.000      -0.657      -0.296\nsigma2         0.0001   9.75e-06     11.892      0.000    9.68e-05       0.000\n===================================================================================\nLjung-Box (Q):                       36.90   Jarque-Bera (JB):                67.61\nProb(Q):                              0.61   Prob(JB):                         0.00\nHeteroskedasticity (H):               2.56   Skew:                             0.70\nProb(H) (two-sided):                  0.00   Kurtosis:                         6.35\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX: Introduction->ARIMA Example 3: Airline Model": [
        [
            "In the previous example, we included a seasonal effect in an additive way, meaning that we added a term allowing the process to depend on the 4th MA lag. It may be instead that we want to model a seasonal effect in a multiplicative way. We often write the model then as an ARIMA $(p,d,q) \\times (P,D,Q)_s$, where the lowercast letters indicate the specification for the non-seasonal component, and the uppercase letters indicate the specification for the seasonal component; $s$ is the periodicity of the seasons (e.g. it is often 4 for quarterly data or 12 for monthly data). The data process can be written generically as:\n$$\n\\phi_p (L) \\tilde \\phi_P (L^s) \\Delta^d \\Delta_s^D y_t = A(t) + \\theta_q (L) \\tilde \\theta_Q (L^s) \\epsilon_t\n$$",
            "markdown"
        ],
        [
            "where:\n\n$\\phi_p (L)$ is the non-seasonal autoregressive lag polynomial\n$\\tilde \\phi_P (L^s)$ is the seasonal autoregressive lag polynomial\n$\\Delta^d \\Delta_s^D y_t$ is the time series, differenced $d$ times, and seasonally differenced $D$ times.\n$A(t)$ is the trend polynomial (including the intercept)\n$\\theta_q (L)$ is the non-seasonal moving average lag polynomial\n$\\tilde \\theta_Q (L^s)$ is the seasonal moving average lag polynomial",
            "markdown"
        ],
        [
            "sometimes we rewrite this as:\n$$\n\\phi_p (L) \\tilde \\phi_P (L^s) y_t^* = A(t) + \\theta_q (L) \\tilde \\theta_Q (L^s) \\epsilon_t\n$$",
            "markdown"
        ],
        [
            "where $y_t^* = \\Delta^d \\Delta_s^D y_t$. This emphasizes that just as in the simple case, after we take differences (here both non-seasonal and seasonal) to make the data stationary, the resulting model is just an ARMA model.",
            "markdown"
        ],
        [
            "As an example, consider the airline model ARIMA $(2,1,0) \\times (1,1,0)_{12}$, with an intercept. The data process can be written in the form above as:\n$$\n(1 - \\phi_1 L - \\phi_2 L^2) (1 - \\tilde \\phi_1 L^{12}) \\Delta \\Delta_{12} y_t = c + \\epsilon_t\n$$",
            "markdown"
        ],
        [
            "Here, we have:\n\n$\\phi_p (L) = (1 - \\phi_1 L - \\phi_2 L^2)$\n$\\tilde \\phi_P (L^s) = (1 - \\phi_1 L^12)$\n$d = 1, D = 1, s=12$ indicating that $y_t^*$ is derived from $y_t$ by taking first-differences and then taking 12-th differences.\n$A(t) = c$ is the constant trend polynomial (i.e. just an intercept)\n$\\theta_q (L) = \\tilde \\theta_Q (L^s) = 1$ (i.e. there is no moving average effect)",
            "markdown"
        ],
        [
            "It may still be confusing to see the two lag polynomials in front of the time-series variable, but notice that we can multiply the lag polynomials together to get the following model:\n$$\n(1 - \\phi_1 L - \\phi_2 L^2 - \\tilde \\phi_1 L^{12} + \\phi_1 \\tilde \\phi_1 L^{13} + \\phi_2 \\tilde \\phi_1 L^{14} ) y_t^* = c + \\epsilon_t\n$$",
            "markdown"
        ],
        [
            "which can be rewritten as:\n$$\ny_t^* = c + \\phi_1 y_{t-1}^* + \\phi_2 y_{t-2}^* + \\tilde \\phi_1 y_{t-12}^* - \\phi_1 \\tilde \\phi_1 y_{t-13}^* - \\phi_2 \\tilde \\phi_1 y_{t-14}^* + \\epsilon_t\n$$",
            "markdown"
        ],
        [
            "This is similar to the additively seasonal model from example 2, but the coefficients in front of the autoregressive lags are actually combinations of the underlying seasonal and non-seasonal parameters.",
            "markdown"
        ],
        [
            "Specifying the model in Statsmodels is done simply by adding the seasonal_order argument, which accepts a tuple of the form (Seasonal AR specification, Seasonal Integration order, Seasonal MA, Seasonal periodicity). The seasonal AR and MA specifications, as before, can be expressed as a maximum polynomial degree or as the lag polynomial itself. Seasonal periodicity is an integer.",
            "markdown"
        ],
        [
            "For the airline model ARIMA $(2,1,0) \\times (1,1,0)_{12}$ with an intercept, the command is:",
            "markdown"
        ],
        [
            "mod = sm.tsa.statespace.SARIMAX(data['lnair'], order=(2,1,0), seasonal_order=(1,1,0,12))\n\n\n\n\n\n\nIn\u00a0[8]:",
            "code"
        ],
        [
            "# Dataset\nair2 = requests.get('https://www.stata-press.com/data/r12/air2.dta').content\ndata = pd.read_stata(BytesIO(air2))\ndata.index = pd.date_range(start=datetime(data.time[0], 1, 1), periods=len(data), freq='MS')\ndata['lnair'] = np.log(data['air'])\n\n# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['lnair'], order=(2,1,0), seasonal_order=(1,1,0,12), simple_differencing=True)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "code"
        ],
        [
            "Statespace Model Results                                 \n==========================================================================================\nDep. Variable:                       D.DS12.lnair   No. Observations:                  131\nModel:             SARIMAX(2, 0, 0)x(1, 0, 0, 12)   Log Likelihood                 240.821\nDate:                            Sun, 24 Nov 2019   AIC                           -473.643\nTime:                                    07:49:36   BIC                           -462.142\nSample:                                02-01-1950   HQIC                          -468.970\n                                     - 12-01-1960                                         \nCovariance Type:                              opg                                         \n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1         -0.4057      0.080     -5.045      0.000      -0.563      -0.248\nar.L2         -0.0799      0.099     -0.809      0.419      -0.274       0.114\nar.S.L12      -0.4723      0.072     -6.592      0.000      -0.613      -0.332\nsigma2         0.0014      0.000      8.403      0.000       0.001       0.002\n===================================================================================\nLjung-Box (Q):                       49.89   Jarque-Bera (JB):                 0.72\nProb(Q):                              0.14   Prob(JB):                         0.70\nHeteroskedasticity (H):               0.54   Skew:                             0.14\nProb(H) (two-sided):                  0.04   Kurtosis:                         3.23\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "Notice that here we used an additional argument simple_differencing=True. This controls how the order of integration is handled in ARIMA models. If simple_differencing=True, then the time series provided as endog is literatlly differenced and an ARMA model is fit to the resulting new time series. This implies that a number of initial periods are lost to the differencing process, however it may be necessary either to compare results to other packages (e.g. Stata's arima always uses  simple differencing) or if the seasonal periodicity is large.",
            "markdown"
        ],
        [
            "The default is simple_differencing=False, in which case the integration component is implemented as part of the state space formulation, and all of the original data can be used in estimation.",
            "markdown"
        ]
    ],
    "Examples->State space models->SARIMAX: Introduction->ARIMA Example 4: ARMAX (Friedman)": [
        [
            "This model demonstrates the use of explanatory variables (the X part of ARMAX). When exogenous regressors are included, the SARIMAX module uses the concept of \"regression with SARIMA errors\" (see  for details of regression with ARIMA errors versus alternative specifications), so that the model is specified as:\n$$\ny_t = \\beta_t x_t + u_t \\\\\n        \\phi_p (L) \\tilde \\phi_P (L^s) \\Delta^d \\Delta_s^D u_t = A(t) +\n            \\theta_q (L) \\tilde \\theta_Q (L^s) \\epsilon_t\n$$",
            "markdown"
        ],
        [
            "Notice that the first equation is just a linear regression, and the second equation just describes the process followed by the error component as SARIMA (as was described in example 3). One reason for this specification is that the estimated parameters have their natural interpretations.",
            "markdown"
        ],
        [
            "This specification nests many simpler specifications. For example, regression with AR(2) errors is:\n$$\ny_t = \\beta_t x_t + u_t \\\\\n(1 - \\phi_1 L - \\phi_2 L^2) u_t = A(t) + \\epsilon_t\n$$",
            "markdown"
        ],
        [
            "The model considered in this example is regression with ARMA(1,1) errors. The process is then written:\n$$\n\\text{consump}_t = \\beta_0 + \\beta_1 \\text{m2}_t + u_t \\\\\n(1 - \\phi_1 L) u_t = (1 - \\theta_1 L) \\epsilon_t\n$$",
            "markdown"
        ],
        [
            "Notice that $\\beta_0$ is, as described in example 1 above, not the same thing as an intercept specified by trend='c'. Whereas in the examples above we estimated the intercept of the model via the trend polynomial, here, we demonstrate how to estimate $\\beta_0$ itself by adding a constant to the exogenous dataset. In the output, the $beta_0$ is called const, whereas above the intercept $c$ was called intercept in the output.\n\n\n\n\n\nIn\u00a0[9]:",
            "markdown"
        ],
        [
            "# Dataset\nfriedman2 = requests.get('https://www.stata-press.com/data/r12/friedman2.dta').content\ndata = pd.read_stata(BytesIO(friedman2))\ndata.index = data.time\n\n# Variables\nendog = data.loc['1959':'1981', 'consump']\nexog = sm.add_constant(data.loc['1959':'1981', 'm2'])\n\n# Fit the model\nmod = sm.tsa.statespace.SARIMAX(endog, exog, order=(1,0,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "code"
        ],
        [
            "/home/travis/miniconda/envs/statsmodels-test/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n/home/travis/build/statsmodels/statsmodels/statsmodels/tsa/base/tsa_model.py:165: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.\n  % freq, ValueWarning)",
            "code"
        ],
        [
            "Statespace Model Results                           \n==============================================================================\nDep. Variable:                consump   No. Observations:                   92\nModel:               SARIMAX(1, 0, 1)   Log Likelihood                -340.508\nDate:                Sun, 24 Nov 2019   AIC                            691.015\nTime:                        07:49:36   BIC                            703.624\nSample:                    01-01-1959   HQIC                           696.105\n                         - 10-01-1981                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -36.0608     56.645     -0.637      0.524    -147.084      74.962\nm2             1.1220      0.036     30.824      0.000       1.051       1.193\nar.L1          0.9348      0.041     22.717      0.000       0.854       1.016\nma.L1          0.3091      0.089      3.488      0.000       0.135       0.483\nsigma2        93.2548     10.888      8.565      0.000      71.914     114.596\n===================================================================================\nLjung-Box (Q):                       38.72   Jarque-Bera (JB):                23.49\nProb(Q):                              0.53   Prob(JB):                         0.00\nHeteroskedasticity (H):              22.51   Skew:                             0.17\nProb(H) (two-sided):                  0.00   Kurtosis:                         5.45\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting": [
        [
            "Here we describe some of the post-estimation capabilities of Statsmodels' SARIMAX.",
            "markdown"
        ],
        [
            "First, using the model from example, we estimate the parameters using data that excludes the last few observations (this is a little artificial as an example, but it allows considering performance of out-of-sample forecasting and facilitates comparison to Stata's documentation).\n\n\n\n\n\nIn\u00a0[10]:",
            "markdown"
        ],
        [
            "# Dataset\nraw = pd.read_stata(BytesIO(friedman2))\nraw.index = raw.time\ndata = raw.loc[:'1981']\n\n# Variables\nendog = data.loc['1959':, 'consump']\nexog = sm.add_constant(data.loc['1959':, 'm2'])\nnobs = endog.shape[0]\n\n# Fit the model\nmod = sm.tsa.statespace.SARIMAX(endog.loc[:'1978-01-01'], exog=exog.loc[:'1978-01-01'], order=(1,0,1))\nfit_res = mod.fit(disp=False)\nprint(fit_res.summary())",
            "code"
        ],
        [
            "/home/travis/build/statsmodels/statsmodels/statsmodels/tsa/base/tsa_model.py:165: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.\n  % freq, ValueWarning)",
            "code"
        ],
        [
            "Statespace Model Results                           \n==============================================================================\nDep. Variable:                consump   No. Observations:                   77\nModel:               SARIMAX(1, 0, 1)   Log Likelihood                -243.316\nDate:                Sun, 24 Nov 2019   AIC                            496.633\nTime:                        07:49:37   BIC                            508.352\nSample:                    01-01-1959   HQIC                           501.320\n                         - 01-01-1978                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.6782     18.492      0.037      0.971     -35.565      36.921\nm2             1.0379      0.021     50.329      0.000       0.997       1.078\nar.L1          0.8775      0.059     14.859      0.000       0.762       0.993\nma.L1          0.2771      0.108      2.572      0.010       0.066       0.488\nsigma2        31.6979      4.683      6.769      0.000      22.520      40.876\n===================================================================================\nLjung-Box (Q):                       46.78   Jarque-Bera (JB):                 6.05\nProb(Q):                              0.21   Prob(JB):                         0.05\nHeteroskedasticity (H):               6.09   Skew:                             0.57\nProb(H) (two-sided):                  0.00   Kurtosis:                         3.76\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "Next, we want to get results for the full dataset but using the estimated parameters (on a subset of the data).\n\n\n\n\n\nIn\u00a0[11]:",
            "markdown"
        ],
        [
            "mod = sm.tsa.statespace.SARIMAX(endog, exog=exog, order=(1,0,1))\nres = mod.filter(fit_res.params)",
            "code"
        ],
        [
            "/home/travis/build/statsmodels/statsmodels/statsmodels/tsa/base/tsa_model.py:165: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.\n  % freq, ValueWarning)",
            "code"
        ],
        [
            "The predict command is first applied here to get in-sample predictions. We use the full_results=True argument to allow us to calculate confidence intervals (the default output of predict is just the predicted values).",
            "markdown"
        ],
        [
            "With no other arguments, predict returns the one-step-ahead in-sample predictions for the entire sample.\n\n\n\n\n\nIn\u00a0[12]:",
            "markdown"
        ],
        [
            "# In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()",
            "code"
        ],
        [
            "We can also get dynamic predictions. One-step-ahead prediction uses the true values of the endogenous values at each step to predict the next in-sample value. Dynamic predictions use one-step-ahead prediction up to some point in the dataset (specified by the dynamic argument); after that, the previous predicted endogenous values are used in place of the true endogenous values for each new predicted element.",
            "markdown"
        ],
        [
            "The dynamic argument is specified to be an offset relative to the start argument. If start is not specified, it is assumed to be 0.",
            "markdown"
        ],
        [
            "Here we perform dynamic prediction starting in the first quarter of 1978.\n\n\n\n\n\nIn\u00a0[13]:",
            "markdown"
        ],
        [
            "# Dynamic predictions\npredict_dy = res.get_prediction(dynamic='1978-01-01')\npredict_dy_ci = predict_dy.conf_int()",
            "code"
        ],
        [
            "We can graph the one-step-ahead and dynamic predictions (and the corresponding confidence intervals) to see their relative performance. Notice that up to the point where dynamic prediction begins (1978:Q1), the two are the same.\n\n\n\n\n\nIn\u00a0[14]:",
            "markdown"
        ],
        [
            "# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Personal consumption', xlabel='Date', ylabel='Billions of dollars')\n\n# Plot data points\ndata.loc['1977-07-01':, 'consump'].plot(ax=ax, style='o', label='Observed')\n\n# Plot predictions\npredict.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='r--', label='One-step-ahead forecast')\nci = predict_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\npredict_dy.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='g', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='g', alpha=0.1)\n\nlegend = ax.legend(loc='lower right')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "code"
        ],
        [
            "Finally, graph the prediction error. It is obvious that, as one would suspect, one-step-ahead prediction is considerably better.\n\n\n\n\n\nIn\u00a0[15]:",
            "markdown"
        ],
        [
            "# Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)": [
        [
            "This notebook contains explanations for frequently asked questions.",
            "markdown"
        ],
        [
            "Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg",
            "markdown"
        ],
        [
            "Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA",
            "markdown"
        ],
        [
            "Initial residuals in SARIMAX and ARIMA",
            "markdown"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg": [
        [
            "ARIMA are formally OLS with ARMA errors. A basic AR(1) in the OLS with ARMA errors is described as\n\n\\[\\begin{split}\\begin{align}\nY_t & = \\delta + \\epsilon_t \\\\\n\\epsilon_t & = \\rho \\epsilon_{t-1} + \\eta_t \\\\\n\\eta_t & \\sim WN(0,\\sigma^2) \\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "In large samples, \\(\\hat{\\delta}\\stackrel{p}{\\rightarrow} E[Y]\\).",
            "markdown"
        ],
        [
            "SARIMAX uses a different representation, so that the model when estimated using SARIMAX is\n\n\\[\\begin{split}\\begin{align}\nY_t & = \\phi + \\rho Y_{t-1} + \\eta_t \\\\\n\\eta_t & \\sim WN(0,\\sigma^2) \\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "This is the same representation that is used when the model is estimated using OLS (AutoReg). In large samples, \\(\\hat{\\phi}\\stackrel{p}{\\rightarrow} E[Y](1-\\rho)\\).",
            "markdown"
        ],
        [
            "In the next cell, we simulate a large sample and verify that these relationship hold in practice.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\n\nrng = np.random.default_rng(20210819)\neta = rng.standard_normal(5200)\nrho = 0.8\nbeta = 10\nepsilon = eta.copy()\nfor i in range(1, eta.shape[0]):\n    epsilon[i] = rho * epsilon[i - 1] + eta[i]\ny = beta + epsilon\ny = y[200:]",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "from statsmodels.tsa.api import SARIMAX, AutoReg\nfrom statsmodels.tsa.arima.model import ARIMA",
            "code"
        ],
        [
            "The three models are specified and estimated in the next cell. An AR(0) is included as a reference. The AR(0) is identical using all three estimators.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "ar0_res = SARIMAX(y, order=(0, 0, 0), trend=\"c\").fit()\nsarimax_res = SARIMAX(y, order=(1, 0, 0), trend=\"c\").fit()\narima_res = ARIMA(y, order=(1, 0, 0), trend=\"c\").fit()\nautoreg_res = AutoReg(y, 1, trend=\"c\").fit()",
            "code"
        ],
        [
            "This problem is unconstrained.\n This problem is unconstrained.",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  1.91760D+00    |proj g|=  3.68860D-06\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      0      1      0     0     0   3.689D-06   1.918D+00\n  F =   1.9175996129577773\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\nRUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            3     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  1.41373D+00    |proj g|=  9.51828D-04\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    3      2      5      1     0     0   4.516D-05   1.414D+00\n  F =   1.4137311050015484\n\nCONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH",
            "code"
        ],
        [
            "The table below contains the estimated parameter in the model, the estimated AR(1) coefficient, and the long-run mean which is either equal to the estimated parameters (AR(0) or ARIMA), or depends on the ratio of the intercept to 1 minus the AR(1) parameter.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "intercept = [\n    ar0_res.params[0],\n    sarimax_res.params[0],\n    arima_res.params[0],\n    autoreg_res.params[0],\n]\nrho_hat = [0] + [r.params[1] for r in (sarimax_res, arima_res, autoreg_res)]\nlong_run = [\n    ar0_res.params[0],\n    sarimax_res.params[0] / (1 - sarimax_res.params[1]),\n    arima_res.params[0],\n    autoreg_res.params[0] / (1 - autoreg_res.params[1]),\n]\ncols = [\"AR(0)\", \"SARIMAX\", \"ARIMA\", \"AutoReg\"]\npd.DataFrame(\n    [intercept, rho_hat, long_run],\n    columns=cols,\n    index=[\"delta-or-phi\", \"rho\", \"long-run mean\"],\n)",
            "code"
        ],
        [
            "[5]:",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Differences between trend and exog in SARIMAX": [
        [
            "When SARIMAX includes exog variables, then the exog are treated as OLS regressors, so that the model estimated is\n\n\\[\\begin{split}\\begin{align}\nY_t - X_t \\beta & = \\delta + \\rho (Y_{t-1} - X_{t-1}\\beta) + \\eta_t \\\\\n\\eta_t & \\sim WN(0,\\sigma^2) \\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "In the next example, we omit the trend and instead include a column of 1, which produces a model that is equivalent, in large samples, to the case with no exogenous regressor and trend=\"c\". Here the estimated value of const matches the value estimated using ARIMA. This happens since both exog in SARIMAX and the trend in ARIMA are treated as linear regression models with ARMA errors.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "sarimax_exog_res = SARIMAX(y, exog=np.ones_like(y), order=(1, 0, 0), trend=\"n\").fit()\nprint(sarimax_exog_res.summary())",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            3     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  1.41373D+00    |proj g|=  1.06920D-04",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ],
        [
            "* * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    3      1      4      1     0     0   4.752D-05   1.414D+00\n  F =   1.4137311099487531\n\nCONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH\n                               SARIMAX Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                 5000\nModel:               SARIMAX(1, 0, 0)   Log Likelihood               -7068.656\nDate:                Wed, 02 Nov 2022   AIC                          14143.311\nTime:                        17:05:13   BIC                          14162.863\nSample:                             0   HQIC                         14150.164\n                               - 5000\nCovariance Type:                  opg\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          9.7745      0.069    141.177      0.000       9.639       9.910\nar.L1          0.7969      0.009     93.691      0.000       0.780       0.814\nsigma2         0.9894      0.020     49.921      0.000       0.951       1.028\n===================================================================================\nLjung-Box (L1) (Q):                   0.42   Jarque-Bera (JB):                 0.08\nProb(Q):                              0.51   Prob(JB):                         0.96\nHeteroskedasticity (H):               0.97   Skew:                            -0.01\nProb(H) (two-sided):                  0.47   Kurtosis:                         2.99\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA": [
        [
            "While exog are treated the same in both models, the intercept continues to differ. Below we add an exogenous regressor to y and then fit the model using all three methods. The data generating process is now\n\n\\[\\begin{split}\\begin{align}\nY_t & = \\delta + X_t \\beta + \\epsilon_t \\\\\n\\epsilon_t & = \\rho \\epsilon_{t-1} + \\eta_t \\\\\n\\eta_t & \\sim WN(0,\\sigma^2) \\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "full_x = rng.standard_normal(eta.shape)\nx = full_x[200:]\ny += 3 * x",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            4     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  1.42683D+00    |proj g|=  2.05943D-01",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ],
        [
            "At iterate    5    f=  1.41332D+00    |proj g|=  1.60874D-03\n\nAt iterate   10    f=  1.41329D+00    |proj g|=  3.11658D-05\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    4     11     15      1     0     0   1.796D-06   1.413D+00\n  F =   1.4132928400115972\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL",
            "code"
        ],
        [
            "Examining the parameter tables, we see that the parameter estimates on x1 are identical while the estimates of the intercept continue to differ due to the differences in the treatment of trends in these estimators.",
            "markdown"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA->SARIMAX": [
        [
            "[9]:",
            "code"
        ],
        [
            "def print_params(s):\n    from io import StringIO\n\n    return pd.read_csv(StringIO(s.tables[1].as_csv()), index_col=0)\n\n\nprint_params(sarimax_exog_res.summary())",
            "code"
        ],
        [
            "[9]:",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA->ARIMA": [
        [
            "[10]:",
            "code"
        ],
        [
            "print_params(arima_exog_res.summary())",
            "code"
        ],
        [
            "[10]:",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->exog in AutoReg": [
        [
            "When using AutoReg to estimate a model using OLS, the model differs from both SARIMAX and ARIMA. The AutoReg specification with exogenous variables is\n\n\\[\\begin{split}\\begin{align}\nY_t & = \\phi + \\rho Y_{t-1} + X_{t}\\beta + \\eta_t \\\\\n\\eta_t & \\sim WN(0,\\sigma^2) \\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "This specification is not equivalent to the specification estimated in SARIMAX and ARIMA. Here the difference is non-trivial, and naive estimation on the same time series results in different parameter values, even in large samples (and the limit). Estimating this model changes the parameter estimates on the AR(1) coefficient.",
            "markdown"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->exog in AutoReg->AutoReg": [
        [
            "[11]:",
            "code"
        ],
        [
            "autoreg_exog_res = AutoReg(y, 1, exog=x, trend=\"c\").fit()\nprint_params(autoreg_exog_res.summary())",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "The key difference can be seen by writing the model in lag operator notation.\n\n\\[\\begin{split}\\begin{align}\n(1-\\phi L ) Y_t & = X_{t}\\beta + \\eta_t \\Rightarrow \\\\\nY_t & = (1-\\phi L )^{-1}\\left(X_{t}\\beta + \\eta_t\\right) \\\\\nY_t & = \\sum_{i=0}^{\\infty} \\phi^i \\left(X_{t-i}\\beta + \\eta_{t-i}\\right)\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "where it is is assumed that \\(|\\phi|&lt;1\\). Here we see that \\(Y_t\\) depends on all lagged values of \\(X_t\\) and \\(\\eta_t\\). This differs from the specification estimated by SARIMAX and ARIMA, which can be seen to be\n\n\\[\\begin{split}\\begin{align}\nY_t - X_t \\beta & = \\delta + \\rho (Y_{t-1} - X_{t-1}\\beta) + \\eta_t \\\\\n\\left(1-\\rho L \\right)\\left(Y_t - X_t  \\beta\\right) & = \\delta +  \\eta_t \\\\\nY_t - X_t  \\beta & = \\frac{\\delta}{1-\\rho} +  \\left(1-\\rho L \\right)^{-1}\\eta_t \\\\\nY_t - X_t  \\beta & = \\frac{\\delta}{1-\\rho} +  \\sum_{i=0}^\\infty \\rho^i \\eta_{t-i} \\\\\nY_t  & = \\frac{\\delta}{1-\\rho} + X_t  \\beta +  \\sum_{i=0}^\\infty \\rho^i \\eta_{t-i} \\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "In this specification, \\(Y_t\\) only depends on \\(X_t\\) and no other lags.",
            "markdown"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using the correct DGP with AutoReg": [
        [
            "Simulating the process that is estimated in AutoReg shows that the parameters are recovered from the true model.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "y = beta + eta\nepsilon = eta.copy()\nfor i in range(1, eta.shape[0]):\n    y[i] = beta * (1 - rho) + rho * y[i - 1] + 3 * full_x[i] + eta[i]\ny = y[200:]",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using the correct DGP with AutoReg->AutoReg with correct DGP": [
        [
            "[13]:",
            "code"
        ],
        [
            "autoreg_alt_exog_res = AutoReg(y, 1, exog=x, trend=\"c\").fit()\nprint_params(autoreg_alt_exog_res.summary())",
            "code"
        ],
        [
            "[13]:",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA": [
        [
            "In models that contain only autoregressive terms, trends and exogenous variables, fitted values and forecasts can be easily reconstructed once the maximum lag length in the model has been reached. In practice, this means after \\((P+D)s+p+d\\) periods. Earlier predictions and residuals are harder to reconstruct since the model builds the best prediction for \\(Y_t|Y_{t-1},Y_{t-2},...\\). When the number of lags of \\(Y\\) is less than the autoregressive order, then the expression for the\noptimal prediction differs from the model. For example, when predicting the very first value, \\(Y_1\\), there is no information available from the history of \\(Y\\), and so the best prediction is the unconditional mean. In the case of an AR(1), the second prediction will follow the model, so that when using ARIMA, the prediction is\n\n\\[Y_2 = \\hat{\\delta} + \\hat{\\rho} \\left(Y_1 - \\hat{\\delta}\\right)\\]",
            "markdown"
        ],
        [
            "since ARIMA treats both exogenous and trend terms as regression with ARMA errors.",
            "markdown"
        ],
        [
            "This can be seen in the next set of cells.",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "arima_res = ARIMA(y, order=(1, 0, 0), trend=\"c\").fit()\nprint_params(arima_res.summary())",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "arima_res.predict(0, 2)",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "array([ 9.93458658, 10.91088035, 11.80415747])",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "delta_hat, rho_hat = arima_res.params[:2]\ndelta_hat + rho_hat * (y[0] - delta_hat)",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "10.910880346330751",
            "code"
        ],
        [
            "SARIMAX treats trend terms differently, and so the one-step forecast from a model estimated using SARIMAX is\n\n\\[Y_2 = \\hat\\delta + \\hat\\rho Y_1\\]",
            "markdown"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "sarima_res = SARIMAX(y, order=(1, 0, 0), trend=\"c\").fit()\nprint_params(sarima_res.summary())",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            3     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  2.58518D+00    |proj g|=  5.99456D-05\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    3      3      5      1     0     0   3.347D-05   2.585D+00\n  F =   2.5851830060985752\n\nCONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "sarima_res.predict(0, 2)",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "array([ 9.93588659, 10.91128867, 11.80469658])",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "delta_hat, rho_hat = sarima_res.params[:2]\ndelta_hat + rho_hat * y[0]",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "10.911288670367867",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->Prediction with MA components": [
        [
            "When a model contains a MA component, the prediction is more complicated since errors are never directly observable. The prediction is still \\(Y_t|Y_{t-1},Y_{t-2},...\\), and when the MA component is invertible, then the optimal prediction can be represented as a \\(t\\)-lag AR process. When \\(t\\) is large, this should be very close to the prediction as if the errors were observable. For short lags, this can differ markedly.",
            "markdown"
        ],
        [
            "In the next cell we simulate an MA(1) process, and fit an MA model.",
            "markdown"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "rho = 0.8\nbeta = 10\nepsilon = eta.copy()\nfor i in range(1, eta.shape[0]):\n    epsilon[i] = rho * eta[i - 1] + eta[i]\ny = beta + epsilon\ny = y[200:]\n\nma_res = ARIMA(y, order=(0, 0, 1), trend=\"c\").fit()\nprint_params(ma_res.summary())",
            "code"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "We start by looking at predictions near the beginning of the sample corresponding y[1], \u2026, y[5].",
            "markdown"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "ma_res.predict(1, 5)",
            "code"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "array([ 8.57011015,  9.19907188,  8.96971353,  9.78987115, 11.11984478])",
            "code"
        ],
        [
            "and the corresponding residuals that are needed to produce the \u201cdirect\u201d forecasts",
            "markdown"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "ma_res.resid[:5]",
            "code"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "array([-2.7621904 , -1.12255005, -1.33557621, -0.17206944,  1.5634041 ])",
            "code"
        ],
        [
            "Using the model parameters, we can produce the \u201cdirect\u201d forecasts using the MA(1) specification\n\n\\[\\hat Y_t = \\hat\\delta + \\hat\\rho \\hat\\epsilon_{t-1}\\]",
            "markdown"
        ],
        [
            "We see that these are not especially close to the actual model predictions for the initial forecasts, but that the gap quickly reduces.",
            "markdown"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "delta_hat, rho_hat = ma_res.params[:2]\ndirect = delta_hat + rho_hat * ma_res.resid[:5]\ndirect",
            "code"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "array([ 7.70168405,  9.01756049,  8.84659855,  9.7803589 , 11.17314527])",
            "code"
        ],
        [
            "The difference is nearly a standard deviation for the first but declines as the index increases.",
            "markdown"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "ma_res.predict(1, 5) - direct",
            "code"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "array([ 0.8684261 ,  0.18151139,  0.12311499,  0.00951225, -0.05330049])",
            "code"
        ],
        [
            "We next look at the end of the sample and the final three predictions.",
            "markdown"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "t = y.shape[0]\nma_res.predict(t - 3, t - 1)",
            "code"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "array([ 9.79692804, 10.51272714, 10.55855562])",
            "code"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "ma_res.resid[-4:-1]",
            "code"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "array([-0.15142355,  0.74049384,  0.79759816])",
            "code"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "direct = delta_hat + rho_hat * ma_res.resid[-4:-1]\ndirect",
            "code"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "array([ 9.79692804, 10.51272714, 10.55855562])",
            "code"
        ],
        [
            "The \u201cdirect\u201d forecasts are identical. This happens since the effect of the short sample has disappeared by the end of the sample (In practice it is negligible by observations 100 or so, and numerically absent by around observation 160).",
            "markdown"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "ma_res.predict(t - 3, t - 1) - direct",
            "code"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "array([0., 0., 0.])",
            "code"
        ],
        [
            "The same principle applies in more complicated model that include multiple lags or seasonal term - predictions in AR models are simple once the effective lag length has been reached, while predictions in models that contains MA components are only simple once the maximum root of the MA lag polynomial is sufficiently small so that the residuals are close to the true residuals.",
            "markdown"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->Prediction differences in SARIMAX and ARIMA": [
        [
            "The formulas used to make predictions from SARIMAX and ARIMA models differ in one key aspect - ARIMA treats all trend terms, e.g, the intercept or time trend, as part of the exogenous regressors. For example, an AR(1) model with an intercept and linear time trend estimated using ARIMA has the specification\n\n\\[\\begin{split}\\begin{align*}\nY_t - \\delta_0 - \\delta_1 t & = \\epsilon_t \\\\\n\\epsilon_t & = \\rho \\epsilon_{t-1} + \\eta_t\n\\end{align*}\\end{split}\\]",
            "markdown"
        ],
        [
            "When the same model is estimated using SARIMAX, the specification is\n\n\\[\\begin{split}\\begin{align*}\nY_t & = \\epsilon_t \\\\\n\\epsilon_t & =  \\delta_0 + \\delta_1 t  + \\rho \\epsilon_{t-1} + \\eta_t\n\\end{align*}\\end{split}\\]",
            "markdown"
        ],
        [
            "The differences are more apparent when the model contains exogenous regressors, \\(X_t\\). The ARIMA specification is\n\n\\[\\begin{split}\\begin{align*}\nY_t - \\delta_0 - \\delta_1 t - X_t \\beta & = \\epsilon_t \\\\\n\\epsilon_t & = \\rho \\epsilon_{t-1} + \\eta_t \\\\\n           & = \\rho \\left(Y_{t-1} - \\delta_0 - \\delta_1 (t-1) - X_{t-1} \\beta\\right) + \\eta_t\n\\end{align*}\\end{split}\\]",
            "markdown"
        ],
        [
            "while the SARIMAX specification is\n\n\\[\\begin{split}\\begin{align*}\nY_t & =  X_t \\beta + \\epsilon_t \\\\\n\\epsilon_t & =  \\delta_0 + \\delta_1 t  + \\rho \\epsilon_{t-1} + \\eta_t \\\\\n           & = \\delta_0 + \\delta_1 t  + \\rho \\left(Y_{t-1} - X_{t-1}\\beta\\right) + \\eta_t\n\\end{align*}\\end{split}\\]",
            "markdown"
        ],
        [
            "The key difference between these two is that the intercept and the trend are effectively equivalent to exogenous regressions in ARIMA while they are more like standard ARMA terms in SARIMAX.",
            "markdown"
        ],
        [
            "The next cell simulates an ARX with a time trend using the specification in ARIMA and estimates the parameters using both estimators.",
            "markdown"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "rho = 0.8\nbeta = 2\ndelta0 = 10\ndelta1 = 0.5\nepsilon = eta.copy()\nfor i in range(1, eta.shape[0]):\n    epsilon[i] = rho * epsilon[i - 1] + eta[i]\nt = np.arange(epsilon.shape[0])\ny = delta0 + delta1 * t + beta * full_x + epsilon\ny = y[200:]",
            "code"
        ],
        [
            "[30]:",
            "code"
        ],
        [
            "start = np.array([110, delta1, beta, rho, 1])\narx_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"ct\").fit()\nmod = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"ct\")\nstart[:2] *= 1 - rho\nsarimax_res = mod.fit(start_params=start, method=\"bfgs\")",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 1.413691\n         Iterations: 46\n         Function evaluations: 58\n         Gradient evaluations: 58",
            "code"
        ],
        [
            "The two estimators fit similarly, although there is a small difference in the log-likelihood. This is a numerical issue and should not materially affect the predictions. Importantly the two trend parameters, const and x1 (unfortunately named for the time trend), differ between the two. The other parameters are effectively identical.",
            "markdown"
        ],
        [
            "[31]:",
            "code"
        ],
        [
            "print(arx_res.summary())",
            "code"
        ],
        [
            "SARIMAX Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                 5000\nModel:                 ARIMA(1, 0, 0)   Log Likelihood               -7069.171\nDate:                Wed, 02 Nov 2022   AIC                          14148.343\nTime:                        17:05:25   BIC                          14180.928\nSample:                             0   HQIC                         14159.763\n                               - 5000\nCovariance Type:                  opg\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        109.2112      0.137    796.186      0.000     108.942     109.480\nx1             0.5000   4.78e-05   1.05e+04      0.000       0.500       0.500\nx2             2.0495      0.011    187.517      0.000       2.028       2.071\nar.L1          0.7965      0.009     93.669      0.000       0.780       0.813\nsigma2         0.9897      0.020     49.854      0.000       0.951       1.029\n===================================================================================\nLjung-Box (L1) (Q):                   0.33   Jarque-Bera (JB):                 0.15\nProb(Q):                              0.57   Prob(JB):                         0.93\nHeteroskedasticity (H):               0.97   Skew:                            -0.01\nProb(H) (two-sided):                  0.53   Kurtosis:                         3.00\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "[32]:",
            "code"
        ],
        [
            "print(sarimax_res.summary())",
            "code"
        ],
        [
            "SARIMAX Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                 5000\nModel:               SARIMAX(1, 0, 0)   Log Likelihood               -7068.457\nDate:                Wed, 02 Nov 2022   AIC                          14146.914\nTime:                        17:05:25   BIC                          14179.500\nSample:                             0   HQIC                         14158.335\n                               - 5000\nCovariance Type:                  opg\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     22.7438      0.929     24.481      0.000      20.923      24.565\ndrift          0.1019      0.004     23.985      0.000       0.094       0.110\nx1             2.0230      0.011    185.290      0.000       2.002       2.044\nar.L1          0.7963      0.008     93.745      0.000       0.780       0.813\nsigma2         0.9894      0.020     49.899      0.000       0.951       1.028\n===================================================================================\nLjung-Box (L1) (Q):                   0.47   Jarque-Bera (JB):                 0.13\nProb(Q):                              0.49   Prob(JB):                         0.94\nHeteroskedasticity (H):               0.97   Skew:                            -0.01\nProb(H) (two-sided):                  0.47   Kurtosis:                         3.00\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Initial residuals SARIMAX and ARIMA": [
        [
            "Residuals for observations before the maximal model order, which depends on the AR, MA, Seasonal AR, Seasonal MA and differencing parameters, are not reliable and should not be used for performance assessment. In general, in an ARIMA with orders \\((p,d,q)\\times(P,D,Q,s)\\), the formula for residuals that are less well behaved is:\n\n\\[\\max((P+D)s+p+d,Qs+q)\\]",
            "markdown"
        ],
        [
            "We can simulate some data from an ARIMA(1,0,0)(1,0,0,12) and examine the residuals.",
            "markdown"
        ],
        [
            "[33]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\n\nrho = 0.8\npsi = -0.6\nbeta = 20\nepsilon = eta.copy()\nfor i in range(13, eta.shape[0]):\n    epsilon[i] = (\n        rho * epsilon[i - 1]\n        + psi * epsilon[i - 12]\n        - (rho * psi) * epsilon[i - 13]\n        + eta[i]\n    )\ny = beta + epsilon\ny = y[200:]",
            "code"
        ],
        [
            "With a large sample, the parameter estimates are very close to the DGP parameters.",
            "markdown"
        ],
        [
            "[34]:",
            "code"
        ],
        [
            "res = ARIMA(y, order=(1, 0, 0), trend=\"c\", seasonal_order=(1, 0, 0, 12)).fit()\nprint(res.summary())",
            "code"
        ],
        [
            "SARIMAX Results\n========================================================================================\nDep. Variable:                                y   No. Observations:                 5000\nModel:             ARIMA(1, 0, 0)x(1, 0, 0, 12)   Log Likelihood               -7076.266\nDate:                          Wed, 02 Nov 2022   AIC                          14160.532\nTime:                                  17:05:28   BIC                          14186.600\nSample:                                       0   HQIC                         14169.668\n                                         - 5000\nCovariance Type:                            opg\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         19.8586      0.043    458.609      0.000      19.774      19.943\nar.L1          0.7972      0.008     93.925      0.000       0.781       0.814\nar.S.L12      -0.6044      0.011    -53.280      0.000      -0.627      -0.582\nsigma2         0.9914      0.020     49.899      0.000       0.952       1.030\n===================================================================================\nLjung-Box (L1) (Q):                   0.50   Jarque-Bera (JB):                 0.11\nProb(Q):                              0.48   Prob(JB):                         0.95\nHeteroskedasticity (H):               0.96   Skew:                            -0.01\nProb(H) (two-sided):                  0.40   Kurtosis:                         2.99\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "We can first examine the initial 13 residuals by plotting against the actual shocks in the model. While there is a correspondence, it is fairly weak and the correlation is much less than 1.",
            "markdown"
        ],
        [
            "[35]:",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n\nplt.rc(\"figure\", figsize=(10, 10))\nplt.rc(\"font\", size=14)\n\n_ = plt.scatter(res.resid[:13], eta[200 : 200 + 13])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_faq_63_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_faq_63_0.png\"/>",
            "code"
        ],
        [
            "Looking at the next 24 residuals and shocks, we see there is nearly perfect correlation. This is expected in large samples once the less accurate residuals are ignored.",
            "markdown"
        ],
        [
            "[36]:",
            "code"
        ],
        [
            "_ = plt.scatter(res.resid[13:37], eta[200 + 13 : 200 + 37])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_faq_65_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_faq_65_0.png\"/>",
            "code"
        ],
        [
            "Next, we simulate an ARIMA(1,1,0), and include a time trend.",
            "markdown"
        ],
        [
            "[37]:",
            "code"
        ],
        [
            "rng = np.random.default_rng(20210819)\neta = rng.standard_normal(5200)\nrho = 0.8\nbeta = 20\nepsilon = eta.copy()\nfor i in range(2, eta.shape[0]):\n    epsilon[i] = (1 + rho) * epsilon[i - 1] - rho * epsilon[i - 2] + eta[i]\nt = np.arange(epsilon.shape[0])\ny = beta + 2 * t + epsilon\ny = y[200:]",
            "code"
        ],
        [
            "Again the parameter estimates are very close to the DGP parameters.",
            "markdown"
        ],
        [
            "[38]:",
            "code"
        ],
        [
            "res = ARIMA(y, order=(1, 1, 0), trend=\"t\").fit()\nprint(res.summary())",
            "code"
        ],
        [
            "SARIMAX Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                 5000\nModel:                 ARIMA(1, 1, 0)   Log Likelihood               -7067.739\nDate:                Wed, 02 Nov 2022   AIC                          14141.479\nTime:                        17:05:29   BIC                          14161.030\nSample:                             0   HQIC                         14148.331\n                               - 5000\nCovariance Type:                  opg\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             1.7747      0.069     25.642      0.000       1.639       1.910\nar.L1          0.7968      0.009     93.658      0.000       0.780       0.813\nsigma2         0.9896      0.020     49.908      0.000       0.951       1.028\n===================================================================================\nLjung-Box (L1) (Q):                   0.43   Jarque-Bera (JB):                 0.09\nProb(Q):                              0.51   Prob(JB):                         0.96\nHeteroskedasticity (H):               0.97   Skew:                            -0.01\nProb(H) (two-sided):                  0.47   Kurtosis:                         2.99\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "The residuals are not accurate, and the first residual is approximately 500. The others are closer, although in this model the first 2 should usually be ignored.",
            "markdown"
        ],
        [
            "[39]:",
            "code"
        ],
        [
            "res.resid[:5]",
            "code"
        ],
        [
            "[39]:",
            "code"
        ],
        [
            "array([ 5.08403002e+02, -1.58904197e+00, -1.54902445e+00,  1.04992619e-01,\n        1.33644383e+00])",
            "code"
        ],
        [
            "The reason why the first residual is so large is that the optimal prediction of this value is the mean of the difference, which is 1.77. Once the first value is known, the second value makes use of the first value in its prediction and the prediction is substantially closer to the truth.",
            "markdown"
        ],
        [
            "[40]:",
            "code"
        ],
        [
            "res.predict(0, 5)",
            "code"
        ],
        [
            "[40]:",
            "code"
        ],
        [
            "array([  1.77472563, 511.95355129, 510.87392196, 508.85708934,\n       509.03356182, 511.85245439])",
            "code"
        ],
        [
            "It is worth noting that the results class contains two parameters than can be helpful in understanding which residuals are problematic, loglikelihood_burn and nobs_diffuse.",
            "markdown"
        ],
        [
            "[41]:",
            "code"
        ],
        [
            "res.loglikelihood_burn, res.nobs_diffuse",
            "code"
        ],
        [
            "[41]:",
            "code"
        ],
        [
            "(1, 0)",
            "code"
        ]
    ],
    "Examples->State space models->VARMAX: Introduction": [
        [
            "This is a brief introduction notebook to VARMAX models in statsmodels. The VARMAX model is generically specified as:\n\n\\[y_t = \\nu + A_1 y_{t-1} + \\dots + A_p y_{t-p} + B x_t + \\epsilon_t +\nM_1 \\epsilon_{t-1} + \\dots M_q \\epsilon_{t-q}\\]",
            "markdown"
        ],
        [
            "where \\(y_t\\) is a \\(\\text{k_endog} \\times 1\\) vector.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "dta = sm.datasets.webuse('lutkepohl2', 'https://www.stata-press.com/data/r12/')\ndta.index = dta.qtr\ndta.index.freq = dta.index.inferred_freq\nendog = dta.loc['1960-04-01':'1978-10-01', ['dln_inv', 'dln_inc', 'dln_consump']]",
            "code"
        ]
    ],
    "Examples->State space models->VARMAX: Introduction->Model specification": [
        [
            "The VARMAX class in statsmodels allows estimation of VAR, VMA, and VARMA models (through the order argument), optionally with a constant term (via the trend argument). Exogenous regressors may also be included (as usual in statsmodels, by the exog argument), and in this way a time trend may be added. Finally, the class allows measurement error (via the measurement_error argument) and allows specifying either a diagonal or unstructured innovation covariance matrix (via the\nerror_cov_type argument).",
            "markdown"
        ]
    ],
    "Examples->State space models->VARMAX: Introduction->Example 1: VAR": [
        [
            "Below is a simple VARX(2) model in two endogenous variables and an exogenous series, but no constant term. Notice that we needed to allow for more iterations than the default (which is maxiter=50) in order for the likelihood estimation to converge. This is not unusual in VAR models which have to estimate a large number of parameters, often on a relatively small number of time series: this model, for example, estimates 27 parameters off of 75 observations of 3 variables.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "exog = endog['dln_consump']\nmod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(2,0), trend='n', exog=exog)\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "code"
        ],
        [
            "Statespace Model Results\n==================================================================================\nDep. Variable:     ['dln_inv', 'dln_inc']   No. Observations:                   75\nModel:                            VARX(2)   Log Likelihood                 361.037\nDate:                    Wed, 02 Nov 2022   AIC                           -696.075\nTime:                            17:07:51   BIC                           -665.947\nSample:                        04-01-1960   HQIC                          -684.045\n                             - 10-01-1978\nCovariance Type:                      opg\n===================================================================================\nLjung-Box (L1) (Q):            0.05, 10.07   Jarque-Bera (JB):          11.05, 2.46\nProb(Q):                        0.82, 0.00   Prob(JB):                   0.00, 0.29\nHeteroskedasticity (H):         0.45, 0.40   Skew:                      0.16, -0.38\nProb(H) (two-sided):            0.05, 0.03   Kurtosis:                   4.85, 3.44\n                            Results for equation dln_inv\n====================================================================================\n                       coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nL1.dln_inv          -0.2399      0.093     -2.578      0.010      -0.422      -0.058\nL1.dln_inc           0.2776      0.449      0.618      0.536      -0.602       1.157\nL2.dln_inv          -0.1654      0.155     -1.066      0.286      -0.470       0.139\nL2.dln_inc           0.0643      0.421      0.153      0.879      -0.761       0.889\nbeta.dln_consump     0.9840      0.637      1.545      0.122      -0.264       2.232\n                            Results for equation dln_inc\n====================================================================================\n                       coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nL1.dln_inv           0.0633      0.036      1.770      0.077      -0.007       0.133\nL1.dln_inc           0.0803      0.107      0.750      0.453      -0.129       0.290\nL2.dln_inv           0.0111      0.033      0.337      0.736      -0.054       0.076\nL2.dln_inc           0.0335      0.134      0.250      0.803      -0.229       0.296\nbeta.dln_consump     0.7756      0.113      6.893      0.000       0.555       0.996\n                                  Error covariance matrix\n============================================================================================\n                               coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------------\nsqrt.var.dln_inv             0.0434      0.004     12.295      0.000       0.036       0.050\nsqrt.cov.dln_inv.dln_inc  6.006e-05      0.002      0.030      0.976      -0.004       0.004\nsqrt.var.dln_inc             0.0109      0.001     11.212      0.000       0.009       0.013\n============================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "From the estimated VAR model, we can plot the impulse response functions of the endogenous variables.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "ax = res.impulse_responses(10, orthogonalized=True, impulse=[1, 0]).plot(figsize=(13,3))\nax.set(xlabel='t', title='Responses to a shock to `dln_inv`');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->VARMAX: Introduction->Example 2: VMA": [
        [
            "A vector moving average model can also be formulated. Below we show a VMA(2) on the same data, but where the innovations to the process are uncorrelated. In this example we leave out the exogenous regressor but now include the constant term.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(0,2), error_cov_type='diagonal')\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "code"
        ],
        [
            "Statespace Model Results\n==================================================================================\nDep. Variable:     ['dln_inv', 'dln_inc']   No. Observations:                   75\nModel:                             VMA(2)   Log Likelihood                 353.883\n                              + intercept   AIC                           -683.766\nDate:                    Wed, 02 Nov 2022   BIC                           -655.956\nTime:                            17:07:56   HQIC                          -672.661\nSample:                        04-01-1960\n                             - 10-01-1978\nCovariance Type:                      opg\n===================================================================================\nLjung-Box (L1) (Q):             0.02, 0.05   Jarque-Bera (JB):         11.85, 13.52\nProb(Q):                        0.88, 0.83   Prob(JB):                   0.00, 0.00\nHeteroskedasticity (H):         0.44, 0.81   Skew:                      0.05, -0.48\nProb(H) (two-sided):            0.05, 0.60   Kurtosis:                   4.95, 4.84\n                           Results for equation dln_inv\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nintercept         0.0182      0.005      3.815      0.000       0.009       0.028\nL1.e(dln_inv)    -0.2710      0.105     -2.579      0.010      -0.477      -0.065\nL1.e(dln_inc)     0.5424      0.631      0.859      0.390      -0.695       1.780\nL2.e(dln_inv)     0.0397      0.146      0.271      0.786      -0.247       0.326\nL2.e(dln_inc)     0.1665      0.478      0.348      0.728      -0.770       1.103\n                           Results for equation dln_inc\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nintercept         0.0207      0.002     12.977      0.000       0.018       0.024\nL1.e(dln_inv)     0.0483      0.042      1.158      0.247      -0.033       0.130\nL1.e(dln_inc)    -0.0742      0.140     -0.532      0.595      -0.348       0.199\nL2.e(dln_inv)     0.0172      0.042      0.406      0.685      -0.066       0.100\nL2.e(dln_inc)     0.1313      0.152      0.861      0.389      -0.168       0.430\n                             Error covariance matrix\n==================================================================================\n                     coef    std err          z      P|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nsigma2.dln_inv     0.0020      0.000      7.384      0.000       0.001       0.003\nsigma2.dln_inc     0.0001   2.34e-05      5.812      0.000       9e-05       0.000\n==================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ]
    ],
    "Examples->State space models->VARMAX: Introduction->Caution: VARMA(p,q) specifications": [
        [
            "Although the model allows estimating VARMA(p,q) specifications, these models are not identified without additional restrictions on the representation matrices, which are not built-in. For this reason, it is recommended that the user proceed with error (and indeed a warning is issued when these models are specified). Nonetheless, they may in some circumstances provide useful information.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(1,1))\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/tsa/statespace/varmax.py:161: EstimationWarning: Estimation of VARMA(p,q) models is not generically robust, due especially to identification issues.\n  warn('Estimation of VARMA(p,q) models is not generically robust,'",
            "code"
        ],
        [
            "Statespace Model Results\n==================================================================================\nDep. Variable:     ['dln_inv', 'dln_inc']   No. Observations:                   75\nModel:                         VARMA(1,1)   Log Likelihood                 354.290\n                              + intercept   AIC                           -682.580\nDate:                    Wed, 02 Nov 2022   BIC                           -652.452\nTime:                            17:07:58   HQIC                          -670.550\nSample:                        04-01-1960\n                             - 10-01-1978\nCovariance Type:                      opg\n===================================================================================\nLjung-Box (L1) (Q):             0.00, 0.05   Jarque-Bera (JB):         11.18, 13.96\nProb(Q):                        0.96, 0.82   Prob(JB):                   0.00, 0.00\nHeteroskedasticity (H):         0.43, 0.91   Skew:                      0.01, -0.45\nProb(H) (two-sided):            0.04, 0.81   Kurtosis:                   4.89, 4.91\n                           Results for equation dln_inv\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nintercept         0.0104      0.066      0.159      0.874      -0.118       0.139\nL1.dln_inv       -0.0051      0.704     -0.007      0.994      -1.385       1.375\nL1.dln_inc        0.3827      2.766      0.138      0.890      -5.039       5.805\nL1.e(dln_inv)    -0.2475      0.714     -0.347      0.729      -1.647       1.152\nL1.e(dln_inc)     0.1232      3.017      0.041      0.967      -5.791       6.037\n                           Results for equation dln_inc\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nintercept         0.0165      0.027      0.600      0.548      -0.037       0.070\nL1.dln_inv       -0.0328      0.282     -0.117      0.907      -0.585       0.519\nL1.dln_inc        0.2351      1.114      0.211      0.833      -1.947       2.418\nL1.e(dln_inv)     0.0887      0.288      0.308      0.758      -0.476       0.654\nL1.e(dln_inc)    -0.2393      1.148     -0.208      0.835      -2.490       2.012\n                                  Error covariance matrix\n============================================================================================\n                               coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------------\nsqrt.var.dln_inv             0.0449      0.003     14.527      0.000       0.039       0.051\nsqrt.cov.dln_inv.dln_inc     0.0017      0.003      0.652      0.514      -0.003       0.007\nsqrt.var.dln_inc             0.0116      0.001     11.729      0.000       0.010       0.013\n============================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ]
    ],
    "Examples->State space models->Dynamic Factor Models: Application": [
        [
            "Factor models generally try to find a small number of unobserved \u201cfactors\u201d that influence a substantial portion of the variation in a larger number of observed variables, and they are related to dimension-reduction techniques such as principal components analysis. Dynamic factor models explicitly model the transition dynamics of the unobserved factors, and so are often applied to time-series data.",
            "markdown"
        ],
        [
            "Macroeconomic coincident indices are designed to capture the common component of the \u201cbusiness cycle\u201d; such a component is assumed to simultaneously affect many macroeconomic variables. Although the estimation and use of coincident indices (for example the ) pre-dates dynamic factor models, in several influential papers Stock and Watson (1989, 1991) used a dynamic factor model\nto provide a theoretical foundation for them.",
            "markdown"
        ],
        [
            "Below, we follow the treatment found in Kim and Nelson (1999), of the Stock and Watson (1991) model, to formulate a dynamic factor model, estimate its parameters via maximum likelihood, and create a coincident index.",
            "markdown"
        ]
    ],
    "Examples->State space models->Dynamic Factor Models: Application->Macroeconomic data": [
        [
            "The coincident index is created by considering the comovements in four macroeconomic variables (versions of these variables are available on ; the ID of the series used below is given in parentheses):",
            "markdown"
        ],
        [
            "Industrial production (IPMAN)",
            "markdown"
        ],
        [
            "Real aggregate income (excluding transfer payments) (W875RX1)",
            "markdown"
        ],
        [
            "Manufacturing and trade sales (CMRMTSPL)",
            "markdown"
        ],
        [
            "Employees on non-farm payrolls (PAYEMS)",
            "markdown"
        ],
        [
            "In all cases, the data is at the monthly frequency and has been seasonally adjusted; the time-frame considered is 1972 - 2005.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nnp.set_printoptions(precision=4, suppress=True, linewidth=120)",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "from pandas_datareader.data import DataReader\n\n# Get the datasets from FRED\nstart = '1979-01-01'\nend = '2014-12-01'\nindprod = DataReader('IPMAN', 'fred', start=start, end=end)\nincome = DataReader('W875RX1', 'fred', start=start, end=end)\nsales = DataReader('CMRMTSPL', 'fred', start=start, end=end)\nemp = DataReader('PAYEMS', 'fred', start=start, end=end)\n# dta = pd.concat((indprod, income, sales, emp), axis=1)\n# dta.columns = ['indprod', 'income', 'sales', 'emp']",
            "code"
        ],
        [
            "<strong>Note</strong>: in a recent update on FRED (8/12/15) the time series CMRMTSPL was truncated to begin in 1997; this is probably a mistake due to the fact that CMRMTSPL is a spliced series, so the earlier period is from the series HMRMT and the latter period is defined by CMRMT.",
            "markdown"
        ],
        [
            "This has since (02/11/16) been corrected, however the series could also be constructed by hand from HMRMT and CMRMT, as shown below (process taken from the notes in the Alfred xls file).",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "# HMRMT = DataReader('HMRMT', 'fred', start='1967-01-01', end=end)\n# CMRMT = DataReader('CMRMT', 'fred', start='1997-01-01', end=end)",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "# HMRMT_growth = HMRMT.diff() / HMRMT.shift()\n# sales = pd.Series(np.zeros(emp.shape[0]), index=emp.index)\n\n# # Fill in the recent entries (1997 onwards)\n# sales[CMRMT.index] = CMRMT\n\n# # Backfill the previous entries (pre 1997)\n# idx = sales.loc[:'1997-01-01'].index\n# for t in range(len(idx)-1, 0, -1):\n#     month = idx[t]\n#     prev_month = idx[t-1]\n#     sales.loc[prev_month] = sales.loc[month] / (1 + HMRMT_growth.loc[prev_month].values)",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "dta = pd.concat((indprod, income, sales, emp), axis=1)\ndta.columns = ['indprod', 'income', 'sales', 'emp']\ndta.index.freq = dta.index.inferred_freq",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "dta.loc[:, 'indprod':'emp'].plot(subplots=True, layout=(2, 2), figsize=(15, 6));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_8_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_8_0.png\"/>",
            "code"
        ],
        [
            "Stock and Watson (1991) report that for their datasets, they could not reject the null hypothesis of a unit root in each series (so the series are integrated), but they did not find strong evidence that the series were co-integrated.",
            "markdown"
        ],
        [
            "As a result, they suggest estimating the model using the first differences (of the logs) of the variables, demeaned and standardized.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "# Create log-differenced series\ndta['dln_indprod'] = (np.log(dta.indprod)).diff() * 100\ndta['dln_income'] = (np.log(dta.income)).diff() * 100\ndta['dln_sales'] = (np.log(dta.sales)).diff() * 100\ndta['dln_emp'] = (np.log(dta.emp)).diff() * 100\n\n# De-mean and standardize\ndta['std_indprod'] = (dta['dln_indprod'] - dta['dln_indprod'].mean()) / dta['dln_indprod'].std()\ndta['std_income'] = (dta['dln_income'] - dta['dln_income'].mean()) / dta['dln_income'].std()\ndta['std_sales'] = (dta['dln_sales'] - dta['dln_sales'].mean()) / dta['dln_sales'].std()\ndta['std_emp'] = (dta['dln_emp'] - dta['dln_emp'].mean()) / dta['dln_emp'].std()",
            "code"
        ]
    ],
    "Examples->State space models->Dynamic Factor Models: Application->Dynamic factors": [
        [
            "A general dynamic factor model is written as:\n\n\\[\\begin{split}\\begin{align}\ny_t & = \\Lambda f_t + B x_t + u_t \\\\\nf_t & = A_1 f_{t-1} + \\dots + A_p f_{t-p} + \\eta_t \\qquad \\eta_t \\sim N(0, I)\\\\\nu_t & = C_1 u_{t-1} + \\dots + C_q u_{t-q} + \\varepsilon_t \\qquad \\varepsilon_t \\sim N(0, \\Sigma)\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(y_t\\) are observed data, \\(f_t\\) are the unobserved factors (evolving as a vector autoregression), \\(x_t\\) are (optional) exogenous variables, and \\(u_t\\) is the error, or \u201cidiosyncratic\u201d, process (\\(u_t\\) is also optionally allowed to be autocorrelated). The \\(\\Lambda\\) matrix is often referred to as the matrix of \u201cfactor loadings\u201d. The variance of the factor error term is set to the identity matrix to ensure identification of the unobserved factors.",
            "markdown"
        ],
        [
            "This model can be cast into state space form, and the unobserved factor estimated via the Kalman filter. The likelihood can be evaluated as a byproduct of the filtering recursions, and maximum likelihood estimation used to estimate the parameters.",
            "markdown"
        ]
    ],
    "Examples->State space models->Dynamic Factor Models: Application->Model specification": [
        [
            "The specific dynamic factor model in this application has 1 unobserved factor which is assumed to follow an AR(2) process. The innovations \\(\\varepsilon_t\\) are assumed to be independent (so that \\(\\Sigma\\) is a diagonal matrix) and the error term associated with each equation, \\(u_{i,t}\\) is assumed to follow an independent AR(2) process.",
            "markdown"
        ],
        [
            "Thus the specification considered here is:\n\n\\[\\begin{split}\\begin{align}\ny_{i,t} & = \\lambda_i f_t + u_{i,t} \\\\\nu_{i,t} & = c_{i,1} u_{1,t-1} + c_{i,2} u_{i,t-2} + \\varepsilon_{i,t} \\qquad & \\varepsilon_{i,t} \\sim N(0, \\sigma_i^2) \\\\\nf_t & = a_1 f_{t-1} + a_2 f_{t-2} + \\eta_t \\qquad & \\eta_t \\sim N(0, I)\\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(i\\) is one of: [indprod, income, sales, emp ].",
            "markdown"
        ],
        [
            "This model can be formulated using the DynamicFactor model built-in to statsmodels. In particular, we have the following specification:",
            "markdown"
        ],
        [
            "k_factors = 1 - (there is 1 unobserved factor)",
            "markdown"
        ],
        [
            "factor_order = 2 - (it follows an AR(2) process)",
            "markdown"
        ],
        [
            "error_var = False - (the errors evolve as independent AR processes rather than jointly as a VAR - note that this is the default option, so it is not specified below)",
            "markdown"
        ],
        [
            "error_order = 2 - (the errors are autocorrelated of order 2: i.e.\u00a0AR(2) processes)",
            "markdown"
        ],
        [
            "error_cov_type = 'diagonal' - (the innovations are uncorrelated; this is again the default)",
            "markdown"
        ],
        [
            "Once the model is created, the parameters can be estimated via maximum likelihood; this is done using the fit() method.",
            "markdown"
        ],
        [
            "<strong>Note</strong>: recall that we have demeaned and standardized the data; this will be important in interpreting the results that follow.",
            "markdown"
        ],
        [
            "<strong>Aside</strong>: in their empirical example, Kim and Nelson (1999) actually consider a slightly different model in which the employment variable is allowed to also depend on lagged values of the factor - this model does not fit into the built-in DynamicFactor class, but can be accommodated by using a subclass to implement the required new parameters and restrictions - see Appendix A, below.",
            "markdown"
        ]
    ],
    "Examples->State space models->Dynamic Factor Models: Application->Parameter estimation": [
        [
            "Multivariate models can have a relatively large number of parameters, and it may be difficult to escape from local minima to find the maximized likelihood. In an attempt to mitigate this problem, I perform an initial maximization step (from the model-defined starting parameters) using the modified Powell method available in Scipy (see the minimize documentation for more information). The resulting parameters are then used as starting parameters in the standard LBFGS optimization method.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "# Get the endogenous data\nendog = dta.loc['1979-02-01':, 'std_indprod':'std_emp']\n\n# Create the model\nmod = sm.tsa.DynamicFactor(endog, k_factors=1, factor_order=2, error_order=2)\ninitial_res = mod.fit(method='powell', disp=False)\nres = mod.fit(initial_res.params, disp=False)",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"",
            "code"
        ]
    ],
    "Examples->State space models->Dynamic Factor Models: Application->Estimates": [
        [
            "Once the model has been estimated, there are two components that we can use for analysis or inference:",
            "markdown"
        ],
        [
            "The estimated parameters",
            "markdown"
        ],
        [
            "The estimated factor",
            "markdown"
        ]
    ],
    "Examples->State space models->Dynamic Factor Models: Application->Estimates->Parameters": [
        [
            "The estimated parameters can be helpful in understanding the implications of the model, although in models with a larger number of observed variables and / or unobserved factors they can be difficult to interpret.",
            "markdown"
        ],
        [
            "One reason for this difficulty is due to identification issues between the factor loadings and the unobserved factors. One easy-to-see identification issue is the sign of the loadings and the factors: an equivalent model to the one displayed below would result from reversing the signs of all factor loadings and the unobserved factor.",
            "markdown"
        ],
        [
            "Here, one of the easy-to-interpret implications in this model is the persistence of the unobserved factor: we find that exhibits substantial persistence.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "print(res.summary(separate_params=False))",
            "code"
        ],
        [
            "Statespace Model Results\n=================================================================================================================\nDep. Variable:     ['std_indprod', 'std_income', 'std_sales', 'std_emp']   No. Observations:                  431\nModel:                                 DynamicFactor(factors=1, order=2)   Log Likelihood               -1589.814\n                                                          + AR(2) errors   AIC                           3215.628\nDate:                                                   Wed, 02 Nov 2022   BIC                           3288.818\nTime:                                                           17:12:03   HQIC                          3244.526\nSample:                                                       02-01-1979\n                                                            - 12-01-2014\nCovariance Type:                                                     opg\n====================================================================================================\n                                       coef    std err          z      P|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------------\nloading.f1.std_indprod              -1.0266      0.020    -52.383      0.000      -1.065      -0.988\nloading.f1.std_income               -0.3131      0.011    -27.546      0.000      -0.335      -0.291\nloading.f1.std_sales                -0.5394      0.021    -25.874      0.000      -0.580      -0.499\nloading.f1.std_emp                  -0.3050      0.014    -21.688      0.000      -0.333      -0.277\nsigma2.std_indprod                5.767e-07   1.39e-06      0.415      0.678   -2.15e-06     3.3e-06\nsigma2.std_income                    0.8754      0.016     53.989      0.000       0.844       0.907\nsigma2.std_sales                     0.5748      0.003    179.695      0.000       0.569       0.581\nsigma2.std_emp                       0.3581      0.009     39.855      0.000       0.340       0.376\nL1.f1.f1                             0.2601      0.013     19.409      0.000       0.234       0.286\nL2.f1.f1                             0.2645      0.009     28.465      0.000       0.246       0.283\nL1.e(std_indprod).e(std_indprod) -1.668e-07   1.14e-09   -145.845      0.000   -1.69e-07   -1.65e-07\nL2.e(std_indprod).e(std_indprod)     1.0000   3.72e-08   2.69e+07      0.000       1.000       1.000\nL1.e(std_income).e(std_income)      -0.1875      0.018    -10.413      0.000      -0.223      -0.152\nL2.e(std_income).e(std_income)      -0.0983      0.010     -9.415      0.000      -0.119      -0.078\nL1.e(std_sales).e(std_sales)        -0.4530      0.006    -70.375      0.000      -0.466      -0.440\nL2.e(std_sales).e(std_sales)        -0.1926      0.002    -84.558      0.000      -0.197      -0.188\nL1.e(std_emp).e(std_emp)             0.1888      0.016     11.557      0.000       0.157       0.221\nL2.e(std_emp).e(std_emp)             0.4421      0.019     23.670      0.000       0.405       0.479\n=========================================================================================================\nLjung-Box (L1) (Q):     8.05, 0.13, 0.01, 1.92   Jarque-Bera (JB):   3050420.53, 11405.19, 10.41, 2599.40\nProb(Q):                0.00, 0.71, 0.90, 0.17   Prob(JB):                         0.00, 0.00, 0.01, 0.00\nHeteroskedasticity (H): 0.00, 4.71, 0.53, 0.48   Skew:                          -20.05, -1.16, 0.01, 0.91\nProb(H) (two-sided):    0.00, 0.00, 0.00, 0.00   Kurtosis:                     413.19, 28.09, 3.76, 14.89\n=========================================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n[2] Covariance matrix is singular or near-singular, with condition number 2.53e+18. Standard errors may be unstable.",
            "code"
        ]
    ],
    "Examples->State space models->Dynamic Factor Models: Application->Estimates->Estimated factors": [
        [
            "While it can be useful to plot the unobserved factors, it is less useful here than one might think for two reasons:",
            "markdown"
        ],
        [
            "The sign-related identification issue described above.",
            "markdown"
        ],
        [
            "Since the data was differenced, the estimated factor explains the variation in the differenced data, not the original data.",
            "markdown"
        ],
        [
            "It is for these reasons that the coincident index is created (see below).",
            "markdown"
        ],
        [
            "With these reservations, the unobserved factor is plotted below, along with the NBER indicators for US recessions. It appears that the factor is successful at picking up some degree of business cycle activity.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(13,3))\n\n# Plot the factor\ndates = endog.index._mpl_repr()\nax.plot(dates, res.factors.filtered[0], label='Factor')\nax.legend()\n\n# Retrieve and also plot the NBER recession indicators\nrec = DataReader('USREC', 'fred', start=start, end=end)\nylim = ax.get_ylim()\nax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_19_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_19_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->Dynamic Factor Models: Application->Post-estimation": [
        [
            "Although here we will be able to interpret the results of the model by constructing the coincident index, there is a useful and generic approach for getting a sense for what is being captured by the estimated factor. By taking the estimated factors as given, regressing them (and a constant) each (one at a time) on each of the observed variables, and recording the coefficients of determination (\\(R^2\\) values), we can get a sense of the variables for which each factor explains a substantial\nportion of the variance and the variables for which it does not.",
            "markdown"
        ],
        [
            "In models with more variables and more factors, this can sometimes lend interpretation to the factors (for example sometimes one factor will load primarily on real variables and another on nominal variables).",
            "markdown"
        ],
        [
            "In this model, with only four endogenous variables and one factor, it is easy to digest a simple table of the \\(R^2\\) values, but in larger models it is not. For this reason, a bar plot is often employed; from the plot we can easily see that the factor explains most of the variation in industrial production index and a large portion of the variation in sales and employment, it is less helpful in explaining income.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "res.plot_coefficients_of_determination(figsize=(8,2));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_21_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->Dynamic Factor Models: Application->Coincident Index": [
        [
            "As described above, the goal of this model was to create an interpretable series which could be used to understand the current status of the macroeconomy. This is what the coincident index is designed to do. It is constructed below. For readers interested in an explanation of the construction, see Kim and Nelson (1999) or Stock and Watson (1991).",
            "markdown"
        ],
        [
            "In essence, what is done is to reconstruct the mean of the (differenced) factor. We will compare it to the coincident index on published by the Federal Reserve Bank of Philadelphia (USPHCI on FRED).",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "usphci = DataReader('USPHCI', 'fred', start='1979-01-01', end='2014-12-01')['USPHCI']\nusphci.plot(figsize=(13,3));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_23_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_23_0.png\"/>",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "dusphci = usphci.diff()[1:].values\ndef compute_coincident_index(mod, res):\n    # Estimate W(1)\n    spec = res.specification\n    design = mod.ssm['design']\n    transition = mod.ssm['transition']\n    ss_kalman_gain = res.filter_results.kalman_gain[:,:,-1]\n    k_states = ss_kalman_gain.shape[0]\n\n    W1 = np.linalg.inv(np.eye(k_states) - np.dot(\n        np.eye(k_states) - np.dot(ss_kalman_gain, design),\n        transition\n    )).dot(ss_kalman_gain)[0]\n\n    # Compute the factor mean vector\n    factor_mean = np.dot(W1, dta.loc['1972-02-01':, 'dln_indprod':'dln_emp'].mean())\n\n    # Normalize the factors\n    factor = res.factors.filtered[0]\n    factor *= np.std(usphci.diff()[1:]) / np.std(factor)\n\n    # Compute the coincident index\n    coincident_index = np.zeros(mod.nobs+1)\n    # The initial value is arbitrary; here it is set to\n    # facilitate comparison\n    coincident_index[0] = usphci.iloc[0] * factor_mean / dusphci.mean()\n    for t in range(0, mod.nobs):\n        coincident_index[t+1] = coincident_index[t] + factor[t] + factor_mean\n\n    # Attach dates\n    coincident_index = pd.Series(coincident_index, index=dta.index).iloc[1:]\n\n    # Normalize to use the same base year as USPHCI\n    coincident_index *= (usphci.loc['1992-07-01'] / coincident_index.loc['1992-07-01'])\n\n    return coincident_index",
            "code"
        ],
        [
            "Below we plot the calculated coincident index along with the US recessions and the comparison coincident index USPHCI.",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(13,3))\n\n# Compute the index\ncoincident_index = compute_coincident_index(mod, res)\n\n# Plot the factor\ndates = endog.index._mpl_repr()\nax.plot(dates, coincident_index, label='Coincident index')\nax.plot(usphci.index._mpl_repr(), usphci, label='USPHCI')\nax.legend(loc='lower right')\n\n# Retrieve and also plot the NBER recession indicators\nylim = ax.get_ylim()\nax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_26_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_26_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->Dynamic Factor Models: Application->Appendix 1: Extending the dynamic factor model": [
        [
            "Recall that the previous specification was described by:\n\n\\[\\begin{split}\\begin{align}\ny_{i,t} & = \\lambda_i f_t + u_{i,t} \\\\\nu_{i,t} & = c_{i,1} u_{1,t-1} + c_{i,2} u_{i,t-2} + \\varepsilon_{i,t} \\qquad & \\varepsilon_{i,t} \\sim N(0, \\sigma_i^2) \\\\\nf_t & = a_1 f_{t-1} + a_2 f_{t-2} + \\eta_t \\qquad & \\eta_t \\sim N(0, I)\\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "Written in state space form, the previous specification of the model had the following observation equation:\n\n\\[\\begin{split}\\begin{bmatrix}\ny_{\\text{indprod}, t} \\\\\ny_{\\text{income}, t} \\\\\ny_{\\text{sales}, t} \\\\\ny_{\\text{emp}, t} \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n\\lambda_\\text{indprod} & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n\\lambda_\\text{income}  & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n\\lambda_\\text{sales}   & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n\\lambda_\\text{emp}     & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_t \\\\\nf_{t-1} \\\\\nu_{\\text{indprod}, t} \\\\\nu_{\\text{income}, t} \\\\\nu_{\\text{sales}, t} \\\\\nu_{\\text{emp}, t} \\\\\nu_{\\text{indprod}, t-1} \\\\\nu_{\\text{income}, t-1} \\\\\nu_{\\text{sales}, t-1} \\\\\nu_{\\text{emp}, t-1} \\\\\n\\end{bmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "and transition equation:\n\n\\[\\begin{split}\\begin{bmatrix}\nf_t \\\\\nf_{t-1} \\\\\nu_{\\text{indprod}, t} \\\\\nu_{\\text{income}, t} \\\\\nu_{\\text{sales}, t} \\\\\nu_{\\text{emp}, t} \\\\\nu_{\\text{indprod}, t-1} \\\\\nu_{\\text{income}, t-1} \\\\\nu_{\\text{sales}, t-1} \\\\\nu_{\\text{emp}, t-1} \\\\\n\\end{bmatrix} = \\begin{bmatrix}\na_1 & a_2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n1   & 0   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0   & 0   & c_{\\text{indprod}, 1} & 0 & 0 & 0 & c_{\\text{indprod}, 2} & 0 & 0 & 0 \\\\\n0   & 0   & 0 & c_{\\text{income}, 1} & 0 & 0 & 0 & c_{\\text{income}, 2} & 0 & 0 \\\\\n0   & 0   & 0 & 0 & c_{\\text{sales}, 1} & 0 & 0 & 0 & c_{\\text{sales}, 2} & 0 \\\\\n0   & 0   & 0 & 0 & 0 & c_{\\text{emp}, 1} & 0 & 0 & 0 & c_{\\text{emp}, 2} \\\\\n0   & 0   & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0   & 0   & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0   & 0   & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n0   & 0   & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{t-1} \\\\\nf_{t-2} \\\\\nu_{\\text{indprod}, t-1} \\\\\nu_{\\text{income}, t-1} \\\\\nu_{\\text{sales}, t-1} \\\\\nu_{\\text{emp}, t-1} \\\\\nu_{\\text{indprod}, t-2} \\\\\nu_{\\text{income}, t-2} \\\\\nu_{\\text{sales}, t-2} \\\\\nu_{\\text{emp}, t-2} \\\\\n\\end{bmatrix}\n+ R \\begin{bmatrix}\n\\eta_t \\\\\n\\varepsilon_{t}\n\\end{bmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "the DynamicFactor model handles setting up the state space representation and, in the DynamicFactor.update method, it fills in the fitted parameter values into the appropriate locations.",
            "markdown"
        ],
        [
            "The extended specification is the same as in the previous example, except that we also want to allow employment to depend on lagged values of the factor. This creates a change to the \\(y_{\\text{emp},t}\\) equation. Now we have:\n\n\\[\\begin{split}\\begin{align}\ny_{i,t} & = \\lambda_i f_t + u_{i,t} \\qquad & i \\in \\{\\text{indprod}, \\text{income}, \\text{sales} \\}\\\\\ny_{i,t} & = \\lambda_{i,0} f_t + \\lambda_{i,1} f_{t-1} + \\lambda_{i,2} f_{t-2} + \\lambda_{i,2} f_{t-3} + u_{i,t} \\qquad & i = \\text{emp} \\\\\nu_{i,t} & = c_{i,1} u_{i,t-1} + c_{i,2} u_{i,t-2} + \\varepsilon_{i,t} \\qquad & \\varepsilon_{i,t} \\sim N(0, \\sigma_i^2) \\\\\nf_t & = a_1 f_{t-1} + a_2 f_{t-2} + \\eta_t \\qquad & \\eta_t \\sim N(0, I)\\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "Now, the corresponding observation equation should look like the following:\n\n\\[\\begin{split}\\begin{bmatrix}\ny_{\\text{indprod}, t} \\\\\ny_{\\text{income}, t} \\\\\ny_{\\text{sales}, t} \\\\\ny_{\\text{emp}, t} \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n\\lambda_\\text{indprod} & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n\\lambda_\\text{income}  & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n\\lambda_\\text{sales}   & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n\\lambda_\\text{emp,1}   & \\lambda_\\text{emp,2} & \\lambda_\\text{emp,3} & \\lambda_\\text{emp,4} & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_t \\\\\nf_{t-1} \\\\\nf_{t-2} \\\\\nf_{t-3} \\\\\nu_{\\text{indprod}, t} \\\\\nu_{\\text{income}, t} \\\\\nu_{\\text{sales}, t} \\\\\nu_{\\text{emp}, t} \\\\\nu_{\\text{indprod}, t-1} \\\\\nu_{\\text{income}, t-1} \\\\\nu_{\\text{sales}, t-1} \\\\\nu_{\\text{emp}, t-1} \\\\\n\\end{bmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "Notice that we have introduced two new state variables, \\(f_{t-2}\\) and \\(f_{t-3}\\), which means we need to update the transition equation:\n\n\\[\\begin{split}\\begin{bmatrix}\nf_t \\\\\nf_{t-1} \\\\\nf_{t-2} \\\\\nf_{t-3} \\\\\nu_{\\text{indprod}, t} \\\\\nu_{\\text{income}, t} \\\\\nu_{\\text{sales}, t} \\\\\nu_{\\text{emp}, t} \\\\\nu_{\\text{indprod}, t-1} \\\\\nu_{\\text{income}, t-1} \\\\\nu_{\\text{sales}, t-1} \\\\\nu_{\\text{emp}, t-1} \\\\\n\\end{bmatrix} = \\begin{bmatrix}\na_1 & a_2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n1   & 0   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0   & 1   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0   & 0   & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0   & 0   & 0 & 0 & c_{\\text{indprod}, 1} & 0 & 0 & 0 & c_{\\text{indprod}, 2} & 0 & 0 & 0 \\\\\n0   & 0   & 0 & 0 & 0 & c_{\\text{income}, 1} & 0 & 0 & 0 & c_{\\text{income}, 2} & 0 & 0 \\\\\n0   & 0   & 0 & 0 & 0 & 0 & c_{\\text{sales}, 1} & 0 & 0 & 0 & c_{\\text{sales}, 2} & 0 \\\\\n0   & 0   & 0 & 0 & 0 & 0 & 0 & c_{\\text{emp}, 1} & 0 & 0 & 0 & c_{\\text{emp}, 2} \\\\\n0   & 0   & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0   & 0   & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0   & 0   & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n0   & 0   & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{t-1} \\\\\nf_{t-2} \\\\\nf_{t-3} \\\\\nf_{t-4} \\\\\nu_{\\text{indprod}, t-1} \\\\\nu_{\\text{income}, t-1} \\\\\nu_{\\text{sales}, t-1} \\\\\nu_{\\text{emp}, t-1} \\\\\nu_{\\text{indprod}, t-2} \\\\\nu_{\\text{income}, t-2} \\\\\nu_{\\text{sales}, t-2} \\\\\nu_{\\text{emp}, t-2} \\\\\n\\end{bmatrix}\n+ R \\begin{bmatrix}\n\\eta_t \\\\\n\\varepsilon_{t}\n\\end{bmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "This model cannot be handled out-of-the-box by the DynamicFactor class, but it can be handled by creating a subclass when alters the state space representation in the appropriate way.",
            "markdown"
        ],
        [
            "First, notice that if we had set factor_order = 4, we would almost have what we wanted. In that case, the last line of the observation equation would be:\n\n\\[\\begin{split}\\begin{bmatrix}\n\\vdots \\\\\ny_{\\text{emp}, t} \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n\\vdots &  &  &  &  &  &  &  &  &  &  & \\vdots \\\\\n\\lambda_\\text{emp,1}   & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_t \\\\\nf_{t-1} \\\\\nf_{t-2} \\\\\nf_{t-3} \\\\\n\\vdots\n\\end{bmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "and the first line of the transition equation would be:\n\n\\[\\begin{split}\\begin{bmatrix}\nf_t \\\\\n\\vdots\n\\end{bmatrix} = \\begin{bmatrix}\na_1 & a_2 & a_3 & a_4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n\\vdots &  &  &  &  &  &  &  &  &  &  & \\vdots \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{t-1} \\\\\nf_{t-2} \\\\\nf_{t-3} \\\\\nf_{t-4} \\\\\n\\vdots\n\\end{bmatrix}\n+ R \\begin{bmatrix}\n\\eta_t \\\\\n\\varepsilon_{t}\n\\end{bmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "Relative to what we want, we have the following differences:",
            "markdown"
        ],
        [
            "In the above situation, the \\(\\lambda_{\\text{emp}, j}\\) are forced to be zero for \\(j > 0\\), and we want them to be estimated as parameters.",
            "markdown"
        ],
        [
            "We only want the factor to transition according to an AR(2), but under the above situation it is an AR(4).",
            "markdown"
        ],
        [
            "Our strategy will be to subclass DynamicFactor, and let it do most of the work (setting up the state space representation, etc.) where it assumes that factor_order = 4. The only things we will actually do in the subclass will be to fix those two issues.",
            "markdown"
        ],
        [
            "First, here is the full code of the subclass; it is discussed below. It is important to note at the outset that none of the methods defined below could have been omitted. In fact, the methods __init__, start_params, param_names, transform_params, untransform_params, and update form the core of all state space models in statsmodels, not just the DynamicFactor class.",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "from statsmodels.tsa.statespace import tools\nclass ExtendedDFM(sm.tsa.DynamicFactor):\n    def __init__(self, endog, **kwargs):\n            # Setup the model as if we had a factor order of 4\n            super(ExtendedDFM, self).__init__(\n                endog, k_factors=1, factor_order=4, error_order=2,\n                **kwargs)\n\n            # Note: `self.parameters` is an ordered dict with the\n            # keys corresponding to parameter types, and the values\n            # the number of parameters of that type.\n            # Add the new parameters\n            self.parameters['new_loadings'] = 3\n\n            # Cache a slice for the location of the 4 factor AR\n            # parameters (a_1, ..., a_4) in the full parameter vector\n            offset = (self.parameters['factor_loadings'] +\n                      self.parameters['exog'] +\n                      self.parameters['error_cov'])\n            self._params_factor_ar = np.s_[offset:offset+2]\n            self._params_factor_zero = np.s_[offset+2:offset+4]\n\n    @property\n    def start_params(self):\n        # Add three new loading parameters to the end of the parameter\n        # vector, initialized to zeros (for simplicity; they could\n        # be initialized any way you like)\n        return np.r_[super(ExtendedDFM, self).start_params, 0, 0, 0]\n\n    @property\n    def param_names(self):\n        # Add the corresponding names for the new loading parameters\n        #  (the name can be anything you like)\n        return super(ExtendedDFM, self).param_names + [\n            'loading.L%d.f1.%s' % (i, self.endog_names[3]) for i in range(1,4)]\n\n    def transform_params(self, unconstrained):\n            # Perform the typical DFM transformation (w/o the new parameters)\n            constrained = super(ExtendedDFM, self).transform_params(\n            unconstrained[:-3])\n\n            # Redo the factor AR constraint, since we only want an AR(2),\n            # and the previous constraint was for an AR(4)\n            ar_params = unconstrained[self._params_factor_ar]\n            constrained[self._params_factor_ar] = (\n                tools.constrain_stationary_univariate(ar_params))\n\n            # Return all the parameters\n            return np.r_[constrained, unconstrained[-3:]]\n\n    def untransform_params(self, constrained):\n            # Perform the typical DFM untransformation (w/o the new parameters)\n            unconstrained = super(ExtendedDFM, self).untransform_params(\n                constrained[:-3])\n\n            # Redo the factor AR unconstrained, since we only want an AR(2),\n            # and the previous unconstrained was for an AR(4)\n            ar_params = constrained[self._params_factor_ar]\n            unconstrained[self._params_factor_ar] = (\n                tools.unconstrain_stationary_univariate(ar_params))\n\n            # Return all the parameters\n            return np.r_[unconstrained, constrained[-3:]]\n\n    def update(self, params, transformed=True, **kwargs):\n        # Peform the transformation, if required\n        if not transformed:\n            params = self.transform_params(params)\n        params[self._params_factor_zero] = 0\n\n        # Now perform the usual DFM update, but exclude our new parameters\n        super(ExtendedDFM, self).update(params[:-3], transformed=True, **kwargs)\n\n        # Finally, set our new parameters in the design matrix\n        self.ssm['design', 3, 1:4] = params[-3:]",
            "code"
        ],
        [
            "So what did we just do?",
            "markdown"
        ],
        [
            "<strong>``__init__``</strong>",
            "markdown"
        ],
        [
            "The important step here was specifying the base dynamic factor model which we were operating with. In particular, as described above, we initialize with factor_order=4, even though we will only end up with an AR(2) model for the factor. We also performed some general setup-related tasks.",
            "markdown"
        ],
        [
            "<strong>``start_params``</strong>",
            "markdown"
        ],
        [
            "start_params are used as initial values in the optimizer. Since we are adding three new parameters, we need to pass those in. If we had not done this, the optimizer would use the default starting values, which would be three elements short.",
            "markdown"
        ],
        [
            "<strong>``param_names``</strong>",
            "markdown"
        ],
        [
            "param_names are used in a variety of places, but especially in the results class. Below we get a full result summary, which is only possible when all the parameters have associated names.",
            "markdown"
        ],
        [
            "<strong>``transform_params``</strong> and <strong>``untransform_params``</strong>",
            "markdown"
        ],
        [
            "The optimizer selects possibly parameter values in an unconstrained way. That\u2019s not usually desired (since variances cannot be negative, for example), and transform_params is used to transform the unconstrained values used by the optimizer to constrained values appropriate to the model. Variances terms are typically squared (to force them to be positive), and AR lag coefficients are often constrained to lead to a stationary model. untransform_params is used for the reverse operation (and\nis important because starting parameters are usually specified in terms of values appropriate to the model, and we need to convert them to parameters appropriate to the optimizer before we can begin the optimization routine).",
            "markdown"
        ],
        [
            "Even though we do not need to transform or untransform our new parameters (the loadings can in theory take on any values), we still need to modify this function for two reasons:",
            "markdown"
        ],
        [
            "The version in the DynamicFactor class is expecting 3 fewer parameters than we have now. At a minimum, we need to handle the three new parameters.",
            "markdown"
        ],
        [
            "The version in the DynamicFactor class constrains the factor lag coefficients to be stationary as though it was an AR(4) model. Since we actually have an AR(2) model, we need to re-do the constraint. We also set the last two autoregressive coefficients to be zero here.",
            "markdown"
        ],
        [
            "<strong>``update``</strong>",
            "markdown"
        ],
        [
            "The most important reason we need to specify a new update method is because we have three new parameters that we need to place into the state space formulation. In particular we let the parent DynamicFactor.update class handle placing all the parameters except the three new ones in to the state space representation, and then we put the last three in manually.",
            "markdown"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "# Create the model\nextended_mod = ExtendedDFM(endog)\ninitial_extended_res = extended_mod.fit(maxiter=1000, disp=False)\nextended_res = extended_mod.fit(initial_extended_res.params, method='nm', maxiter=1000)\nprint(extended_res.summary(separate_params=False))",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 4.685876\n         Iterations: 290\n         Function evaluations: 488\n                                             Statespace Model Results\n=================================================================================================================\nDep. Variable:     ['std_indprod', 'std_income', 'std_sales', 'std_emp']   No. Observations:                  431\nModel:                                 DynamicFactor(factors=1, order=4)   Log Likelihood               -2019.612\n                                                          + AR(2) errors   AIC                           4085.225\nDate:                                                   Wed, 02 Nov 2022   BIC                           4178.745\nTime:                                                           17:12:09   HQIC                          4122.150\nSample:                                                       02-01-1979\n                                                            - 12-01-2014\nCovariance Type:                                                     opg\n====================================================================================================\n                                       coef    std err          z      P|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------------\nloading.f1.std_indprod              -0.7047      0.036    -19.635      0.000      -0.775      -0.634\nloading.f1.std_income               -0.2593      0.039     -6.701      0.000      -0.335      -0.183\nloading.f1.std_sales                -0.4496      0.023    -19.232      0.000      -0.495      -0.404\nloading.f1.std_emp                  -0.4083      0.038    -10.620      0.000      -0.484      -0.333\nsigma2.std_indprod                   0.2351      0.045      5.198      0.000       0.146       0.324\nsigma2.std_income                    0.8709      0.030     29.035      0.000       0.812       0.930\nsigma2.std_sales                     0.5222      0.035     15.087      0.000       0.454       0.590\nsigma2.std_emp                       0.2618      0.023     11.373      0.000       0.217       0.307\nL1.f1.f1                             0.2834      0.054      5.253      0.000       0.178       0.389\nL2.f1.f1                             0.3923      0.058      6.718      0.000       0.278       0.507\nL3.f1.f1                                  0   1.15e-10          0      1.000   -2.26e-10    2.26e-10\nL4.f1.f1                                  0   1.15e-10          0      1.000   -2.26e-10    2.26e-10\nL1.e(std_indprod).e(std_indprod)    -0.2123      0.121     -1.762      0.078      -0.449       0.024\nL2.e(std_indprod).e(std_indprod)    -0.1923      0.094     -2.035      0.042      -0.377      -0.007\nL1.e(std_income).e(std_income)      -0.1928      0.023     -8.459      0.000      -0.238      -0.148\nL2.e(std_income).e(std_income)      -0.0926      0.048     -1.916      0.055      -0.187       0.002\nL1.e(std_sales).e(std_sales)        -0.4850      0.047    -10.241      0.000      -0.578      -0.392\nL2.e(std_sales).e(std_sales)        -0.2287      0.050     -4.548      0.000      -0.327      -0.130\nL1.e(std_emp).e(std_emp)             0.2289      0.041      5.639      0.000       0.149       0.308\nL2.e(std_emp).e(std_emp)             0.4899      0.048     10.170      0.000       0.395       0.584\nloading.L1.f1.std_emp               -0.0855      0.037     -2.341      0.019      -0.157      -0.014\nloading.L2.f1.std_emp               -0.0066      0.036     -0.186      0.853      -0.076       0.063\nloading.L3.f1.std_emp               -0.1723      0.028     -6.074      0.000      -0.228      -0.117\n====================================================================================================\nLjung-Box (L1) (Q):     0.14, 0.01, 1.04, 4.33   Jarque-Bera (JB):   275.13, 9978.10, 26.07, 3786.11\nProb(Q):                0.70, 0.94, 0.31, 0.04   Prob(JB):                    0.00, 0.00, 0.00, 0.00\nHeteroskedasticity (H): 0.75, 4.81, 0.44, 0.43   Skew:                       0.26, -0.97, 0.26, 0.79\nProb(H) (two-sided):    0.08, 0.00, 0.00, 0.00   Kurtosis:                  6.88, 26.49, 4.09, 17.43\n====================================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n[2] Covariance matrix is singular or near-singular, with condition number 1.2e+18. Standard errors may be unstable.",
            "code"
        ],
        [
            "Although this model increases the likelihood, it is not preferred by the AIC and BIC measures which penalize the additional three parameters.",
            "markdown"
        ],
        [
            "Furthermore, the qualitative results are unchanged, as we can see from the updated \\(R^2\\) chart and the new coincident index, both of which are practically identical to the previous results.",
            "markdown"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "extended_res.plot_coefficients_of_determination(figsize=(8,2));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_34_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_34_0.png\"/>",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(13,3))\n\n# Compute the index\nextended_coincident_index = compute_coincident_index(extended_mod, extended_res)\n\n# Plot the factor\ndates = endog.index._mpl_repr()\nax.plot(dates, coincident_index, '-', linewidth=1, label='Basic model')\nax.plot(dates, extended_coincident_index, '--', linewidth=3, label='Extended model')\nax.plot(usphci.index._mpl_repr(), usphci, label='USPHCI')\nax.legend(loc='lower right')\nax.set(title='Coincident indices, comparison')\n\n# Retrieve and also plot the NBER recession indicators\nylim = ax.get_ylim()\nax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_35_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_dfm_coincident_35_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->Unobserved Components: Application": [
        [
            "In an influential article, Harvey and Jaeger (1993) described the use of unobserved components models (also known as \u201cstructural time series models\u201d) to derive stylized facts of the business cycle.",
            "markdown"
        ],
        [
            "Their paper begins:",
            "markdown"
        ],
        [
            "\"Establishing the 'stylized facts' associated with a set of time series is widely considered a crucial step\nin macroeconomic research ... For such facts to be useful they should (1) be consistent with the stochastic\nproperties of the data and (2) present meaningful information.\"",
            "code"
        ],
        [
            "In particular, they make the argument that these goals are often better met using the unobserved components approach rather than the popular Hodrick-Prescott filter or Box-Jenkins ARIMA modeling techniques.",
            "markdown"
        ],
        [
            "statsmodels has the ability to perform all three types of analysis, and below we follow the steps of their paper, using a slightly updated dataset.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display, Latex",
            "code"
        ]
    ],
    "Examples->State space models->Unobserved Components: Application->Unobserved Components": [
        [
            "The unobserved components model available in statsmodels can be written as:\n\n\\[y_t = \\underbrace{\\mu_{t}}_{\\text{trend}} + \\underbrace{\\gamma_{t}}_{\\text{seasonal}} + \\underbrace{c_{t}}_{\\text{cycle}} + \\sum_{j=1}^k \\underbrace{\\beta_j x_{jt}}_{\\text{explanatory}} + \\underbrace{\\varepsilon_t}_{\\text{irregular}}\\]",
            "markdown"
        ],
        [
            "see Durbin and Koopman 2012, Chapter 3 for notation and additional details. Notice that different specifications for the different individual components can support a wide range of models. The specific models considered in the paper and below are specializations of this general equation.",
            "markdown"
        ]
    ],
    "Examples->State space models->Unobserved Components: Application->Unobserved Components->Trend": [
        [
            "The trend component is a dynamic extension of a regression model that includes an intercept and linear time-trend.\n\n\\[\\begin{split}\\begin{align}\n\\underbrace{\\mu_{t+1}}_{\\text{level}} & = \\mu_t + \\nu_t + \\eta_{t+1} \\qquad & \\eta_{t+1} \\sim N(0, \\sigma_\\eta^2) \\\\\\\\\n\\underbrace{\\nu_{t+1}}_{\\text{trend}} & = \\nu_t + \\zeta_{t+1} & \\zeta_{t+1} \\sim N(0, \\sigma_\\zeta^2) \\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "where the level is a generalization of the intercept term that can dynamically vary across time, and the trend is a generalization of the time-trend such that the slope can dynamically vary across time.",
            "markdown"
        ],
        [
            "For both elements (level and trend), we can consider models in which:",
            "markdown"
        ],
        [
            "The element is included vs excluded (if the trend is included, there must also be a level included).",
            "markdown"
        ],
        [
            "The element is deterministic vs stochastic (i.e.\u00a0whether or not the variance on the error term is confined to be zero or not)",
            "markdown"
        ],
        [
            "The only additional parameters to be estimated via MLE are the variances of any included stochastic components.",
            "markdown"
        ],
        [
            "This leads to the following specifications:",
            "markdown"
        ]
    ],
    "Examples->State space models->Unobserved Components: Application->Unobserved Components->Seasonal": [
        [
            "The seasonal component is written as:\n\n\\[\\gamma_t = - \\sum_{j=1}^{s-1} \\gamma_{t+1-j} + \\omega_t \\qquad \\omega_t \\sim N(0, \\sigma_\\omega^2)\\]",
            "markdown"
        ],
        [
            "The periodicity (number of seasons) is s, and the defining character is that (without the error term), the seasonal components sum to zero across one complete cycle. The inclusion of an error term allows the seasonal effects to vary over time.",
            "markdown"
        ],
        [
            "The variants of this model are:",
            "markdown"
        ],
        [
            "The periodicity s",
            "markdown"
        ],
        [
            "Whether or not to make the seasonal effects stochastic.",
            "markdown"
        ],
        [
            "If the seasonal effect is stochastic, then there is one additional parameter to estimate via MLE (the variance of the error term).",
            "markdown"
        ]
    ],
    "Examples->State space models->Unobserved Components: Application->Unobserved Components->Cycle": [
        [
            "The cyclical component is intended to capture cyclical effects at time frames much longer than captured by the seasonal component. For example, in economics the cyclical term is often intended to capture the business cycle, and is then expected to have a period between \u201c1.5 and 12 years\u201d (see Durbin and Koopman).",
            "markdown"
        ],
        [
            "The cycle is written as:\n\n\\[\\begin{split}\\begin{align}\nc_{t+1} & = c_t \\cos \\lambda_c + c_t^* \\sin \\lambda_c + \\tilde \\omega_t \\qquad & \\tilde \\omega_t \\sim N(0, \\sigma_{\\tilde \\omega}^2) \\\\\\\\\nc_{t+1}^* & = -c_t \\sin \\lambda_c + c_t^* \\cos \\lambda_c + \\tilde \\omega_t^* & \\tilde \\omega_t^* \\sim N(0, \\sigma_{\\tilde \\omega}^2)\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "The parameter \\(\\lambda_c\\) (the frequency of the cycle) is an additional parameter to be estimated by MLE. If the seasonal effect is stochastic, then there is one another parameter to estimate (the variance of the error term - note that both of the error terms here share the same variance, but are assumed to have independent draws).",
            "markdown"
        ]
    ],
    "Examples->State space models->Unobserved Components: Application->Unobserved Components->Irregular": [
        [
            "The irregular component is assumed to be a white noise error term. Its variance is a parameter to be estimated by MLE; i.e.\n\n\\[\\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)\\]",
            "markdown"
        ],
        [
            "In some cases, we may want to generalize the irregular component to allow for autoregressive effects:\n\n\\[\\varepsilon_t = \\rho(L) \\varepsilon_{t-1} + \\epsilon_t, \\qquad \\epsilon_t \\sim N(0, \\sigma_\\epsilon^2)\\]",
            "markdown"
        ],
        [
            "In this case, the autoregressive parameters would also be estimated via MLE.",
            "markdown"
        ]
    ],
    "Examples->State space models->Unobserved Components: Application->Unobserved Components->Regression effects": [
        [
            "We may want to allow for explanatory variables by including additional terms\n\n\\[\\sum_{j=1}^k \\beta_j x_{jt}\\]",
            "markdown"
        ],
        [
            "or for intervention effects by including\n\n\\[\\begin{split}\\begin{align}\n\\delta w_t \\qquad \\text{where} \\qquad w_t & = 0, \\qquad t &lt; \\tau, \\\\\\\\\n& = 1, \\qquad t \\ge \\tau\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "These additional parameters could be estimated via MLE or by including them as components of the state space formulation.",
            "markdown"
        ]
    ],
    "Examples->State space models->Unobserved Components: Application->Data": [
        [
            "Following Harvey and Jaeger, we will consider the following time series:",
            "markdown"
        ],
        [
            "US real GNP, \u201coutput\u201d, ()",
            "markdown"
        ],
        [
            "US GNP implicit price deflator, \u201cprices\u201d, ()",
            "markdown"
        ],
        [
            "US monetary base, \u201cmoney\u201d, ()",
            "markdown"
        ],
        [
            "The time frame in the original paper varied across series, but was broadly 1954-1989. Below we use data from the period 1948-2008 for all series. Although the unobserved components approach allows isolating a seasonal component within the model, the series considered in the paper, and here, are already seasonally adjusted.",
            "markdown"
        ],
        [
            "All data series considered here are taken from . Conveniently, the Python library  has the ability to download data from FRED directly.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "# Datasets\nfrom pandas_datareader.data import DataReader\n\n# Get the raw data\nstart = '1948-01'\nend = '2008-01'\nus_gnp = DataReader('GNPC96', 'fred', start=start, end=end)\nus_gnp_deflator = DataReader('GNPDEF', 'fred', start=start, end=end)\nus_monetary_base = DataReader('AMBSL', 'fred', start=start, end=end).resample('QS').mean()\nrecessions = DataReader('USRECQ', 'fred', start=start, end=end).resample('QS').last().values[:,0]\n\n# Construct the dataframe\ndta = pd.concat(map(np.log, (us_gnp, us_gnp_deflator, us_monetary_base)), axis=1)\ndta.columns = ['US GNP','US Prices','US monetary base']\ndta.index.freq = dta.index.inferred_freq\ndates = dta.index._mpl_repr()",
            "code"
        ],
        [
            "To get a sense of these three variables over the timeframe, we can plot them:",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "# Plot the data\nax = dta.plot(figsize=(13,3))\nylim = ax.get_ylim()\nax.xaxis.grid()\nax.fill_between(dates, ylim[0]+1e-5, ylim[1]-1e-5, recessions, facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->Unobserved Components: Application->Model": [
        [
            "Since the data is already seasonally adjusted and there are no obvious explanatory variables, the generic model considered is:\n\n\\[y_t = \\underbrace{\\mu_{t}}_{\\text{trend}} + \\underbrace{c_{t}}_{\\text{cycle}} + \\underbrace{\\varepsilon_t}_{\\text{irregular}}\\]",
            "markdown"
        ],
        [
            "The irregular will be assumed to be white noise, and the cycle will be stochastic and damped. The final modeling choice is the specification to use for the trend component. Harvey and Jaeger consider two models:",
            "markdown"
        ],
        [
            "Local linear trend (the \u201cunrestricted\u201d model)",
            "markdown"
        ],
        [
            "Smooth trend (the \u201crestricted\u201d model, since we are forcing \\(\\sigma_\\eta = 0\\))",
            "markdown"
        ],
        [
            "Below, we construct kwargs dictionaries for each of these model types. Notice that rather that there are two ways to specify the models. One way is to specify components directly, as in the table above. The other way is to use string names which map to various specifications.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "# Model specifications\n\n# Unrestricted model, using string specification\nunrestricted_model = {\n    'level': 'local linear trend', 'cycle': True, 'damped_cycle': True, 'stochastic_cycle': True\n}\n\n# Unrestricted model, setting components directly\n# This is an equivalent, but less convenient, way to specify a\n# local linear trend model with a stochastic damped cycle:\n# unrestricted_model = {\n#     'irregular': True, 'level': True, 'stochastic_level': True, 'trend': True, 'stochastic_trend': True,\n#     'cycle': True, 'damped_cycle': True, 'stochastic_cycle': True\n# }\n\n# The restricted model forces a smooth trend\nrestricted_model = {\n    'level': 'smooth trend', 'cycle': True, 'damped_cycle': True, 'stochastic_cycle': True\n}\n\n# Restricted model, setting components directly\n# This is an equivalent, but less convenient, way to specify a\n# smooth trend model with a stochastic damped cycle. Notice\n# that the difference from the local linear trend model is that\n# `stochastic_level=False` here.\n# unrestricted_model = {\n#     'irregular': True, 'level': True, 'stochastic_level': False, 'trend': True, 'stochastic_trend': True,\n#     'cycle': True, 'damped_cycle': True, 'stochastic_cycle': True\n# }",
            "code"
        ],
        [
            "We now fit the following models:",
            "markdown"
        ],
        [
            "Output, unrestricted model",
            "markdown"
        ],
        [
            "Prices, unrestricted model",
            "markdown"
        ],
        [
            "Prices, restricted model",
            "markdown"
        ],
        [
            "Money, unrestricted model",
            "markdown"
        ],
        [
            "Money, restricted model",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "# Output\noutput_mod = sm.tsa.UnobservedComponents(dta['US GNP'], **unrestricted_model)\noutput_res = output_mod.fit(method='powell', disp=False)\n\n# Prices\nprices_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **unrestricted_model)\nprices_res = prices_mod.fit(method='powell', disp=False)\n\nprices_restricted_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **restricted_model)\nprices_restricted_res = prices_restricted_mod.fit(method='powell', disp=False)\n\n# Money\nmoney_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **unrestricted_model)\nmoney_res = money_mod.fit(method='powell', disp=False)\n\nmoney_restricted_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **restricted_model)\nmoney_restricted_res = money_restricted_mod.fit(method='powell', disp=False)",
            "code"
        ],
        [
            "Once we have fit these models, there are a variety of ways to display the information. Looking at the model of US GNP, we can summarize the fit of the model using the summary method on the fit object.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "print(output_res.summary())",
            "code"
        ],
        [
            "Unobserved Components Results\n=====================================================================================\nDep. Variable:                        US GNP   No. Observations:                  241\nModel:                    local linear trend   Log Likelihood                 770.016\n                   + damped stochastic cycle   AIC                          -1528.032\nDate:                       Wed, 02 Nov 2022   BIC                          -1507.223\nTime:                               17:10:52   HQIC                         -1519.645\nSample:                           01-01-1948\n                                - 01-01-2008\nCovariance Type:                         opg\n====================================================================================\n                       coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nsigma2.irregular  1.284e-06   7.31e-06      0.176      0.861    -1.3e-05    1.56e-05\nsigma2.level      3.263e-06   4.94e-05      0.066      0.947   -9.36e-05       0.000\nsigma2.trend      3.113e-06   1.45e-06      2.149      0.032    2.73e-07    5.95e-06\nsigma2.cycle      3.878e-05   2.55e-05      1.522      0.128   -1.12e-05    8.87e-05\nfrequency.cycle      0.4479      0.047      9.547      0.000       0.356       0.540\ndamping.cycle        0.8679      0.042     20.662      0.000       0.786       0.950\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):                 9.38\nProb(Q):                              0.98   Prob(JB):                         0.01\nHeteroskedasticity (H):               0.26   Skew:                            -0.05\nProb(H) (two-sided):                  0.00   Kurtosis:                         3.97\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "For unobserved components models, and in particular when exploring stylized facts in line with point (2) from the introduction, it is often more instructive to plot the estimated unobserved components (e.g.\u00a0the level, trend, and cycle) themselves to see if they provide a meaningful description of the data.",
            "markdown"
        ],
        [
            "The plot_components method of the fit object can be used to show plots and confidence intervals of each of the estimated states, as well as a plot of the observed data versus the one-step-ahead predictions of the model to assess fit.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "fig = output_res.plot_components(legend_loc='lower right', figsize=(15, 9));",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/tsa/statespace/structural.py:1738: RuntimeWarning: invalid value encountered in sqrt\n  std_errors = np.sqrt(component_bunch['%s_cov' % which])\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_14_1.png\" src=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_14_1.png\"/>",
            "code"
        ],
        [
            "Finally, Harvey and Jaeger summarize the models in another way to highlight the relative importances of the trend and cyclical components; below we replicate their Table I. The values we find are broadly consistent with, but different in the particulars from, the values from their table.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "# Create Table I\ntable_i = np.zeros((5,6))\n\nstart = dta.index[0]\nend = dta.index[-1]\ntime_range = '%d:%d-%d:%d' % (start.year, start.quarter, end.year, end.quarter)\nmodels = [\n    ('US GNP', time_range, 'None'),\n    ('US Prices', time_range, 'None'),\n    ('US Prices', time_range, r'$\\sigma_\\eta^2 = 0$'),\n    ('US monetary base', time_range, 'None'),\n    ('US monetary base', time_range, r'$\\sigma_\\eta^2 = 0$'),\n]\nindex = pd.MultiIndex.from_tuples(models, names=['Series', 'Time range', 'Restrictions'])\nparameter_symbols = [\n    r'$\\sigma_\\zeta^2$', r'$\\sigma_\\eta^2$', r'$\\sigma_\\kappa^2$', r'$\\rho$',\n    r'$2 \\pi / \\lambda_c$', r'$\\sigma_\\varepsilon^2$',\n]\n\ni = 0\nfor res in (output_res, prices_res, prices_restricted_res, money_res, money_restricted_res):\n    if res.model.stochastic_level:\n        (sigma_irregular, sigma_level, sigma_trend,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n    else:\n        (sigma_irregular, sigma_level,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n        sigma_trend = np.nan\n    period_cycle = 2 * np.pi / frequency_cycle\n\n    table_i[i, :] = [\n        sigma_level*1e7, sigma_trend*1e7,\n        sigma_cycle*1e7, damping_cycle, period_cycle,\n        sigma_irregular*1e7\n    ]\n    i += 1\n\npd.set_option('float_format', lambda x: '%.4g' % np.round(x, 2) if not np.isnan(x) else '-')\ntable_i = pd.DataFrame(table_i, index=index, columns=parameter_symbols)\ntable_i",
            "code"
        ],
        [
            "[8]:",
            "code"
        ]
    ],
    "Examples->State space models->Trends and cycles in unemployment": [
        [
            "Here we consider three methods for separating a trend and cycle in economic data. Supposing we have a time series \\(y_t\\), the basic idea is to decompose it into these two components:\n\n\\[y_t = \\mu_t + \\eta_t\\]",
            "markdown"
        ],
        [
            "where \\(\\mu_t\\) represents the trend or level and \\(\\eta_t\\) represents the cyclical component. In this case, we consider a stochastic trend, so that \\(\\mu_t\\) is a random variable and not a deterministic function of time. Two of methods fall under the heading of \u201cunobserved components\u201d models, and the third is the popular Hodrick-Prescott (HP) filter. Consistent with e.g.\u00a0Harvey and Jaeger (1993), we find that these models all produce similar decompositions.",
            "markdown"
        ],
        [
            "This notebook demonstrates applying these models to separate trend from cycle in the U.S. unemployment rate.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "from pandas_datareader.data import DataReader\nendog = DataReader('UNRATE', 'fred', start='1954-01-01')\nendog.index.freq = endog.index.inferred_freq",
            "code"
        ]
    ],
    "Examples->State space models->Trends and cycles in unemployment->Hodrick-Prescott (HP) filter": [
        [
            "The first method is the Hodrick-Prescott filter, which can be applied to a data series in a very straightforward method. Here we specify the parameter \\(\\lambda=129600\\) because the unemployment rate is observed monthly.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "hp_cycle, hp_trend = sm.tsa.filters.hpfilter(endog, lamb=129600)",
            "code"
        ]
    ],
    "Examples->State space models->Trends and cycles in unemployment->Unobserved components and ARIMA model (UC-ARIMA)": [
        [
            "The next method is an unobserved components model, where the trend is modeled as a random walk and the cycle is modeled with an ARIMA model - in particular, here we use an AR(4) model. The process for the time series can be written as:\n\n\\[\\begin{split}\\begin{align}\ny_t & = \\mu_t + \\eta_t \\\\\n\\mu_{t+1} & = \\mu_t + \\epsilon_{t+1} \\\\\n\\phi(L) \\eta_t & = \\nu_t\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(\\phi(L)\\) is the AR(4) lag polynomial and \\(\\epsilon_t\\) and \\(\\nu_t\\) are white noise.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "mod_ucarima = sm.tsa.UnobservedComponents(endog, 'rwalk', autoregressive=4)\n# Here the powell method is used, since it achieves a\n# higher loglikelihood than the default L-BFGS method\nres_ucarima = mod_ucarima.fit(method='powell', disp=False)\nprint(res_ucarima.summary())",
            "code"
        ],
        [
            "Unobserved Components Results\n==============================================================================\nDep. Variable:                 UNRATE   No. Observations:                  825\nModel:                    random walk   Log Likelihood                -458.267\n                              + AR(4)   AIC                            928.535\nDate:                Wed, 02 Nov 2022   BIC                            956.820\nTime:                        17:11:00   HQIC                           939.386\nSample:                    01-01-1954\n                         - 09-01-2022\nCovariance Type:                  opg\n================================================================================\n                   coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nsigma2.level     0.0784      0.168      0.466      0.641      -0.251       0.408\nsigma2.ar        0.0980      0.171      0.574      0.566      -0.236       0.432\nar.L1            1.0638      0.116      9.203      0.000       0.837       1.290\nar.L2           -0.1799      0.327     -0.551      0.582      -0.820       0.460\nar.L3            0.0916      0.199      0.461      0.645      -0.298       0.481\nar.L4           -0.0214      0.082     -0.260      0.795      -0.183       0.140\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):           6101480.12\nProb(Q):                              0.95   Prob(JB):                         0.00\nHeteroskedasticity (H):               9.36   Skew:                            17.14\nProb(H) (two-sided):                  0.00   Kurtosis:                       423.16\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ]
    ],
    "Examples->State space models->Trends and cycles in unemployment->Unobserved components with stochastic cycle (UC)": [
        [
            "The final method is also an unobserved components model, but where the cycle is modeled explicitly.\n\n\\[\\begin{split}\\begin{align}\ny_t & = \\mu_t + \\eta_t \\\\\n\\mu_{t+1} & = \\mu_t + \\epsilon_{t+1} \\\\\n\\eta_{t+1} & = \\eta_t \\cos \\lambda_\\eta + \\eta_t^* \\sin \\lambda_\\eta + \\tilde \\omega_t \\qquad & \\tilde \\omega_t \\sim N(0, \\sigma_{\\tilde \\omega}^2) \\\\\n\\eta_{t+1}^* & = -\\eta_t \\sin \\lambda_\\eta + \\eta_t^* \\cos \\lambda_\\eta + \\tilde \\omega_t^* & \\tilde \\omega_t^* \\sim N(0, \\sigma_{\\tilde \\omega}^2)\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "mod_uc = sm.tsa.UnobservedComponents(\n    endog, 'rwalk',\n    cycle=True, stochastic_cycle=True, damped_cycle=True,\n)\n# Here the powell method gets close to the optimum\nres_uc = mod_uc.fit(method='powell', disp=False)\n# but to get to the highest loglikelihood we do a\n# second round using the L-BFGS method.\nres_uc = mod_uc.fit(res_uc.params, disp=False)\nprint(res_uc.summary())",
            "code"
        ],
        [
            "Unobserved Components Results\n=====================================================================================\nDep. Variable:                        UNRATE   No. Observations:                  825\nModel:                           random walk   Log Likelihood                -461.552\n                   + damped stochastic cycle   AIC                            931.103\nDate:                       Wed, 02 Nov 2022   BIC                            949.950\nTime:                               17:11:00   HQIC                           938.334\nSample:                           01-01-1954\n                                - 09-01-2022\nCovariance Type:                         opg\n===================================================================================\n                      coef    std err          z      P|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nsigma2.level        0.1419      0.022      6.416      0.000       0.099       0.185\nsigma2.cycle        0.0279      0.021      1.344      0.179      -0.013       0.069\nfrequency.cycle     0.3491      0.206      1.698      0.090      -0.054       0.752\ndamping.cycle       0.7710      0.068     11.386      0.000       0.638       0.904\n===================================================================================\nLjung-Box (L1) (Q):                   1.72   Jarque-Bera (JB):           6215485.79\nProb(Q):                              0.19   Prob(JB):                         0.00\nHeteroskedasticity (H):               9.34   Skew:                            17.32\nProb(H) (two-sided):                  0.00   Kurtosis:                       427.59\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"",
            "code"
        ]
    ],
    "Examples->State space models->Trends and cycles in unemployment->Graphical comparison": [
        [
            "The output of each of these models is an estimate of the trend component \\(\\mu_t\\) and an estimate of the cyclical component \\(\\eta_t\\). Qualitatively the estimates of trend and cycle are very similar, although the trend component from the HP filter is somewhat more variable than those from the unobserved components models. This means that relatively mode of the movement in the unemployment rate is attributed to changes in the underlying trend rather than to temporary cyclical movements.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "fig, axes = plt.subplots(2, figsize=(13,5));\naxes[0].set(title='Level/trend component')\naxes[0].plot(endog.index, res_uc.level.smoothed, label='UC')\naxes[0].plot(endog.index, res_ucarima.level.smoothed, label='UC-ARIMA(2,0)')\naxes[0].plot(hp_trend, label='HP Filter')\naxes[0].legend(loc='upper left')\naxes[0].grid()\n\naxes[1].set(title='Cycle component')\naxes[1].plot(endog.index, res_uc.cycle.smoothed, label='UC')\naxes[1].plot(endog.index, res_ucarima.autoregressive.smoothed, label='UC-ARIMA(2,0)')\naxes[1].plot(hp_cycle, label='HP Filter')\naxes[1].legend(loc='upper left')\naxes[1].grid()\n\nfig.tight_layout();\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_cycles_11_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_cycles_11_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->State space modeling: Local Linear Trends": [
        [
            "This notebook describes how to extend the statsmodels statespace classes to create and estimate a custom model. Here we develop a local linear trend model.",
            "markdown"
        ],
        [
            "The Local Linear Trend model has the form (see Durbin and Koopman 2012, Chapter 3.2 for all notation and details):\n\n\\[\\begin{split}\\begin{align}\ny_t & = \\mu_t + \\varepsilon_t \\qquad & \\varepsilon_t \\sim\n    N(0, \\sigma_\\varepsilon^2) \\\\\n\\mu_{t+1} & = \\mu_t + \\nu_t + \\xi_t & \\xi_t \\sim N(0, \\sigma_\\xi^2) \\\\\n\\nu_{t+1} & = \\nu_t + \\zeta_t & \\zeta_t \\sim N(0, \\sigma_\\zeta^2)\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "It is easy to see that this can be cast into state space form as:\n\n\\[\\begin{split}\\begin{align}\ny_t & = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\mu_t \\\\ \\nu_t \\end{pmatrix} + \\varepsilon_t \\\\\n\\begin{pmatrix} \\mu_{t+1} \\\\ \\nu_{t+1} \\end{pmatrix} & = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{pmatrix} \\mu_t \\\\ \\nu_t \\end{pmatrix} + \\begin{pmatrix} \\xi_t \\\\ \\zeta_t \\end{pmatrix}\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "Notice that much of the state space representation is composed of known values; in fact the only parts in which parameters to be estimated appear are in the variance / covariance matrices:\n\n\\[\\begin{split}\\begin{align}\nH_t & = \\begin{bmatrix} \\sigma_\\varepsilon^2 \\end{bmatrix} \\\\\nQ_t & = \\begin{bmatrix} \\sigma_\\xi^2 & 0 \\\\ 0 & \\sigma_\\zeta^2 \\end{bmatrix}\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "code"
        ],
        [
            "To take advantage of the existing infrastructure, including Kalman filtering and maximum likelihood estimation, we create a new class which extends from statsmodels.tsa.statespace.MLEModel. There are a number of things that must be specified:",
            "markdown"
        ],
        [
            "<strong>k_states</strong>, <strong>k_posdef</strong>: These two parameters must be provided to the base classes in initialization. The inform the statespace model about the size of, respectively, the state vector, above \\(\\begin{pmatrix} \\mu_t & \\nu_t \\end{pmatrix}'\\), and the state error vector, above \\(\\begin{pmatrix} \\xi_t & \\zeta_t \\end{pmatrix}'\\). Note that the dimension of the endogenous vector does not have to be specified, since it can be inferred from the endog array.",
            "markdown"
        ],
        [
            "<strong>update</strong>: The method update, with argument params, must be specified (it is used when fit() is called to calculate the MLE). It takes the parameters and fills them into the appropriate state space matrices. For example, below, the params vector contains variance parameters \\(\\begin{pmatrix} \\sigma_\\varepsilon^2 & \\sigma_\\xi^2 & \\sigma_\\zeta^2\\end{pmatrix}\\), and the update method must place them in the observation and state covariance matrices. More generally, the\nparameter vector might be mapped into many different places in all of the statespace matrices.",
            "markdown"
        ],
        [
            "<strong>statespace matrices</strong>: by default, all state space matrices (obs_intercept, design, obs_cov, state_intercept, transition, selection, state_cov) are set to zeros. Values that are fixed (like the ones in the design and transition matrices here) can be set in initialization, whereas values that vary with the parameters should be set in the update method. Note that it is easy to forget to set the selection matrix, which is often just the identity matrix (as it is here), but not setting\nit will lead to a very different model (one where there is not a stochastic component to the transition equation).",
            "markdown"
        ],
        [
            "<strong>start params</strong>: start parameters must be set, even if it is just a vector of zeros, although often good start parameters can be found from the data. Maximum likelihood estimation by gradient methods (as employed here) can be sensitive to the starting parameters, so it is important to select good ones if possible. Here it does not matter too much (although as variances, they should\u2019t be set zero).",
            "markdown"
        ],
        [
            "<strong>initialization</strong>: in addition to defined state space matrices, all state space models must be initialized with the mean and variance for the initial distribution of the state vector. If the distribution is known, initialize_known(initial_state, initial_state_cov) can be called, or if the model is stationary (e.g.\u00a0an ARMA model), initialize_stationary can be used. Otherwise, initialize_approximate_diffuse is a reasonable generic initialization (exact diffuse initialization is not\nyet available). Since the local linear trend model is not stationary (it is composed of random walks) and since the distribution is not generally known, we use initialize_approximate_diffuse below.",
            "markdown"
        ],
        [
            "The above are the minimum necessary for a successful model. There are also a number of things that do not have to be set, but which may be helpful or important for some applications:",
            "markdown"
        ],
        [
            "<strong>transform / untransform</strong>: when fit is called, the optimizer in the background will use gradient methods to select the parameters that maximize the likelihood function. By default it uses unbounded optimization, which means that it may select any parameter value. In many cases, that is not the desired behavior; variances, for example, cannot be negative. To get around this, the transform method takes the unconstrained vector of parameters provided by the optimizer and returns a\nconstrained vector of parameters used in likelihood evaluation. untransform provides the reverse operation.",
            "markdown"
        ],
        [
            "<strong>param_names</strong>: this internal method can be used to set names for the estimated parameters so that e.g.\u00a0the summary provides meaningful names. If not present, parameters are named param0, param1, etc.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "\"\"\"\nUnivariate Local Linear Trend Model\n\"\"\"\nclass LocalLinearTrend(sm.tsa.statespace.MLEModel):\n    def __init__(self, endog):\n        # Model order\n        k_states = k_posdef = 2\n\n        # Initialize the statespace\n        super(LocalLinearTrend, self).__init__(\n            endog, k_states=k_states, k_posdef=k_posdef,\n            initialization='approximate_diffuse',\n            loglikelihood_burn=k_states\n        )\n\n        # Initialize the matrices\n        self.ssm['design'] = np.array([1, 0])\n        self.ssm['transition'] = np.array([[1, 1],\n                                       [0, 1]])\n        self.ssm['selection'] = np.eye(k_states)\n\n        # Cache some indices\n        self._state_cov_idx = ('state_cov',) + np.diag_indices(k_posdef)\n\n    @property\n    def param_names(self):\n        return ['sigma2.measurement', 'sigma2.level', 'sigma2.trend']\n\n    @property\n    def start_params(self):\n        return [np.std(self.endog)]*3\n\n    def transform_params(self, unconstrained):\n        return unconstrained**2\n\n    def untransform_params(self, constrained):\n        return constrained**0.5\n\n    def update(self, params, *args, **kwargs):\n        params = super(LocalLinearTrend, self).update(params, *args, **kwargs)\n\n        # Observation covariance\n        self.ssm['obs_cov',0,0] = params[0]\n\n        # State covariance\n        self.ssm[self._state_cov_idx] = params[1:]",
            "code"
        ],
        [
            "Using this simple model, we can estimate the parameters from a local linear trend model. The following example is from Commandeur and Koopman (2007), section 3.4., modeling motor vehicle fatalities in Finland.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "import requests\nfrom io import BytesIO\nfrom zipfile import ZipFile\n\n# Download the dataset\nck = requests.get('http://staff.feweb.vu.nl/koopman/projects/ckbook/OxCodeAll.zip').content\nzipped = ZipFile(BytesIO(ck))\ndf = pd.read_table(\n    BytesIO(zipped.read('OxCodeIntroStateSpaceBook/Chapter_2/NorwayFinland.txt')),\n    skiprows=1, header=None, sep='\\s+', engine='python',\n    names=['date','nf', 'ff']\n)",
            "code"
        ],
        [
            "Since we defined the local linear trend model as extending from MLEModel, the fit() method is immediately available, just as in other statsmodels maximum likelihood classes. Similarly, the returned results class supports many of the same post-estimation results, like the summary method.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "# Load Dataset\ndf.index = pd.date_range(start='%d-01-01' % df.date[0], end='%d-01-01' % df.iloc[-1, 0], freq='AS')\n\n# Log transform\ndf['lff'] = np.log(df['ff'])\n\n# Setup the model\nmod = LocalLinearTrend(df['lff'])\n\n# Fit it using MLE (recall that we are fitting the three variance parameters)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "code"
        ],
        [
            "Statespace Model Results\n==============================================================================\nDep. Variable:                    lff   No. Observations:                   34\nModel:               LocalLinearTrend   Log Likelihood                  27.510\nDate:                Wed, 02 Nov 2022   AIC                            -49.020\nTime:                        17:02:10   BIC                            -44.623\nSample:                    01-01-1970   HQIC                           -47.563\n                         - 01-01-2003\nCovariance Type:                  opg\n======================================================================================\n                         coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nsigma2.measurement     0.0010      0.003      0.346      0.730      -0.005       0.007\nsigma2.level           0.0074      0.005      1.564      0.118      -0.002       0.017\nsigma2.trend        2.501e-11      0.000   1.67e-07      1.000      -0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):                 0.68\nProb(Q):                              0.95   Prob(JB):                         0.71\nHeteroskedasticity (H):               0.75   Skew:                            -0.02\nProb(H) (two-sided):                  0.64   Kurtosis:                         2.29\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "Finally, we can do post-estimation prediction and forecasting. Notice that the end period can be specified as a date.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "# Perform prediction and forecasting\npredict = res.get_prediction()\nforecast = res.get_forecast('2014')",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(10,4))\n\n# Plot the results\ndf['lff'].plot(ax=ax, style='k.', label='Observations')\npredict.predicted_mean.plot(ax=ax, label='One-step-ahead Prediction')\npredict_ci = predict.conf_int(alpha=0.05)\npredict_index = np.arange(len(predict_ci))\nax.fill_between(predict_index[2:], predict_ci.iloc[2:, 0], predict_ci.iloc[2:, 1], alpha=0.1)\n\nforecast.predicted_mean.plot(ax=ax, style='r', label='Forecast')\nforecast_ci = forecast.conf_int()\nforecast_index = np.arange(len(predict_ci), len(predict_ci) + len(forecast_ci))\nax.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.1)\n\n# Cleanup the image\nax.set_ylim((4, 8));\nlegend = ax.legend(loc='lower left');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_local_linear_trend_11_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_local_linear_trend_11_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->State space modeling: Local Linear Trends->References": [
        [
            "Commandeur, Jacques J. F., and Siem Jan Koopman. 2007.\nAn Introduction to State Space Time Series Analysis.\nOxford\u202f; New York: Oxford University Press.\n\nDurbin, James, and Siem Jan Koopman. 2012.\nTime Series Analysis by State Space Methods: Second Edition.\nOxford University Press.",
            "code"
        ]
    ],
    "Examples->State space models->Statespace ARMA: Sunspots Data": [
        [
            "This notebook replicates the existing ARMA notebook using the statsmodels.tsa.statespace.SARIMAX class rather than the statsmodels.tsa.ARMA class.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "from statsmodels.graphics.api import qqplot",
            "code"
        ]
    ],
    "Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data": [
        [
            "[4]:",
            "code"
        ],
        [
            "print(sm.datasets.sunspots.NOTE)",
            "code"
        ],
        [
            "::\n\n    Number of Observations - 309 (Annual 1700 - 2008)\n    Number of Variables - 1\n    Variable name definitions::\n\n        SUNACTIVITY - Number of sunspots for each year\n\n    The data file contains a 'YEAR' variable that is not returned by load.",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "dta = sm.datasets.sunspots.load_pandas().data",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "dta.index = pd.Index(pd.date_range(\"1700\", end=\"2009\", freq=\"A-DEC\"))\ndel dta[\"YEAR\"]",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "dta.plot(figsize=(12,4));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_arma_0_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_arma_0_9_0.png\"/>",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(dta.values.squeeze(), lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(dta, lags=40, ax=ax2)",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/graphics/tsaplots.py:348: FutureWarning: The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n  warnings.warn(\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_arma_0_10_1.png\" src=\"../../../_images/examples_notebooks_generated_statespace_arma_0_10_1.png\"/>",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "arma_mod20 = sm.tsa.statespace.SARIMAX(dta, order=(2,0,0), trend='c').fit(disp=False)\nprint(arma_mod20.params)",
            "code"
        ],
        [
            "intercept     14.793947\nar.L1          1.390659\nar.L2         -0.688568\nsigma2       274.761105\ndtype: float64",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "arma_mod30 = sm.tsa.statespace.SARIMAX(dta, order=(3,0,0), trend='c').fit(disp=False)",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "print(arma_mod20.aic, arma_mod20.bic, arma_mod20.hqic)",
            "code"
        ],
        [
            "2622.636338141521 2637.5697032491116 2628.606725986767",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "print(arma_mod30.params)",
            "code"
        ],
        [
            "intercept     16.762205\nar.L1          1.300810\nar.L2         -0.508122\nar.L3         -0.129612\nsigma2       270.102651\ndtype: float64",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "print(arma_mod30.aic, arma_mod30.bic, arma_mod30.hqic)",
            "code"
        ],
        [
            "2619.4036296635304 2638.0703360480193 2626.866614470088",
            "code"
        ],
        [
            "Does our model obey the theory?",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "sm.stats.durbin_watson(arma_mod30.resid)",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "1.9564844805409447",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12,4))\nax = fig.add_subplot(111)\nax = plt.plot(arma_mod30.resid)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_arma_0_18_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_arma_0_18_0.png\"/>",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "resid = arma_mod30.resid",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "stats.normaltest(resid)",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "NormaltestResult(statistic=49.847006530010574, pvalue=1.4992016872414017e-11)",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12,4))\nax = fig.add_subplot(111)\nfig = qqplot(resid, line='q', ax=ax, fit=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_arma_0_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_arma_0_21_0.png\"/>",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(resid, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(resid, lags=40, ax=ax2)",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/graphics/tsaplots.py:348: FutureWarning: The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n  warnings.warn(\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_arma_0_22_1.png\" src=\"../../../_images/examples_notebooks_generated_statespace_arma_0_22_1.png\"/>",
            "code"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "r,q,p = sm.tsa.acf(resid, fft=True, qstat=True)\ndata = np.c_[r[1:], q, p]\nindex = pd.Index(range(1,q.shape[0]+1), name=\"lag\")\ntable = pd.DataFrame(data, columns=[\"AC\", \"Q\", \"Prob(Q)\"], index=index)\nprint(table)",
            "code"
        ],
        [
            "AC          Q      Prob(Q)\nlag\n1    0.009176   0.026273  8.712350e-01\n2    0.041820   0.573727  7.506142e-01\n3   -0.001342   0.574292  9.022915e-01\n4    0.136064   6.407488  1.707135e-01\n5    0.092433   9.108334  1.048203e-01\n6    0.091919  11.788018  6.686843e-02\n7    0.068735  13.291374  6.531942e-02\n8   -0.015021  13.363411  9.994250e-02\n9    0.187599  24.636915  3.400198e-03\n10   0.213724  39.317881  2.233182e-05\n11   0.201092  52.358270  2.347759e-07\n12   0.117192  56.802110  8.581666e-08\n13  -0.014051  56.866210  1.895534e-07\n14   0.015394  56.943403  4.001105e-07\n15  -0.024986  57.147464  7.747084e-07\n16   0.080892  59.293626  6.880520e-07\n17   0.041120  59.850085  1.112486e-06\n18  -0.052030  60.744064  1.550379e-06\n19   0.062500  62.038494  1.833802e-06\n20  -0.010292  62.073718  3.385223e-06\n21   0.074467  63.924062  3.196544e-06\n22   0.124962  69.152771  8.984833e-07\n23   0.093170  72.069532  5.802915e-07\n24  -0.082149  74.345042  4.715786e-07",
            "code"
        ],
        [
            "This indicates a lack of fit.",
            "markdown"
        ],
        [
            "In-sample dynamic prediction. How good does our model do?",
            "markdown"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "predict_sunspots = arma_mod30.predict(start='1990', end='2012', dynamic=True)",
            "code"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(12, 8))\ndta.loc['1950':].plot(ax=ax)\npredict_sunspots.plot(ax=ax, style='r');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_arma_0_27_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_arma_0_27_0.png\"/>",
            "code"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "def mean_forecast_err(y, yhat):\n    return y.sub(yhat).mean()",
            "code"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "mean_forecast_err(dta.SUNACTIVITY, predict_sunspots)",
            "code"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "5.63554983393475",
            "code"
        ]
    ],
    "Examples->State space models->Seasonality in Time Series Data": [
        [
            "Consider the problem of modeling time series data with multiple seasonal components with different periodicities. Let us take the time series \\(y_t\\) and decompose it explicitly to have a level component and two seasonal components.\n\n\\[y_t = \\mu_t + \\gamma^{(1)}_t + \\gamma^{(2)}_t\\]",
            "markdown"
        ],
        [
            "where \\(\\mu_t\\) represents the trend or level, \\(\\gamma^{(1)}_t\\) represents a seasonal component with a relatively short period, and \\(\\gamma^{(2)}_t\\) represents another seasonal component of longer period. We will have a fixed intercept term for our level and consider both \\(\\gamma^{(2)}_t\\) and \\(\\gamma^{(2)}_t\\) to be stochastic so that the seasonal patterns can vary over time.",
            "markdown"
        ],
        [
            "In this notebook, we will generate synthetic data conforming to this model and showcase modeling of the seasonal terms in a few different ways under the unobserved components modeling framework.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nplt.rc(\"figure\", figsize=(16,8))\nplt.rc(\"font\", size=14)",
            "code"
        ]
    ],
    "Examples->State space models->Seasonality in Time Series Data->Synthetic data creation": [
        [
            "We will create data with multiple seasonal patterns by following equations (3.7) and (3.8) in Durbin and Koopman (2012). We will simulate 300 periods and two seasonal terms parametrized in the frequency domain having periods 10 and 100, respectively, and 3 and 2 number of harmonics, respectively. Further, the variances of their stochastic parts are 4 and 9, respectively.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "# First we'll simulate the synthetic data\ndef simulate_seasonal_term(periodicity, total_cycles, noise_std=1.,\n                           harmonics=None):\n    duration = periodicity * total_cycles\n    assert duration == int(duration)\n    duration = int(duration)\n    harmonics = harmonics if harmonics else int(np.floor(periodicity / 2))\n\n    lambda_p = 2 * np.pi / float(periodicity)\n\n    gamma_jt = noise_std * np.random.randn((harmonics))\n    gamma_star_jt = noise_std * np.random.randn((harmonics))\n\n    total_timesteps = 100 * duration # Pad for burn in\n    series = np.zeros(total_timesteps)\n    for t in range(total_timesteps):\n        gamma_jtp1 = np.zeros_like(gamma_jt)\n        gamma_star_jtp1 = np.zeros_like(gamma_star_jt)\n        for j in range(1, harmonics + 1):\n            cos_j = np.cos(lambda_p * j)\n            sin_j = np.sin(lambda_p * j)\n            gamma_jtp1[j - 1] = (gamma_jt[j - 1] * cos_j\n                                 + gamma_star_jt[j - 1] * sin_j\n                                 + noise_std * np.random.randn())\n            gamma_star_jtp1[j - 1] = (- gamma_jt[j - 1] * sin_j\n                                      + gamma_star_jt[j - 1] * cos_j\n                                      + noise_std * np.random.randn())\n        series[t] = np.sum(gamma_jtp1)\n        gamma_jt = gamma_jtp1\n        gamma_star_jt = gamma_star_jtp1\n    wanted_series = series[-duration:] # Discard burn in\n\n    return wanted_series",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "duration = 100 * 3\nperiodicities = [10, 100]\nnum_harmonics = [3, 2]\nstd = np.array([2, 3])\nnp.random.seed(8678309)\n\nterms = []\nfor ix, _ in enumerate(periodicities):\n    s = simulate_seasonal_term(\n        periodicities[ix],\n        duration / periodicities[ix],\n        harmonics=num_harmonics[ix],\n        noise_std=std[ix])\n    terms.append(s)\nterms.append(np.ones_like(terms[0]) * 10.)\nseries = pd.Series(np.sum(terms, axis=0))\ndf = pd.DataFrame(data={'total': series,\n                        '10(3)': terms[0],\n                        '100(2)': terms[1],\n                        'level':terms[2]})\nh1, = plt.plot(df['total'])\nh2, = plt.plot(df['10(3)'])\nh3, = plt.plot(df['100(2)'])\nh4, = plt.plot(df['level'])\nplt.legend(['total','10(3)','100(2)', 'level'])\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_5_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_5_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->Seasonality in Time Series Data->Unobserved components (frequency domain modeling)": [
        [
            "The next method is an unobserved components model, where the trend is modeled as a fixed intercept and the seasonal components are modeled using trigonometric functions with primary periodicities of 10 and 100, respectively, and number of harmonics 3 and 2, respectively. Note that this is the correct, generating model. The process for the time series can be written as:\n\n\\[\\begin{split}\\begin{align}\ny_t & = \\mu_t + \\gamma^{(1)}_t + \\gamma^{(2)}_t + \\epsilon_t\\\\\n\\mu_{t+1} & = \\mu_t \\\\\n\\gamma^{(1)}_{t} &= \\sum_{j=1}^2 \\gamma^{(1)}_{j, t} \\\\\n\\gamma^{(2)}_{t} &= \\sum_{j=1}^3 \\gamma^{(2)}_{j, t}\\\\\n\\gamma^{(1)}_{j, t+1} &= \\gamma^{(1)}_{j, t}\\cos(\\lambda_j) + \\gamma^{*, (1)}_{j, t}\\sin(\\lambda_j) + \\omega^{(1)}_{j,t}, ~j = 1, 2, 3\\\\\n\\gamma^{*, (1)}_{j, t+1} &= -\\gamma^{(1)}_{j, t}\\sin(\\lambda_j) + \\gamma^{*, (1)}_{j, t}\\cos(\\lambda_j) + \\omega^{*, (1)}_{j, t}, ~j = 1, 2, 3\\\\\n\\gamma^{(2)}_{j, t+1} &= \\gamma^{(2)}_{j, t}\\cos(\\lambda_j) + \\gamma^{*, (2)}_{j, t}\\sin(\\lambda_j) + \\omega^{(2)}_{j,t}, ~j = 1, 2\\\\\n\\gamma^{*, (2)}_{j, t+1} &= -\\gamma^{(2)}_{j, t}\\sin(\\lambda_j) + \\gamma^{*, (2)}_{j, t}\\cos(\\lambda_j) + \\omega^{*, (2)}_{j, t}, ~j = 1, 2\\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(\\epsilon_t\\) is white noise, \\(\\omega^{(1)}_{j,t}\\) are i.i.d. \\(N(0, \\sigma^2_1)\\), and \\(\\omega^{(2)}_{j,t}\\) are i.i.d. \\(N(0, \\sigma^2_2)\\), where \\(\\sigma_1 = 2.\\)",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "model = sm.tsa.UnobservedComponents(series.values,\n                                    level='fixed intercept',\n                                    freq_seasonal=[{'period': 10,\n                                                    'harmonics': 3},\n                                                   {'period': 100,\n                                                    'harmonics': 2}])\nres_f = model.fit(disp=False)\nprint(res_f.summary())\n# The first state variable holds our estimate of the intercept\nprint(\"fixed intercept estimated as {0:.3f}\".format(res_f.smoother_results.smoothed_state[0,-1:][0]))\n\nres_f.plot_components()\nplt.show()\n<br/>",
            "code"
        ],
        [
            "Unobserved Components Results\n==============================================================================================\nDep. Variable:                                      y   No. Observations:                  300\nModel:                                fixed intercept   Log Likelihood               -1145.631\n                    + stochastic freq_seasonal(10(3))   AIC                           2295.261\n                   + stochastic freq_seasonal(100(2))   BIC                           2302.594\nDate:                                Wed, 02 Nov 2022   HQIC                          2298.200\nTime:                                        17:09:52\nSample:                                             0\n                                                - 300\nCovariance Type:                                  opg\n===============================================================================================\n                                  coef    std err          z      P|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nsigma2.freq_seasonal_10(3)      4.5942      0.565      8.126      0.000       3.486       5.702\nsigma2.freq_seasonal_100(2)     9.7904      2.483      3.942      0.000       4.923      14.658\n===================================================================================\nLjung-Box (L1) (Q):                   0.06   Jarque-Bera (JB):                 0.08\nProb(Q):                              0.81   Prob(JB):                         0.96\nHeteroskedasticity (H):               1.17   Skew:                             0.01\nProb(H) (two-sided):                  0.45   Kurtosis:                         3.08\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\nfixed intercept estimated as 4.053\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_7_1.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_7_1.png\"/>",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "model.ssm.transition[:, :, 0]",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "array([[ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 0.        ,  0.80901699,  0.58778525,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 0.        , -0.58778525,  0.80901699,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.30901699,  0.95105652,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 0.        ,  0.        ,  0.        , -0.95105652,  0.30901699,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        -0.30901699,  0.95105652,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        -0.95105652, -0.30901699,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.99802673,  0.06279052,  0.        ,\n         0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        , -0.06279052,  0.99802673,  0.        ,\n         0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.9921147 ,\n         0.12533323],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        , -0.12533323,\n         0.9921147 ]])",
            "code"
        ],
        [
            "Observe that the fitted variances are pretty close to the true variances of 4 and 9. Further, the individual seasonal components look pretty close to the true seasonal components. The smoothed level term is kind of close to the true level of 10. Finally, our diagnostics look solid; the test statistics are small enough to fail to reject our three tests.",
            "markdown"
        ]
    ],
    "Examples->State space models->Seasonality in Time Series Data->Unobserved components (mixed time and frequency domain modeling)": [
        [
            "The second method is an unobserved components model, where the trend is modeled as a fixed intercept and the seasonal components are modeled using 10 constants summing to 0 and trigonometric functions with a primary periodicities of 100 with 2 harmonics total. Note that this is not the generating model, as it presupposes that there are more state errors for the shorter seasonal component than in reality. The process for the time series can be written as:\n\n\\[\\begin{split}\\begin{align}\ny_t & = \\mu_t + \\gamma^{(1)}_t + \\gamma^{(2)}_t + \\epsilon_t\\\\\n\\mu_{t+1} & = \\mu_t \\\\\n\\gamma^{(1)}_{t + 1} &= - \\sum_{j=1}^9 \\gamma^{(1)}_{t + 1 - j} + \\omega^{(1)}_t\\\\\n\\gamma^{(2)}_{j, t+1} &= \\gamma^{(2)}_{j, t}\\cos(\\lambda_j) + \\gamma^{*, (2)}_{j, t}\\sin(\\lambda_j) + \\omega^{(2)}_{j,t}, ~j = 1, 2\\\\\n\\gamma^{*, (2)}_{j, t+1} &= -\\gamma^{(2)}_{j, t}\\sin(\\lambda_j) + \\gamma^{*, (2)}_{j, t}\\cos(\\lambda_j) + \\omega^{*, (2)}_{j, t}, ~j = 1, 2\\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(\\epsilon_t\\) is white noise, \\(\\omega^{(1)}_{t}\\) are i.i.d. \\(N(0, \\sigma^2_1)\\), and \\(\\omega^{(2)}_{j,t}\\) are i.i.d. \\(N(0, \\sigma^2_2)\\).",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "model = sm.tsa.UnobservedComponents(series,\n                                    level='fixed intercept',\n                                    seasonal=10,\n                                    freq_seasonal=[{'period': 100,\n                                                    'harmonics': 2}])\nres_tf = model.fit(disp=False)\nprint(res_tf.summary())\n# The first state variable holds our estimate of the intercept\nprint(\"fixed intercept estimated as {0:.3f}\".format(res_tf.smoother_results.smoothed_state[0,-1:][0]))\n\nfig = res_tf.plot_components()\nfig.tight_layout(pad=1.0)",
            "code"
        ],
        [
            "Unobserved Components Results\n==============================================================================================\nDep. Variable:                                      y   No. Observations:                  300\nModel:                                fixed intercept   Log Likelihood               -1238.113\n                            + stochastic seasonal(10)   AIC                           2480.226\n                   + stochastic freq_seasonal(100(2))   BIC                           2487.538\nDate:                                Wed, 02 Nov 2022   HQIC                          2483.157\nTime:                                        17:09:53\nSample:                                             0\n                                                - 300\nCovariance Type:                                  opg\n===============================================================================================\n                                  coef    std err          z      P|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nsigma2.seasonal                55.2934      7.114      7.773      0.000      41.351      69.236\nsigma2.freq_seasonal_100(2)    28.6897      4.008      7.159      0.000      20.835      36.544\n===================================================================================\nLjung-Box (L1) (Q):                  26.35   Jarque-Bera (JB):                 1.20\nProb(Q):                              0.00   Prob(JB):                         0.55\nHeteroskedasticity (H):               1.27   Skew:                            -0.14\nProb(H) (two-sided):                  0.24   Kurtosis:                         2.87\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\nfixed intercept estimated as 4.468\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_11_1.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_11_1.png\"/>",
            "code"
        ],
        [
            "The plotted components look good. However, the estimated variance of the second seasonal term is inflated from reality. Additionally, we reject the Ljung-Box statistic, indicating we may have remaining autocorrelation after accounting for our components.",
            "markdown"
        ]
    ],
    "Examples->State space models->Seasonality in Time Series Data->Unobserved components (lazy frequency domain modeling)": [
        [
            "The third method is an unobserved components model with a fixed intercept and one seasonal component, which is modeled using trigonometric functions with primary periodicity 100 and 50 harmonics. Note that this is not the generating model, as it presupposes that there are more harmonics then in reality. Because the variances are tied together, we are not able to drive the estimated covariance of the non-existent harmonics to 0. What is lazy about this model specification is that we have not\nbothered to specify the two different seasonal components and instead chosen to model them using a single component with enough harmonics to cover both. We will not be able to capture any differences in variances between the two true components. The process for the time series can be written as:\n\n\\[\\begin{split}\\begin{align}\ny_t & = \\mu_t + \\gamma^{(1)}_t + \\epsilon_t\\\\\n\\mu_{t+1} &= \\mu_t\\\\\n\\gamma^{(1)}_{t} &= \\sum_{j=1}^{50}\\gamma^{(1)}_{j, t}\\\\\n\\gamma^{(1)}_{j, t+1} &= \\gamma^{(1)}_{j, t}\\cos(\\lambda_j) + \\gamma^{*, (1)}_{j, t}\\sin(\\lambda_j) + \\omega^{(1}_{j,t}, ~j = 1, 2, \\dots, 50\\\\\n\\gamma^{*, (1)}_{j, t+1} &= -\\gamma^{(1)}_{j, t}\\sin(\\lambda_j) + \\gamma^{*, (1)}_{j, t}\\cos(\\lambda_j) + \\omega^{*, (1)}_{j, t}, ~j = 1, 2, \\dots, 50\\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(\\epsilon_t\\) is white noise, \\(\\omega^{(1)}_{t}\\) are i.i.d. \\(N(0, \\sigma^2_1)\\).",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "model = sm.tsa.UnobservedComponents(series,\n                                    level='fixed intercept',\n                                    freq_seasonal=[{'period': 100}])\nres_lf = model.fit(disp=False)\nprint(res_lf.summary())\n# The first state variable holds our estimate of the intercept\nprint(\"fixed intercept estimated as {0:.3f}\".format(res_lf.smoother_results.smoothed_state[0,-1:][0]))\n\nfig = res_lf.plot_components()\nfig.tight_layout(pad=1.0)",
            "code"
        ],
        [
            "Unobserved Components Results\n===============================================================================================\nDep. Variable:                                       y   No. Observations:                  300\nModel:                                 fixed intercept   Log Likelihood               -1101.455\n                   + stochastic freq_seasonal(100(50))   AIC                           2204.910\nDate:                                 Wed, 02 Nov 2022   BIC                           2208.204\nTime:                                         17:09:57   HQIC                          2206.243\nSample:                                              0\n                                                 - 300\nCovariance Type:                                   opg\n================================================================================================\n                                   coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------------------------\nsigma2.freq_seasonal_100(50)     0.7591      0.082      9.233      0.000       0.598       0.920\n===================================================================================\nLjung-Box (L1) (Q):                  85.96   Jarque-Bera (JB):                 0.72\nProb(Q):                              0.00   Prob(JB):                         0.70\nHeteroskedasticity (H):               1.00   Skew:                            -0.01\nProb(H) (two-sided):                  0.99   Kurtosis:                         2.71\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\nfixed intercept estimated as 4.426\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_14_1.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_14_1.png\"/>",
            "code"
        ],
        [
            "Note that one of our diagnostic tests would be rejected at the .05 level.",
            "markdown"
        ]
    ],
    "Examples->State space models->Seasonality in Time Series Data->Unobserved components (lazy time domain seasonal modeling)": [
        [
            "The fourth method is an unobserved components model with a fixed intercept and a single seasonal component modeled using a time-domain seasonal model of 100 constants. The process for the time series can be written as:\n\n\\[\\begin{split}\\begin{align}\ny_t & =\\mu_t + \\gamma^{(1)}_t + \\epsilon_t\\\\\n\\mu_{t+1} &= \\mu_{t} \\\\\n\\gamma^{(1)}_{t + 1} &= - \\sum_{j=1}^{99} \\gamma^{(1)}_{t + 1 - j} + \\omega^{(1)}_t\\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(\\epsilon_t\\) is white noise, \\(\\omega^{(1)}_{t}\\) are i.i.d. \\(N(0, \\sigma^2_1)\\).",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "model = sm.tsa.UnobservedComponents(series,\n                                    level='fixed intercept',\n                                    seasonal=100)\nres_lt = model.fit(disp=False)\nprint(res_lt.summary())\n# The first state variable holds our estimate of the intercept\nprint(\"fixed intercept estimated as {0:.3f}\".format(res_lt.smoother_results.smoothed_state[0,-1:][0]))\n\nfig = res_lt.plot_components()\nfig.tight_layout(pad=1.0)",
            "code"
        ],
        [
            "Unobserved Components Results\n======================================================================================\nDep. Variable:                              y   No. Observations:                  300\nModel:                        fixed intercept   Log Likelihood               -1564.378\n                   + stochastic seasonal(100)   AIC                           3130.756\nDate:                        Wed, 02 Nov 2022   BIC                           3134.054\nTime:                                17:10:01   HQIC                          3132.091\nSample:                                     0\n                                        - 300\nCovariance Type:                          opg\n===================================================================================\n                      coef    std err          z      P|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nsigma2.seasonal  3.558e+05   2.96e+04     12.012      0.000    2.98e+05    4.14e+05\n===================================================================================\nLjung-Box (L1) (Q):                 200.79   Jarque-Bera (JB):                25.29\nProb(Q):                              0.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.49   Skew:                             0.85\nProb(H) (two-sided):                  0.00   Kurtosis:                         3.37\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\nfixed intercept estimated as 4.690\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_17_1.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_17_1.png\"/>",
            "code"
        ],
        [
            "The seasonal component itself looks good\u2013it is the primary signal. The estimated variance of the seasonal term is very high (\\(>10^5\\)), leading to a lot of uncertainty in our one-step-ahead predictions and slow responsiveness to new data, as evidenced by large errors in one-step ahead predictions and observations. Finally, all three of our diagnostic tests were rejected.",
            "markdown"
        ]
    ],
    "Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates": [
        [
            "The plots below show that explicitly modeling the individual components results in the filtered state being close to the true state within roughly half a period. The lazy models took longer (almost a full period) to do the same on the combined true state.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "# Assign better names for our seasonal terms\ntrue_seasonal_10_3 = terms[0]\ntrue_seasonal_100_2 = terms[1]\ntrue_sum = true_seasonal_10_3 + true_seasonal_100_2\n<br/>",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "time_s = np.s_[:50]  # After this they basically agree\nfig2 = plt.figure()\nax2 = fig2.add_subplot(111)\nh21, = ax2.plot(idx[time_s], res_f.freq_seasonal[1].filtered[time_s], label='Double Freq. Seas')\nh22, = ax2.plot(idx[time_s], res_tf.freq_seasonal[0].filtered[time_s], label='Mixed Domain Seas')\nh23, = ax2.plot(idx[time_s], true_seasonal_100_2[time_s], label='True Seasonal 100(2)')\nplt.legend([h21, h22, h23], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 100(2) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_22_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_22_0.png\"/>",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "time_s = np.s_[:100]\n\nfig3 = plt.figure()\nax3 = fig3.add_subplot(111)\nh31, = ax3.plot(idx[time_s], res_f.freq_seasonal[1].filtered[time_s] + res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh32, = ax3.plot(idx[time_s], res_tf.freq_seasonal[0].filtered[time_s] + res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh33, = ax3.plot(idx[time_s], true_sum[time_s], label='True Seasonal 100(2)')\nh34, = ax3.plot(idx[time_s], res_lf.freq_seasonal[0].filtered[time_s], label='Lazy Freq. Seas')\nh35, = ax3.plot(idx[time_s], res_lt.seasonal.filtered[time_s], label='Lazy Time Seas')\n\nplt.legend([h31, h32, h33, h34, h35], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth', 'Lazy Freq. Seas', 'Lazy Time Seas'], loc=1)\nplt.title('Seasonal components combined')\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_23_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_23_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates->Conclusions": [
        [
            "In this notebook, we simulated a time series with two seasonal components of different periods. We modeled them using structural time series models with (a) two frequency domain components of correct periods and numbers of harmonics, (b) time domain seasonal component for the shorter term and a frequency domain term with correct period and number of harmonics, (c) a single frequency domain term with the longer period and full number of harmonics, and (d) a single time domain term with the longer\nperiod. We saw a variety of diagnostic results, with only the correct generating model, (a), failing to reject any of the tests. Thus, more flexible seasonal modeling allowing for multiple components with specifiable harmonics can be a useful tool for time series modeling. Finally, we can represent seasonal components with fewer total states in this way, allowing for the user to attempt to make the bias-variance trade-off themselves instead of being forced to choose \u201clazy\u201d models, which use a\nlarge number of states and incur additional variance as a result.",
            "markdown"
        ]
    ],
    "Examples->State space models->Fixed / constrained parameters in state space models": [
        [
            "In this notebook we show how to fix specific values of certain parameters in statsmodels\u2019 state space models while estimating others.",
            "markdown"
        ],
        [
            "In general, state space models allow users to:",
            "markdown"
        ],
        [
            "Estimate all parameters by maximum likelihood",
            "markdown"
        ],
        [
            "Fix some parameters and estimate the rest",
            "markdown"
        ],
        [
            "Fix all parameters (so that no parameters are estimated)",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nfrom importlib import reload\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom pandas_datareader.data import DataReader",
            "code"
        ],
        [
            "To illustrate, we will use the Consumer Price Index for Apparel, which has a time-varying level and a strong seasonal component.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "endog = DataReader('CPIAPPNS', 'fred', start='1980').asfreq('MS')\nendog.plot(figsize=(15, 3));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_fixed_params_3_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_fixed_params_3_0.png\"/>",
            "code"
        ],
        [
            "It is well known (e.g.\u00a0Harvey and Jaeger [1993]) that the HP filter output can be generated by an unobserved components model given certain restrictions on the parameters.",
            "markdown"
        ],
        [
            "The unobserved components model is:\n\n\\[\\begin{split}\\begin{aligned}\ny_t & = \\mu_t + \\varepsilon_t & \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2) \\\\\n\\mu_t &= \\mu_{t-1} + \\beta_{t-1} + \\eta_t & \\eta_t \\sim N(0, \\sigma_\\eta^2) \\\\\n\\beta_t &= \\beta_{t-1} + \\zeta_t & \\zeta_t \\sim N(0, \\sigma_\\zeta^2) \\\\\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "For the trend to match the output of the HP filter, the parameters must be set as follows:\n\n\\[\\begin{split}\\begin{aligned}\n\\frac{\\sigma_\\varepsilon^2}{\\sigma_\\zeta^2} & = \\lambda \\\\\n\\sigma_\\eta^2 & = 0\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(\\lambda\\) is the parameter of the associated HP filter. For the monthly data that we use here, it is usually recommended that \\(\\lambda = 129600\\).",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "# Run the HP filter with lambda = 129600\nhp_cycle, hp_trend = sm.tsa.filters.hpfilter(endog, lamb=129600)\n\n# The unobserved components model above is the local linear trend, or \"lltrend\", specification\nmod = sm.tsa.UnobservedComponents(endog, 'lltrend')\nprint(mod.param_names)",
            "code"
        ],
        [
            "['sigma2.irregular', 'sigma2.level', 'sigma2.trend']",
            "code"
        ],
        [
            "The parameters of the unobserved components model (UCM) are written as:",
            "markdown"
        ],
        [
            "\\(\\sigma_\\varepsilon^2 = \\text{sigma2.irregular}\\)",
            "markdown"
        ],
        [
            "\\(\\sigma_\\eta^2 = \\text{sigma2.level}\\)",
            "markdown"
        ],
        [
            "\\(\\sigma_\\zeta^2 = \\text{sigma2.trend}\\)",
            "markdown"
        ],
        [
            "To satisfy the above restrictions, we will set \\((\\sigma_\\varepsilon^2, \\sigma_\\eta^2, \\sigma_\\zeta^2) = (1, 0, 1 / 129600)\\).",
            "markdown"
        ],
        [
            "Since we are fixing all parameters here, we do not need to use the fit method at all, since that method is used to perform maximum likelihood estimation. Instead, we can directly run the Kalman filter and smoother at our chosen parameters using the smooth method.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "res = mod.smooth([1., 0, 1. / 129600])\nprint(res.summary())",
            "code"
        ],
        [
            "Unobserved Components Results\n==============================================================================\nDep. Variable:               CPIAPPNS   No. Observations:                  513\nModel:             local linear trend   Log Likelihood               -2788.429\nDate:                Wed, 02 Nov 2022   AIC                           5582.858\nTime:                        17:08:17   BIC                           5595.567\nSample:                    01-01-1980   HQIC                          5587.840\n                         - 09-01-2022\nCovariance Type:                  opg\n====================================================================================\n                       coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nsigma2.irregular     1.0000      0.009    108.686      0.000       0.982       1.018\nsigma2.level              0      0.000          0      1.000      -0.000       0.000\nsigma2.trend      7.716e-06   2.06e-07     37.535      0.000    7.31e-06    8.12e-06\n===================================================================================\nLjung-Box (L1) (Q):                 229.15   Jarque-Bera (JB):                 1.27\nProb(Q):                              0.00   Prob(JB):                         0.53\nHeteroskedasticity (H):               2.30   Skew:                             0.02\nProb(H) (two-sided):                  0.00   Kurtosis:                         2.76\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "The estimate that corresponds to the HP filter\u2019s trend estimate is given by the smoothed estimate of the level (which is \\(\\mu_t\\) in the notation above):",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "ucm_trend = pd.Series(res.level.smoothed, index=endog.index)",
            "code"
        ],
        [
            "It is easy to see that the estimate of the smoothed level from the UCM is equal to the output of the HP filter:",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(15, 3))\n\nax.plot(hp_trend, label='HP estimate')\nax.plot(ucm_trend, label='UCM estimate')\nax.legend();\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_fixed_params_11_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_fixed_params_11_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->Fixed / constrained parameters in state space models->Adding a seasonal component": [
        [
            "However, unobserved components models are more flexible than the HP filter. For example, the data shown above is clearly seasonal, but with time-varying seasonal effects (the seasonality is much weaker at the beginning than at the end). One of the benefits of the unobserved components framework is that we can add a stochastic seasonal component. In this case, we will estimate the variance of the seasonal component by maximum likelihood while still including the restriction on the parameters\nimplied above so that the trend corresponds to the HP filter concept.",
            "markdown"
        ],
        [
            "Adding the stochastic seasonal component adds one new parameter, sigma2.seasonal.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "# Construct a local linear trend model with a stochastic seasonal component of period 1 year\nmod = sm.tsa.UnobservedComponents(endog, 'lltrend', seasonal=12, stochastic_seasonal=True)\nprint(mod.param_names)",
            "code"
        ],
        [
            "['sigma2.irregular', 'sigma2.level', 'sigma2.trend', 'sigma2.seasonal']",
            "code"
        ],
        [
            "In this case, we will continue to restrict the first three parameters as described above, but we want to estimate the value of sigma2.seasonal by maximum likelihood. Therefore, we will use the fit method along with the fix_params context manager.",
            "markdown"
        ],
        [
            "The fix_params method takes a dictionary of parameters names and associated values. Within the generated context, those parameters will be used in all cases. In the case of the fit method, only the parameters that were not fixed will be estimated.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "# Here we restrict the first three parameters to specific values\nwith mod.fix_params({'sigma2.irregular': 1, 'sigma2.level': 0, 'sigma2.trend': 1. / 129600}):\n    # Now we fit any remaining parameters, which in this case\n    # is just `sigma2.seasonal`\n    res_restricted = mod.fit()",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            1     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  3.65530D+00    |proj g|=  2.09914D-01\n\nAt iterate    5    f=  3.07793D+00    |proj g|=  8.21054D-06\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    1      5     11      1     0     0   8.211D-06   3.078D+00\n  F =   3.0779342481049738\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ],
        [
            "Alternatively, we could have simply used the fit_constrained method, which also accepts a dictionary of constraints:",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "res_restricted = mod.fit_constrained({'sigma2.irregular': 1, 'sigma2.level': 0, 'sigma2.trend': 1. / 129600})",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            1     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  3.65530D+00    |proj g|=  2.09914D-01",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ],
        [
            "At iterate    5    f=  3.07793D+00    |proj g|=  8.21054D-06\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    1      5     11      1     0     0   8.211D-06   3.078D+00\n  F =   3.0779342481049738\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL",
            "code"
        ],
        [
            "The summary output includes all parameters, but indicates that the first three were fixed (and so were not estimated).",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "print(res_restricted.summary())",
            "code"
        ],
        [
            "Unobserved Components Results\n=====================================================================================\nDep. Variable:                      CPIAPPNS   No. Observations:                  513\nModel:                    local linear trend   Log Likelihood               -1578.980\n                   + stochastic seasonal(12)   AIC                           3159.961\nDate:                       Wed, 02 Nov 2022   BIC                           3164.175\nTime:                               17:08:18   HQIC                          3161.614\nSample:                           01-01-1980\n                                - 09-01-2022\nCovariance Type:                         opg\n============================================================================================\n                               coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------------\nsigma2.irregular (fixed)     1.0000        nan        nan        nan         nan         nan\nsigma2.level (fixed)              0        nan        nan        nan         nan         nan\nsigma2.trend (fixed)      7.716e-06        nan        nan        nan         nan         nan\nsigma2.seasonal              0.0977      0.009     11.371      0.000       0.081       0.115\n===================================================================================\nLjung-Box (L1) (Q):                 427.41   Jarque-Bera (JB):                49.94\nProb(Q):                              0.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               2.43   Skew:                             0.23\nProb(H) (two-sided):                  0.00   Kurtosis:                         4.48\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "For comparison, we construct the unrestricted maximum likelihood estimates (MLE). In this case, the estimate of the level will no longer correspond to the HP filter concept.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "res_unrestricted = mod.fit()",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            4     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  3.63572D+00    |proj g|=  1.06815D-01",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ],
        [
            "At iterate    5    f=  2.03262D+00    |proj g|=  4.18918D-01\n\nAt iterate   10    f=  1.62777D+00    |proj g|=  1.40075D+00\n\nAt iterate   15    f=  1.53457D+00    |proj g|=  1.18624D-02\n\nAt iterate   20    f=  1.53105D+00    |proj g|=  4.43139D-02\n\nAt iterate   25    f=  1.44441D+00    |proj g|=  3.53844D-01\n\nAt iterate   30    f=  1.39156D+00    |proj g|=  2.32794D-01\n\nAt iterate   35    f=  1.38532D+00    |proj g|=  2.75610D-03\n\nAt iterate   40    f=  1.38532D+00    |proj g|=  9.53104D-06\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    4     40     68      1     0     0   9.531D-06   1.385D+00\n  F =   1.3853189469230238\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL",
            "code"
        ],
        [
            "Finally, we can retrieve the smoothed estimates of the trend and seasonal components.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "# Construct the smoothed level estimates\nunrestricted_trend = pd.Series(res_unrestricted.level.smoothed, index=endog.index)\nrestricted_trend = pd.Series(res_restricted.level.smoothed, index=endog.index)\n\n# Construct the smoothed estimates of the seasonal pattern\nunrestricted_seasonal = pd.Series(res_unrestricted.seasonal.smoothed, index=endog.index)\nrestricted_seasonal = pd.Series(res_restricted.seasonal.smoothed, index=endog.index)",
            "code"
        ],
        [
            "Comparing the estimated level, it is clear that the seasonal UCM with fixed parameters still produces a trend that corresponds very closely (although no longer exactly) to the HP filter output.",
            "markdown"
        ],
        [
            "Meanwhile, the estimated level from the model with no parameter restrictions (the MLE model) is much less smooth than these.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(15, 3))\n\nax.plot(unrestricted_trend, label='MLE, with seasonal')\nax.plot(restricted_trend, label='Fixed parameters, with seasonal')\nax.plot(hp_trend, label='HP filter, no seasonal')\nax.legend();\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_fixed_params_26_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_fixed_params_26_0.png\"/>",
            "code"
        ],
        [
            "Finally, the UCM with the parameter restrictions is still able to pick up the time-varying seasonal component quite well.",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(15, 3))\n\nax.plot(unrestricted_seasonal, label='MLE')\nax.plot(restricted_seasonal, label='Fixed parameters')\nax.legend();\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_fixed_params_28_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_fixed_params_28_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nfrom importlib import reload\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import invwishart, invgamma\n\n# Get the macro dataset\ndta = sm.datasets.macrodata.load_pandas().data\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')",
            "code"
        ]
    ],
    "Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Background": [
        [
            "Bayesian analysis of linear Gaussian state space models via Markov chain Monte Carlo (MCMC) methods has become both commonplace and relatively straightforward in recent years, due especially to advances in sampling from the joint posterior of the unobserved state vector conditional on the data and model parameters (see especially Carter and Kohn (1994), de Jong and Shephard (1995), and Durbin and Koopman (2002)). This is particularly useful for Gibbs sampling MCMC approaches.",
            "markdown"
        ],
        [
            "While these procedures make use of the forward/backward application of the recursive Kalman filter and smoother, another recent line of research takes a different approach and constructs the posterior joint distribution of the entire vector of states at once - see in particular Chan and Jeliazkov (2009) for an econometric time series treatment and McCausland et al.\u00a0(2011) for a more general survey. In particular, the posterior mean and precision matrix are constructed explicitly, with the latter\na sparse band matrix. Advantage is then taken of efficient algorithms for Cholesky factorization of sparse band matrices; this reduces memory costs and can improve performance. Following McCausland et al.\u00a0(2011), we refer to this method as the \u201cCholesky Factor Algorithm\u201d (CFA) approach.",
            "markdown"
        ],
        [
            "The CFA-based simulation smoother has some advantages and some drawbacks compared to that based on the more typical Kalman filter and smoother (KFS).",
            "markdown"
        ],
        [
            "<strong>Advantages of CFA</strong>:",
            "markdown"
        ],
        [
            "Derivation of the joint posterior distribution is relatively straightforward and easy to understand.",
            "markdown"
        ],
        [
            "In some cases can be both faster and less memory-intensive than the KFS approach",
            "markdown"
        ],
        [
            "In the Appendix at the end of this notebook, we briefly discuss the performance of the two simulation smoothers for the TVP-VAR model. In summary: simple tests on a single machine suggest that for the TVP-VAR model, the CFA and KFS implementations in Statsmodels have about the same runtimes, while both implementations are about twice as fast as the replication code, written in Matlab, provided by Chan and Jeliazkov (2009).",
            "markdown"
        ],
        [
            "<strong>Drawbacks of CFA</strong>:",
            "markdown"
        ],
        [
            "The main drawback is that this method has not (at least so far) reached the generality of the KFS approach. For example:",
            "markdown"
        ],
        [
            "It can not be used with models that have reduced-rank error terms in the observation or state equations.",
            "markdown"
        ],
        [
            "One implication of this is that the typical state space model trick of including identities in the state equation to accommodate, for example, higher-order lags in autoregressive models is not applicable. These models can still be handled by the CFA approach, but at the cost of requiring a slightly different implementation for each lag that is included.",
            "markdown"
        ],
        [
            "As an example, standard ways of representing ARMA and VARMA processes in state space form do include identities in the observation and/or state equations, and so the basic formulas presented in Chan and Jeliazkov (2009) do not apply immediately to these models.",
            "markdown"
        ],
        [
            "Less flexibility is available in the state initialization / prior.",
            "markdown"
        ]
    ],
    "Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Implementation in Statsmodels": [
        [
            "A CFA simulation smoother along the lines of the basic formulas presented in Chan and Jeliazkov (2009) has been implemented in Statsmodels.",
            "markdown"
        ],
        [
            "<strong>Notes</strong>:",
            "markdown"
        ],
        [
            "Therefore, the CFA simulation smoother in Statsmodels so-far only supports the case that the state transition is truly a first-order Markov process (i.e.\u00a0it does not support a p-th order Markov process that has been stacked using identities into a first-order process).",
            "markdown"
        ],
        [
            "By contrast, the KFS smoother in Statsmodels is fully general any can be used for any state space model, including those with stacked p-th order Markov processes or other identities in the observation and state equations.",
            "markdown"
        ],
        [
            "Either a KFS or the CFA simulation smoothers can be constructed from a state space model using the simulation_smoother method. To show the basic idea, we first consider a simple example.",
            "markdown"
        ]
    ],
    "Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Implementation in Statsmodels->Local level model": [
        [
            "A local level model decomposes an observed series \\(y_t\\) into a persistent trend \\(\\mu_t\\) and a transitory error component\n\n\\[\\begin{split}\\begin{aligned}\ny_t & = \\mu_t + \\varepsilon_t, \\qquad \\varepsilon_t \\sim N(0, \\sigma_\\text{irregular}^2) \\\\\n\\mu_t & = \\mu_{t-1} + \\eta_t, \\quad ~ \\eta_t \\sim N(0, \\sigma_\\text{level}^2)\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "This model satisfies the requirements of the CFA simulation smoother because both the observation error term \\(\\varepsilon_t\\) and the state innovation term \\(\\eta_t\\) are non-degenerate - that is, their covariance matrices are full rank.",
            "markdown"
        ],
        [
            "We apply this model to inflation, and consider simulating draws from the posterior of the joint state vector. That is, we are interested in sampling from\n\n\\[p(\\mu^t \\mid y^t, \\sigma_\\text{irregular}^2, \\sigma_\\text{level}^2)\\]",
            "markdown"
        ],
        [
            "where we define \\(\\mu^t \\equiv (\\mu_1, \\dots, \\mu_T)'\\) and \\(y^t \\equiv (y_1, \\dots, y_T)'\\).",
            "markdown"
        ],
        [
            "In Statsmodels, the local level model falls into the more general class of \u201cunobserved components\u201d models, and can be constructed as follows:",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "# Construct a local level model for inflation\nmod = sm.tsa.UnobservedComponents(dta.infl, 'llevel')\n\n# Fit the model's parameters (sigma2_varepsilon and sigma2_eta)\n# via maximum likelihood\nres = mod.fit()\nprint(res.params)\n\n# Create simulation smoother objects\nsim_kfs = mod.simulation_smoother()              # default method is KFS\nsim_cfa = mod.simulation_smoother(method='cfa')  # can specify CFA method",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  2.40814D+00    |proj g|=  1.27979D-01\n\nAt iterate    5    f=  2.24982D+00    |proj g|=  9.50562D-04\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      7      9      1     0     0   2.952D-07   2.250D+00\n  F =   2.2498167187366014\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\nsigma2.irregular    3.373368\nsigma2.level        0.744712\ndtype: float64",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ],
        [
            "The simulation smoother objects sim_kfs and sim_cfa have simulate methods that perform simulation smoothing. Each time that simulate is called, the simulated_state attribute will be re-populated with a new simulated draw from the posterior.",
            "markdown"
        ],
        [
            "Below, we construct 20 simulated paths for the trend, using the KFS and CFA approaches, where the simulation is at the maximum likelihood parameter estimates.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "nsimulations = 20\nsimulated_state_kfs = pd.DataFrame(\n    np.zeros((mod.nobs, nsimulations)), index=dta.index)\nsimulated_state_cfa = pd.DataFrame(\n    np.zeros((mod.nobs, nsimulations)), index=dta.index)\n\nfor i in range(nsimulations):\n    # Apply KFS simulation smoothing\n    sim_kfs.simulate()\n    # Save the KFS simulated state\n    simulated_state_kfs.iloc[:, i] = sim_kfs.simulated_state[0]\n\n    # Apply CFA simulation smoothing\n    sim_cfa.simulate()\n    # Save the CFA simulated state\n    simulated_state_cfa.iloc[:, i] = sim_cfa.simulated_state[0]",
            "code"
        ],
        [
            "Plotting the observed data and the simulations created using each method below, it is not too hard to see that these two methods are doing the same thing.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "# Plot the inflation data along with simulated trends\nfig, axes = plt.subplots(2, figsize=(15, 6))\n\n# Plot data and KFS simulations\ndta.infl.plot(ax=axes[0], color='k')\naxes[0].set_title('Simulations based on KFS approach, MLE parameters')\nsimulated_state_kfs.plot(ax=axes[0], color='C0', alpha=0.25, legend=False)\n\n# Plot data and CFA simulations\ndta.infl.plot(ax=axes[1], color='k')\naxes[1].set_title('Simulations based on CFA approach, MLE parameters')\nsimulated_state_cfa.plot(ax=axes[1], color='C0', alpha=0.25, legend=False)\n\n# Add a legend, clean up layout\nhandles, labels = axes[0].get_legend_handles_labels()\naxes[0].legend(handles[:2], ['Data', 'Simulated state'])\nfig.tight_layout();\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_9_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Implementation in Statsmodels->Updating the model\u2019s parameters": [
        [
            "The simulation smoothers are tied to the model instance, here the variable mod. Whenever the model instance is updated with new parameters, the simulation smoothers will take those new parameters into account in future calls to the simulate method.",
            "markdown"
        ],
        [
            "This is convenient for MCMC algorithms, which repeatedly (a) update the model\u2019s parameters, (b) draw a sample of the state vector, and then (c) draw new values for the model\u2019s parameters.",
            "markdown"
        ],
        [
            "Here we will change the model to a different parameterization that yields a smoother trend, and show how the simulated values change (for brevity we only show the simulations from the KFS approach, but simulations from the CFA approach would be the same).",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(15, 3))\n\n# Update the model's parameterization to one that attributes more\n# variation in inflation to the observation error and so has less\n# variation in the trend component\nmod.update([4, 0.05])\n\n# Plot simulations\nfor i in range(nsimulations):\n    sim_kfs.simulate()\n    ax.plot(dta.index, sim_kfs.simulated_state[0],\n            color='C0', alpha=0.25, label='Simulated state')\n\n# Plot data\ndta.infl.plot(ax=ax, color='k', label='Data', zorder=-1)\n\n# Add title, legend, clean up layout\nax.set_title('Simulations with alternative parameterization yielding a smoother trend')\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles[-2:], labels[-2:])\nfig.tight_layout();\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_11_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_11_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC": [
        [
            "One of the applications that Chan and Jeliazkov (2009) consider is the time-varying parameters vector autoregression (TVP-VAR) model, estimated with Bayesian Gibb sampling (MCMC) methods. They apply this to model the co-movements in four macroeconomic time series:",
            "markdown"
        ],
        [
            "Real GDP growth",
            "markdown"
        ],
        [
            "Inflation",
            "markdown"
        ],
        [
            "Unemployment rate",
            "markdown"
        ],
        [
            "Short-term interest rates",
            "markdown"
        ],
        [
            "We will replicate their example, using a very similar dataset that is included in Statsmodels.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "# Subset to the four variables of interest\ny = dta[['realgdp', 'cpi', 'unemp', 'tbilrate']].copy()\ny.columns = ['gdp', 'inf', 'unemp', 'int']\n\n# Convert to real GDP growth and CPI inflation rates\ny[['gdp', 'inf']] = np.log(y[['gdp', 'inf']]).diff() * 100\ny = y.iloc[1:]\n\nfig, ax = plt.subplots(figsize=(15, 5))\ny.plot(ax=ax)\nax.set_title('Evolution of macroeconomic variables included in TVP-VAR exercise');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_13_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_13_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->TVP-VAR model": [
        [
            "<strong>Note</strong>: this section is based on Chan and Jeliazkov (2009) section 3.1, which can be consulted for additional details.",
            "markdown"
        ],
        [
            "The usual (time-invariant) VAR(1) model is typically written:\n\n\\[\\begin{aligned}\ny_t & = \\mu + \\Phi y_{t-1} + \\varepsilon_t, \\qquad \\varepsilon_t \\sim N(0, H)\n\\end{aligned}\\]",
            "markdown"
        ],
        [
            "where \\(y_t\\) is a \\(p \\times 1\\) vector of variables observed at time \\(t\\) and \\(H\\) is a covariance matrix.",
            "markdown"
        ],
        [
            "The TVP-VAR(1) model generalizes this to allow the coefficients to vary over time according. Stacking all the parameters into a vector according to \\(\\alpha_t = \\text{vec}([\\mu_t : \\Phi_t])\\), where \\(\\text{vec}\\) denotes the operation that stacks columns of a matrix into a vector, we model their evolution over time according to:\n\n\\[\\alpha_{i,t+1} = \\alpha_{i, t} + \\eta_{i,t}, \\qquad \\eta_{i, t} \\sim N(0, \\sigma_i^2)\\]",
            "markdown"
        ],
        [
            "In other words, each parameter evolves independently according to a random walk.",
            "markdown"
        ],
        [
            "Note that there are \\(p\\) coefficients in \\(\\mu_t\\) and \\(p^2\\) coefficients in \\(\\Phi_t\\), so the full state vector \\(\\alpha\\) is shaped \\(p * (p + 1) \\times 1\\).",
            "markdown"
        ],
        [
            "Putting the TVP-VAR(1) model into state-space form is relatively straightforward, and in fact we just have to re-write the observation equation into SUR form:\n\n\\[\\begin{split}\\begin{aligned}\ny_t & = Z_t \\alpha_t + \\varepsilon_t, \\qquad \\varepsilon_t \\sim N(0, H) \\\\\n\\alpha_{t+1} & = \\alpha_t + \\eta_t, \\qquad \\eta_t \\sim N(0, \\text{diag}(\\{\\sigma_i^2\\}))\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "where\n\n\\[\\begin{split}Z_t = \\begin{bmatrix}\n1 & y_{t-1}' & 0 & \\dots & &  0 \\\\\n0 & 0 & 1 & y_{t-1}' &  & 0 \\\\\n\\vdots & & & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & 0 & 1 & y_{t-1}'  \\\\\n\\end{bmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "As long as \\(H\\) is full rank and each of the variances \\(\\sigma_i^2\\) is non-zero, the model satisfies the requirements of the CFA simulation smoother.",
            "markdown"
        ],
        [
            "We also need to specify the initialization / prior for the initial state, \\(\\alpha_1\\). Here we will follow Chan and Jeliazkov (2009) in using \\(\\alpha_1 \\sim N(0, 5 I)\\), although we could also model it as diffuse.",
            "markdown"
        ],
        [
            "Aside from the time-varying coefficients \\(\\alpha_t\\), the other parameters that we will need to estimate are terms in the covariance matrix \\(H\\) and the random walk variances \\(\\sigma_i^2\\).",
            "markdown"
        ]
    ],
    "Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->TVP-VAR model in Statsmodels": [
        [
            "Constructing this model programatically in Statsmodels is also relatively straightforward, since there are basically four steps:",
            "markdown"
        ],
        [
            "Create a new TVPVAR class as a subclass of sm.tsa.statespace.MLEModel",
            "markdown"
        ],
        [
            "Fill in the fixed values of the state space system matrices",
            "markdown"
        ],
        [
            "Specify the initialization of \\(\\alpha_1\\)",
            "markdown"
        ],
        [
            "Create a method for updating the state space system matrices with new values of the covariance matrix \\(H\\) and the random walk variances \\(\\sigma_i^2\\).",
            "markdown"
        ],
        [
            "To do this, first note that the general state space representation used by Statsmodels is:\n\n\\[\\begin{split}\\begin{aligned}\ny_t & = d_t + Z_t \\alpha_t + \\varepsilon_t, \\qquad \\varepsilon_t \\sim N(0, H_t) \\\\\n\\alpha_{t+1} & = c_t + T_t \\alpha_t + R_t \\eta_t, \\qquad \\eta_t \\sim N(0, Q_t) \\\\\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "Then the TVP-VAR(1) model implies the following specializations:",
            "markdown"
        ],
        [
            "The intercept terms are zero, i.e.\u00a0\\(c_t = d_t = 0\\)",
            "markdown"
        ],
        [
            "The design matrix \\(Z_t\\) is time-varying but its values are fixed as described above (i.e.\u00a0its values contain ones and lags of \\(y_t\\))",
            "markdown"
        ],
        [
            "The observation covariance matrix is not time-varying, i.e.\u00a0\\(H_t = H_{t+1} = H\\)",
            "markdown"
        ],
        [
            "The transition matrix is not time-varying and is equal to the identity matrix, i.e.\u00a0\\(T_t = T_{t+1} = I\\)",
            "markdown"
        ],
        [
            "The selection matrix \\(R_t\\) is not time-varying and is also equal to the identity matrix, i.e.\u00a0\\(R_t = R_{t+1} = I\\)",
            "markdown"
        ],
        [
            "The state covariance matrix \\(Q_t\\) is not time-varying and is diagonal, i.e.\u00a0\\(Q_t = Q_{t+1} = \\text{diag}(\\{\\sigma_i^2\\})\\)",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "# 1. Create a new TVPVAR class as a subclass of sm.tsa.statespace.MLEModel\nclass TVPVAR(sm.tsa.statespace.MLEModel):\n    # Steps 2-3 are best done in the class \"constructor\", i.e. the __init__ method\n    def __init__(self, y):\n        # Create a matrix with [y_t' : y_{t-1}'] for t = 2, ..., T\n        augmented = sm.tsa.lagmat(y, 1, trim='both', original='in', use_pandas=True)\n        # Separate into y_t and z_t = [1 : y_{t-1}']\n        p = y.shape[1]\n        y_t = augmented.iloc[:, :p]\n        z_t = sm.add_constant(augmented.iloc[:, p:])\n\n        # Recall that the length of the state vector is p * (p + 1)\n        k_states = p * (p + 1)\n        super().__init__(y_t, exog=z_t, k_states=k_states)\n\n        # Note that the state space system matrices default to contain zeros,\n        # so we don't need to explicitly set c_t = d_t = 0.\n\n        # Construct the design matrix Z_t\n        # Notes:\n        # - self.k_endog = p is the dimension of the observed vector\n        # - self.k_states = p * (p + 1) is the dimension of the observed vector\n        # - self.nobs = T is the number of observations in y_t\n        self['design'] = np.zeros((self.k_endog, self.k_states, self.nobs))\n        for i in range(self.k_endog):\n            start = i * (self.k_endog + 1)\n            end = start + self.k_endog + 1\n            self['design', i, start:end, :] = z_t.T\n\n        # Construct the transition matrix T = I\n        self['transition'] = np.eye(k_states)\n\n        # Construct the selection matrix R = I\n        self['selection'] = np.eye(k_states)\n\n        # Step 3: Initialize the state vector as alpha_1 ~ N(0, 5I)\n        self.ssm.initialize('known', stationary_cov=5 * np.eye(self.k_states))\n\n    # Step 4. Create a method that we can call to update H and Q\n    def update_variances(self, obs_cov, state_cov_diag):\n        self['obs_cov'] = obs_cov\n        self['state_cov'] = np.diag(state_cov_diag)\n\n    # Finally, it can be convenient to define human-readable names for\n    # each element of the state vector. These will be available in output\n    @property\n    def state_names(self):\n        state_names = np.empty((self.k_endog, self.k_endog + 1), dtype=object)\n        for i in range(self.k_endog):\n            endog_name = self.endog_names[i]\n            state_names[i] = (\n                ['intercept.%s' % endog_name] +\n                ['L1.%s-%s' % (other_name, endog_name) for other_name in self.endog_names])\n        return state_names.ravel().tolist()",
            "code"
        ],
        [
            "The above class defined the state space model for any given dataset. Now we need to create a specific instance of it with the dataset that we created earlier containing real GDP growth, inflation, unemployment, and interest rates.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "# Create an instance of our TVPVAR class with our observed dataset y\nmod = TVPVAR(y)",
            "code"
        ]
    ],
    "Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Preliminary investigation with ad-hoc parameters in H, Q": [
        [
            "In our analysis below, we will need to begin our MCMC iterations with some initial parameterization. Following Chan and Jeliazkov (2009) we will set \\(H\\) to be the sample covariance matrix of our dataset, and we will set \\(\\sigma_i^2 = 0.01\\) for each \\(i\\).",
            "markdown"
        ],
        [
            "Before discussing the MCMC scheme that will allow us to make inferences about the model, first we can consider the output of the model when simply plugging in these initial parameters. To fill in these parameters, we use the update_variances method that we defined earlier and then perform Kalman filtering and smoothing conditional on those parameters.",
            "markdown"
        ],
        [
            "<strong>Warning: This exercise is just by way of explanation - we must wait for the output of the MCMC exercise to study the actual implications of the model in a meaningful way</strong>.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "initial_obs_cov = np.cov(y.T)\ninitial_state_cov_diag = [0.01] * mod.k_states\n\n# Update H and Q\nmod.update_variances(initial_obs_cov, initial_state_cov_diag)\n\n# Perform Kalman filtering and smoothing\n# (the [] is just an empty list that in some models might contain\n# additional parameters. Here, we don't have any additional parameters\n# so we just pass an empty list)\ninitial_res = mod.smooth([])",
            "code"
        ],
        [
            "The initial_res variable contains the output of Kalman filtering and smoothing, conditional on those initial parameters. In particular, we may be interested in the \u201csmoothed states\u201d, which are \\(E[\\alpha_t \\mid y^t, H, \\{\\sigma_i^2\\}]\\).",
            "markdown"
        ],
        [
            "First, lets create a function that graphs the coefficients over time, separated into the equations for equation of the observed variables.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "def plot_coefficients_by_equation(states):\n    fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n\n    # The way we defined Z_t implies that the first 5 elements of the\n    # state vector correspond to the first variable in y_t, which is GDP growth\n    ax = axes[0, 0]\n    states.iloc[:, :5].plot(ax=ax)\n    ax.set_title('GDP growth')\n    ax.legend()\n\n    # The next 5 elements correspond to inflation\n    ax = axes[0, 1]\n    states.iloc[:, 5:10].plot(ax=ax)\n    ax.set_title('Inflation rate')\n    ax.legend();\n\n    # The next 5 elements correspond to unemployment\n    ax = axes[1, 0]\n    states.iloc[:, 10:15].plot(ax=ax)\n    ax.set_title('Unemployment equation')\n    ax.legend()\n\n    # The last 5 elements correspond to the interest rate\n    ax = axes[1, 1]\n    states.iloc[:, 15:20].plot(ax=ax)\n    ax.set_title('Interest rate equation')\n    ax.legend();\n\n    return ax\n<br/>",
            "code"
        ],
        [
            "Now, we are interested in the smoothed states, which are available in the states.smoothed attribute out our results object initial_res.",
            "markdown"
        ],
        [
            "As the graph below shows, the initial parameterization implies substantial time-variation in some of the coefficients.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "# Here, for illustration purposes only, we plot the time-varying\n# coefficients conditional on an ad-hoc parameterization\n\n# Recall that `initial_res` contains the Kalman filtering and smoothing,\n# and the `states.smoothed` attribute contains the smoothed states\nplot_coefficients_by_equation(initial_res.states.smoothed);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Bayesian estimation via MCMC": [
        [
            "We will now implement the Gibbs sampler scheme described in Chan and Jeliazkov (2009), Algorithm 2.",
            "markdown"
        ],
        [
            "We use the following (conditionally conjugate) priors:\n\n\\[\\begin{split}\\begin{aligned}\nH & \\sim \\mathcal{IW}(\\nu_1^0, S_1^0) \\\\\n\\sigma_i^2 & \\sim \\mathcal{IG} \\left ( \\frac{\\nu_{i2}^0}{2}, \\frac{S_{i2}^0}{2} \\right )\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(\\mathcal{IW}\\) denotes the inverse-Wishart distribution and \\(\\mathcal{IG}\\) denotes the inverse-Gamma distribution. We set the prior hyperparameters as:\n\n\\[\\begin{split}\\begin{aligned}\nv_1^0 = T + 3, & \\quad S_1^0 = I \\\\\nv_{i2}^0 = 6, & \\quad S_{i2}^0 = 0.01 \\qquad \\text{for each} ~ i\\\\\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "# Prior hyperparameters\n\n# Prior for obs. cov. is inverse-Wishart(v_1^0=k + 3, S10=I)\nv10 = mod.k_endog + 3\nS10 = np.eye(mod.k_endog)\n\n# Prior for state cov. variances is inverse-Gamma(v_{i2}^0 / 2 = 3, S+{i2}^0 / 2 = 0.005)\nvi20 = 6\nSi20 = 0.01",
            "code"
        ],
        [
            "Before running the MCMC iterations, there are a couple of practical steps:",
            "markdown"
        ],
        [
            "Create arrays to store the draws of our state vector, observation covariance matrix, and state error variances.",
            "markdown"
        ],
        [
            "Put the initial values for H and Q (described above) into the storage vectors",
            "markdown"
        ],
        [
            "Construct the simulation smoother object associated with our TVPVAR instance to make draws of the state vector",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "# Gibbs sampler setup\nniter = 11000\nnburn = 1000\n\n# 1. Create storage arrays\nstore_states = np.zeros((niter + 1, mod.nobs, mod.k_states))\nstore_obs_cov = np.zeros((niter + 1, mod.k_endog, mod.k_endog))\nstore_state_cov = np.zeros((niter + 1, mod.k_states))\n\n# 2. Put in the initial values\nstore_obs_cov[0] = initial_obs_cov\nstore_state_cov[0] = initial_state_cov_diag\nmod.update_variances(store_obs_cov[0], store_state_cov[0])\n\n# 3. Construct posterior samplers\nsim = mod.simulation_smoother(method='cfa')",
            "code"
        ],
        [
            "As before, we could have used either the simulation smoother based on the Kalman filter and smoother or that based on the Cholesky Factor Algorithm.",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "for i in range(niter):\n    mod.update_variances(store_obs_cov[i], store_state_cov[i])\n    sim.simulate()\n\n    # 1. Sample states\n    store_states[i + 1] = sim.simulated_state.T\n\n    # 2. Simulate obs cov\n    fitted = np.matmul(mod['design'].transpose(2, 0, 1), store_states[i + 1][..., None])[..., 0]\n    resid = mod.endog - fitted\n    store_obs_cov[i + 1] = invwishart.rvs(v10 + mod.nobs, S10 + resid.T @ resid)\n\n    # 3. Simulate state cov variances\n    resid = store_states[i + 1, 1:] - store_states[i + 1, :-1]\n    sse = np.sum(resid**2, axis=0)\n\n    for j in range(mod.k_states):\n        rv = invgamma.rvs((vi20 + mod.nobs - 1) / 2, scale=(Si20 + sse[j]) / 2)\n        store_state_cov[i + 1, j] = rv",
            "code"
        ],
        [
            "After removing a number of initial draws, the remaining draws from the posterior allow us to conduct inference. Below, we plot the posterior mean of the time-varying regression coefficients.",
            "markdown"
        ],
        [
            "(<strong>Note</strong>: these plots are different from those in Figure 1 of the published version of Chan and Jeliazkov (2009), but they are very similar to those produced by the Matlab replication code available at )",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "# Collect the posterior means of each time-varying coefficient\nstates_posterior_mean = pd.DataFrame(\n    np.mean(store_states[nburn + 1:], axis=0),\n    index=mod._index, columns=mod.state_names)\n\n# Plot these means over time\nplot_coefficients_by_equation(states_posterior_mean);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_35_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_35_0.png\"/>",
            "code"
        ],
        [
            "Python also has a number of libraries to assist with exploring Bayesian models. Here we\u2019ll just use the  package to explore the credible intervals of each of the covariance and variance parameters, although it makes available a much wider set of tools for analysis.",
            "markdown"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "import arviz as az\n\n# Collect the observation error covariance parameters\naz_obs_cov = az.convert_to_inference_data({\n    ('Var[%s]' % mod.endog_names[i] if i == j else\n     'Cov[%s, %s]' % (mod.endog_names[i], mod.endog_names[j])):\n    store_obs_cov[nburn + 1:, i, j]\n    for i in range(mod.k_endog) for j in range(i, mod.k_endog)})\n\n# Plot the credible intervals\naz.plot_forest(az_obs_cov, figsize=(8, 7));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_37_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_37_0.png\"/>",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "# Collect the state innovation variance parameters\naz_state_cov = az.convert_to_inference_data({\n    r'$\\sigma^2$[%s]' % mod.state_names[i]: store_state_cov[nburn + 1:, i]\n    for i in range(mod.k_states)})\n\n# Plot the credible intervals\naz.plot_forest(az_state_cov, figsize=(8, 7));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_38_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_38_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Appendix: performance": [
        [
            "Finally, we run a few simple tests to compare the performance of the KFS and CFA simulation smoothers by using the %timeit Jupyter notebook magic.",
            "markdown"
        ],
        [
            "One caveat is that the KFS simulation smoother can produce a variety of output beyond just simulations of the posterior state vector, and these additional computations could bias the results. To make the results comparable, we will tell the KFS simulation smoother to only compute simulations of the state by using the simulation_output argument.",
            "markdown"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "from statsmodels.tsa.statespace.simulation_smoother import SIMULATION_STATE\n\nsim_cfa = mod.simulation_smoother(method='cfa')\nsim_kfs = mod.simulation_smoother(simulation_output=SIMULATION_STATE)",
            "code"
        ],
        [
            "Then we can use the following code to perform a basic timing exercise:",
            "markdown"
        ],
        [
            "%timeit -n 10000 -r 3 sim_cfa.simulate()\n%timeit -n 10000 -r 3 sim_kfs.simulate()",
            "code"
        ],
        [
            "On the machine this was tested on, this resulted in the following:",
            "markdown"
        ],
        [
            "2.06 ms \u00b1 26.5 \u00b5s per loop (mean \u00b1 std. dev. of 3 runs, 10000 loops each)\n2.02 ms \u00b1 68.4 \u00b5s per loop (mean \u00b1 std. dev. of 3 runs, 10000 loops each)",
            "code"
        ],
        [
            "These results suggest that - at least for this model - there are not noticeable computational gains from the CFA approach relative to the KFS approach. However, this does not rule out the following:",
            "markdown"
        ],
        [
            "The Statsmodels implementation of the CFA simulation smoother could possibly be further optimized",
            "markdown"
        ],
        [
            "The CFA approach may only show improvement for certain models (for example with a large number of endog variables)",
            "markdown"
        ],
        [
            "One simple way to take a first pass at assessing the first possibility is to compare the runtime of the Statsmodels implementation of the CFA simulation smoother to the Matlab implementation in the replication codes of Chan and Jeliazkov (2009), available at .",
            "markdown"
        ],
        [
            "While the Statsmodels version of the CFA simulation smoother is written in Cython and compiled to C code, the Matlab version takes advantage of the Matlab\u2019s sparse matrix capabilities. As a result, even though it is not compiled code, we might expect it to have relatively good performance.",
            "markdown"
        ],
        [
            "On the machine this was tested on, the Matlab version typically ran the MCMC loop with 11,000 iterations in 70-75 seconds, while the MCMC loop in this notebook using the Statsmodels CFA simulation smoother (see above), also with 11,0000 iterations, ran in 40-45 seconds. This is some evidence that the Statsmodels implementation of the CFA smoother already performs relatively well (although it does not rule out that there are additional gains possible).",
            "markdown"
        ]
    ],
    "Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Bibliography": [
        [
            "Carter, Chris K., and Robert Kohn. \u201cOn Gibbs sampling for state space models.\u201d Biometrika 81, no. 3 (1994): 541-553.",
            "markdown"
        ],
        [
            "Chan, Joshua CC, and Ivan Jeliazkov. \u201cEfficient simulation and integrated likelihood estimation in state space models.\u201d International Journal of Mathematical Modelling and Numerical Optimisation 1, no. 1-2 (2009): 101-120.",
            "markdown"
        ],
        [
            "De Jong, Piet, and Neil Shephard. \u201cThe simulation smoother for time series models.\u201d Biometrika 82, no. 2 (1995): 339-350.",
            "markdown"
        ],
        [
            "Durbin, James, and Siem Jan Koopman. \u201cA simple and efficient simulation smoother for state space time series analysis.\u201d Biometrika 89, no. 3 (2002): 603-616.",
            "markdown"
        ],
        [
            "McCausland, William J., Shirley Miller, and Denis Pelletier. \u201cSimulation smoothing for state\u2013space models: A computational efficiency analysis.\u201d Computational Statistics & Data Analysis 55, no. 1 (2011): 199-212.",
            "markdown"
        ]
    ],
    "Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction": [
        [
            "This notebook will show how to use fast Bayesian methods to estimate SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors) models. These methods can also be parallelized across multiple cores.",
            "markdown"
        ],
        [
            "Here, fast methods means a version of Hamiltonian Monte Carlo called the No-U-Turn Sampler (NUTS) developed by Hoffmann and Gelman: see . As they say, \u201cthe cost of HMC per independent sample from a target distribution of dimension \\(D\\) is roughly \\(\\mathcal{O}(D^{5/4})\\), which\nstands in sharp contrast with the \\(\\mathcal{O}(D^{2})\\) cost of random-walk Metropolis\u201d. So for problems of larger dimension, the time-saving with HMC is significant. However it does require the gradient, or Jacobian, of the model to be provided.",
            "markdown"
        ],
        [
            "This notebook will combine the Python libraries , which does econometrics, and , which is for Bayesian estimation, to perform fast Bayesian estimation of a simple SARIMAX model, in this case an ARMA(1, 1) model for US CPI.",
            "markdown"
        ],
        [
            "Note that, for simple models like AR(p), base PyMC3 is a quicker way to fit a model; there\u2019s an . The advantage of using statsmodels is that it gives access to methods that can solve a vast range of statespace models.",
            "markdown"
        ],
        [
            "The model we\u2019ll solve is given by\n\n\\[y_t = \\phi y_{t-1} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1}, \\qquad \\varepsilon_t \\sim N(0, \\sigma^2)\\]",
            "markdown"
        ],
        [
            "with 1 auto-regressive term and 1 moving average term. In statespace form it is written as:\n\n\\[\\begin{split}\\begin{align}\ny_t & = \\underbrace{\\begin{bmatrix} 1 & \\theta_1 \\end{bmatrix}}_{Z} \\underbrace{\\begin{bmatrix} \\alpha_{1,t} \\\\ \\alpha_{2,t} \\end{bmatrix}}_{\\alpha_t} \\\\\n    \\begin{bmatrix} \\alpha_{1,t+1} \\\\ \\alpha_{2,t+1} \\end{bmatrix} & = \\underbrace{\\begin{bmatrix}\n        \\phi & 0 \\\\\n        1      & 0     \\\\\n    \\end{bmatrix}}_{T} \\begin{bmatrix} \\alpha_{1,t} \\\\ \\alpha_{2,t} \\end{bmatrix} +\n    \\underbrace{\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}}_{R} \\underbrace{\\varepsilon_{t+1}}_{\\eta_t} \\\\\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "The code will follow these steps: 1. Import external dependencies 2. Download and plot the data on US CPI 3. Simple maximum likelihood estimation (MLE) as an example 4. Definitions of helper functions to provide tensors to the library doing Bayesian estimation 5. Bayesian estimation via NUTS 6. Application to US CPI series",
            "markdown"
        ],
        [
            "Finally, Appendix A shows how to re-use the helper functions from step (4) to estimate a different state space model, UnobservedComponents, using the same Bayesian methods.",
            "markdown"
        ]
    ],
    "Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->1. Import external dependencies": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc3 as pm\nimport statsmodels.api as sm\nimport theano\nimport theano.tensor as tt\nfrom pandas.plotting import register_matplotlib_converters\nfrom pandas_datareader.data import DataReader\n\nplt.style.use(\"seaborn\")\nregister_matplotlib_converters()",
            "code"
        ],
        [
            "/tmp/ipykernel_4908/3117040453.py:12: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style'. Alternatively, directly use the seaborn API instead.\n  plt.style.use(\"seaborn\")",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->2. Download and plot the data on US CPI": [
        [
            "We\u2019ll get the data from FRED:",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "cpi = DataReader(\"CPIAUCNS\", \"fred\", start=\"1971-01\", end=\"2018-12\")\ncpi.index = pd.DatetimeIndex(cpi.index, freq=\"MS\")\n\n# Define the inflation series that we'll use in analysis\ninf = np.log(cpi).resample(\"QS\").mean().diff()[1:] * 400\ninf = inf.dropna()\nprint(inf.head())",
            "code"
        ],
        [
            "CPIAUCNS\nDATE\n1971-04-01  4.316424\n1971-07-01  4.279518\n1971-10-01  1.956799\n1972-01-01  2.917767\n1972-04-01  3.219096",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "# Plot the series\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\nax.plot(inf.index, inf, label=r\"$\\Delta \\log CPI$\", lw=2)\nax.legend(loc=\"lower left\")\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_6_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->3. Fit the model with maximum likelihood": [
        [
            "Statsmodels does all of the hard work of this for us - creating and fitting the model takes just two lines of code. The model order parameters correspond to auto-regressive, difference, and moving average orders respectively.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "# Create an SARIMAX model instance - here we use it to estimate\n# the parameters via MLE using the `fit` method, but we can\n# also re-use it below for the Bayesian estimation\nmod = sm.tsa.statespace.SARIMAX(inf, order=(1, 0, 1))\n\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())",
            "code"
        ],
        [
            "SARIMAX Results\n==============================================================================\nDep. Variable:               CPIAUCNS   No. Observations:                  191\nModel:               SARIMAX(1, 0, 1)   Log Likelihood                -448.685\nDate:                Wed, 02 Nov 2022   AIC                            903.370\nTime:                        17:02:14   BIC                            913.127\nSample:                    04-01-1971   HQIC                           907.322\n                         - 10-01-2018\nCovariance Type:                  opg\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1          0.9785      0.015     64.545      0.000       0.949       1.008\nma.L1         -0.6342      0.057    -11.073      0.000      -0.747      -0.522\nsigma2         6.3682      0.323     19.695      0.000       5.734       7.002\n===================================================================================\nLjung-Box (L1) (Q):                   4.77   Jarque-Bera (JB):               699.70\nProb(Q):                              0.03   Prob(JB):                         0.00\nHeteroskedasticity (H):               1.72   Skew:                            -1.48\nProb(H) (two-sided):                  0.03   Kurtosis:                        11.90\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "It\u2019s a good fit. We can also get the series of one-step ahead predictions and plot it next to the actual data, along with a confidence band.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "predict_mle = res_mle.get_prediction()\npredict_mle_ci = predict_mle.conf_int()\nlower = predict_mle_ci[\"lower CPIAUCNS\"]\nupper = predict_mle_ci[\"upper CPIAUCNS\"]\n\n# Graph\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n\n# Plot data points\ninf.plot(ax=ax, style=\"-\", label=\"Observed\")\n\n# Plot predictions\npredict_mle.predicted_mean.plot(ax=ax, style=\"r.\", label=\"One-step-ahead forecast\")\nax.fill_between(predict_mle_ci.index, lower, upper, color=\"r\", alpha=0.1)\nax.legend(loc=\"lower left\")\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_10_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_10_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->4. Helper functions to provide tensors to the library doing Bayesian estimation": [
        [
            "We\u2019re almost on to the magic but there are a few preliminaries. Feel free to skip this section if you\u2019re not interested in the technical details.",
            "markdown"
        ]
    ],
    "Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->Technical Details": [
        [
            "PyMC3 is a Bayesian estimation library (\u201cProbabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with Theano\u201d) that is a) fast and b) optimized for Bayesian machine learning, for instance . To do all of this, it is built on top of a Theano, a library that aims to evaluate tensors very efficiently and provide symbolic differentiation (necessary for any kind of deep\nlearning). It is the symbolic differentiation that means PyMC3 can use NUTS on any problem formulated within PyMC3.",
            "markdown"
        ],
        [
            "We are not formulating a problem directly in PyMC3; we\u2019re using statsmodels to specify the statespace model and solve it with the Kalman filter. So we need to put the plumbing of statsmodels and PyMC3 together, which means wrapping the statsmodels SARIMAX model object in a Theano-flavored wrapper before passing information to PyMC3 for estimation.",
            "markdown"
        ],
        [
            "Because of this, we can\u2019t use the Theano auto-differentiation directly. Happily, statsmodels SARIMAX objects have a method to return the Jacobian evaluated at the parameter values. We\u2019ll be making use of this to provide gradients so that we can use NUTS.",
            "markdown"
        ]
    ],
    "Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->Technical Details->Defining helper functions to translate models into a PyMC3 friendly form": [
        [
            "First, we\u2019ll create the Theano wrappers. They will be in the form of \u2018Ops\u2019, operation objects, that \u2018perform\u2019 particular tasks. They are initialized with a statsmodels model instance.",
            "markdown"
        ],
        [
            "Although this code may look somewhat opaque, it is generic for any state space model in statsmodels.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "class Loglike(tt.Op):\n\n    itypes = [tt.dvector]  # expects a vector of parameter values when called\n    otypes = [tt.dscalar]  # outputs a single scalar value (the log likelihood)\n\n    def __init__(self, model):\n        self.model = model\n        self.score = Score(self.model)\n\n    def perform(self, node, inputs, outputs):\n        (theta,) = inputs  # contains the vector of parameters\n        llf = self.model.loglike(theta)\n        outputs[0][0] = np.array(llf)  # output the log-likelihood\n\n    def grad(self, inputs, g):\n        # the method that calculates the gradients - it actually returns the\n        # vector-Jacobian product - g[0] is a vector of parameter values\n        (theta,) = inputs  # our parameters\n        out = [g[0] * self.score(theta)]\n        return out\n\n\nclass Score(tt.Op):\n    itypes = [tt.dvector]\n    otypes = [tt.dvector]\n\n    def __init__(self, model):\n        self.model = model\n\n    def perform(self, node, inputs, outputs):\n        (theta,) = inputs\n        outputs[0][0] = self.model.score(theta)",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->5. Bayesian estimation with NUTS": [
        [
            "The next step is to set the parameters for the Bayesian estimation, specify our priors, and run it.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "# Set sampling params\nndraws = 3000  # number of draws from the distribution\nnburn = 600  # number of \"burn-in points\" (which will be discarded)",
            "code"
        ],
        [
            "Now for the fun part! There are three parameters to estimate: \\(\\phi\\), \\(\\theta_1\\), and \\(\\sigma\\). We\u2019ll use uninformative uniform priors for the first two, and an inverse gamma for the last one. Then we\u2019ll run the inference optionally using as many computer cores as I have.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "# Construct an instance of the Theano wrapper defined above, which\n# will allow PyMC3 to compute the likelihood and Jacobian in a way\n# that it can make use of. Here we are using the same model instance\n# created earlier for MLE analysis (we could also create a new model\n# instance if we preferred)\nloglike = Loglike(mod)\n\nwith pm.Model() as m:\n    # Priors\n    arL1 = pm.Uniform(\"ar.L1\", -0.99, 0.99)\n    maL1 = pm.Uniform(\"ma.L1\", -0.99, 0.99)\n    sigma2 = pm.InverseGamma(\"sigma2\", 2, 4)\n\n    # convert variables to tensor vectors\n    theta = tt.as_tensor_variable([arL1, maL1, sigma2])\n\n    # use a DensityDist (use a lamdba function to \"call\" the Op)\n    pm.DensityDist(\"likelihood\", loglike, observed=theta)\n\n    # Draw samples\n    trace = pm.sample(\n        ndraws,\n        tune=nburn,\n        return_inferencedata=True,\n        cores=1,\n        compute_convergence_checks=False,\n    )",
            "code"
        ],
        [
            "Auto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [sigma2, ma.L1, ar.L1]\n\n\n\n\n\n\n\n\n\n\n\n\n\n<progress class=\"\" max=\"3600\" style=\"width:300px; height:20px; vertical-align: middle;\" value=\"3600\"></progress>\n  100.00% [3600/3600 01:31&lt;00:00 Sampling chain 0, 8 divergences]\n\n\n\n\n\n\n\n\n\n\n\n\n\n<progress class=\"\" max=\"3600\" style=\"width:300px; height:20px; vertical-align: middle;\" value=\"3600\"></progress>\n  100.00% [3600/3600 01:09&lt;00:00 Sampling chain 1, 6 divergences]",
            "code"
        ],
        [
            "Sampling 2 chains for 600 tune and 3_000 draw iterations (1_200 + 6_000 draws total) took 161 seconds.\nThere were 8 divergences after tuning. Increase `target_accept` or reparameterize.\nThere were 14 divergences after tuning. Increase `target_accept` or reparameterize.",
            "code"
        ],
        [
            "Note that the NUTS sampler is auto-assigned because we provided gradients. PyMC3 will use Metropolis or Slicing samplers if it does not find that gradients are available. There are an impressive number of draws per second for a \u201cblock box\u201d style computation! However, note that if the model can be represented directly by PyMC3 (like the AR(p) models mentioned above), then computation can be substantially faster.",
            "markdown"
        ],
        [
            "Inference is complete, but are the results any good? There are a number of ways to check. The first is to look at the posterior distributions (with lines showing the MLE values):",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "plt.tight_layout()\n# Note: the syntax here for the lines argument is required for\n# PyMC3 versions = 3.7\n# For version &lt;= 3.6 you can use lines=dict(res_mle.params) instead\n_ = pm.plot_trace(\n    trace,\n    lines=[(k, {}, [v]) for k, v in dict(res_mle.params).items()],\n    combined=True,\n    figsize=(12, 12),\n)",
            "code"
        ],
        [
            "&lt;Figure size 800x550 with 0 Axes\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_20_1.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_20_1.png\"/>",
            "code"
        ],
        [
            "The estimated posteriors clearly peak close to the parameters found by MLE. We can also see a summary of the estimated values:",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "pm.summary(trace)",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "Here \\(\\hat{R}\\) is the Gelman-Rubin statistic. It tests for lack of convergence by comparing the variance between multiple chains to the variance within each chain. If convergence has been achieved, the between-chain and within-chain variances should be identical. If \\(\\hat{R}&lt;1.2\\) for all model parameters, we can have some confidence that convergence has been reached.",
            "markdown"
        ],
        [
            "Additionally, the highest posterior density interval (the gap between the two values of HPD in the table) is small for each of the variables.",
            "markdown"
        ]
    ],
    "Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->6. Application of Bayesian estimates of parameters": [
        [
            "We\u2019ll now re-instigate a version of the model but using the parameters from the Bayesian estimation, and again plot the one-step-ahead forecasts.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "# Retrieve the posterior means\nparams = pm.summary(trace)[\"mean\"].values\n\n# Construct results using these posterior means as parameter values\nres_bayes = mod.smooth(params)\n\npredict_bayes = res_bayes.get_prediction()\npredict_bayes_ci = predict_bayes.conf_int()\nlower = predict_bayes_ci[\"lower CPIAUCNS\"]\nupper = predict_bayes_ci[\"upper CPIAUCNS\"]\n\n# Graph\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n\n# Plot data points\ninf.plot(ax=ax, style=\"-\", label=\"Observed\")\n\n# Plot predictions\npredict_bayes.predicted_mean.plot(ax=ax, style=\"r.\", label=\"One-step-ahead forecast\")\nax.fill_between(predict_bayes_ci.index, lower, upper, color=\"r\", alpha=0.1)\nax.legend(loc=\"lower left\")\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_24_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_24_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models": [
        [
            "We can reuse the Loglike and Score wrappers defined above to consider a different state space model. For example, we might want to model inflation as the combination of a random walk trend and autoregressive error term:\n\n\\[\\begin{split}\\begin{aligned}\ny_t & = \\mu_t + \\varepsilon_t \\\\\n\\mu_t & = \\mu_{t-1} + \\eta_t \\\\\n\\varepsilon_t &= \\phi \\varepsilon_t + \\zeta_t\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "This model can be constructed in Statsmodels with the UnobservedComponents class using the rwalk and autoregressive specifications. As before, we can fit the model using maximum likelihood via the fit method.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "# Construct the model instance\nmod_uc = sm.tsa.UnobservedComponents(inf, \"rwalk\", autoregressive=1)\n\n# Fit the model via maximum likelihood\nres_uc_mle = mod_uc.fit()\nprint(res_uc_mle.summary())",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            3     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  2.43820D+00    |proj g|=  1.07332D-01\n\nAt iterate    5    f=  2.31427D+00    |proj g|=  2.20639D-02\n\nAt iterate   10    f=  2.30814D+00    |proj g|=  1.67045D-05\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    3     11     14      1     0     0   7.403D-08   2.308D+00\n  F =   2.3081388933012512\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n                        Unobserved Components Results\n==============================================================================\nDep. Variable:               CPIAUCNS   No. Observations:                  191\nModel:                    random walk   Log Likelihood                -440.855\n                              + AR(1)   AIC                            887.709\nDate:                Wed, 02 Nov 2022   BIC                            897.450\nTime:                        17:06:03   HQIC                           891.655\nSample:                    04-01-1971\n                         - 10-01-2018\nCovariance Type:                  opg\n================================================================================\n                   coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nsigma2.level     0.2037      0.156      1.310      0.190      -0.101       0.508\nsigma2.ar        5.2920      0.338     15.665      0.000       4.630       5.954\nar.L1            0.4005      0.096      4.161      0.000       0.212       0.589\n===================================================================================\nLjung-Box (L1) (Q):                   1.46   Jarque-Bera (JB):               521.69\nProb(Q):                              0.23   Prob(JB):                         0.00\nHeteroskedasticity (H):               1.59   Skew:                            -1.30\nProb(H) (two-sided):                  0.07   Kurtosis:                        10.69\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ],
        [
            "As noted earlier, the Theano wrappers (Loglike and Score) that we created above are generic, so we can re-use essentially the same code to explore the model with Bayesian methods.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "# Set sampling params\nndraws = 3000  # number of draws from the distribution\nnburn = 600  # number of \"burn-in points\" (which will be discarded)",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "# Here we follow the same procedure as above, but now we instantiate the\n# Theano wrapper `Loglike` with the UC model instance instead of the\n# SARIMAX model instance\nloglike_uc = Loglike(mod_uc)\n\nwith pm.Model():\n    # Priors\n    sigma2level = pm.InverseGamma(\"sigma2.level\", 1, 1)\n    sigma2ar = pm.InverseGamma(\"sigma2.ar\", 1, 1)\n    arL1 = pm.Uniform(\"ar.L1\", -0.99, 0.99)\n\n    # convert variables to tensor vectors\n    theta_uc = tt.as_tensor_variable([sigma2level, sigma2ar, arL1])\n\n    # use a DensityDist (use a lamdba function to \"call\" the Op)\n    pm.DensityDist(\"likelihood\", loglike_uc, observed=theta_uc)\n\n    # Draw samples\n    trace_uc = pm.sample(\n        ndraws,\n        tune=nburn,\n        return_inferencedata=True,\n        cores=1,\n        compute_convergence_checks=False,\n    )",
            "code"
        ],
        [
            "Auto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [ar.L1, sigma2.ar, sigma2.level]\n\n\n\n\n\n\n\n\n\n\n\n\n\n<progress class=\"\" max=\"3600\" style=\"width:300px; height:20px; vertical-align: middle;\" value=\"3600\"></progress>\n  100.00% [3600/3600 00:32&lt;00:00 Sampling chain 0, 0 divergences]\n\n\n\n\n\n\n\n\n\n\n\n\n\n<progress class=\"\" max=\"3600\" style=\"width:300px; height:20px; vertical-align: middle;\" value=\"3600\"></progress>\n  100.00% [3600/3600 00:31&lt;00:00 Sampling chain 1, 0 divergences]",
            "code"
        ],
        [
            "Sampling 2 chains for 600 tune and 3_000 draw iterations (1_200 + 6_000 draws total) took 64 seconds.",
            "code"
        ],
        [
            "And as before we can plot the marginal posteriors. In contrast to the SARIMAX example, here the posterior modes are somewhat different from the MLE estimates.",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "plt.tight_layout()\n# Note: the syntax here for the lines argument is required for\n# PyMC3 versions = 3.7\n# For version &lt;= 3.6 you can use lines=dict(res_mle.params) instead\n_ = pm.plot_trace(\n    trace_uc,\n    lines=[(k, {}, [v]) for k, v in dict(res_uc_mle.params).items()],\n    combined=True,\n    figsize=(12, 12),\n)",
            "code"
        ],
        [
            "&lt;Figure size 800x550 with 0 Axes\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_32_1.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_32_1.png\"/>",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "pm.summary(trace_uc)",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "# Retrieve the posterior means\nparams = pm.summary(trace_uc)[\"mean\"].values\n\n# Construct results using these posterior means as parameter values\nres_uc_bayes = mod_uc.smooth(params)",
            "code"
        ],
        [
            "One benefit of this model is that it gives us an estimate of the underling \u201clevel\u201d of inflation, using the smoothed estimate of \\(\\mu_t\\), which we can access as the \u201clevel\u201d column in the results objects\u2019 states.smoothed attribute. In this case, because the Bayesian posterior mean of the level\u2019s variance is larger than the MLE estimate, its estimated level is a little more volatile.",
            "markdown"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "# Graph\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n\n# Plot data points\ninf[\"CPIAUCNS\"].plot(ax=ax, style=\"-\", label=\"Observed data\")\n\n# Plot estimate of the level term\nres_uc_mle.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (MLE)\")\nres_uc_bayes.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (Bayesian)\")\n\nax.legend(loc=\"lower left\");\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\"/>",
            "code"
        ]
    ],
    "Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d": [
        [
            "In this notebook, we describe how to use Statsmodels to compute the impacts of updated or revised datasets on out-of-sample forecasts or in-sample estimates of missing data. We follow the approach of the \u201cNowcasting\u201d literature (see references at the end), by using a state space model to compute the \u201cnews\u201d and impacts of incoming data.",
            "markdown"
        ],
        [
            "<strong>Note</strong>: this notebook applies to Statsmodels v0.12+. In addition, it only applies to the state space models or related classes, which are: sm.tsa.statespace.ExponentialSmoothing, sm.tsa.arima.ARIMA, sm.tsa.SARIMAX, sm.tsa.UnobservedComponents, sm.tsa.VARMAX, and sm.tsa.DynamicFactor.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nmacrodata = sm.datasets.macrodata.load_pandas().data\nmacrodata.index = pd.period_range('1959Q1', '2009Q3', freq='Q')",
            "code"
        ],
        [
            "Forecasting exercises often start with a fixed set of historical data that is used for model selection and parameter estimation. Then, the fitted selected model (or models) can be used to create out-of-sample forecasts. Most of the time, this is not the end of the story. As new data comes in, you may need to evaluate your forecast errors, possibly update your models, and create updated out-of-sample forecasts. This is sometimes called a \u201creal-time\u201d forecasting exercise (by contrast, a pseudo\nreal-time exercise is one in which you simulate this procedure).",
            "markdown"
        ],
        [
            "If all that matters is minimizing some loss function based on forecast errors (like MSE), then when new data comes in you may just want to completely redo model selection, parameter estimation and out-of-sample forecasting, using the updated datapoints. If you do this, your new forecasts will have changed for two reasons:",
            "markdown"
        ],
        [
            "You have received new data that gives you new information",
            "markdown"
        ],
        [
            "Your forecasting model or the estimated parameters are different",
            "markdown"
        ],
        [
            "In this notebook, we focus on methods for isolating the first effect. The way we do this comes from the so-called \u201cnowcasting\u201d literature, and in particular Ba\u0144bura, Giannone, and Reichlin (2011), Ba\u0144bura and Modugno (2014), and Ba\u0144bura et al.\u00a0(2014). They describe this exercise as computing the \u201c<strong>news</strong>\u201d, and we follow them in using this language in Statsmodels.",
            "markdown"
        ],
        [
            "These methods are perhaps most useful with multivariate models, since there multiple variables may update at the same time, and it is not immediately obvious what forecast change was created by what updated variable. However, they can still be useful for thinking about forecast revisions in univariate models. We will therefore start with the simpler univariate case to explain how things work, and then move to the multivariate case afterwards.",
            "markdown"
        ],
        [
            "<strong>Note on revisions</strong>: the framework that we are using is designed to decompose changes to forecasts from newly observed datapoints. It can also take into account revisions to previously published datapoints, but it does not decompose them separately. Instead, it only shows the aggregate effect of \u201crevisions\u201d.",
            "markdown"
        ],
        [
            "<strong>Note on ``exog`` data</strong>: the framework that we are using only decomposes changes to forecasts from newly observed datapoints for modeled variables. These are the \u201cleft-hand-side\u201d variables that in Statsmodels are given in the endog arguments. This framework does not decompose or account for changes to unmodeled \u201cright-hand-side\u201d variables, like those included in the exog argument.",
            "markdown"
        ]
    ],
    "Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)": [
        [
            "We will begin with a simple autoregressive model, an AR(1):\n\n\\[y_t = \\phi y_{t-1} + \\varepsilon_t\\]",
            "markdown"
        ],
        [
            "The parameter \\(\\phi\\) captures the persistence of the series",
            "markdown"
        ],
        [
            "We will use this model to forecast inflation.",
            "markdown"
        ],
        [
            "To make it simpler to describe the forecast updates in this notebook, we will work with inflation data that has been de-meaned, but it is straightforward in practice to augment the model with a mean term.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "# De-mean the inflation series\ny = macrodata['infl'] - macrodata['infl'].mean()",
            "code"
        ]
    ],
    "Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 1: fitting the model on the available dataset": [
        [
            "Here, we\u2019ll simulate an out-of-sample exercise, by constructing and fitting our model using all of the data except the last five observations. We\u2019ll assume that we haven\u2019t observed these values yet, and then in subsequent steps we\u2019ll add them back into the analysis.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "y_pre = y.iloc[:-5]\ny_pre.plot(figsize=(15, 3), title='Inflation');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\"/>",
            "code"
        ],
        [
            "To construct forecasts, we first estimate the parameters of the model. This returns a results object that we will be able to use produce forecasts.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "mod_pre = sm.tsa.arima.ARIMA(y_pre, order=(1, 0, 0), trend='n')\nres_pre = mod_pre.fit()\nprint(res_pre.summary())",
            "code"
        ],
        [
            "SARIMAX Results\n==============================================================================\nDep. Variable:                   infl   No. Observations:                  198\nModel:                 ARIMA(1, 0, 0)   Log Likelihood                -446.407\nDate:                Wed, 02 Nov 2022   AIC                            896.813\nTime:                        17:10:08   BIC                            903.390\nSample:                    03-31-1959   HQIC                           899.475\n                         - 06-30-2008\nCovariance Type:                  opg\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1          0.6751      0.043     15.858      0.000       0.592       0.759\nsigma2         5.3027      0.367     14.459      0.000       4.584       6.022\n===================================================================================\nLjung-Box (L1) (Q):                  15.65   Jarque-Bera (JB):                43.04\nProb(Q):                              0.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.85   Skew:                             0.18\nProb(H) (two-sided):                  0.50   Kurtosis:                         5.26\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "Creating the forecasts from the results object res is easy - you can just call the forecast method with the number of forecasts you want to construct. In this case, we\u2019ll construct four out-of-sample forecasts.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "# Compute the forecasts\nforecasts_pre = res_pre.forecast(4)\n\n# Plot the last 3 years of data and the four out-of-sample forecasts\ny_pre.iloc[-12:].plot(figsize=(15, 3), label='Data', legend=True)\nforecasts_pre.plot(label='Forecast', legend=True);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_13_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_13_0.png\"/>",
            "code"
        ],
        [
            "For the AR(1) model, it is also easy to manually construct the forecasts. Denoting the last observed variable as \\(y_T\\) and the \\(h\\)-step-ahead forecast as \\(y_{T+h|T}\\), we have:\n\n\\[y_{T+h|T} = \\hat \\phi^h y_T\\]",
            "markdown"
        ],
        [
            "Where \\(\\hat \\phi\\) is our estimated value for the AR(1) coefficient. From the summary output above, we can see that this is the first parameter of the model, which we can access from the params attribute of the results object.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "# Get the estimated AR(1) coefficient\nphi_hat = res_pre.params[0]\n\n# Get the last observed value of the variable\ny_T = y_pre.iloc[-1]\n\n# Directly compute the forecasts at the horizons h=1,2,3,4\nmanual_forecasts = pd.Series([phi_hat * y_T, phi_hat**2 * y_T,\n                              phi_hat**3 * y_T, phi_hat**4 * y_T],\n                             index=forecasts_pre.index)\n\n# We'll print the two to double-check that they're the same\nprint(pd.concat([forecasts_pre, manual_forecasts], axis=1))",
            "code"
        ],
        [
            "predicted_mean         0\n2008Q3        3.084388  3.084388\n2008Q4        2.082323  2.082323\n2009Q1        1.405812  1.405812\n2009Q2        0.949088  0.949088",
            "code"
        ]
    ],
    "Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 2: computing the \u201cnews\u201d from a new observation": [
        [
            "Suppose that time has passed, and we have now received another observation. Our dataset is now larger, and we can evaluate our forecast error and produce updated forecasts for the subsequent quarters.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "# Get the next observation after the \"pre\" dataset\ny_update = y.iloc[-5:-4]\n\n# Print the forecast error\nprint('Forecast error: %.2f' % (y_update.iloc[0] - forecasts_pre.iloc[0]))",
            "code"
        ],
        [
            "Forecast error: -10.21",
            "code"
        ],
        [
            "To compute forecasts based on our updated dataset, we will create an updated results object res_post using the append method, to append on our new observation to the previous dataset.",
            "markdown"
        ],
        [
            "Note that by default, the append method does not re-estimate the parameters of the model. This is exactly what we want here, since we want to isolate the effect on the forecasts of the new information only.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "# Create a new results object by passing the new observations to the `append` method\nres_post = res_pre.append(y_update)\n\n# Since we now know the value for 2008Q3, we will only use `res_post` to\n# produce forecasts for 2008Q4 through 2009Q2\nforecasts_post = pd.concat([y_update, res_post.forecast('2009Q2')])\nprint(forecasts_post)",
            "code"
        ],
        [
            "2008Q3   -7.121330\n2008Q4   -4.807732\n2009Q1   -3.245783\n2009Q2   -2.191284\nFreq: Q-DEC, dtype: float64",
            "code"
        ],
        [
            "In this case, the forecast error is quite large - inflation was more than 10 percentage points below the AR(1) models\u2019 forecast. (This was largely because of large swings in oil prices around the global financial crisis).",
            "markdown"
        ],
        [
            "To analyse this in more depth, we can use Statsmodels to isolate the effect of the new information - or the \u201c<strong>news</strong>\u201d - on our forecasts. This means that we do not yet want to change our model or re-estimate the parameters. Instead, we will use the news method that is available in the results objects of state space models.",
            "markdown"
        ],
        [
            "Computing the news in Statsmodels always requires a previous results object or dataset, and an updated results object or dataset. Here we will use the original results object res_pre as the previous results and the res_post results object that we just created as the updated results.",
            "markdown"
        ],
        [
            "Once we have previous and updated results objects or datasets, we can compute the news by calling the news method. Here, we will call res_pre.news, and the first argument will be the updated results, res_post (however, if you have two results objects, the news method could can be called on either one).",
            "markdown"
        ],
        [
            "In addition to specifying the comparison object or dataset as the first argument, there are a variety of other arguments that are accepted. The most important specify the \u201cimpact periods\u201d that you want to consider. These \u201cimpact periods\u201d correspond to the forecasted periods of interest; i.e.\u00a0these dates specify with periods will have forecast revisions decomposed.",
            "markdown"
        ],
        [
            "To specify the impact periods, you must pass two of start, end, and periods (similar to the Pandas date_range method). If your time series was a Pandas object with an associated date or period index, then you can pass dates as values for start and end, as we do below.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "# Compute the impact of the news on the four periods that we previously\n# forecasted: 2008Q3 through 2009Q2\nnews = res_pre.news(res_post, start='2008Q3', end='2009Q2')\n# Note: one alternative way to specify these impact dates is\n# `start='2008Q3', periods=4`",
            "code"
        ],
        [
            "The variable news is an object of the class NewsResults, and it contains details about the updates to the data in res_post compared to res_pre, the new information in the updated dataset, and the impact that the new information had on the forecasts in the period between start and end.",
            "markdown"
        ],
        [
            "One easy way to summarize the results are with the summary method.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "print(news.summary())",
            "code"
        ],
        [
            "News\n==============================================================================\nModel:                          ARIMA   Original sample:                1959Q1\nDate:                Wed, 02 Nov 2022                                 - 2008Q2\nTime:                        17:10:08   Update through:                 2008Q3\n                                        No. Revisions:                       0\n                                        No. New datapoints:                  1\n          Impacts for [impacted variable = infl]\n=========================================================\nimpact date estimate (prev) impact of news estimate (new)\n---------------------------------------------------------\n     2008Q3            3.08         -10.21          -7.12\n     2008Q4            2.08          -6.89          -4.81\n     2009Q1            1.41          -4.65          -3.25\n     2009Q2            0.95          -3.14          -2.19\n                  News from updated observations:\n===================================================================\nupdate date updated variable   observed forecast (prev)        news\n-------------------------------------------------------------------\n     2008Q3             infl      -7.12            3.08      -10.21\n           Details for [updated variable = infl, impacted variable = infl]\n=====================================================================================\nupdate date   observed forecast (prev) impact date        news      weight     impact\n-------------------------------------------------------------------------------------\n     2008Q3      -7.12            3.08      2008Q3      -10.21         1.0     -10.21\n                                            2008Q4      -10.21        0.68      -6.89\n                                            2009Q1      -10.21        0.46      -4.65\n                                            2009Q2      -10.21        0.31      -3.14\n=====================================================================================",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/tsa/statespace/news.py:591: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n  impacts.iloc[:, 0] = impacts.iloc[:, 0].map(str)",
            "code"
        ],
        [
            "<strong>Summary output</strong>: the default summary for this news results object printed four tables:",
            "markdown"
        ],
        [
            "Summary of the model and datasets",
            "markdown"
        ],
        [
            "Details of the news from updated data",
            "markdown"
        ],
        [
            "Summary of the impacts of the new information on the forecasts between start='2008Q3' and end='2009Q2'",
            "markdown"
        ],
        [
            "Details of how the updated data led to the impacts on the forecasts between start='2008Q3' and end='2009Q2'",
            "markdown"
        ],
        [
            "These are described in more detail below.",
            "markdown"
        ],
        [
            "Notes:",
            "markdown"
        ],
        [
            "There are a number of arguments that can be passed to the summary method to control this output. Check the documentation / docstring for details.",
            "markdown"
        ],
        [
            "Table (4), showing details of the updates and impacts, can become quite large if the model is multivariate, there are multiple updates, or a large number of impact dates are selected. It is only shown by default for univariate models.",
            "markdown"
        ],
        [
            "<strong>First table: summary of the model and datasets</strong>",
            "markdown"
        ],
        [
            "The first table, above, shows:",
            "markdown"
        ],
        [
            "The type of model from which the forecasts were made. Here this is an ARIMA model, since an AR(1) is a special case of an ARIMA(p,d,q) model.",
            "markdown"
        ],
        [
            "The date and time at which the analysis was computed.",
            "markdown"
        ],
        [
            "The original sample period, which here corresponds to y_pre",
            "markdown"
        ],
        [
            "The endpoint of the updated sample period, which here is the last date in y_post",
            "markdown"
        ],
        [
            "<strong>Second table: the news from updated data</strong>",
            "markdown"
        ],
        [
            "This table simply shows the forecasts from the previous results for observations that were updated in the updated sample.",
            "markdown"
        ],
        [
            "Notes:",
            "markdown"
        ],
        [
            "Our updated dataset y_post did not contain any revisions to previously observed datapoints. If it had, there would be an additional table showing the previous and updated values of each such revision.",
            "markdown"
        ],
        [
            "<strong>Third table: summary of the impacts of the new information</strong>",
            "markdown"
        ],
        [
            "Columns:",
            "markdown"
        ],
        [
            "The third table, above, shows:",
            "markdown"
        ],
        [
            "The previous forecast for each of the impact dates, in the \u201cestimate (prev)\u201d column",
            "markdown"
        ],
        [
            "The impact that the new information (the \u201cnews\u201d) had on the forecasts for each of the impact dates, in the \u201cimpact of news\u201d column",
            "markdown"
        ],
        [
            "The updated forecast for each of the impact dates, in the \u201cestimate (new)\u201d column",
            "markdown"
        ],
        [
            "Notes:",
            "markdown"
        ],
        [
            "In multivariate models, this table contains additional columns describing the relevant impacted variable for each row.",
            "markdown"
        ],
        [
            "Our updated dataset y_post did not contain any revisions to previously observed datapoints. If it had, there would be additional columns in this table showing the impact of those revisions on the forecasts for the impact dates.",
            "markdown"
        ],
        [
            "Note that estimate (new) = estimate (prev) + impact of news",
            "markdown"
        ],
        [
            "This table can be accessed independently using the summary_impacts method.",
            "markdown"
        ],
        [
            "In our example:",
            "markdown"
        ],
        [
            "Notice that in our example, the table shows the values that we computed earlier:",
            "markdown"
        ],
        [
            "The \u201cestimate (prev)\u201d column is identical to the forecasts from our previous model, contained in the forecasts_pre variable.",
            "markdown"
        ],
        [
            "The \u201cestimate (new)\u201d column is identical to our forecasts_post variable, which contains the observed value for 2008Q3 and the forecasts from the updated model for 2008Q4 - 2009Q2.",
            "markdown"
        ],
        [
            "<strong>Fourth table: details of updates and their impacts</strong>",
            "markdown"
        ],
        [
            "The fourth table, above, shows how each new observation translated into specific impacts at each impact date.",
            "markdown"
        ],
        [
            "Columns:",
            "markdown"
        ],
        [
            "The first three columns table described the relevant <strong>update</strong> (an \u201cupdated\u201d is a new observation):",
            "markdown"
        ],
        [
            "The first column (\u201cupdate date\u201d) shows the date of the variable that was updated.",
            "markdown"
        ],
        [
            "The second column (\u201cforecast (prev)\u201d) shows the value that would have been forecasted for the update variable at the update date based on the previous results / dataset.",
            "markdown"
        ],
        [
            "The third column (\u201cobserved\u201d) shows the actual observed value of that updated variable / update date in the updated results / dataset.",
            "markdown"
        ],
        [
            "The last four columns described the <strong>impact</strong> of a given update (an impact is a changed forecast within the \u201cimpact periods\u201d).",
            "markdown"
        ],
        [
            "The fourth column (\u201cimpact date\u201d) gives the date at which the given update made an impact.",
            "markdown"
        ],
        [
            "The fifth column (\u201cnews\u201d) shows the \u201cnews\u201d associated with the given update (this is the same for each impact of a given update, but is just not sparsified by default)",
            "markdown"
        ],
        [
            "The sixth column (\u201cweight\u201d) describes the weight that the \u201cnews\u201d from the given update has on the impacted variable at the impact date. In general, weights will be different between each \u201cupdated variable\u201d / \u201cupdate date\u201d / \u201cimpacted variable\u201d / \u201cimpact date\u201d combination.",
            "markdown"
        ],
        [
            "The seventh column (\u201cimpact\u201d) shows the impact that the given update had on the given \u201cimpacted variable\u201d / \u201cimpact date\u201d.",
            "markdown"
        ],
        [
            "Notes:",
            "markdown"
        ],
        [
            "In multivariate models, this table contains additional columns to show the relevant variable that was updated and variable that was impacted for each row. Here, there is only one variable (\u201cinfl\u201d), so those columns are suppressed to save space.",
            "markdown"
        ],
        [
            "By default, the updates in this table are \u201csparsified\u201d with blanks, to avoid repeating the same values for \u201cupdate date\u201d, \u201cforecast (prev)\u201d, and \u201cobserved\u201d for each row of the table. This behavior can be overridden using the sparsify argument.",
            "markdown"
        ],
        [
            "Note that impact = news * weight.",
            "markdown"
        ],
        [
            "This table can be accessed independently using the summary_details method.",
            "markdown"
        ],
        [
            "In our example:",
            "markdown"
        ],
        [
            "For the update to 2008Q3 and impact date 2008Q3, the weight is equal to 1. This is because we only have one variable, and once we have incorporated the data for 2008Q3, there is no no remaining ambiguity about the \u201cforecast\u201d for this date. Thus all of the \u201cnews\u201d about this variable at 2008Q3 passes through to the \u201cforecast\u201d directly.",
            "markdown"
        ]
    ],
    "Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Addendum: manually computing the news, weights, and impacts": [
        [
            "For this simple example with a univariate model, it is straightforward to compute all of the values shown above by hand. First, recall the formula for forecasting \\(y_{T+h|T} = \\phi^h y_T\\), and note that it follows that we also have \\(y_{T+h|T+1} = \\phi^h y_{T+1}\\). Finally, note that \\(y_{T|T+1} = y_T\\), because if we know the value of the observations through \\(T+1\\), we know the value of \\(y_T\\).",
            "markdown"
        ],
        [
            "<strong>News</strong>: The \u201cnews\u201d is nothing more than the forecast error associated with one of the new observations. So the news associated with observation \\(T+1\\) is:\n\n\\[n_{T+1} = y_{T+1} - y_{T+1|T} = Y_{T+1} - \\phi Y_T\\]",
            "markdown"
        ],
        [
            "<strong>Impacts</strong>: The impact of the news is the difference between the updated and previous forecasts, \\(i_h \\equiv y_{T+h|T+1} - y_{T+h|T}\\).",
            "markdown"
        ],
        [
            "The previous forecasts for \\(h=1, \\dots, 4\\) are: \\(\\begin{pmatrix} \\phi y_T & \\phi^2 y_T & \\phi^3 y_T & \\phi^4 y_T \\end{pmatrix}'\\).",
            "markdown"
        ],
        [
            "The updated forecasts for \\(h=1, \\dots, 4\\) are: \\(\\begin{pmatrix} y_{T+1} & \\phi y_{T+1} & \\phi^2 y_{T+1} & \\phi^3 y_{T+1} \\end{pmatrix}'\\).",
            "markdown"
        ],
        [
            "The impacts are therefore:\n\n\\[\\begin{split}\\{ i_h \\}_{h=1}^4 = \\begin{pmatrix} y_{T+1} - \\phi y_T \\\\ \\phi (Y_{T+1} - \\phi y_T) \\\\ \\phi^2 (Y_{T+1} - \\phi y_T) \\\\ \\phi^3 (Y_{T+1} - \\phi y_T) \\end{pmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "<strong>Weights</strong>: To compute the weights, we just need to note that it is immediate that we can rewrite the impacts in terms of the forecast errors, \\(n_{T+1}\\).\n\n\\[\\begin{split}\\{ i_h \\}_{h=1}^4 = \\begin{pmatrix} 1 \\\\ \\phi \\\\ \\phi^2 \\\\ \\phi^3 \\end{pmatrix} n_{T+1}\\end{split}\\]",
            "markdown"
        ],
        [
            "The weights are then simply \\(w = \\begin{pmatrix} 1 \\\\ \\phi \\\\ \\phi^2 \\\\ \\phi^3 \\end{pmatrix}\\)",
            "markdown"
        ],
        [
            "We can check that this is what the news method has computed.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "# Print the news, computed by the `news` method\nprint(news.news)\n\n# Manually compute the news\nprint()\nprint((y_update.iloc[0] - phi_hat * y_pre.iloc[-1]).round(6))",
            "code"
        ],
        [
            "update date  updated variable\n2008Q3       infl               -10.205718\nName: news, dtype: float64\n\n-10.205718",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "# Print the total impacts, computed by the `news` method\n# (Note: news.total_impacts = news.revision_impacts + news.update_impacts, but\n# here there are no data revisions, so total and update impacts are the same)\nprint(news.total_impacts)\n\n# Manually compute the impacts\nprint()\nprint(forecasts_post - forecasts_pre)",
            "code"
        ],
        [
            "infl\n2008Q3 -10.205718\n2008Q4  -6.890055\n2009Q1  -4.651595\n2009Q2  -3.140371\n\n2008Q3   -10.205718\n2008Q4    -6.890055\n2009Q1    -4.651595\n2009Q2    -3.140371\nFreq: Q-DEC, dtype: float64",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "# Print the weights, computed by the `news` method\nprint(news.weights)\n\n# Manually compute the weights\nprint()\nprint(np.array([1, phi_hat, phi_hat**2, phi_hat**3]).round(6))",
            "code"
        ],
        [
            "impact date                  2008Q3    2008Q4    2009Q1    2009Q2\nimpacted variable              infl      infl      infl      infl\nupdate date updated variable\n2008Q3      infl                1.0  0.675117  0.455783  0.307707\n\n[1.       0.675117 0.455783 0.307707]",
            "code"
        ]
    ],
    "Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor": [
        [
            "In this example, we\u2019ll consider forecasting monthly core price inflation based on the Personal Consumption Expenditures (PCE) price index and the Consumer Price Index (CPI), using a Dynamic Factor model. Both of these measures track prices in the US economy and are based on similar source data, but they have a number of definitional differences. Nonetheless, they track each other relatively well, so modeling them jointly using a single dynamic factor seems reasonable.",
            "markdown"
        ],
        [
            "One reason that this kind of approach can be useful is that the CPI is released earlier in the month than the PCE. One the CPI is released, therefore, we can update our dynamic factor model with that additional datapoint, and obtain an improved forecast for that month\u2019s PCE release. A more involved version of this kind of analysis is available in Knotek and Zaman (2017).",
            "markdown"
        ],
        [
            "We start by downloading the core CPI and PCE price index data from , converting them to annualized monthly inflation rates, removing two outliers, and de-meaning each series (the dynamic factor model does not",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "import pandas_datareader as pdr\nlevels = pdr.get_data_fred(['PCEPILFE', 'CPILFESL'], start='1999', end='2019').to_period('M')\ninfl = np.log(levels).diff().iloc[1:] * 1200\ninfl.columns = ['PCE', 'CPI']\n\n# Remove two outliers and de-mean the series\ninfl['PCE'].loc['2001-09':'2001-10'] = np.nan",
            "code"
        ],
        [
            "To show how this works, we\u2019ll imagine that it is April 14, 2017, which is the data of the March 2017 CPI release. So that we can show the effect of multiple updates at once, we\u2019ll assume that we haven\u2019t updated our data since the end of January, so that:",
            "markdown"
        ],
        [
            "Our <strong>previous dataset</strong> will consist of all values for the PCE and CPI through January 2017",
            "markdown"
        ],
        [
            "Our <strong>updated dataset</strong> will additionally incorporate the CPI for February and March 2017 and the PCE data for February 2017. But it will not yet the PCE (the March 2017 PCE price index was not released until May 1, 2017).",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "# Previous dataset runs through 2017-02\ny_pre = infl.loc[:'2017-01'].copy()\nconst_pre = np.ones(len(y_pre))\nprint(y_pre.tail())",
            "code"
        ],
        [
            "PCE       CPI\nDATE\n2016-09  1.385688  2.022262\n2016-10  1.777645  1.445830\n2016-11  0.584472  1.631694\n2016-12  1.572165  2.109728\n2017-01  3.093392  2.623570",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "# For the updated dataset, we'll just add in the\n# CPI value for 2017-03\ny_post = infl.loc[:'2017-03'].copy()\ny_post.loc['2017-03', 'PCE'] = np.nan\nconst_post = np.ones(len(y_post))\n\n# Notice the missing value for PCE in 2017-03\nprint(y_post.tail())",
            "code"
        ],
        [
            "PCE       CPI\nDATE\n2016-11  0.584472  1.631694\n2016-12  1.572165  2.109728\n2017-01  3.093392  2.623570\n2017-02  2.337166  2.541355\n2017-03       NaN -0.258197",
            "code"
        ],
        [
            "We chose this particular example because in March 2017, core CPI prices fell for the first time since 2010, and this information may be useful in forecast core PCE prices for that month. The graph below shows the CPI and PCE price data as it would have been observed on April 14th\\(^\\dagger\\).",
            "markdown"
        ],
        [
            "\\(\\dagger\\) This statement is not entirely true, because both the CPI and PCE price indexes can be revised to a certain extent after the fact. As a result, the series that we\u2019re pulling are not exactly like those observed on April 14, 2017. This could be fixed by pulling the archived data from  instead of , but the data we have is good enough for this tutorial.",
            "markdown"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "# Plot the updated dataset\nfig, ax = plt.subplots(figsize=(15, 3))\ny_post.plot(ax=ax)\nax.hlines(0, '2009', '2017-06', linewidth=1.0)\nax.set_xlim('2009', '2017-06');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_43_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_43_0.png\"/>",
            "code"
        ],
        [
            "To perform the exercise, we first construct and fit a DynamicFactor model. Specifically:",
            "markdown"
        ],
        [
            "We are using a single dynamic factor (k_factors=1)",
            "markdown"
        ],
        [
            "We are modeling the factor\u2019s dynamics with an AR(6) model (factor_order=6)",
            "markdown"
        ],
        [
            "We have included a vector of ones as an exogenous variable (exog=const_pre), because the inflation series we are working with are not mean-zero.",
            "markdown"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "mod_pre = sm.tsa.DynamicFactor(y_pre, exog=const_pre, k_factors=1, factor_order=6)\nres_pre = mod_pre.fit()\nprint(res_pre.summary())",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =           12     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  4.82073D+00    |proj g|=  3.19124D-01",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ],
        [
            "At iterate    5    f=  4.63641D+00    |proj g|=  3.14144D-01\n  ys=-8.605E-01  -gs= 5.450E-01 BFGS update SKIPPED\n\nAt iterate   10    f=  2.95557D+00    |proj g|=  2.77682D-01\n\nAt iterate   15    f=  2.57945D+00    |proj g|=  2.07981D-01\n\nAt iterate   20    f=  2.43842D+00    |proj g|=  8.68367D-02\n\nAt iterate   25    f=  2.43091D+00    |proj g|=  1.15711D-02\n\nAt iterate   30    f=  2.42946D+00    |proj g|=  8.06022D-03\n\nAt iterate   35    f=  2.42937D+00    |proj g|=  8.24406D-05\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n   12     37     51      1     1     0   2.265D-05   2.429D+00\n  F =   2.4293664173073029\n\nCONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH\n                                   Statespace Model Results\n=============================================================================================\nDep. Variable:                        ['PCE', 'CPI']   No. Observations:                  216\nModel:             DynamicFactor(factors=1, order=6)   Log Likelihood                -524.743\n                                      + 1 regressors   AIC                           1073.486\nDate:                               Wed, 02 Nov 2022   BIC                           1113.990\nTime:                                       17:10:11   HQIC                          1089.850\nSample:                                   02-28-1999\n                                        - 01-31-2017\nCovariance Type:                                 opg\n===================================================================================\nLjung-Box (L1) (Q):             4.33, 0.55   Jarque-Bera (JB):         12.52, 10.68\nProb(Q):                        0.04, 0.46   Prob(JB):                   0.00, 0.00\nHeteroskedasticity (H):         0.57, 0.47   Skew:                      0.16, -0.14\nProb(H) (two-sided):            0.02, 0.00   Kurtosis:                   4.13, 4.05\n                           Results for equation PCE\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nloading.f1     0.5407      0.061      8.836      0.000       0.421       0.661\nbeta.const     1.7154      0.094     18.269      0.000       1.531       1.899\n                           Results for equation CPI\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nloading.f1     0.9033      0.104      8.717      0.000       0.700       1.106\nbeta.const     1.9620      0.136     14.375      0.000       1.694       2.230\n                        Results for factor equation f1\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nL1.f1          0.1246      0.069      1.805      0.071      -0.011       0.260\nL2.f1          0.1823      0.072      2.543      0.011       0.042       0.323\nL3.f1          0.0177      0.073      0.244      0.807      -0.125       0.160\nL4.f1         -0.0700      0.078     -0.893      0.372      -0.224       0.084\nL5.f1          0.1561      0.068      2.304      0.021       0.023       0.289\nL6.f1          0.1376      0.075      1.838      0.066      -0.009       0.284\n                           Error covariance matrix\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nsigma2.PCE     0.5517      0.066      8.381      0.000       0.423       0.681\nsigma2.CPI  3.132e-10      0.149   2.11e-09      1.000      -0.291       0.291\n==============================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "With the fitted model in hand, we now construct the news and impacts associated with observing the CPI for March 2017. The updated data is for February 2017 and part of March 2017, and we\u2019ll examining the impacts on both March and April.",
            "markdown"
        ],
        [
            "In the univariate example, we first created an updated results object, and then passed that to the news method. Here, we\u2019re creating the news by directly passing the updated dataset.",
            "markdown"
        ],
        [
            "Notice that:",
            "markdown"
        ],
        [
            "y_post contains the entire updated dataset (not just the new datapoints)",
            "markdown"
        ],
        [
            "We also had to pass an updated exog array. This array must cover <strong>both</strong>:",
            "markdown"
        ],
        [
            "The entire period associated with y_post",
            "markdown"
        ],
        [
            "Any additional datapoints after the end of y_post through the last impact date, specified by end",
            "markdown"
        ],
        [
            "Here, y_post ends in March 2017, so we needed our exog to extend one more period, to April 2017.",
            "markdown"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "code"
        ],
        [
            "<strong>Note</strong>:",
            "markdown"
        ],
        [
            "In the univariate example, above, we first constructed a new results object, and then passed that to the news method. We could have done that here too, although there is an extra step required. Since we are requesting an impact for a period beyond the end of y_post, we would still need to pass the additional value for the exog variable during that period to news:",
            "markdown"
        ],
        [
            "res_post = res_pre.apply(y_post, exog=const_post)\nnews = res_pre.news(res_post, exog=[1.], start='2017-03', end='2017-04')\n\n\n</blockquote>",
            "code"
        ],
        [
            "Now that we have computed the news, printing summary is a convenient way to see the results.",
            "markdown"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "# Show the summary of the news results\nprint(news.summary())",
            "code"
        ],
        [
            "News\n==============================================================================\nModel:                  DynamicFactor   Original sample:               1999-02\nDate:                Wed, 02 Nov 2022                                - 2017-01\nTime:                        17:10:11   Update through:                2017-04\n                                        No. Revisions:                       0\n                                        No. New datapoints:                  3\n                                  Impacts\n===========================================================================\nimpact date impacted variable estimate (prev) impact of news estimate (new)\n---------------------------------------------------------------------------\n    2017-03               CPI            2.07          -2.33          -0.26\n                          PCE            1.78          -1.39           0.39\n    2017-04               CPI            1.90          -0.23           1.67\n                          PCE            1.68          -0.14           1.54\n                  News from updated observations:\n===================================================================\nupdate date updated variable   observed forecast (prev)        news\n-------------------------------------------------------------------\n    2017-02              CPI       2.54            2.24        0.30\n                         PCE       2.34            1.88        0.46\n    2017-03              CPI      -0.26            2.07       -2.33\n===================================================================",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/tsa/statespace/news.py:594: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n  impacts.iloc[:, :2] = impacts.iloc[:, :2].applymap(str)",
            "code"
        ],
        [
            "Because we have multiple variables, by default the summary only shows the news from updated data along and the total impacts.",
            "markdown"
        ],
        [
            "From the first table, we can see that our updated dataset contains three new data points, with most of the \u201cnews\u201d from these data coming from the very low reading in March 2017.",
            "markdown"
        ],
        [
            "The second table shows that these three datapoints substantially impacted the estimate for PCE in March 2017 (which was not yet observed). This estimate revised down by nearly 1.5 percentage points.",
            "markdown"
        ],
        [
            "The updated data also impacted the forecasts in the first out-of-sample month, April 2017. After incorporating the new data, the model\u2019s forecasts for CPI and PCE inflation in that month revised down 0.29 and 0.17 percentage point, respectively.",
            "markdown"
        ],
        [
            "While these tables show the \u201cnews\u201d and the total impacts, they do not show how much of each impact was caused by each updated datapoint. To see that information, we need to look at the details tables.",
            "markdown"
        ],
        [
            "One way to see the details tables is to pass include_details=True to the summary method. To avoid repeating the tables above, however, we\u2019ll just call the summary_details method directly.",
            "markdown"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "print(news.summary_details())",
            "code"
        ],
        [
            "Details for [updated variable = CPI]\n======================================================================================================\nupdate date   observed forecast (prev) impact date impacted variable        news     weight     impact\n------------------------------------------------------------------------------------------------------\n    2017-02       2.54            2.24     2017-04               CPI        0.30       0.18       0.06\n                                                                 PCE        0.30       0.11       0.03\n    2017-03      -0.26            2.07     2017-03               CPI       -2.33       1.00      -2.33\n                                                                 PCE       -2.33       0.60      -1.39\n                                           2017-04               CPI       -2.33       0.12      -0.29\n                                                                 PCE       -2.33       0.07      -0.17\n======================================================================================================",
            "code"
        ],
        [
            "This table shows that most of the revisions to the estimate of PCE in April 2017, described above, came from the news associated with the CPI release in March 2017. By contrast, the CPI release in February had only a little effect on the April forecast, and the PCE release in February had essentially no effect.",
            "markdown"
        ]
    ],
    "Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Bibliography": [
        [
            "Ba\u0144bura, Marta, Domenico Giannone, and Lucrezia Reichlin. \u201cNowcasting.\u201d The Oxford Handbook of Economic Forecasting. July 8, 2011.",
            "markdown"
        ],
        [
            "Ba\u0144bura, Marta, Domenico Giannone, Michele Modugno, and Lucrezia Reichlin. \u201cNow-casting and the real-time data flow.\u201d In Handbook of economic forecasting, vol.\u00a02, pp.\u00a0195-237. Elsevier, 2013.",
            "markdown"
        ],
        [
            "Ba\u0144bura, Marta, and Michele Modugno. \u201cMaximum likelihood estimation of factor models on datasets with arbitrary pattern of missing data.\u201d Journal of Applied Econometrics 29, no. 1 (2014): 133-160.",
            "markdown"
        ],
        [
            "Knotek, Edward S., and Saeed Zaman. \u201cNowcasting US headline and core inflation.\u201d Journal of Money, Credit and Banking 49, no. 5 (2017): 931-968.",
            "markdown"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models": [
        [
            "The true power of the state space model is to allow the creation and estimation of custom models. This notebook shows various statespace models that subclass sm.tsa.statespace.MLEModel.",
            "markdown"
        ],
        [
            "Remember the general state space model can be written in the following general way:\n\n\\[\\begin{split}\\begin{aligned}\ny_t & = Z_t \\alpha_{t} + d_t +  \\varepsilon_t \\\\\n\\alpha_{t+1} & = T_t \\alpha_{t} + c_t + R_t \\eta_{t}\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "You can check the details and the dimensions of the objects",
            "markdown"
        ],
        [
            "Most models won\u2019t include all of these elements. For example, the design matrix \\(Z_t\\) might not depend on time (\\(\\forall t \\;Z_t = Z\\)), or the model won\u2019t have an observation intercept \\(d_t\\).",
            "markdown"
        ],
        [
            "We\u2019ll start with something relatively simple and then show how to extend it bit by bit to include more elements.",
            "markdown"
        ],
        [
            "Model 1: time-varying coefficients. One observation equation with two state equations",
            "markdown"
        ],
        [
            "Model 2: time-varying parameters with non identity transition matrix",
            "markdown"
        ],
        [
            "Model 3: multiple observation and multiple state equations",
            "markdown"
        ],
        [
            "Bonus: pymc3 for Bayesian estimation",
            "markdown"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nfrom collections import OrderedDict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=15)",
            "code"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models->Model 1: time-varying coefficients": [
        [
            "The observed data is \\(y_t, x_t, w_t\\). With \\(x_t, w_t\\) being the exogenous variables. Notice that the design matrix is time-varying, so it will have three dimensions (k_endog x k_states x nobs)",
            "markdown"
        ],
        [
            "The states are \\(\\beta_{x,t}\\) and \\(\\beta_{w,t}\\). The state equation tells us these states evolve with a random walk. Thus, in this case the transition matrix is a 2 by 2 identity matrix.",
            "markdown"
        ],
        [
            "We\u2019ll first simulate the data, the construct a model and finally estimate it.",
            "markdown"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "def gen_data_for_model1():\n    nobs = 1000\n\n    rs = np.random.RandomState(seed=93572)\n\n    d = 5\n    var_y = 5\n    var_coeff_x = 0.01\n    var_coeff_w = 0.5\n\n    x_t = rs.uniform(size=nobs)\n    w_t = rs.uniform(size=nobs)\n    eps = rs.normal(scale=var_y ** 0.5, size=nobs)\n\n    beta_x = np.cumsum(rs.normal(size=nobs, scale=var_coeff_x ** 0.5))\n    beta_w = np.cumsum(rs.normal(size=nobs, scale=var_coeff_w ** 0.5))\n\n    y_t = d + beta_x * x_t + beta_w * w_t + eps\n    return y_t, x_t, w_t, beta_x, beta_w\n\n\ny_t, x_t, w_t, beta_x, beta_w = gen_data_for_model1()\n_ = plt.plot(y_t)",
            "code"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "class TVRegression(sm.tsa.statespace.MLEModel):\n    def __init__(self, y_t, x_t, w_t):\n        exog = np.c_[x_t, w_t]  # shaped nobs x 2\n\n        super(TVRegression, self).__init__(\n            endog=y_t, exog=exog, k_states=2, initialization=\"diffuse\"\n        )\n\n        # Since the design matrix is time-varying, it must be\n        # shaped k_endog x k_states x nobs\n        # Notice that exog.T is shaped k_states x nobs, so we\n        # just need to add a new first axis with shape 1\n        self.ssm[\"design\"] = exog.T[np.newaxis, :, :]  # shaped 1 x 2 x nobs\n        self.ssm[\"selection\"] = np.eye(self.k_states)\n        self.ssm[\"transition\"] = np.eye(self.k_states)\n\n        # Which parameters need to be positive?\n        self.positive_parameters = slice(1, 4)\n\n    @property\n    def param_names(self):\n        return [\"intercept\", \"var.e\", \"var.x.coeff\", \"var.w.coeff\"]\n\n    @property\n    def start_params(self):\n        \"\"\"\n        Defines the starting values for the parameters\n        The linear regression gives us reasonable starting values for the constant\n        d and the variance of the epsilon error\n        \"\"\"\n        exog = sm.add_constant(self.exog)\n        res = sm.OLS(self.endog, exog).fit()\n        params = np.r_[res.params[0], res.scale, 0.001, 0.001]\n        return params\n\n    def transform_params(self, unconstrained):\n        \"\"\"\n        We constraint the last three parameters\n        ('var.e', 'var.x.coeff', 'var.w.coeff') to be positive,\n        because they are variances\n        \"\"\"\n        constrained = unconstrained.copy()\n        constrained[self.positive_parameters] = (\n            constrained[self.positive_parameters] ** 2\n        )\n        return constrained\n\n    def untransform_params(self, constrained):\n        \"\"\"\n        Need to unstransform all the parameters you transformed\n        in the `transform_params` function\n        \"\"\"\n        unconstrained = constrained.copy()\n        unconstrained[self.positive_parameters] = (\n            unconstrained[self.positive_parameters] ** 0.5\n        )\n        return unconstrained\n\n    def update(self, params, **kwargs):\n        params = super(TVRegression, self).update(params, **kwargs)\n\n        self[\"obs_intercept\", 0, 0] = params[0]\n        self[\"obs_cov\", 0, 0] = params[1]\n        self[\"state_cov\"] = np.diag(params[2:4])",
            "code"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models->Model 1: time-varying coefficients->And then estimate it with our custom model class": [
        [
            "[ ]:",
            "code"
        ],
        [
            "mod = TVRegression(y_t, x_t, w_t)\nres = mod.fit()\n\nprint(res.summary())",
            "code"
        ],
        [
            "The values that generated the data were:",
            "markdown"
        ],
        [
            "intercept = 5",
            "markdown"
        ],
        [
            "var.e = 5",
            "markdown"
        ],
        [
            "var.x.coeff = 0.01",
            "markdown"
        ],
        [
            "var.w.coeff = 0.5",
            "markdown"
        ],
        [
            "As you can see, the estimation recovered the real parameters pretty well.",
            "markdown"
        ],
        [
            "We can also recover the estimated evolution of the underlying coefficients (or states in Kalman filter talk)",
            "markdown"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "fig, axes = plt.subplots(2, figsize=(16, 8))\n\nss = pd.DataFrame(res.smoothed_state.T, columns=[\"x\", \"w\"])\n\naxes[0].plot(beta_x, label=\"True\")\naxes[0].plot(ss[\"x\"], label=\"Smoothed estimate\")\naxes[0].set(title=\"Time-varying coefficient on x_t\")\naxes[0].legend()\n\naxes[1].plot(beta_w, label=\"True\")\naxes[1].plot(ss[\"w\"], label=\"Smoothed estimate\")\naxes[1].set(title=\"Time-varying coefficient on w_t\")\naxes[1].legend()\n\nfig.tight_layout();",
            "code"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models->Model 2: time-varying parameters with non identity transition matrix": [
        [
            "This is a small extension from Model 1. Instead of having an identity transition matrix, we\u2019ll have one with two parameters (\\(\\rho_1, \\rho_2\\)) that we need to estimate.\n\n\\[\\begin{split}\\begin{aligned}\ny_t & = d + x_t \\beta_{x,t} + w_t \\beta_{w,t} + \\varepsilon_t \\hspace{4em} \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)\\\\\n\\begin{bmatrix} \\beta_{x,t} \\\\ \\beta_{w,t} \\end{bmatrix} & = \\begin{bmatrix} \\rho_1 & 0 \\\\ 0 & \\rho_2 \\end{bmatrix} \\begin{bmatrix} \\beta_{x,t-1} \\\\ \\beta_{w,t-1} \\end{bmatrix} + \\begin{bmatrix} \\zeta_{x,t} \\\\ \\zeta_{w,t} \\end{bmatrix} \\hspace{3.7em} \\begin{bmatrix} \\zeta_{x,t} \\\\ \\zeta_{w,t} \\end{bmatrix} \\sim N \\left ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_{\\beta, x}^2 & 0 \\\\ 0 & \\sigma_{\\beta, w}^2 \\end{bmatrix} \\right )\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "What should we modify in our previous class to make things work? + Good news: not a lot! + Bad news: we need to be careful about a few things",
            "markdown"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models->Model 2: time-varying parameters with non identity transition matrix->1) Change the starting parameters function": [
        [
            "We need to add names for the new parameters \\(\\rho_1, \\rho_2\\) and we need to start corresponding starting values.",
            "markdown"
        ],
        [
            "The param_names function goes from:",
            "markdown"
        ],
        [
            "def param_names(self):\n    return ['intercept', 'var.e', 'var.x.coeff', 'var.w.coeff']",
            "code"
        ],
        [
            "to",
            "markdown"
        ],
        [
            "def param_names(self):\n    return ['intercept', 'var.e', 'var.x.coeff', 'var.w.coeff',\n           'rho1', 'rho2']",
            "code"
        ],
        [
            "and we change the start_params function from",
            "markdown"
        ],
        [
            "def start_params(self):\n    exog = sm.add_constant(self.exog)\n    res = sm.OLS(self.endog, exog).fit()\n    params = np.r_[res.params[0], res.scale, 0.001, 0.001]\n    return params",
            "code"
        ],
        [
            "to",
            "markdown"
        ],
        [
            "def start_params(self):\n    exog = sm.add_constant(self.exog)\n    res = sm.OLS(self.endog, exog).fit()\n    params = np.r_[res.params[0], res.scale, 0.001, 0.001, 0.8, 0.8]\n    return params",
            "code"
        ],
        [
            "Change the update function",
            "markdown"
        ],
        [
            "It goes from",
            "markdown"
        ],
        [
            "def update(self, params, **kwargs):\n    params = super(TVRegression, self).update(params, **kwargs)\n\n    self['obs_intercept', 0, 0] = params[0]\n    self['obs_cov', 0, 0] = params[1]\n    self['state_cov'] = np.diag(params[2:4])",
            "code"
        ],
        [
            "to",
            "markdown"
        ],
        [
            "def update(self, params, **kwargs):\n    params = super(TVRegression, self).update(params, **kwargs)\n\n    self['obs_intercept', 0, 0] = params[0]\n    self['obs_cov', 0, 0] = params[1]\n    self['state_cov'] = np.diag(params[2:4])\n    self['transition', 0, 0] = params[4]\n    self['transition', 1, 1] = params[5]",
            "code"
        ],
        [
            "(optional) change transform_params and untransform_params",
            "markdown"
        ],
        [
            "This is not required, but you might wanna restrict \\(\\rho_1, \\rho_2\\) to lie between -1 and 1. In that case, we first import two utility functions from statsmodels.",
            "markdown"
        ],
        [
            "from statsmodels.tsa.statespace.tools import (\n    constrain_stationary_univariate, unconstrain_stationary_univariate)",
            "code"
        ],
        [
            "constrain_stationary_univariate constraint the value to be within -1 and 1. unconstrain_stationary_univariate provides the inverse function. The transform and untransform parameters function would look like this (remember that \\(\\rho_1, \\rho_2\\) are in the 4 and 5th index):",
            "markdown"
        ],
        [
            "def transform_params(self, unconstrained):\n    constrained = unconstrained.copy()\n    constrained[self.positive_parameters] = constrained[self.positive_parameters]**2\n    constrained[4] = constrain_stationary_univariate(constrained[4:5])\n    constrained[5] = constrain_stationary_univariate(constrained[5:6])\n    return constrained\n\ndef untransform_params(self, constrained):\n    unconstrained = constrained.copy()\n    unconstrained[self.positive_parameters] = unconstrained[self.positive_parameters]**0.5\n    unconstrained[4] = unconstrain_stationary_univariate(constrained[4:5])\n    unconstrained[5] = unconstrain_stationary_univariate(constrained[5:6])\n    return unconstrained",
            "code"
        ],
        [
            "I\u2019ll write the full class below (without the optional changes I have just discussed)",
            "markdown"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "class TVRegressionExtended(sm.tsa.statespace.MLEModel):\n    def __init__(self, y_t, x_t, w_t):\n        exog = np.c_[x_t, w_t]  # shaped nobs x 2\n\n        super(TVRegressionExtended, self).__init__(\n            endog=y_t, exog=exog, k_states=2, initialization=\"diffuse\"\n        )\n\n        # Since the design matrix is time-varying, it must be\n        # shaped k_endog x k_states x nobs\n        # Notice that exog.T is shaped k_states x nobs, so we\n        # just need to add a new first axis with shape 1\n        self.ssm[\"design\"] = exog.T[np.newaxis, :, :]  # shaped 1 x 2 x nobs\n        self.ssm[\"selection\"] = np.eye(self.k_states)\n        self.ssm[\"transition\"] = np.eye(self.k_states)\n\n        # Which parameters need to be positive?\n        self.positive_parameters = slice(1, 4)\n\n    @property\n    def param_names(self):\n        return [\"intercept\", \"var.e\", \"var.x.coeff\", \"var.w.coeff\", \"rho1\", \"rho2\"]\n\n    @property\n    def start_params(self):\n        \"\"\"\n        Defines the starting values for the parameters\n        The linear regression gives us reasonable starting values for the constant\n        d and the variance of the epsilon error\n        \"\"\"\n\n        exog = sm.add_constant(self.exog)\n        res = sm.OLS(self.endog, exog).fit()\n        params = np.r_[res.params[0], res.scale, 0.001, 0.001, 0.7, 0.8]\n        return params\n\n    def transform_params(self, unconstrained):\n        \"\"\"\n        We constraint the last three parameters\n        ('var.e', 'var.x.coeff', 'var.w.coeff') to be positive,\n        because they are variances\n        \"\"\"\n        constrained = unconstrained.copy()\n        constrained[self.positive_parameters] = (\n            constrained[self.positive_parameters] ** 2\n        )\n        return constrained\n\n    def untransform_params(self, constrained):\n        \"\"\"\n        Need to unstransform all the parameters you transformed\n        in the `transform_params` function\n        \"\"\"\n        unconstrained = constrained.copy()\n        unconstrained[self.positive_parameters] = (\n            unconstrained[self.positive_parameters] ** 0.5\n        )\n        return unconstrained\n\n    def update(self, params, **kwargs):\n        params = super(TVRegressionExtended, self).update(params, **kwargs)\n\n        self[\"obs_intercept\", 0, 0] = params[0]\n        self[\"obs_cov\", 0, 0] = params[1]\n        self[\"state_cov\"] = np.diag(params[2:4])\n        self[\"transition\", 0, 0] = params[4]\n        self[\"transition\", 1, 1] = params[5]",
            "code"
        ],
        [
            "To estimate, we\u2019ll use the same data as in model 1 and expect the \\(\\rho_1, \\rho_2\\) to be near 1.",
            "markdown"
        ],
        [
            "The results look pretty good! Note that this estimation can be quite sensitive to the starting value of \\(\\rho_1, \\rho_2\\). If you try lower values, you\u2019ll see it fails to converge.",
            "markdown"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "mod = TVRegressionExtended(y_t, x_t, w_t)\nres = mod.fit(maxiter=2000)  # it doesn't converge with 50 iters\nprint(res.summary())",
            "code"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations": [
        [
            "We\u2019ll keep the time-varying parameters, but this time we\u2019ll also have two observation equations.",
            "markdown"
        ],
        [
            "State equations\n\n\\[\\alpha_{1, t+1} = \\delta_1 \\alpha_{1, t} + \\delta_2 \\alpha_{2, t} + W_1\\]\n\n\\[\\alpha_{2, t+1} = \\delta_3  \\alpha_{2, t} + W_2\\]",
            "markdown"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations->Observation equations": [
        [
            "\\(\\hat{i_t}, \\hat{M_t}, \\hat{s_t}\\) are observed each period.",
            "markdown"
        ],
        [
            "The model for the observation equation has two equations:\n\n\\[\\hat{i_t} = \\alpha_1 * \\hat{s_t} + \\varepsilon_1\\]\n\n\\[\\hat{M_t} = \\alpha_2 + \\varepsilon_2\\]",
            "markdown"
        ],
        [
            "Following the , the endogenous part of the observation equation is \\(y_t = (\\hat{i_t}, \\hat{M_t})\\) and we only have one exogenous variable \\(\\hat{s_t}\\)",
            "markdown"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations->Matrix notation for the state space model": [
        [
            "I\u2019ll simulate some data, talk about what we need to modify and finally estimate the model to see if we\u2019re recovering something reasonable.",
            "markdown"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "true_values = {\n    \"var_e1\": 0.01,\n    \"var_e2\": 0.01,\n    \"var_w1\": 0.01,\n    \"var_w2\": 0.01,\n    \"delta1\": 0.8,\n    \"delta2\": 0.5,\n    \"delta3\": 0.7,\n}\n\n\ndef gen_data_for_model3():\n    # Starting values\n    alpha1_0 = 2.1\n    alpha2_0 = 1.1\n\n    t_max = 500\n\n    def gen_i(alpha1, s):\n        return alpha1 * s + np.sqrt(true_values[\"var_e1\"]) * np.random.randn()\n\n    def gen_m_hat(alpha2):\n        return 1 * alpha2 + np.sqrt(true_values[\"var_e2\"]) * np.random.randn()\n\n    def gen_alpha1(alpha1, alpha2):\n        w1 = np.sqrt(true_values[\"var_w1\"]) * np.random.randn()\n        return true_values[\"delta1\"] * alpha1 + true_values[\"delta2\"] * alpha2 + w1\n\n    def gen_alpha2(alpha2):\n        w2 = np.sqrt(true_values[\"var_w2\"]) * np.random.randn()\n        return true_values[\"delta3\"] * alpha2 + w2\n\n    s_t = 0.3 + np.sqrt(1.4) * np.random.randn(t_max)\n    i_hat = np.empty(t_max)\n    m_hat = np.empty(t_max)\n\n    current_alpha1 = alpha1_0\n    current_alpha2 = alpha2_0\n    for t in range(t_max):\n        # Obs eqns\n        i_hat[t] = gen_i(current_alpha1, s_t[t])\n        m_hat[t] = gen_m_hat(current_alpha2)\n\n        # state eqns\n        new_alpha1 = gen_alpha1(current_alpha1, current_alpha2)\n        new_alpha2 = gen_alpha2(current_alpha2)\n\n        # Update states for next period\n        current_alpha1 = new_alpha1\n        current_alpha2 = new_alpha2\n\n    return i_hat, m_hat, s_t\n\n\ni_hat, m_hat, s_t = gen_data_for_model3()",
            "code"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations->What do we need to modify?": [
        [
            "Once again, we don\u2019t need to change much, but we need to be careful about the dimensions.",
            "markdown"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations->What do we need to modify?->1) The __init__ function changes from": [
        [
            "def __init__(self, y_t, x_t, w_t):\n        exog = np.c_[x_t, w_t]\n\n        super(TVRegressionExtended, self).__init__(\n            endog=y_t, exog=exog, k_states=2,\n            initialization='diffuse')\n\n        self.ssm['design'] = exog.T[np.newaxis, :, :]  # shaped 1 x 2 x nobs\n        self.ssm['selection'] = np.eye(self.k_states)\n        self.ssm['transition'] = np.eye(self.k_states)",
            "code"
        ],
        [
            "to",
            "markdown"
        ],
        [
            "def __init__(self, i_t: np.array, s_t: np.array, m_t: np.array):\n\n        exog = np.c_[s_t, np.repeat(1, len(s_t))]  # exog.shape = (nobs, 2)\n\n        super(MultipleYsModel, self).__init__(\n            endog=np.c_[i_t, m_t], exog=exog, k_states=2,\n            initialization='diffuse')\n\n        self.ssm['design'] = np.zeros((self.k_endog, self.k_states, self.nobs))\n        self.ssm['design', 0, 0, :] = s_t\n        self.ssm['design', 1, 1, :] = 1",
            "code"
        ],
        [
            "Note that we did not have to specify k_endog anywhere. The initialization does this for us after checking the dimensions of the endog matrix.",
            "markdown"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations->What do we need to modify?->2) The update() function": [
        [
            "changes from",
            "markdown"
        ],
        [
            "def update(self, params, **kwargs):\n    params = super(TVRegressionExtended, self).update(params, **kwargs)\n\n    self['obs_intercept', 0, 0] = params[0]\n    self['obs_cov', 0, 0] = params[1]\n\n    self['state_cov'] = np.diag(params[2:4])\n    self['transition', 0, 0] = params[4]\n    self['transition', 1, 1] = params[5]",
            "code"
        ],
        [
            "to",
            "markdown"
        ],
        [
            "def update(self, params, **kwargs):\n    params = super(MultipleYsModel, self).update(params, **kwargs)\n\n\n    #The following line is not needed (by default, this matrix is initialized by zeroes),\n    #But I leave it here so the dimensions are clearer\n    self['obs_intercept'] = np.repeat([np.array([0, 0])], self.nobs, axis=0).T\n    self['obs_cov', 0, 0] = params[0]\n    self['obs_cov', 1, 1] = params[1]\n\n    self['state_cov'] = np.diag(params[2:4])\n    #delta1, delta2, delta3\n    self['transition', 0, 0] = params[4]\n    self['transition', 0, 1] = params[5]\n    self['transition', 1, 1] = params[6]",
            "code"
        ],
        [
            "The rest of the methods change in pretty obvious ways (need to add parameter names, make sure the indexes work, etc). The full code for the function is right below",
            "markdown"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "starting_values = {\n    \"var_e1\": 0.2,\n    \"var_e2\": 0.1,\n    \"var_w1\": 0.15,\n    \"var_w2\": 0.18,\n    \"delta1\": 0.7,\n    \"delta2\": 0.1,\n    \"delta3\": 0.85,\n}\n\n\nclass MultipleYsModel(sm.tsa.statespace.MLEModel):\n    def __init__(self, i_t: np.array, s_t: np.array, m_t: np.array):\n\n        exog = np.c_[s_t, np.repeat(1, len(s_t))]  # exog.shape = (nobs, 2)\n\n        super(MultipleYsModel, self).__init__(\n            endog=np.c_[i_t, m_t], exog=exog, k_states=2, initialization=\"diffuse\"\n        )\n\n        self.ssm[\"design\"] = np.zeros((self.k_endog, self.k_states, self.nobs))\n        self.ssm[\"design\", 0, 0, :] = s_t\n        self.ssm[\"design\", 1, 1, :] = 1\n\n        # These have ok shape. Placeholders since I'm changing them\n        # in the update() function\n        self.ssm[\"selection\"] = np.eye(self.k_states)\n        self.ssm[\"transition\"] = np.eye(self.k_states)\n\n        # Dictionary of positions to names\n        self.position_dict = OrderedDict(\n            var_e1=1, var_e2=2, var_w1=3, var_w2=4, delta1=5, delta2=6, delta3=7\n        )\n        self.initial_values = starting_values\n        self.positive_parameters = slice(0, 4)\n\n    @property\n    def param_names(self):\n        return list(self.position_dict.keys())\n\n    @property\n    def start_params(self):\n        \"\"\"\n        Initial values\n        \"\"\"\n        # (optional) Use scale for var_e1 and var_e2 starting values\n        params = np.r_[\n            self.initial_values[\"var_e1\"],\n            self.initial_values[\"var_e2\"],\n            self.initial_values[\"var_w1\"],\n            self.initial_values[\"var_w2\"],\n            self.initial_values[\"delta1\"],\n            self.initial_values[\"delta2\"],\n            self.initial_values[\"delta3\"],\n        ]\n        return params\n\n    def transform_params(self, unconstrained):\n        \"\"\"\n        If you need to restrict parameters\n        For example, variances should be  0\n        Parameters maybe have to be within -1 and 1\n        \"\"\"\n        constrained = unconstrained.copy()\n        constrained[self.positive_parameters] = (\n            constrained[self.positive_parameters] ** 2\n        )\n        return constrained\n\n    def untransform_params(self, constrained):\n        \"\"\"\n        Need to reverse what you did in transform_params()\n        \"\"\"\n        unconstrained = constrained.copy()\n        unconstrained[self.positive_parameters] = (\n            unconstrained[self.positive_parameters] ** 0.5\n        )\n        return unconstrained\n\n    def update(self, params, **kwargs):\n        params = super(MultipleYsModel, self).update(params, **kwargs)\n\n        # The following line is not needed (by default, this matrix is initialized by zeroes),\n        # But I leave it here so the dimensions are clearer\n        self[\"obs_intercept\"] = np.repeat([np.array([0, 0])], self.nobs, axis=0).T\n\n        self[\"obs_cov\", 0, 0] = params[0]\n        self[\"obs_cov\", 1, 1] = params[1]\n\n        self[\"state_cov\"] = np.diag(params[2:4])\n\n        # delta1, delta2, delta3\n        self[\"transition\", 0, 0] = params[4]\n        self[\"transition\", 0, 1] = params[5]\n        self[\"transition\", 1, 1] = params[6]",
            "code"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "mod = MultipleYsModel(i_hat, s_t, m_hat)\nres = mod.fit()\n\nprint(res.summary())",
            "code"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation": [
        [
            "In this section I\u2019ll show how you can take your custom state space model and easily plug it to pymc3 and estimate it with Bayesian methods. In particular, this example will show you an estimation with a version of Hamiltonian Monte Carlo called the No-U-Turn Sampler (NUTS).",
            "markdown"
        ],
        [
            "I\u2019m basically copying the ideas contained , so make sure to check that for more details.",
            "markdown"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "# Extra requirements\nimport pymc3 as pm\nimport theano\nimport theano.tensor as tt",
            "code"
        ],
        [
            "We need to define some helper functions to connect theano to the likelihood function that is implied in our model",
            "markdown"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "class Loglike(tt.Op):\n\n    itypes = [tt.dvector]  # expects a vector of parameter values when called\n    otypes = [tt.dscalar]  # outputs a single scalar value (the log likelihood)\n\n    def __init__(self, model):\n        self.model = model\n        self.score = Score(self.model)\n\n    def perform(self, node, inputs, outputs):\n        (theta,) = inputs  # contains the vector of parameters\n        llf = self.model.loglike(theta)\n        outputs[0][0] = np.array(llf)  # output the log-likelihood\n\n    def grad(self, inputs, g):\n        # the method that calculates the gradients - it actually returns the\n        # vector-Jacobian product - g[0] is a vector of parameter values\n        (theta,) = inputs  # our parameters\n        out = [g[0] * self.score(theta)]\n        return out\n\n\nclass Score(tt.Op):\n    itypes = [tt.dvector]\n    otypes = [tt.dvector]\n\n    def __init__(self, model):\n        self.model = model\n\n    def perform(self, node, inputs, outputs):\n        (theta,) = inputs\n        outputs[0][0] = self.model.score(theta)",
            "code"
        ],
        [
            "We\u2019ll simulate again the data we used for model 1. We\u2019ll also fit it again and save the results to compare them to the Bayesian posterior we get.",
            "markdown"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "y_t, x_t, w_t, beta_x, beta_w = gen_data_for_model1()\nplt.plot(y_t)",
            "code"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())",
            "code"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->Bayesian estimation": [
        [
            "We need to define a prior for each parameter and the number of draws and burn-in points",
            "markdown"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "# Set sampling params\nndraws = 3000  #  3000 number of draws from the distribution\nnburn = 600  # 600 number of \"burn-in points\" (which will be discarded)",
            "code"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "# Construct an instance of the Theano wrapper defined above, which\n# will allow PyMC3 to compute the likelihood and Jacobian in a way\n# that it can make use of. Here we are using the same model instance\n# created earlier for MLE analysis (we could also create a new model\n# instance if we preferred)\nloglike = Loglike(mod)\n\nwith pm.Model():\n    # Priors\n    intercept = pm.Uniform(\"intercept\", 1, 10)\n    var_e = pm.InverseGamma(\"var.e\", 2.3, 0.5)\n    var_x_coeff = pm.InverseGamma(\"var.x.coeff\", 2.3, 0.1)\n    var_w_coeff = pm.InverseGamma(\"var.w.coeff\", 2.3, 0.1)\n\n    # convert variables to tensor vectors\n    theta = tt.as_tensor_variable([intercept, var_e, var_x_coeff, var_w_coeff])\n\n    # use a DensityDist (use a lamdba function to \"call\" the Op)\n    pm.DensityDist(\"likelihood\", loglike, observed=theta)\n\n    # Draw samples\n    trace = pm.sample(\n        ndraws,\n        tune=nburn,\n        return_inferencedata=True,\n        cores=1,\n        compute_convergence_checks=False,\n    )",
            "code"
        ]
    ],
    "Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->How does the posterior distribution compare with the MLE estimation?": [
        [
            "The clearly peak around the MLE estimate.",
            "markdown"
        ],
        [
            "[ ]:",
            "code"
        ],
        [
            "results_dict = {\n    \"intercept\": res_mle.params[0],\n    \"var.e\": res_mle.params[1],\n    \"var.x.coeff\": res_mle.params[2],\n    \"var.w.coeff\": res_mle.params[3],\n}\nplt.tight_layout()\n_ = pm.plot_trace(\n    trace,\n    lines=[(k, {}, [v]) for k, v in dict(results_dict).items()],\n    combined=True,\n    figsize=(12, 12),\n)",
            "code"
        ]
    ],
    "Examples->State space models->ETS models": [
        [
            "The ETS models are a family of time series models with an underlying state space model consisting of a level component, a trend component (T), a seasonal component (S), and an error term (E).",
            "markdown"
        ],
        [
            "This notebook shows how they can be used with statsmodels. For a more thorough treatment we refer to [1], chapter 8 (free online resource), on which the implementation in statsmodels and the examples used in this notebook are based.",
            "markdown"
        ],
        [
            "statsmodels implements all combinations of: - additive and multiplicative error model - additive and multiplicative trend, possibly dampened - additive and multiplicative seasonality",
            "markdown"
        ],
        [
            "However, not all of these methods are stable. Refer to [1] and references therein for more info about model stability.",
            "markdown"
        ],
        [
            "[1] Hyndman, Rob J., and George Athanasopoulos. Forecasting: principles and practice, 3rd edition, OTexts, 2019.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline\nfrom statsmodels.tsa.exponential_smoothing.ets import ETSModel",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "plt.rcParams[\"figure.figsize\"] = (12, 8)",
            "code"
        ]
    ],
    "Examples->State space models->ETS models->Simple exponential smoothing": [
        [
            "The simplest of the ETS models is also known as simple exponential smoothing. In ETS terms, it corresponds to the (A, N, N) model, that is, a model with additive errors, no trend, and no seasonality. The state space formulation of Holt\u2019s method is:",
            "markdown"
        ],
        [
            "\\begin{align}\ny_{t} &= y_{t-1} + e_t\\\\\nl_{t} &= l_{t-1} + \\alpha e_t\\\\\n\\end{align}",
            "markdown"
        ],
        [
            "This state space formulation can be turned into a different formulation, a forecast and a smoothing equation (as can be done with all ETS models):",
            "markdown"
        ],
        [
            "\\begin{align}\n\\hat{y}_{t|t-1} &= l_{t-1}\\\\\nl_{t} &= \\alpha y_{t-1} + (1 - \\alpha) l_{t-1}\n\\end{align}",
            "markdown"
        ],
        [
            "Here, \\(\\hat{y}_{t|t-1}\\) is the forecast/expectation of \\(y_t\\) given the information of the previous step. In the simple exponential smoothing model, the forecast corresponds to the previous level. The second equation (smoothing equation) calculates the next level as weighted average of the previous level and the previous observation.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "oildata = [\n    111.0091,\n    130.8284,\n    141.2871,\n    154.2278,\n    162.7409,\n    192.1665,\n    240.7997,\n    304.2174,\n    384.0046,\n    429.6622,\n    359.3169,\n    437.2519,\n    468.4008,\n    424.4353,\n    487.9794,\n    509.8284,\n    506.3473,\n    340.1842,\n    240.2589,\n    219.0328,\n    172.0747,\n    252.5901,\n    221.0711,\n    276.5188,\n    271.1480,\n    342.6186,\n    428.3558,\n    442.3946,\n    432.7851,\n    437.2497,\n    437.2092,\n    445.3641,\n    453.1950,\n    454.4096,\n    422.3789,\n    456.0371,\n    440.3866,\n    425.1944,\n    486.2052,\n    500.4291,\n    521.2759,\n    508.9476,\n    488.8889,\n    509.8706,\n    456.7229,\n    473.8166,\n    525.9509,\n    549.8338,\n    542.3405,\n]\noil = pd.Series(oildata, index=pd.date_range(\"1965\", \"2013\", freq=\"AS\"))\noil.plot()\nplt.ylabel(\"Annual oil production in Saudi Arabia (Mt)\")",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "Text(0, 0.5, 'Annual oil production in Saudi Arabia (Mt)')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ets_4_1.png\" src=\"../../../_images/examples_notebooks_generated_ets_4_1.png\"/>",
            "code"
        ],
        [
            "The plot above shows annual oil production in Saudi Arabia in million tonnes. The data are taken from the R package fpp2 (companion package to prior version [1]). Below you can see how to fit a simple exponential smoothing model using statsmodels\u2019s ETS implementation to this data. Additionally, the fit using forecast in R is shown as comparison.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "model = ETSModel(oil)\nfit = model.fit(maxiter=10000)\noil.plot(label=\"data\")\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nplt.ylabel(\"Annual oil production in Saudi Arabia (Mt)\")\n\n# obtained from R\nparams_R = [0.99989969, 0.11888177503085334, 0.80000197, 36.46466837, 34.72584983]\nyhat = model.smooth(params_R).fittedvalues\nyhat.plot(label=\"R fit\", linestyle=\"--\")\n\nplt.legend()",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  6.27365D+00    |proj g|=  8.99900D-01\n\nAt iterate    1    f=  5.31675D+00    |proj g|=  6.49880D-04\n\nAt iterate    2    f=  5.30939D+00    |proj g|=  5.55467D-04\n\nAt iterate    3    f=  5.29115D+00    |proj g|=  5.87086D-05\n\nAt iterate    4    f=  5.29096D+00    |proj g|=  1.86518D-06\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      4     13      5     0     1   1.865D-06   5.291D+00\n  F =   5.2909564503744404\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f19b560d990\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ets_6_2.png\" src=\"../../../_images/examples_notebooks_generated_ets_6_2.png\"/>",
            "code"
        ],
        [
            "By default the initial states are considered to be fitting parameters and are estimated by maximizing log-likelihood. It is possible to only use a heuristic for the initial values:",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "model_heuristic = ETSModel(oil, initialization_method=\"heuristic\")\nfit_heuristic = model_heuristic.fit()\noil.plot(label=\"data\")\nfit.fittedvalues.plot(label=\"estimated\")\nfit_heuristic.fittedvalues.plot(label=\"heuristic\", linestyle=\"--\")\nplt.ylabel(\"Annual oil production in Saudi Arabia (Mt)\")\n\n# obtained from R\nparams = [0.99989969, 0.11888177503085334, 0.80000197, 36.46466837, 34.72584983]\nyhat = model.smooth(params).fittedvalues\nyhat.plot(label=\"with R params\", linestyle=\":\")\n\nplt.legend()",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            1     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  6.27365D+00    |proj g|=  8.99900D-01\n\nAt iterate    1    f=  5.31675D+00    |proj g|=  0.00000D+00\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    1      1      2      1     0     1   0.000D+00   5.317D+00\n  F =   5.3167544390512402\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f19b5491ab0\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ets_8_2.png\" src=\"../../../_images/examples_notebooks_generated_ets_8_2.png\"/>",
            "code"
        ],
        [
            "The fitted parameters and some other measures are shown using fit.summary(). Here we can see that the log-likelihood of the model using fitted initial states is fractionally lower than the one using a heuristic for the initial states.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "print(fit.summary())",
            "code"
        ],
        [
            "ETS Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                   49\nModel:                       ETS(ANN)   Log Likelihood                -259.257\nDate:                Wed, 02 Nov 2022   AIC                            524.514\nTime:                        17:06:12   BIC                            530.189\nSample:                    01-01-1965   HQIC                           526.667\n                         - 01-01-2013   Scale                         2307.767\nCovariance Type:               approx\n===================================================================================\n                      coef    std err          z      P|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nsmoothing_level     0.9999      0.132      7.551      0.000       0.740       1.259\ninitial_level     110.7930     48.110      2.303      0.021      16.499     205.087\n===================================================================================\nLjung-Box (Q):                        1.87   Jarque-Bera (JB):                20.78\nProb(Q):                              0.39   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.49   Skew:                            -1.04\nProb(H) (two-sided):                  0.16   Kurtosis:                         5.42\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using numerical (complex-step) differentiation.",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "print(fit_heuristic.summary())",
            "code"
        ],
        [
            "ETS Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                   49\nModel:                       ETS(ANN)   Log Likelihood                -260.521\nDate:                Wed, 02 Nov 2022   AIC                            525.042\nTime:                        17:06:12   BIC                            528.826\nSample:                    01-01-1965   HQIC                           526.477\n                         - 01-01-2013   Scale                         2429.964\nCovariance Type:               approx\n===================================================================================\n                      coef    std err          z      P|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nsmoothing_level     0.9999      0.132      7.559      0.000       0.741       1.259\n==============================================\n              initialization method: heuristic\n----------------------------------------------\ninitial_level                          33.6309\n===================================================================================\nLjung-Box (Q):                        1.85   Jarque-Bera (JB):                18.42\nProb(Q):                              0.40   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.44   Skew:                            -1.02\nProb(H) (two-sided):                  0.11   Kurtosis:                         5.21\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using numerical (complex-step) differentiation.",
            "code"
        ]
    ],
    "Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method": [
        [
            "The exponential smoothing method can be modified to incorporate a trend and a seasonal component. In the additive Holt-Winters\u2019 method, the seasonal component is added to the rest. This model corresponds to the ETS(A, A, A) model, and has the following state space formulation:",
            "markdown"
        ],
        [
            "\\begin{align}\ny_t &= l_{t-1} + b_{t-1} + s_{t-m} + e_t\\\\\nl_{t} &= l_{t-1} + b_{t-1} + \\alpha e_t\\\\\nb_{t} &= b_{t-1} + \\beta e_t\\\\\ns_{t} &= s_{t-m} + \\gamma e_t\n\\end{align}",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "austourists_data = [\n    30.05251300,\n    19.14849600,\n    25.31769200,\n    27.59143700,\n    32.07645600,\n    23.48796100,\n    28.47594000,\n    35.12375300,\n    36.83848500,\n    25.00701700,\n    30.72223000,\n    28.69375900,\n    36.64098600,\n    23.82460900,\n    29.31168300,\n    31.77030900,\n    35.17787700,\n    19.77524400,\n    29.60175000,\n    34.53884200,\n    41.27359900,\n    26.65586200,\n    28.27985900,\n    35.19115300,\n    42.20566386,\n    24.64917133,\n    32.66733514,\n    37.25735401,\n    45.24246027,\n    29.35048127,\n    36.34420728,\n    41.78208136,\n    49.27659843,\n    31.27540139,\n    37.85062549,\n    38.83704413,\n    51.23690034,\n    31.83855162,\n    41.32342126,\n    42.79900337,\n    55.70835836,\n    33.40714492,\n    42.31663797,\n    45.15712257,\n    59.57607996,\n    34.83733016,\n    44.84168072,\n    46.97124960,\n    60.01903094,\n    38.37117851,\n    46.97586413,\n    50.73379646,\n    61.64687319,\n    39.29956937,\n    52.67120908,\n    54.33231689,\n    66.83435838,\n    40.87118847,\n    51.82853579,\n    57.49190993,\n    65.25146985,\n    43.06120822,\n    54.76075713,\n    59.83447494,\n    73.25702747,\n    47.69662373,\n    61.09776802,\n    66.05576122,\n]\nindex = pd.date_range(\"1999-03-01\", \"2015-12-01\", freq=\"3MS\")\naustourists = pd.Series(austourists_data, index=index)\naustourists.plot()\nplt.ylabel(\"Australian Tourists\")",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "Text(0, 0.5, 'Australian Tourists')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ets_13_1.png\" src=\"../../../_images/examples_notebooks_generated_ets_13_1.png\"/>",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "# fit in statsmodels\nmodel = ETSModel(\n    austourists,\n    error=\"add\",\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    seasonal_periods=4,\n)\nfit = model.fit()\n\n# fit with R params\nparams_R = [\n    0.35445427,\n    0.03200749,\n    0.39993387,\n    0.97999997,\n    24.01278357,\n    0.97770147,\n    1.76951063,\n    -0.50735902,\n    -6.61171798,\n    5.34956637,\n]\nfit_R = model.smooth(params_R)\n\naustourists.plot(label=\"data\")\nplt.ylabel(\"Australian Tourists\")\n\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nfit_R.fittedvalues.plot(label=\"R fit\", linestyle=\"--\")\nplt.legend()",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            9     M =           10\n\nAt X0         1 variables are exactly at the bounds\n\nAt iterate    0    f=  3.40132D+00    |proj g|=  9.88789D-01\n\nAt iterate    1    f=  2.58255D+00    |proj g|=  9.99244D-01\n\nAt iterate    2    f=  2.49918D+00    |proj g|=  2.90033D-01\n\nAt iterate    3    f=  2.48198D+00    |proj g|=  2.44942D-01\n\nAt iterate    4    f=  2.43118D+00    |proj g|=  7.29715D-02\n\nAt iterate    5    f=  2.42924D+00    |proj g|=  7.03572D-02\n\nAt iterate    6    f=  2.42851D+00    |proj g|=  4.66401D-02\n\nAt iterate    7    f=  2.42794D+00    |proj g|=  2.92420D-02\n\nAt iterate    8    f=  2.42784D+00    |proj g|=  2.53311D-02\n\nAt iterate    9    f=  2.42721D+00    |proj g|=  1.89532D-02\n\nAt iterate   10    f=  2.42622D+00    |proj g|=  3.18651D-02\n\nAt iterate   11    f=  2.42512D+00    |proj g|=  3.53011D-02\n\nAt iterate   12    f=  2.42383D+00    |proj g|=  3.65638D-02\n\nAt iterate   13    f=  2.42196D+00    |proj g|=  4.83560D-02\n\nAt iterate   14    f=  2.41828D+00    |proj g|=  5.99631D-02\n\nAt iterate   15    f=  2.41131D+00    |proj g|=  6.89763D-02\n\nAt iterate   16    f=  2.40200D+00    |proj g|=  7.65493D-02\n\nAt iterate   17    f=  2.39385D+00    |proj g|=  8.77260D-02\n\nAt iterate   18    f=  2.37917D+00    |proj g|=  1.52245D-01\n\nAt iterate   19    f=  2.35438D+00    |proj g|=  2.40363D-01\n\nAt iterate   20    f=  2.33832D+00    |proj g|=  2.53143D-01\n\nAt iterate   21    f=  2.33774D+00    |proj g|=  2.54902D-01\n\nAt iterate   22    f=  2.33766D+00    |proj g|=  2.49994D-01\n\nAt iterate   23    f=  2.33766D+00    |proj g|=  2.50104D-01\n  Positive dir derivative in projection\n  Using the backtracking step\n\nAt iterate   24    f=  2.32726D+00    |proj g|=  2.22975D-01\n\nAt iterate   25    f=  2.29784D+00    |proj g|=  1.16941D-01\n\nAt iterate   26    f=  2.29783D+00    |proj g|=  1.19878D-01\n\nAt iterate   27    f=  2.29783D+00    |proj g|=  1.21984D-01\n\nAt iterate   28    f=  2.29781D+00    |proj g|=  1.25790D-01\n\nAt iterate   29    f=  2.29113D+00    |proj g|=  8.44534D-02\n\nAt iterate   30    f=  2.27689D+00    |proj g|=  2.30209D-01\n\nAt iterate   31    f=  2.27528D+00    |proj g|=  1.25984D-01\n\nAt iterate   32    f=  2.27316D+00    |proj g|=  6.97733D-02\n\nAt iterate   33    f=  2.27205D+00    |proj g|=  2.60402D-02\n\nAt iterate   34    f=  2.27180D+00    |proj g|=  1.96301D-02\n\nAt iterate   35    f=  2.27153D+00    |proj g|=  4.38765D-03\n\nAt iterate   36    f=  2.27146D+00    |proj g|=  4.78666D-03\n\nAt iterate   37    f=  2.27062D+00    |proj g|=  2.37355D-02\n\nAt iterate   38    f=  2.26946D+00    |proj g|=  3.82704D-02\n\nAt iterate   39    f=  2.26699D+00    |proj g|=  4.70319D-02\n\nAt iterate   40    f=  2.26462D+00    |proj g|=  3.43058D-02\n\nAt iterate   41    f=  2.26203D+00    |proj g|=  2.18685D-02\n\nAt iterate   42    f=  2.26006D+00    |proj g|=  2.83495D-02\n\nAt iterate   43    f=  2.25881D+00    |proj g|=  4.32037D-02\n\nAt iterate   44    f=  2.25609D+00    |proj g|=  4.94953D-02\n\nAt iterate   45    f=  2.25369D+00    |proj g|=  1.20176D-01\n\nAt iterate   46    f=  2.25097D+00    |proj g|=  2.60216D-02\n\nAt iterate   47    f=  2.24903D+00    |proj g|=  3.29519D-02\n\nAt iterate   48    f=  2.24813D+00    |proj g|=  4.94365D-02\n\nAt iterate   49    f=  2.24642D+00    |proj g|=  4.11819D-02\n\nAt iterate   50    f=  2.24591D+00    |proj g|=  3.68314D-02\n\nAt iterate   51    f=  2.24526D+00    |proj g|=  1.04519D-02\n\nAt iterate   52    f=  2.24519D+00    |proj g|=  2.51985D-03\n\nAt iterate   53    f=  2.24519D+00    |proj g|=  2.27445D-03\n\nAt iterate   54    f=  2.24517D+00    |proj g|=  5.45697D-03\n\nAt iterate   55    f=  2.24516D+00    |proj g|=  1.00389D-02\n\nAt iterate   56    f=  2.24512D+00    |proj g|=  1.65810D-02\n\nAt iterate   57    f=  2.24506D+00    |proj g|=  1.84056D-02\n\nAt iterate   58    f=  2.24493D+00    |proj g|=  1.66877D-02\n\nAt iterate   59    f=  2.24475D+00    |proj g|=  1.22406D-02\n\nAt iterate   60    f=  2.24455D+00    |proj g|=  3.20739D-03\n\nAt iterate   61    f=  2.24452D+00    |proj g|=  1.36433D-03\n\nAt iterate   62    f=  2.24452D+00    |proj g|=  1.45146D-03\n\nAt iterate   63    f=  2.24452D+00    |proj g|=  9.39737D-04\n\nAt iterate   64    f=  2.24452D+00    |proj g|=  9.74687D-04\n\nAt iterate   65    f=  2.24452D+00    |proj g|=  6.42419D-04\n\nAt iterate   66    f=  2.24452D+00    |proj g|=  3.26539D-04\n\nAt iterate   67    f=  2.24452D+00    |proj g|=  3.99325D-04\n\nAt iterate   68    f=  2.24452D+00    |proj g|=  7.01572D-04\n\nAt iterate   69    f=  2.24452D+00    |proj g|=  1.01759D-03\n\nAt iterate   70    f=  2.24452D+00    |proj g|=  1.39750D-03\n\nAt iterate   71    f=  2.24451D+00    |proj g|=  1.67200D-03\n\nAt iterate   72    f=  2.24451D+00    |proj g|=  1.42490D-03\n\nAt iterate   73    f=  2.24451D+00    |proj g|=  1.05196D-03\n\nAt iterate   74    f=  2.24451D+00    |proj g|=  4.36984D-04\n\nAt iterate   75    f=  2.24451D+00    |proj g|=  1.07470D-04\n\nAt iterate   76    f=  2.24451D+00    |proj g|=  1.10312D-04\n\nAt iterate   77    f=  2.24451D+00    |proj g|=  1.12932D-04\n\nAt iterate   78    f=  2.24451D+00    |proj g|=  7.29194D-05\n\nAt iterate   79    f=  2.24451D+00    |proj g|=  5.37348D-05\n\nAt iterate   80    f=  2.24451D+00    |proj g|=  9.29035D-05\n\nAt iterate   81    f=  2.24451D+00    |proj g|=  1.07736D-04\n\nAt iterate   82    f=  2.24451D+00    |proj g|=  2.11209D-04\n\nAt iterate   83    f=  2.24451D+00    |proj g|=  2.05702D-04\n\nAt iterate   84    f=  2.24451D+00    |proj g|=  9.85434D-05\n\nAt iterate   85    f=  2.24451D+00    |proj g|=  6.38600D-05\n\nAt iterate   86    f=  2.24451D+00    |proj g|=  1.66445D-04\n\nAt iterate   87    f=  2.24451D+00    |proj g|=  1.44196D-04\n\nAt iterate   88    f=  2.24451D+00    |proj g|=  2.32481D-04\n\nAt iterate   89    f=  2.24451D+00    |proj g|=  2.30749D-04\n\nAt iterate   90    f=  2.24451D+00    |proj g|=  1.80833D-04\n\nAt iterate   91    f=  2.24451D+00    |proj g|=  9.96536D-05\n\nAt iterate   92    f=  2.24451D+00    |proj g|=  4.37872D-05\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    9     92    113    100     0     1   4.379D-05   2.245D+00\n  F =   2.2445125684008076\n\nCONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f19b0f4add0\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ets_14_2.png\" src=\"../../../_images/examples_notebooks_generated_ets_14_2.png\"/>",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "print(fit.summary())",
            "code"
        ],
        [
            "ETS Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                   68\nModel:                      ETS(AAdA)   Log Likelihood                -152.627\nDate:                Wed, 02 Nov 2022   AIC                            327.254\nTime:                        17:06:13   BIC                            351.668\nSample:                    03-01-1999   HQIC                           336.928\n                         - 12-01-2015   Scale                            5.213\nCovariance Type:               approx\n======================================================================================\n                         coef    std err          z      P|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nsmoothing_level        0.3398      0.111      3.070      0.002       0.123       0.557\nsmoothing_trend        0.0259      0.008      3.158      0.002       0.010       0.042\nsmoothing_seasonal     0.4011      0.080      5.041      0.000       0.245       0.557\ndamping_trend          0.9800        nan        nan        nan         nan         nan\ninitial_level         29.4435   2.82e+04      0.001      0.999   -5.52e+04    5.52e+04\ninitial_trend          0.6142      0.392      1.565      0.118      -0.155       1.383\ninitial_seasonal.0    -3.4340   2.82e+04     -0.000      1.000   -5.52e+04    5.52e+04\ninitial_seasonal.1    -5.9537   2.82e+04     -0.000      1.000   -5.52e+04    5.52e+04\ninitial_seasonal.2   -11.4807   2.82e+04     -0.000      1.000   -5.52e+04    5.52e+04\ninitial_seasonal.3          0   2.82e+04          0      1.000   -5.52e+04    5.52e+04\n===================================================================================\nLjung-Box (Q):                        5.76   Jarque-Bera (JB):                 7.70\nProb(Q):                              0.67   Prob(JB):                         0.02\nHeteroskedasticity (H):               0.46   Skew:                            -0.63\nProb(H) (two-sided):                  0.07   Kurtosis:                         4.05\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using numerical (complex-step) differentiation.",
            "code"
        ]
    ],
    "Examples->State space models->ETS models->Predictions": [
        [
            "The ETS model can also be used for predicting. There are several different methods available: - forecast: makes out of sample predictions - predict: in sample and out of sample predictions - simulate: runs simulations of the statespace model - get_prediction: in sample and out of sample predictions, as well as prediction intervals",
            "markdown"
        ],
        [
            "We can use them on our previously fitted model to predict from 2014 to 2020.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "pred = fit.get_prediction(start=\"2014\", end=\"2020\")",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "df = pred.summary_frame(alpha=0.05)\ndf",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "In this case the prediction intervals were calculated using an analytical formula. This is not available for all models. For these other models, prediction intervals are calculated by performing multiple simulations (1000 by default) and using the percentiles of the simulation results. This is done internally by the get_prediction method.",
            "markdown"
        ],
        [
            "We can also manually run simulations, e.g.\u00a0to plot them. Since the data ranges until end of 2015, we have to simulate from the first quarter of 2016 to the first quarter of 2020, which means 17 steps.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "simulated = fit.simulate(anchor=\"end\", nsimulations=17, repetitions=100)",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7f19b0b13d60\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ets_21_1.png\" src=\"../../../_images/examples_notebooks_generated_ets_21_1.png\"/>",
            "code"
        ],
        [
            "In this case, we chose \u201cend\u201d as simulation anchor, which means that the first simulated value will be the first out of sample value. It is also possible to choose other anchor inside the sample.",
            "markdown"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: concentrating out the scale": [
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\ndta = sm.datasets.macrodata.load_pandas().data\ndta.index = pd.date_range(start='1959Q1', end='2009Q4', freq='Q')",
            "code"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: concentrating out the scale->Introduction": [
        [
            "(much of this is based on Harvey (1989); see especially section 3.4)",
            "markdown"
        ],
        [
            "State space models can generically be written as follows (here we focus on time-invariant state space models, but similar results apply also to time-varying models):\n\n\\[\\begin{split}\\begin{align}\ny_t & = Z \\alpha_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, H) \\\\\n\\alpha_{t+1} & = T \\alpha_t + R \\eta_t \\quad \\eta_t \\sim N(0, Q)\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "Often, some or all of the values in the matrices \\(Z, H, T, R, Q\\) are unknown and must be estimated; in statsmodels, estimation is often done by finding the parameters that maximize the likelihood function. In particular, if we collect the parameters in a vector \\(\\psi\\), then each of these matrices can be thought of as functions of those parameters, for example \\(Z = Z(\\psi)\\), etc.",
            "markdown"
        ],
        [
            "Usually, the likelihood function is maximized numerically, for example by applying quasi-Newton \u201chill-climbing\u201d algorithms, and this becomes more and more difficult the more parameters there are. It turns out that in many cases we can reparameterize the model as \\([\\psi_*', \\sigma_*^2]'\\), where \\(\\sigma_*^2\\) is the \u201cscale\u201d of the model (usually, it replaces one of the error variance terms) and it is possible to find the maximum likelihood estimate of \\(\\sigma_*^2\\) analytically, by\ndifferentiating the likelihood function. This implies that numerical methods are only required to estimate the parameters \\(\\psi_*\\), which has dimension one less than that of \\(\\psi\\).",
            "markdown"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model": [
        [
            "(see, for example, section 4.2 of Harvey (1989))",
            "markdown"
        ],
        [
            "As a specific example, consider the local level model, which can be written as:\n\n\\[\\begin{split}\\begin{align}\ny_t & = \\alpha_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2) \\\\\n\\alpha_{t+1} & = \\alpha_t + \\eta_t \\quad \\eta_t \\sim N(0, \\sigma_\\eta^2)\n\\end{align}\\end{split}\\]",
            "markdown"
        ],
        [
            "In this model, \\(Z, T,\\) and \\(R\\) are all fixed to be equal to \\(1\\), and there are two unknown parameters, so that \\(\\psi = [\\sigma_\\varepsilon^2, \\sigma_\\eta^2]\\).",
            "markdown"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach": [
        [
            "First, we show how to define this model without concentrating out the scale, using statsmodels\u2019 state space library:",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "class LocalLevel(sm.tsa.statespace.MLEModel):\n    _start_params = [1., 1.]\n    _param_names = ['var.level', 'var.irregular']\n\n    def __init__(self, endog):\n        super(LocalLevel, self).__init__(endog, k_states=1, initialization='diffuse')\n\n        self['design', 0, 0] = 1\n        self['transition', 0, 0] = 1\n        self['selection', 0, 0] = 1\n\n    def transform_params(self, unconstrained):\n        return unconstrained**2\n\n    def untransform_params(self, unconstrained):\n        return unconstrained**0.5\n\n    def update(self, params, **kwargs):\n        params = super(LocalLevel, self).update(params, **kwargs)\n\n        self['state_cov', 0, 0] = params[0]\n        self['obs_cov', 0, 0] = params[1]\n<br/>",
            "code"
        ],
        [
            "There are two parameters in this model that must be chosen: var.level \\((\\sigma_\\eta^2)\\) and var.irregular \\((\\sigma_\\varepsilon^2)\\). We can use the built-in fit method to choose them by numerically maximizing the likelihood function.",
            "markdown"
        ],
        [
            "In our example, we are applying the local level model to consumer price index inflation.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "code"
        ],
        [
            "Statespace Model Results\n==============================================================================\nDep. Variable:                   infl   No. Observations:                  203\nModel:                     LocalLevel   Log Likelihood                -457.632\nDate:                Wed, 02 Nov 2022   AIC                            921.263\nTime:                        17:10:55   BIC                            931.203\nSample:                    03-31-1959   HQIC                           925.285\n                         - 09-30-2009\nCovariance Type:                  opg\n=================================================================================\n                    coef    std err          z      P|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nvar.level         0.7447      0.156      4.766      0.000       0.438       1.051\nvar.irregular     3.3733      0.315     10.715      0.000       2.756       3.990\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):               182.26\nProb(Q):                              0.99   Prob(JB):                         0.00\nHeteroskedasticity (H):               1.75   Skew:                            -1.02\nProb(H) (two-sided):                  0.02   Kurtosis:                         7.18\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "We can look at the results from the numerical optimizer in the results attribute mle_retvals:",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "print(res.mle_retvals)",
            "code"
        ],
        [
            "{'fopt': 2.254343511382187, 'gopt': array([-7.10311809e-06, -9.72870673e-06]), 'fcalls': 27, 'warnflag': 0, 'converged': True, 'iterations': 7}",
            "code"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale": [
        [
            "Now, there are two ways to reparameterize this model as above:",
            "markdown"
        ],
        [
            "The first way is to set \\(\\sigma_*^2 \\equiv \\sigma_\\varepsilon^2\\) so that \\(\\psi_* = \\psi / \\sigma_\\varepsilon^2 = [1, q_\\eta]\\) where \\(q_\\eta = \\sigma_\\eta^2 / \\sigma_\\varepsilon^2\\).",
            "markdown"
        ],
        [
            "The second way is to set \\(\\sigma_*^2 \\equiv \\sigma_\\eta^2\\) so that \\(\\psi_* = \\psi / \\sigma_\\eta^2 = [h, 1]\\) where \\(h = \\sigma_\\varepsilon^2 / \\sigma_\\eta^2\\).",
            "markdown"
        ],
        [
            "In the first case, we only need to numerically maximize the likelihood with respect to \\(q_\\eta\\), and in the second case we only need to numerically maximize the likelihood with respect to \\(h\\).",
            "markdown"
        ],
        [
            "Either approach would work well in most cases, and in the example below we will use the second method.",
            "markdown"
        ],
        [
            "To reformulate the model to take advantage of the concentrated likelihood function, we need to write the model in terms of the parameter vector \\(\\psi_* = [g, 1]\\). Because this parameter vector defines \\(\\sigma_\\eta^2 \\equiv 1\\), we now include a new line self['state_cov', 0, 0] = 1 and the only unknown parameter is \\(h\\). Because our parameter \\(h\\) is no longer a variance, we renamed it here to be ratio.irregular.",
            "markdown"
        ],
        [
            "The key piece that is required to formulate the model so that the scale can be computed from the Kalman filter recursions (rather than selected numerically) is setting the flag self.ssm.filter_concentrated = True.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "class LocalLevelConcentrated(sm.tsa.statespace.MLEModel):\n    _start_params = [1.]\n    _param_names = ['ratio.irregular']\n\n    def __init__(self, endog):\n        super(LocalLevelConcentrated, self).__init__(endog, k_states=1, initialization='diffuse')\n\n        self['design', 0, 0] = 1\n        self['transition', 0, 0] = 1\n        self['selection', 0, 0] = 1\n        self['state_cov', 0, 0] = 1\n\n        self.ssm.filter_concentrated = True\n\n    def transform_params(self, unconstrained):\n        return unconstrained**2\n\n    def untransform_params(self, unconstrained):\n        return unconstrained**0.5\n\n    def update(self, params, **kwargs):\n        params = super(LocalLevelConcentrated, self).update(params, **kwargs)\n        self['obs_cov', 0, 0] = params[0]\n<br/>",
            "code"
        ],
        [
            "Again, we can use the built-in fit method to find the maximum likelihood estimate of \\(h\\).",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())",
            "code"
        ],
        [
            "Statespace Model Results\n==================================================================================\nDep. Variable:                       infl   No. Observations:                  203\nModel:             LocalLevelConcentrated   Log Likelihood                -457.632\nDate:                    Wed, 02 Nov 2022   AIC                            921.263\nTime:                            17:10:55   BIC                            931.203\nSample:                        03-31-1959   HQIC                           925.285\n                             - 09-30-2009   Scale                            0.745\nCovariance Type:                      opg\n===================================================================================\n                      coef    std err          z      P|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nratio.irregular     4.5297      1.226      3.694      0.000       2.126       6.933\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):               182.26\nProb(Q):                              0.99   Prob(JB):                         0.00\nHeteroskedasticity (H):               1.75   Skew:                            -1.02\nProb(H) (two-sided):                  0.02   Kurtosis:                         7.18\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "The estimate of \\(h\\) is provided in the middle table of parameters (ratio.irregular), while the estimate of the scale is provided in the upper table. Below, we will show that these estimates are consistent with those from the previous approach.",
            "markdown"
        ],
        [
            "And we can again look at the results from the numerical optimizer in the results attribute mle_retvals. It turns out that two fewer iterations were required in this case, since there was one fewer parameter to select. Moreover, since the numerical maximization problem was easier, the optimizer was able to find a value that made the gradient for this parameter slightly closer to zero than it was above.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "print(res_conc.mle_retvals)",
            "code"
        ],
        [
            "{'fopt': 2.2543435111703576, 'gopt': array([-6.71906974e-08]), 'fcalls': 12, 'warnflag': 0, 'converged': True, 'iterations': 5}",
            "code"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Comparing estimates": [
        [
            "Recall that \\(h = \\sigma_\\varepsilon^2 / \\sigma_\\eta^2\\) and the scale is \\(\\sigma_*^2 = \\sigma_\\eta^2\\). Using these definitions, we can see that both models produce nearly identical results:",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "print('Original model')\nprint('var.level     = %.5f' % res.params[0])\nprint('var.irregular = %.5f' % res.params[1])\n\nprint('\\nConcentrated model')\nprint('scale         = %.5f' % res_conc.scale)\nprint('h * scale     = %.5f' % (res_conc.params[0] * res_conc.scale))",
            "code"
        ],
        [
            "Original model\nvar.level     = 0.74469\nvar.irregular = 3.37330\n\nConcentrated model\nscale         = 0.74472\nh * scale     = 3.37338",
            "code"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: SARIMAX": [
        [
            "By default in SARIMAX models, the variance term is chosen by numerically maximizing the likelihood function, but an option has been added to allow concentrating the scale out.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "# Typical approach\nmod_ar = sm.tsa.SARIMAX(dta.cpi, order=(1, 0, 0), trend='ct')\nres_ar = mod_ar.fit(disp=False)\n\n# Estimating the model with the scale concentrated out\nmod_ar_conc = sm.tsa.SARIMAX(dta.cpi, order=(1, 0, 0), trend='ct', concentrate_scale=True)\nres_ar_conc = mod_ar_conc.fit(disp=False)",
            "code"
        ],
        [
            "These two approaches produce about the same loglikelihood and parameters, although the model with the concentrated scale was able to improve the fit very slightly:",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "print('Loglikelihood')\nprint('- Original model:     %.4f' % res_ar.llf)\nprint('- Concentrated model: %.4f' % res_ar_conc.llf)\n\nprint('\\nParameters')\nprint('- Original model:     %.4f, %.4f, %.4f, %.4f' % tuple(res_ar.params))\nprint('- Concentrated model: %.4f, %.4f, %.4f, %.4f' % (tuple(res_ar_conc.params) + (res_ar_conc.scale,)))",
            "code"
        ],
        [
            "Loglikelihood\n- Original model:     -245.8275\n- Concentrated model: -245.8264\n\nParameters\n- Original model:     0.4921, 0.0243, 0.9808, 0.6490\n- Concentrated model: 0.4864, 0.0242, 0.9809, 0.6492",
            "code"
        ],
        [
            "This time, about 1/3 fewer iterations of the optimizer are required under the concentrated approach:",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "print('Optimizer iterations')\nprint('- Original model:     %d' % res_ar.mle_retvals['iterations'])\nprint('- Concentrated model: %d' % res_ar_conc.mle_retvals['iterations'])",
            "code"
        ],
        [
            "Optimizer iterations\n- Original model:     36\n- Concentrated model: 22",
            "code"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: Chandrasekhar recursions": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom pandas_datareader.data import DataReader",
            "code"
        ],
        [
            "Although most operations related to state space models rely on the Kalman filtering recursions, in some special cases one can use a separate method often called \u201cChandrasekhar recursions\u201d. These provide an alternative way to iteratively compute the conditional moments of the state vector, and in some cases they can be substantially less computationally intensive than the Kalman filter recursions. For complete details, see the paper \u201cUsing the \u2018Chandrasekhar Recursions\u2019 for Likelihood Evaluation\nof DSGE Models\u201d (Herbst, 2015). Here we just sketch the basic idea.",
            "markdown"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->State space models and the Kalman filter": [
        [
            "Recall that a time-invariant state space model can be written:\n\n\\[\\begin{split}\\begin{aligned}\ny_t &= Z \\alpha_t + \\varepsilon_t, \\qquad \\varepsilon_t \\sim N(0, H) \\\\\n\\alpha_{t+1} & = T \\alpha_t + R \\eta_t, \\qquad \\eta_t \\sim N(0, Q) \\\\\n\\alpha_1 & \\sim N(a_1, P_1)\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(y_t\\) is a \\(p \\times 1\\) vector and \\(\\alpha_t\\) is an \\(m \\times 1\\) vector.",
            "markdown"
        ],
        [
            "Each iteration of the Kalman filter, say at time \\(t\\), can be split into three parts:",
            "markdown"
        ],
        [
            "<strong>Initialization</strong>: specification of \\(a_t\\) and \\(P_t\\) that define the conditional state distribution, \\(\\alpha_t \\mid y^{t-1} \\sim N(a_t, P_t)\\).",
            "markdown"
        ],
        [
            "<strong>Updating</strong>: computation of \\(a_{t|t}\\) and \\(P_{t|t}\\) that define the conditional state distribution, \\(\\alpha_t \\mid y^{t} \\sim N(a_{t|t}, P_{t|t})\\).",
            "markdown"
        ],
        [
            "<strong>Prediction</strong>: computation of \\(a_{t+1}\\) and \\(P_{t+1}\\) that define the conditional state distribution, \\(\\alpha_{t+1} \\mid y^{t} \\sim N(a_{t+1}, P_{t+1})\\).",
            "markdown"
        ],
        [
            "Of course after the first iteration, the prediction part supplies the values required for initialization of the next step.",
            "markdown"
        ],
        [
            "Focusing on the prediction step, the Kalman filter recursions yield:\n\n\\[\\begin{split}\\begin{aligned}\na_{t+1} & = T a_{t|t} \\\\\nP_{t+1} & = T P_{t|t} T' + R Q R' \\\\\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "where the matrices \\(T\\) and \\(P_{t|t}\\) are each \\(m \\times m\\), where \\(m\\) is the size of the state vector \\(\\alpha\\). In some cases, the state vector can become extremely large, which can imply that the matrix multiplications required to produce \\(P_{t+1}\\) can be become computationally intensive.",
            "markdown"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->Example: seasonal autoregression": [
        [
            "As an example, notice that an AR(r) model (we use \\(r\\) here since we already used \\(p\\) as the dimension of the observation vector) can be put into state space form as:\n\n\\[\\begin{split}\\begin{aligned}\ny_t &= \\alpha_t \\\\\n\\alpha_{t+1} & = T \\alpha_t + R \\eta_t, \\qquad \\eta_t \\sim N(0, Q)\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "where:\n\n\\[\\begin{split}\\begin{aligned}\nT = \\begin{bmatrix}\n\\phi_1 & \\phi_2 & \\dots & \\phi_r \\\\\n1 & 0 & & 0 \\\\\n\\vdots & \\ddots & & \\vdots \\\\\n0 &  & 1 & 0 \\\\\n\\end{bmatrix} \\qquad\nR = \\begin{bmatrix}\n1 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{bmatrix} \\qquad\nQ = \\begin{bmatrix}\n\\sigma^2\n\\end{bmatrix}\n\\end{aligned}\\end{split}\\]",
            "markdown"
        ],
        [
            "In an AR model with daily data that exhibits annual seasonality, we might want to fit a model that incorporates lags up to \\(r=365\\), in which case the state vector would be at least \\(m = 365\\). The matrices \\(T\\) and \\(P_{t|t}\\) then each have \\(365^2 = 133225\\) elements, and so most of the time spent computing the likelihood function (via the Kalman filter) can become dominated by the matrix multiplications in the prediction step.",
            "markdown"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->State space models and the Chandrasekhar recursions": [
        [
            "The Chandrasekhar recursions replace equation \\(P_{t+1} = T P_{t|t} T' + R Q R'\\) with a different recursion:\n\n\\[P_{t+1} = P_t + W_t M_t W_t'\\]",
            "markdown"
        ],
        [
            "but where \\(W_t\\) is a matrix with dimension \\(m \\times p\\) and \\(M_t\\) is a matrix with dimension \\(p \\times p\\), where \\(p\\) is the dimension of the observed vector \\(y_t\\). These matrices themselves have recursive formulations. For more general details and for the formulas for computing \\(W_t\\) and \\(M_t\\), see Herbst (2015).",
            "markdown"
        ],
        [
            "<strong>Important note</strong>: unlike the Kalman filter, the Chandrasekhar recursions can not be used for every state space model. In particular, the latter has the following restrictions (that are not required for the use of the former):",
            "markdown"
        ],
        [
            "The model must be time-invariant, except that time-varying intercepts are permitted.",
            "markdown"
        ],
        [
            "Stationary initialization of the state vector must be used (this rules out all models in non-stationary components)",
            "markdown"
        ],
        [
            "Missing data is not permitted",
            "markdown"
        ],
        [
            "To understand why this formula can imply more efficient computations, consider again the SARIMAX case, above. In this case, \\(p = 1\\), so that \\(M_t\\) is a scalar and we can rewrite the Chandrasekhar recursion as:\n\n\\[P_{t+1} = P_t + M_t \\times W_t W_t'\\]",
            "markdown"
        ],
        [
            "The matrices being multiplied, \\(W_t\\), are then of dimension \\(m \\times 1\\), and in the case \\(r=365\\), they each only have \\(365\\) elements, rather than \\(365^2\\) elements. This implies substantially fewer computations are required to complete the prediction step.",
            "markdown"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->Convergence": [
        [
            "A factor that complicates a straightforward discussion of performance implications is the well-known fact that in time-invariant models, the predicted state covariance matrix will converge to a constant matrix. This implies that there exists an \\(S\\) such that, for every \\(t > S\\), \\(P_t = P_{t+1}\\). Once convergence has been achieved, we can eliminate the equation for \\(P_{t+1}\\) from the prediction step altogether.",
            "markdown"
        ],
        [
            "In simple time series models, like AR(r) models, convergence is achieved fairly quickly, and this can limit the performance benefit to using the Chandrasekhar recursions. Herbst (2015) focuses instead on DSGE (Dynamic Stochastic General Equilibrium) models instead, which often have a large state vector and often a large number of periods to achieve convergence. In these cases, the performance gains can be quite substantial.",
            "markdown"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->Practical example": [
        [
            "As a practical example, we will consider monthly data that has a clear seasonal component. In this case, we look at the inflation rate of apparel, as measured by the consumer price index. A graph of the data indicates strong seasonality.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "cpi_apparel = DataReader('CPIAPPNS', 'fred', start='1986')\ncpi_apparel.index = pd.DatetimeIndex(cpi_apparel.index, freq='MS')\ninf_apparel = np.log(cpi_apparel).diff().iloc[1:] * 1200\ninf_apparel.plot(figsize=(15, 5));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_chandrasekhar_10_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_chandrasekhar_10_0.png\"/>",
            "code"
        ],
        [
            "We will construct two model instances. The first will be set to use the Kalman filter recursions, while the second will be set to use the Chandrasekhar recursions. This setting is controlled by the ssm.filter_chandrasekhar property, as shown below.",
            "markdown"
        ],
        [
            "The model we have in mind is a seasonal autoregression, where we include the first 6 months as lags as well as the given month in each of the previous 15 years as lags. This implies that the state vector has dimension \\(m = 186\\), which is large enough that we might expect to see some substantial performance gains by using the Chandrasekhar recursions.",
            "markdown"
        ],
        [
            "<strong>Remark</strong>: We set tolerance=0 in each model - this has the effect of preventing the filter from ever recognizing that the prediction covariance matrix has converged. This is not recommended in practice. We do this here to highlight the superior performance of the Chandrasekhar recursions when they are used in every period instead of the typical Kalman filter recursions. Later, we will show the performance in a more realistic setting that we do allow for convergence.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "# Model that will apply Kalman filter recursions\nmod_kf = sm.tsa.SARIMAX(inf_apparel, order=(6, 0, 0), seasonal_order=(15, 0, 0, 12), tolerance=0)\nprint(mod_kf.k_states)\n\n# Model that will apply Chandrasekhar recursions\nmod_ch = sm.tsa.SARIMAX(inf_apparel, order=(6, 0, 0), seasonal_order=(15, 0, 0, 12), tolerance=0)\nmod_ch.ssm.filter_chandrasekhar = True",
            "code"
        ],
        [
            "186",
            "code"
        ],
        [
            "We time computation of the log-likelihood function, using the following code:",
            "markdown"
        ],
        [
            "%timeit mod_kf.loglike(mod_kf.start_params)\n%timeit mod_ch.loglike(mod_ch.start_params)",
            "code"
        ],
        [
            "This results in:",
            "markdown"
        ],
        [
            "171 ms \u00b1 19.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n85 ms \u00b1 4.97 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)",
            "code"
        ],
        [
            "The implication is that in this experiment, the Chandrasekhar recursions improved performance by about a factor of 2.",
            "markdown"
        ],
        [
            "As we mentioned above, in the previous experiment we disabled convergence of the predicted covariance matrices, so the results there are an upper bound. Now we allow for convergence, as usual, by removing the tolerance=0 argument:",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "# Model that will apply Kalman filter recursions\nmod_kf = sm.tsa.SARIMAX(inf_apparel, order=(6, 0, 0), seasonal_order=(15, 0, 0, 12))\nprint(mod_kf.k_states)\n\n# Model that will apply Chandrasekhar recursions\nmod_ch = sm.tsa.SARIMAX(inf_apparel, order=(6, 0, 0), seasonal_order=(15, 0, 0, 12))\nmod_ch.ssm.filter_chandrasekhar = True",
            "code"
        ],
        [
            "186",
            "code"
        ],
        [
            "Again, we time computation of the log-likelihood function, using the following code:",
            "markdown"
        ],
        [
            "%timeit mod_kf.loglike(mod_kf.start_params)\n%timeit mod_ch.loglike(mod_ch.start_params)",
            "code"
        ],
        [
            "This results in:",
            "markdown"
        ],
        [
            "114 ms \u00b1 7.64 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n70.5 ms \u00b1 2.43 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)",
            "code"
        ],
        [
            "The Chandrasekhar recursions still improve performance, but now only by about 33%. The reason for this is that after convergence, we no longer need to compute the predicted covariance matrices, so that for those post-convergence periods, there will be no difference in computation time between the two approaches. Below we check the period in which convergence was achieved:",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "res_kf = mod_kf.filter(mod_kf.start_params)\nprint('Convergence at t=%d, of T=%d total observations' %\n      (res_kf.filter_results.period_converged, res_kf.nobs))",
            "code"
        ],
        [
            "Convergence at t=186, of T=440 total observations",
            "code"
        ],
        [
            "Since convergence happened relatively early, we are already avoiding the expensive matrix multiplications in more than half of the periods.",
            "markdown"
        ],
        [
            "However, as mentioned above, larger DSGE models may not achieve convergence for most or all of the periods in the sample, and so we could perhaps expect to achieve performance gains more similar to the first example. In their 2019 paper \u201cEuro area real-time density forecasting with financial or labor market frictions\u201d, McAdam and Warne note that in their applications, \u201cCompared with the standard Kalman filter, it is our experience that these recursions speed up the calculation of the\nlog-likelihood for the three models by roughly 50 percent\u201d. This is about the same result as we found in our first example.",
            "markdown"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->Aside on multithreaded matrix algebra routines": [
        [
            "The timings above are based on the Numpy installation installed via Anaconda, which uses Intel\u2019s MKL BLAS and LAPACK libraries. These implement multithreaded processing to speed up matrix algebra, which can be particularly helpful for operations on the larger matrices we\u2019re working with here. To get a sense of how this affects results, we could turn off multithreading by putting the following in the first cell of this notebook.",
            "markdown"
        ],
        [
            "import os\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"",
            "code"
        ],
        [
            "When we do this, the timings of the first example change to:",
            "markdown"
        ],
        [
            "307 ms \u00b1 3.08 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n97.5 ms \u00b1 1.64 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)",
            "code"
        ],
        [
            "and the timings of the second example change to:",
            "markdown"
        ],
        [
            "178 ms \u00b1 2.78 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n78.9 ms \u00b1 950 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)",
            "code"
        ],
        [
            "Both are slower, but the typical Kalman filter is affected much more.",
            "markdown"
        ],
        [
            "This is not unexpected; the performance differential between single and multithreaded linear algebra is much greater in the typical Kalman filter case, because the whole point of the Chandrasekhar recursions is to reduce the size of the matrix operations. It means that if multithreaded linear algebra is unavailable, the Chandrasekhar recursions offer even greater performance gains.",
            "markdown"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->Chandrasekhar recursions and the univariate filtering approach": [
        [
            "It is also possible to combine the Chandrasekhar recursions with the univariate filtering approach of Koopman and Durbin (2000), by making use of the results of Aknouche and Hamdi in their 2007 paper \u201cPeriodic Chandrasekhar recursions\u201d. An initial implementation of this combination is included in Statsmodels. However, experiments suggest that this tends to degrade performance compared to even the usual Kalman filter. This accords with the computational savings reported for the univariate\nfiltering method, which suggest that savings are highest when the state vector is small relative to the observation vector.",
            "markdown"
        ]
    ],
    "Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->Bibliography": [
        [
            "Aknouche, Abdelhakim, and Fay\u00e7al Hamdi. \u201cPeriodic Chandrasekhar recursions.\u201d arXiv preprint arXiv:0711.3857 (2007).",
            "markdown"
        ],
        [
            "Herbst, Edward. \u201cUsing the \u201cChandrasekhar Recursions\u201d for likelihood evaluation of DSGE models.\u201d Computational Economics 45, no. 4 (2015): 693-705.",
            "markdown"
        ],
        [
            "Koopman, Siem J., and James Durbin. \u201cFast filtering and smoothing for multivariate state space models.\u201d Journal of Time Series Analysis 21, no. 3 (2000): 281-296.",
            "markdown"
        ],
        [
            "McAdam, Peter, and Anders Warne. \u201cEuro area real-time density forecasting with financial or labor market frictions.\u201d International Journal of Forecasting 35, no. 2 (2019): 580-600.",
            "markdown"
        ]
    ],
    "Examples->Forecasting->Forecasting using the Theta Model": [
        [
            "The Theta model of Assimakopoulos & Nikolopoulos (2000) is a simple method for forecasting the involves fitting two \\(\\theta\\)-lines, forecasting the lines using a Simple Exponential Smoother, and then combining the forecasts from the two lines to produce the final forecast. The model is implemented in steps:",
            "markdown"
        ],
        [
            "Test for seasonality",
            "markdown"
        ],
        [
            "Deseasonalize if seasonality detected",
            "markdown"
        ],
        [
            "Estimate \\(\\alpha\\) by fitting a SES model to the data and \\(b_0\\) by OLS.",
            "markdown"
        ],
        [
            "Forecast the series",
            "markdown"
        ],
        [
            "Reseasonalize if the data was deseasonalized.",
            "markdown"
        ],
        [
            "The seasonality test examines the ACF at the seasonal lag \\(m\\). If this lag is significantly different from zero then the data is deseasonalize using statsmodels.tsa.seasonal_decompose use either a multiplicative method (default) or additive.",
            "markdown"
        ],
        [
            "The parameters of the model are \\(b_0\\) and \\(\\alpha\\) where \\(b_0\\) is estimated from the OLS regression\n\n\\[X_t = a_0 + b_0 (t-1) + \\epsilon_t\\]",
            "markdown"
        ],
        [
            "and \\(\\alpha\\) is the SES smoothing parameter in\n\n\\[\\tilde{X}_t = (1-\\alpha) X_t + \\alpha \\tilde{X}_{t-1}\\]",
            "markdown"
        ],
        [
            "The forecasts are then\n\n\\[\\hat{X}_{T+h|T} = \\frac{\\theta-1}{\\theta} \\hat{b}_0\n                    \\left[h - 1 + \\frac{1}{\\hat{\\alpha}}\n                    - \\frac{(1-\\hat{\\alpha})^T}{\\hat{\\alpha}} \\right]\n                    + \\tilde{X}_{T+h|T}\\]",
            "markdown"
        ],
        [
            "Ultimately \\(\\theta\\) only plays a role in determining how much the trend is damped. If \\(\\theta\\) is very large, then the forecast of the model is identical to that from an Integrated Moving Average with a drift,\n\n\\[X_t = X_{t-1} + b_0 + (\\alpha-1)\\epsilon_{t-1} + \\epsilon_t.\\]",
            "markdown"
        ],
        [
            "Finally, the forecasts are reseasonalized if needed.",
            "markdown"
        ],
        [
            "This module is based on:",
            "markdown"
        ],
        [
            "Assimakopoulos, V., & Nikolopoulos, K. (2000). The theta model: a decomposition approach to forecasting. International journal of forecasting, 16(4), 521-530.",
            "markdown"
        ],
        [
            "Hyndman, R. J., & Billah, B. (2003). Unmasking the Theta method. International Journal of Forecasting, 19(2), 287-290.",
            "markdown"
        ],
        [
            "Fioruci, J. A., Pellegrini, T. R., Louzada, F., & Petropoulos, F. (2015). The optimized theta method. arXiv preprint arXiv:1503.03529.",
            "markdown"
        ]
    ],
    "Examples->Forecasting->Forecasting using the Theta Model->Imports": [
        [
            "We start with the standard set of imports and some tweaks to the default matplotlib style.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn as sns\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=15)\nplt.rc(\"lines\", linewidth=3)\nsns.set_style(\"darkgrid\")",
            "code"
        ]
    ],
    "Examples->Forecasting->Forecasting using the Theta Model->Load some Data": [
        [
            "We will first look at housing starts using US data. This series is clearly seasonal but does not have a clear trend during the same.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "reader = pdr.fred.FredReader([\"HOUST\"], start=\"1980-01-01\", end=\"2020-04-01\")\ndata = reader.read()\nhousing = data.HOUST\nhousing.index.freq = housing.index.inferred_freq\nax = housing.plot()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_4_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_4_0.png\"/>",
            "code"
        ],
        [
            "We fit specify the model without any options and fit it. The summary shows that the data was deseasonalized using the multiplicative method. The drift is modest and negative, and the smoothing parameter is fairly low.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "from statsmodels.tsa.forecasting.theta import ThetaModel\n\ntm = ThetaModel(housing)\nres = tm.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "ThetaModel Results\n==============================================================================\nDep. Variable:                  HOUST   No. Observations:                  484\nMethod:                       OLS/SES   Deseasonalized:                   True\nDate:                Wed, 02 Nov 2022   Deseas. Method:         Multiplicative\nTime:                        17:10:36   Period:                             12\nSample:                    01-01-1980\n                         - 04-01-2020\n   Parameter Estimates\n=========================\n           Parameters\n-------------------------\nb0    -0.9182227350685341\nalpha  0.6150174992660694\n-------------------------",
            "code"
        ],
        [
            "The model is first and foremost a forecasting method. Forecasts are produced using the forecast method from fitted model. Below we produce a hedgehog plot by forecasting 2-years ahead every 2 years.",
            "markdown"
        ],
        [
            "<strong>Note</strong>: the default \\(\\theta\\) is 2.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "forecasts = {\"housing\": housing}\nfor year in range(1995, 2020, 2):\n    sub = housing[: str(year)]\n    res = ThetaModel(sub).fit()\n    fcast = res.forecast(24)\n    forecasts[str(year)] = fcast\nforecasts = pd.DataFrame(forecasts)\nax = forecasts[\"1995\":].plot(legend=False)\nchildren = ax.get_children()\nchildren[0].set_linewidth(4)\nchildren[0].set_alpha(0.3)\nchildren[0].set_color(\"#000000\")\nax.set_title(\"Housing Starts\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_8_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_8_0.png\"/>",
            "code"
        ],
        [
            "We could alternatively fit the log of the data. Here it makes more sense to force the deseasonalizing to use the additive method, if needed. We also fit the model parameters using MLE. This method fits the IMA\n\n\\[X_t = X_{t-1} + \\gamma\\epsilon_{t-1} + \\epsilon_t\\]",
            "markdown"
        ],
        [
            "where \\(\\hat{\\alpha}\\) = \\(\\min(\\hat{\\gamma}+1, 0.9998)\\) using statsmodels.tsa.SARIMAX. The parameters are similar although the drift is closer to zero.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "tm = ThetaModel(np.log(housing), method=\"additive\")\nres = tm.fit(use_mle=True)\nprint(res.summary())",
            "code"
        ],
        [
            "ThetaModel Results\n==============================================================================\nDep. Variable:                  HOUST   No. Observations:                  484\nMethod:                           MLE   Deseasonalized:                   True\nDate:                Wed, 02 Nov 2022   Deseas. Method:               Additive\nTime:                        17:10:37   Period:                             12\nSample:                    01-01-1980\n                         - 04-01-2020\n    Parameter Estimates\n===========================\n            Parameters\n---------------------------\nb0    -0.000424561126389559\nalpha    0.6686378512512897\n---------------------------",
            "code"
        ],
        [
            "The forecast only depends on the forecast trend component,\n\n\\[\\hat{b}_0\n                     \\left[h - 1 + \\frac{1}{\\hat{\\alpha}}\n                     - \\frac{(1-\\hat{\\alpha})^T}{\\hat{\\alpha}} \\right],\\]",
            "markdown"
        ],
        [
            "the forecast from the SES (which does not change with the horizon), and the seasonal. These three components are available using the forecast_components. This allows forecasts to be constructed using multiple choices of \\(\\theta\\) using the weight expression above.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "res.forecast_components(12)",
            "code"
        ],
        [
            "[6]:",
            "code"
        ]
    ],
    "Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure": [
        [
            "We next look at personal consumption expenditure. This series has a clear seasonal component and a drift.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "reader = pdr.fred.FredReader([\"NA000349Q\"], start=\"1980-01-01\", end=\"2020-04-01\")\npce = reader.read()\npce.columns = [\"PCE\"]\npce.index.freq = \"QS-OCT\"\n_ = pce.plot()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_14_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_14_0.png\"/>",
            "code"
        ],
        [
            "Since this series is always positive, we model the \\(\\ln\\).",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "mod = ThetaModel(np.log(pce))\nres = mod.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "ThetaModel Results\n==============================================================================\nDep. Variable:                    PCE   No. Observations:                  162\nMethod:                       OLS/SES   Deseasonalized:                   True\nDate:                Wed, 02 Nov 2022   Deseas. Method:         Multiplicative\nTime:                        17:10:38   Period:                              4\nSample:                    01-01-1980\n                         - 04-01-2020\n   Parameter Estimates\n==========================\n           Parameters\n--------------------------\nb0    0.013017709693005357\nalpha   0.9998843148384222\n--------------------------",
            "code"
        ],
        [
            "Next we explore differenced in the forecast as \\(\\theta\\) changes. When \\(\\theta\\) is close to 1, the drift is nearly absent. As \\(\\theta\\) increases, the drift becomes more obvious.",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "forecasts = pd.DataFrame(\n    {\n        \"ln PCE\": np.log(pce.PCE),\n        \"theta=1.2\": res.forecast(12, theta=1.2),\n        \"theta=2\": res.forecast(12),\n        \"theta=3\": res.forecast(12, theta=3),\n        \"No damping\": res.forecast(12, theta=np.inf),\n    }\n)\n_ = forecasts.tail(36).plot()\nplt.title(\"Forecasts of ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_18_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_18_0.png\"/>",
            "code"
        ],
        [
            "Finally, plot_predict can be used to visualize the predictions and prediction intervals which are constructed assuming the IMA is true.",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "ax = res.plot_predict(24, theta=2)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_20_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_20_0.png\"/>",
            "code"
        ],
        [
            "We conclude be producing a hedgehog plot using 2-year non-overlapping samples.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "ln_pce = np.log(pce.PCE)\nforecasts = {\"ln PCE\": ln_pce}\nfor year in range(1995, 2020, 3):\n    sub = ln_pce[: str(year)]\n    res = ThetaModel(sub).fit()\n    fcast = res.forecast(12)\n    forecasts[str(year)] = fcast\nforecasts = pd.DataFrame(forecasts)\nax = forecasts[\"1995\":].plot(legend=False)\nchildren = ax.get_children()\nchildren[0].set_linewidth(4)\nchildren[0].set_alpha(0.3)\nchildren[0].set_color(\"#000000\")\nax.set_title(\"ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_22_0.png\"/>",
            "code"
        ]
    ],
    "Examples->Multivariate Methods->Principal Component Analysis": [
        [
            "Key ideas: Principal component analysis, world bank data, fertility",
            "markdown"
        ],
        [
            "In this notebook, we use principal components analysis (PCA) to analyze the time series of fertility rates in 192 countries, using data obtained from the World Bank. The main goal is to understand how the trends in fertility over time differ from country to country. This is a slightly atypical illustration of PCA because the data are time series. Methods such as functional PCA have been developed for this setting, but since the fertility data are very smooth, there is no real disadvantage to\nusing standard PCA in this case.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.multivariate.pca import PCA\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "code"
        ],
        [
            "The data can be obtained from the , but here we work with a slightly cleaned-up version of the data:",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "data = sm.datasets.fertility.load_pandas().data\ndata.head()",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "5 rows \u00d7 58 columns",
            "markdown"
        ],
        [
            "Here we construct a DataFrame that contains only the numerical fertility rate data and set the index to the country names. We also drop all the countries with any missing data.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "columns = list(map(str, range(1960, 2012)))\ndata.set_index(\"Country Name\", inplace=True)\ndta = data[columns]\ndta = dta.dropna()\ndta.head()",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "5 rows \u00d7 52 columns",
            "markdown"
        ],
        [
            "There are two ways to use PCA to analyze a rectangular matrix: we can treat the rows as the \u201cobjects\u201d and the columns as the \u201cvariables\u201d, or vice-versa. Here we will treat the fertility measures as \u201cvariables\u201d used to measure the countries as \u201cobjects\u201d. Thus the goal will be to reduce the yearly fertility rate values to a small number of fertility rate \u201cprofiles\u201d or \u201cbasis functions\u201d that capture most of the variation over time in the different countries.",
            "markdown"
        ],
        [
            "The mean trend is removed in PCA, but its worthwhile taking a look at it. It shows that fertility has dropped steadily over the time period covered in this dataset. Note that the mean is calculated using a country as the unit of analysis, ignoring population size. This is also true for the PC analysis conducted below. A more sophisticated analysis might weight the countries, say by population in 1980.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "(0.0, 51.0)\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_9_1.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_9_1.png\"/>",
            "code"
        ],
        [
            "Next we perform the PCA:",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "pca_model = PCA(dta.T, standardize=False, demean=True)",
            "code"
        ],
        [
            "Based on the eigenvalues, we see that the first PC dominates, with perhaps a small amount of meaningful variation captured in the second and third PC\u2019s.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "fig = pca_model.plot_scree(log_scale=False)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_13_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_13_0.png\"/>",
            "code"
        ],
        [
            "Next we will plot the PC factors. The dominant factor is monotonically increasing. Countries with a positive score on the first factor will increase faster (or decrease slower) compared to the mean shown above. Countries with a negative score on the first factor will decrease faster than the mean. The second factor is U-shaped with a positive peak at around 1985. Countries with a large positive score on the second factor will have lower than average fertilities at the beginning and end of the\ndata range, but higher than average fertility in the middle of the range.",
            "markdown"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(8, 4))\nlines = ax.plot(pca_model.factors.iloc[:, :3], lw=4, alpha=0.6)\nax.set_xticklabels(dta.columns.values[::10])\nax.set_xlim(0, 51)\nax.set_xlabel(\"Year\", size=17)\nfig.subplots_adjust(0.1, 0.1, 0.85, 0.9)\nlegend = fig.legend(lines, [\"PC 1\", \"PC 2\", \"PC 3\"], loc=\"center right\")\nlegend.draw_frame(False)",
            "code"
        ],
        [
            "/tmp/ipykernel_7045/427128218.py:3: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(dta.columns.values[::10])\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_15_1.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_15_1.png\"/>",
            "code"
        ],
        [
            "To better understand what is going on, we will plot the fertility trajectories for sets of countries with similar PC scores. The following convenience function produces such a plot.",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "idx = pca_model.loadings.iloc[:, 0].argsort()",
            "code"
        ],
        [
            "First we plot the five countries with the greatest scores on PC 1. These countries have a higher rate of fertility increase than the global mean (which is decreasing).",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "def make_plot(labels):\n    fig, ax = plt.subplots(figsize=(9, 5))\n    ax = dta.loc[labels].T.plot(legend=False, grid=False, ax=ax)\n    dta.mean().plot(ax=ax, grid=False, label=\"Mean\")\n    ax.set_xlim(0, 51)\n    fig.subplots_adjust(0.1, 0.1, 0.75, 0.9)\n    ax.set_xlabel(\"Year\", size=17)\n    ax.set_ylabel(\"Fertility\", size=17)\n    legend = ax.legend(\n        *ax.get_legend_handles_labels(), loc=\"center left\", bbox_to_anchor=(1, 0.5)\n    )\n    legend.draw_frame(False)",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "labels = dta.index[idx[-5:]]\nmake_plot(labels)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_20_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_20_0.png\"/>",
            "code"
        ],
        [
            "Here are the five countries with the greatest scores on factor 2. These are countries that reached peak fertility around 1980, later than much of the rest of the world, followed by a rapid decrease in fertility.",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "idx = pca_model.loadings.iloc[:, 1].argsort()\nmake_plot(dta.index[idx[-5:]])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_22_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_22_0.png\"/>",
            "code"
        ],
        [
            "Finally we have the countries with the most negative scores on PC 2. These are the countries where the fertility rate declined much faster than the global mean during the 1960\u2019s and 1970\u2019s, then flattened out.",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "make_plot(dta.index[idx[:5]])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\"/>",
            "code"
        ],
        [
            "We can also look at a scatterplot of the first two principal component scores. We see that the variation among countries is fairly continuous, except perhaps that the two countries with highest scores for PC 2 are somewhat separated from the other points. These countries, Oman and Yemen, are unique in having a sharp spike in fertility around 1980. No other country has such a spike. In contrast, the countries with high scores on PC 1 (that have continuously increasing fertility), are part of a\ncontinuum of variation.",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots()\npca_model.loadings.plot.scatter(x=\"comp_00\", y=\"comp_01\", ax=ax)\nax.set_xlabel(\"PC 1\", size=17)\nax.set_ylabel(\"PC 2\", size=17)\ndta.index[pca_model.loadings.iloc[:, 1]  0.2].values",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "array(['Oman', 'Yemen, Rep.'], dtype=object)\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_26_1.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_26_1.png\"/>",
            "code"
        ]
    ],
    "Examples->User Notes->Contrasts": [
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np\nimport statsmodels.api as sm",
            "code"
        ],
        [
            "This document is based heavily on this excellent resource from UCLA",
            "markdown"
        ],
        [
            "A categorical variable of K categories, or levels, usually enters a regression as a sequence of K-1 dummy variables. This amounts to a linear hypothesis on the level means. That is, each test statistic for these variables amounts to testing whether the mean for that level is statistically significantly different from the mean of the base category. This dummy coding is called Treatment coding in R parlance, and we will follow this convention. There are, however, different coding methods that\namount to different sets of linear hypotheses.",
            "markdown"
        ],
        [
            "In fact, the dummy coding is not technically a contrast coding. This is because the dummy variables add to one and are not functionally independent of the model\u2019s intercept. On the other hand, a set of contrasts for a categorical variable with k levels is a set of k-1 functionally independent linear combinations of the factor level means that are also independent of the sum of the dummy variables. The dummy coding is not wrong per se. It captures all of the coefficients, but it\ncomplicates matters when the model assumes independence of the coefficients such as in ANOVA. Linear regression models do not assume independence of the coefficients and thus dummy coding is often the only coding that is taught in this context.",
            "markdown"
        ],
        [
            "To have a look at the contrast matrices in Patsy, we will use data from UCLA ATS. First let\u2019s load the data.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Contrasts->Example Data": [
        [
            "[2]:",
            "code"
        ],
        [
            "import pandas as pd\n\nurl = \"https://stats.idre.ucla.edu/stat/data/hsb2.csv\"\nhsb2 = pd.read_table(url, delimiter=\",\")",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "hsb2.head(10)",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "It will be instructive to look at the mean of the dependent variable, write, for each level of race ((1 = Hispanic, 2 = Asian, 3 = African American and 4 = Caucasian)).",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "hsb2.groupby(\"race\")[\"write\"].mean()",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "race\n1    46.458333\n2    58.000000\n3    48.200000\n4    54.055172\nName: write, dtype: float64",
            "code"
        ]
    ],
    "Examples->User Notes->Contrasts->Treatment (Dummy) Coding": [
        [
            "Dummy coding is likely the most well known coding scheme. It compares each level of the categorical variable to a base reference level. The base reference level is the value of the intercept. It is the default contrast in Patsy for unordered categorical factors. The Treatment contrast matrix for race would be",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "from patsy.contrasts import Treatment\n\nlevels = [1, 2, 3, 4]\ncontrast = Treatment(reference=0).code_without_intercept(levels)\nprint(contrast.matrix)",
            "code"
        ],
        [
            "[[0. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]",
            "code"
        ],
        [
            "Here we used reference=0, which implies that the first level, Hispanic, is the reference category against which the other level effects are measured. As mentioned above, the columns do not sum to zero and are thus not independent of the intercept. To be explicit, let\u2019s look at how this would encode the race variable.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "hsb2.race.head(10)",
            "code"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "0    4\n1    4\n2    4\n3    4\n4    4\n5    4\n6    3\n7    1\n8    4\n9    3\nName: race, dtype: int64",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "print(contrast.matrix[hsb2.race - 1, :][:20])",
            "code"
        ],
        [
            "[[0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]]",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "pd.get_dummies(hsb2.race.values, drop_first=False)",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "200 rows \u00d7 4 columns",
            "markdown"
        ],
        [
            "This is a bit of a trick, as the race category conveniently maps to zero-based indices. If it does not, this conversion happens under the hood, so this will not work in general but nonetheless is a useful exercise to fix ideas. The below illustrates the output using the three contrasts above",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "from statsmodels.formula.api import ols\n\nmod = ols(\"write ~ C(race, Treatment)\", data=hsb2)\nres = mod.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                  write   R-squared:                       0.107\nModel:                            OLS   Adj. R-squared:                  0.093\nMethod:                 Least Squares   F-statistic:                     7.833\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           5.78e-05\nTime:                        17:10:45   Log-Likelihood:                -721.77\nNo. Observations:                 200   AIC:                             1452.\nDf Residuals:                     196   BIC:                             1465.\nDf Model:                           3\nCovariance Type:            nonrobust\n===========================================================================================\n                              coef    std err          t      P|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------------\nIntercept                  46.4583      1.842     25.218      0.000      42.825      50.091\nC(race, Treatment)[T.2]    11.5417      3.286      3.512      0.001       5.061      18.022\nC(race, Treatment)[T.3]     1.7417      2.732      0.637      0.525      -3.647       7.131\nC(race, Treatment)[T.4]     7.5968      1.989      3.820      0.000       3.675      11.519\n==============================================================================\nOmnibus:                       10.487   Durbin-Watson:                   1.779\nProb(Omnibus):                  0.005   Jarque-Bera (JB):               11.031\nSkew:                          -0.551   Prob(JB):                      0.00402\nKurtosis:                       2.670   Cond. No.                         8.25\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "We explicitly gave the contrast for race; however, since Treatment is the default, we could have omitted this.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Contrasts->Treatment (Dummy) Coding->Simple Coding": [
        [
            "Like Treatment Coding, Simple Coding compares each level to a fixed reference level. However, with simple coding, the intercept is the grand mean of all the levels of the factors. Patsy does not have the Simple contrast included, but you can easily define your own contrasts. To do so, write a class that contains a code_with_intercept and a code_without_intercept method that returns a patsy.contrast.ContrastMatrix instance",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "from patsy.contrasts import ContrastMatrix\n\n\ndef _name_levels(prefix, levels):\n    return [\"[%s%s]\" % (prefix, level) for level in levels]\n\n\nclass Simple(object):\n    def _simple_contrast(self, levels):\n        nlevels = len(levels)\n        contr = -1.0 / nlevels * np.ones((nlevels, nlevels - 1))\n        contr[1:][np.diag_indices(nlevels - 1)] = (nlevels - 1.0) / nlevels\n        return contr\n\n    def code_with_intercept(self, levels):\n        contrast = np.column_stack(\n            (np.ones(len(levels)), self._simple_contrast(levels))\n        )\n        return ContrastMatrix(contrast, _name_levels(\"Simp.\", levels))\n\n    def code_without_intercept(self, levels):\n        contrast = self._simple_contrast(levels)\n        return ContrastMatrix(contrast, _name_levels(\"Simp.\", levels[:-1]))",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "hsb2.groupby(\"race\")[\"write\"].mean().mean()",
            "code"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "51.67837643678162",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "contrast = Simple().code_without_intercept(levels)\nprint(contrast.matrix)",
            "code"
        ],
        [
            "[[-0.25 -0.25 -0.25]\n [ 0.75 -0.25 -0.25]\n [-0.25  0.75 -0.25]\n [-0.25 -0.25  0.75]]",
            "code"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "mod = ols(\"write ~ C(race, Simple)\", data=hsb2)\nres = mod.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                  write   R-squared:                       0.107\nModel:                            OLS   Adj. R-squared:                  0.093\nMethod:                 Least Squares   F-statistic:                     7.833\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           5.78e-05\nTime:                        17:10:45   Log-Likelihood:                -721.77\nNo. Observations:                 200   AIC:                             1452.\nDf Residuals:                     196   BIC:                             1465.\nDf Model:                           3\nCovariance Type:            nonrobust\n===========================================================================================\n                              coef    std err          t      P|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------------\nIntercept                  51.6784      0.982     52.619      0.000      49.741      53.615\nC(race, Simple)[Simp.1]    11.5417      3.286      3.512      0.001       5.061      18.022\nC(race, Simple)[Simp.2]     1.7417      2.732      0.637      0.525      -3.647       7.131\nC(race, Simple)[Simp.3]     7.5968      1.989      3.820      0.000       3.675      11.519\n==============================================================================\nOmnibus:                       10.487   Durbin-Watson:                   1.779\nProb(Omnibus):                  0.005   Jarque-Bera (JB):               11.031\nSkew:                          -0.551   Prob(JB):                      0.00402\nKurtosis:                       2.670   Cond. No.                         7.03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ]
    ],
    "Examples->User Notes->Contrasts->Treatment (Dummy) Coding->Sum (Deviation) Coding": [
        [
            "Sum coding compares the mean of the dependent variable for a given level to the overall mean of the dependent variable over all the levels. That is, it uses contrasts between each of the first k-1 levels and level k In this example, level 1 is compared to all the others, level 2 to all the others, and level 3 to all the others.",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "from patsy.contrasts import Sum\n\ncontrast = Sum().code_without_intercept(levels)\nprint(contrast.matrix)",
            "code"
        ],
        [
            "[[ 1.  0.  0.]\n [ 0.  1.  0.]\n [ 0.  0.  1.]\n [-1. -1. -1.]]",
            "code"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "mod = ols(\"write ~ C(race, Sum)\", data=hsb2)\nres = mod.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                  write   R-squared:                       0.107\nModel:                            OLS   Adj. R-squared:                  0.093\nMethod:                 Least Squares   F-statistic:                     7.833\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           5.78e-05\nTime:                        17:10:45   Log-Likelihood:                -721.77\nNo. Observations:                 200   AIC:                             1452.\nDf Residuals:                     196   BIC:                             1465.\nDf Model:                           3\nCovariance Type:            nonrobust\n=====================================================================================\n                        coef    std err          t      P|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nIntercept            51.6784      0.982     52.619      0.000      49.741      53.615\nC(race, Sum)[S.1]    -5.2200      1.631     -3.200      0.002      -8.437      -2.003\nC(race, Sum)[S.2]     6.3216      2.160      2.926      0.004       2.061      10.582\nC(race, Sum)[S.3]    -3.4784      1.732     -2.008      0.046      -6.895      -0.062\n==============================================================================\nOmnibus:                       10.487   Durbin-Watson:                   1.779\nProb(Omnibus):                  0.005   Jarque-Bera (JB):               11.031\nSkew:                          -0.551   Prob(JB):                      0.00402\nKurtosis:                       2.670   Cond. No.                         6.72\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "This corresponds to a parameterization that forces all the coefficients to sum to zero. Notice that the intercept here is the grand mean where the grand mean is the mean of means of the dependent variable by each level.",
            "markdown"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "hsb2.groupby(\"race\")[\"write\"].mean().mean()",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "51.67837643678162",
            "code"
        ]
    ],
    "Examples->User Notes->Contrasts->Treatment (Dummy) Coding->Backward Difference Coding": [
        [
            "In backward difference coding, the mean of the dependent variable for a level is compared with the mean of the dependent variable for the prior level. This type of coding may be useful for a nominal or an ordinal variable.",
            "markdown"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "from patsy.contrasts import Diff\n\ncontrast = Diff().code_without_intercept(levels)\nprint(contrast.matrix)",
            "code"
        ],
        [
            "[[-0.75 -0.5  -0.25]\n [ 0.25 -0.5  -0.25]\n [ 0.25  0.5  -0.25]\n [ 0.25  0.5   0.75]]",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "mod = ols(\"write ~ C(race, Diff)\", data=hsb2)\nres = mod.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                  write   R-squared:                       0.107\nModel:                            OLS   Adj. R-squared:                  0.093\nMethod:                 Least Squares   F-statistic:                     7.833\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           5.78e-05\nTime:                        17:10:45   Log-Likelihood:                -721.77\nNo. Observations:                 200   AIC:                             1452.\nDf Residuals:                     196   BIC:                             1465.\nDf Model:                           3\nCovariance Type:            nonrobust\n======================================================================================\n                         coef    std err          t      P|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept             51.6784      0.982     52.619      0.000      49.741      53.615\nC(race, Diff)[D.1]    11.5417      3.286      3.512      0.001       5.061      18.022\nC(race, Diff)[D.2]    -9.8000      3.388     -2.893      0.004     -16.481      -3.119\nC(race, Diff)[D.3]     5.8552      2.153      2.720      0.007       1.610      10.101\n==============================================================================\nOmnibus:                       10.487   Durbin-Watson:                   1.779\nProb(Omnibus):                  0.005   Jarque-Bera (JB):               11.031\nSkew:                          -0.551   Prob(JB):                      0.00402\nKurtosis:                       2.670   Cond. No.                         8.30\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "For example, here the coefficient on level 1 is the mean of write at level 2 compared with the mean at level 1. Ie.,",
            "markdown"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "res.params[\"C(race, Diff)[D.1]\"]\nhsb2.groupby(\"race\").mean()[\"write\"][2] - hsb2.groupby(\"race\").mean()[\"write\"][1]",
            "code"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "11.541666666666664",
            "code"
        ]
    ],
    "Examples->User Notes->Contrasts->Treatment (Dummy) Coding->Helmert Coding": [
        [
            "Our version of Helmert coding is sometimes referred to as Reverse Helmert Coding. The mean of the dependent variable for a level is compared to the mean of the dependent variable over all previous levels. Hence, the name \u2018reverse\u2019 being sometimes applied to differentiate from forward Helmert coding. This comparison does not make much sense for a nominal variable such as race, but we would use the Helmert contrast like so:",
            "markdown"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "from patsy.contrasts import Helmert\n\ncontrast = Helmert().code_without_intercept(levels)\nprint(contrast.matrix)",
            "code"
        ],
        [
            "[[-1. -1. -1.]\n [ 1. -1. -1.]\n [ 0.  2. -1.]\n [ 0.  0.  3.]]",
            "code"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "mod = ols(\"write ~ C(race, Helmert)\", data=hsb2)\nres = mod.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                  write   R-squared:                       0.107\nModel:                            OLS   Adj. R-squared:                  0.093\nMethod:                 Least Squares   F-statistic:                     7.833\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           5.78e-05\nTime:                        17:10:45   Log-Likelihood:                -721.77\nNo. Observations:                 200   AIC:                             1452.\nDf Residuals:                     196   BIC:                             1465.\nDf Model:                           3\nCovariance Type:            nonrobust\n=========================================================================================\n                            coef    std err          t      P|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nIntercept                51.6784      0.982     52.619      0.000      49.741      53.615\nC(race, Helmert)[H.2]     5.7708      1.643      3.512      0.001       2.530       9.011\nC(race, Helmert)[H.3]    -1.3431      0.867     -1.548      0.123      -3.054       0.368\nC(race, Helmert)[H.4]     0.7923      0.372      2.130      0.034       0.059       1.526\n==============================================================================\nOmnibus:                       10.487   Durbin-Watson:                   1.779\nProb(Omnibus):                  0.005   Jarque-Bera (JB):               11.031\nSkew:                          -0.551   Prob(JB):                      0.00402\nKurtosis:                       2.670   Cond. No.                         7.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "To illustrate, the comparison on level 4 is the mean of the dependent variable at the previous three levels taken from the mean at level 4",
            "markdown"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "grouped = hsb2.groupby(\"race\")\ngrouped.mean()[\"write\"][4] - grouped.mean()[\"write\"][:3].mean()",
            "code"
        ],
        [
            "/tmp/ipykernel_6705/44341998.py:2: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n  grouped.mean()[\"write\"][4] - grouped.mean()[\"write\"][:3].mean()",
            "code"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "3.169061302681982",
            "code"
        ],
        [
            "As you can see, these are only equal up to a constant. Other versions of the Helmert contrast give the actual difference in means. Regardless, the hypothesis tests are the same.",
            "markdown"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "k = 4\n1.0 / k * (grouped.mean()[\"write\"][k] - grouped.mean()[\"write\"][: k - 1].mean())\nk = 3\n1.0 / k * (grouped.mean()[\"write\"][k] - grouped.mean()[\"write\"][: k - 1].mean())",
            "code"
        ],
        [
            "/tmp/ipykernel_6705/3808679336.py:2: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n  1.0 / k * (grouped.mean()[\"write\"][k] - grouped.mean()[\"write\"][: k - 1].mean())\n/tmp/ipykernel_6705/3808679336.py:4: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n  1.0 / k * (grouped.mean()[\"write\"][k] - grouped.mean()[\"write\"][: k - 1].mean())",
            "code"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "-1.3430555555555561",
            "code"
        ]
    ],
    "Examples->User Notes->Contrasts->Treatment (Dummy) Coding->Orthogonal Polynomial Coding": [
        [
            "The coefficients taken on by polynomial coding for k=4 levels are the linear, quadratic, and cubic trends in the categorical variable. The categorical variable here is assumed to be represented by an underlying, equally spaced numeric variable. Therefore, this type of encoding is used only for ordered categorical variables with equal spacing. In general, the polynomial contrast produces polynomials of order k-1. Since race is not an ordered factor variable let\u2019s use read as an\nexample. First we need to create an ordered categorical from read.",
            "markdown"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "hsb2[\"readcat\"] = np.asarray(pd.cut(hsb2.read, bins=4))\nhsb2[\"readcat\"] = hsb2[\"readcat\"].astype(object)\nhsb2.groupby(\"readcat\").mean()[\"write\"]",
            "code"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "readcat\n(27.952, 40.0]    42.772727\n(40.0, 52.0]      49.978495\n(52.0, 64.0]      56.563636\n(64.0, 76.0]      61.833333\nName: write, dtype: float64",
            "code"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "from patsy.contrasts import Poly\n\nlevels = hsb2.readcat.unique()\ncontrast = Poly().code_without_intercept(levels)\nprint(contrast.matrix)",
            "code"
        ],
        [
            "[[-0.67082039  0.5        -0.2236068 ]\n [-0.2236068  -0.5         0.67082039]\n [ 0.2236068  -0.5        -0.67082039]\n [ 0.67082039  0.5         0.2236068 ]]",
            "code"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "mod = ols(\"write ~ C(readcat, Poly)\", data=hsb2)\nres = mod.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                  write   R-squared:                       0.346\nModel:                            OLS   Adj. R-squared:                  0.336\nMethod:                 Least Squares   F-statistic:                     34.51\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           5.95e-18\nTime:                        17:10:45   Log-Likelihood:                -690.69\nNo. Observations:                 200   AIC:                             1389.\nDf Residuals:                     196   BIC:                             1403.\nDf Model:                           3\nCovariance Type:            nonrobust\n==============================================================================================\n                                 coef    std err          t      P|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------------\nIntercept                     52.7870      0.634     83.268      0.000      51.537      54.037\nC(readcat, Poly).Linear       14.2587      1.484      9.607      0.000      11.332      17.186\nC(readcat, Poly).Quadratic    -0.9680      1.268     -0.764      0.446      -3.468       1.532\nC(readcat, Poly).Cubic        -0.1554      1.006     -0.154      0.877      -2.140       1.829\n==============================================================================\nOmnibus:                        4.467   Durbin-Watson:                   1.768\nProb(Omnibus):                  0.107   Jarque-Bera (JB):                4.289\nSkew:                          -0.307   Prob(JB):                        0.117\nKurtosis:                       2.628   Cond. No.                         3.01\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "As you can see, readcat has a significant linear effect on the dependent variable write but the quadratic and cubic effects are insignificant.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Formulas": [
        [
            "Since version 0.5.0, statsmodels allows users to fit statistical models using R-style formulas. Internally, statsmodels uses the  package to convert formulas and data to the matrices that are used in model fitting. The formula framework is quite powerful; this tutorial only scratches the surface. A full description of the formula language can be found in the patsy docs:",
            "markdown"
        ],
        [
            "",
            "markdown"
        ]
    ],
    "Examples->User Notes->Formulas->Loading modules and functions": [
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np  # noqa:F401  needed in namespace for patsy\nimport statsmodels.api as sm",
            "code"
        ]
    ],
    "Examples->User Notes->Formulas->Loading modules and functions->Import convention": [
        [
            "You can import explicitly from statsmodels.formula.api",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "from statsmodels.formula.api import ols",
            "code"
        ],
        [
            "Alternatively, you can just use the formula namespace of the main statsmodels.api.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "sm.formula.ols",
            "code"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "&lt;bound method Model.from_formula of &lt;class 'statsmodels.regression.linear_model.OLS'",
            "code"
        ],
        [
            "Or you can use the following convention",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "import statsmodels.formula.api as smf",
            "code"
        ],
        [
            "These names are just a convenient way to get access to each model\u2019s from_formula classmethod. See, for instance",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "sm.OLS.from_formula",
            "code"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "&lt;bound method Model.from_formula of &lt;class 'statsmodels.regression.linear_model.OLS'",
            "code"
        ],
        [
            "All of the lower case models accept formula and data arguments, whereas upper case ones take endog and exog design matrices. formula accepts a string which describes the model in terms of a patsy formula. data takes a  data frame or any other data structure that defines a __getitem__ for variable names like a structured array or a dictionary of variables.",
            "markdown"
        ],
        [
            "dir(sm.formula) will print a list of available models.",
            "markdown"
        ],
        [
            "Formula-compatible models have the following generic call signature: (formula, data, subset=None, *args, **kwargs)",
            "markdown"
        ]
    ],
    "Examples->User Notes->Formulas->OLS regression using formulas": [
        [
            "To begin, we fit the linear model described on the  page. Download the data, subset columns, and list-wise delete to remove missing observations:",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "dta = sm.datasets.get_rdataset(\"Guerry\", \"HistData\", cache=True)",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "df = dta.data[[\"Lottery\", \"Literacy\", \"Wealth\", \"Region\"]].dropna()\ndf.head()",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "Fit the model:",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "mod = ols(formula=\"Lottery ~ Literacy + Wealth + Region\", data=df)\nres = mod.fit()\nprint(res.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                Lottery   R-squared:                       0.338\nModel:                            OLS   Adj. R-squared:                  0.287\nMethod:                 Least Squares   F-statistic:                     6.636\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           1.07e-05\nTime:                        17:10:39   Log-Likelihood:                -375.30\nNo. Observations:                  85   AIC:                             764.6\nDf Residuals:                      78   BIC:                             781.7\nDf Model:                           6\nCovariance Type:            nonrobust\n===============================================================================\n                  coef    std err          t      P|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      38.6517      9.456      4.087      0.000      19.826      57.478\nRegion[T.E]   -15.4278      9.727     -1.586      0.117     -34.793       3.938\nRegion[T.N]   -10.0170      9.260     -1.082      0.283     -28.453       8.419\nRegion[T.S]    -4.5483      7.279     -0.625      0.534     -19.039       9.943\nRegion[T.W]   -10.0913      7.196     -1.402      0.165     -24.418       4.235\nLiteracy       -0.1858      0.210     -0.886      0.378      -0.603       0.232\nWealth          0.4515      0.103      4.390      0.000       0.247       0.656\n==============================================================================\nOmnibus:                        3.049   Durbin-Watson:                   1.785\nProb(Omnibus):                  0.218   Jarque-Bera (JB):                2.694\nSkew:                          -0.340   Prob(JB):                        0.260\nKurtosis:                       2.454   Cond. No.                         371.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ]
    ],
    "Examples->User Notes->Formulas->Categorical variables": [
        [
            "Looking at the summary printed above, notice that patsy determined that elements of Region were text strings, so it treated Region as a categorical variable. patsy\u2019s default is also to include an intercept, so we automatically dropped one of the Region categories.",
            "markdown"
        ],
        [
            "If Region had been an integer variable that we wanted to treat explicitly as categorical, we could have done so by using the C() operator:",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "res = ols(formula=\"Lottery ~ Literacy + Wealth + C(Region)\", data=df).fit()\nprint(res.params)",
            "code"
        ],
        [
            "Intercept         38.651655\nC(Region)[T.E]   -15.427785\nC(Region)[T.N]   -10.016961\nC(Region)[T.S]    -4.548257\nC(Region)[T.W]   -10.091276\nLiteracy          -0.185819\nWealth             0.451475\ndtype: float64",
            "code"
        ],
        [
            "Patsy\u2019s mode advanced features for categorical variables are discussed in:",
            "markdown"
        ]
    ],
    "Examples->User Notes->Formulas->Operators": [
        [
            "We have already seen that \u201c~\u201d separates the left-hand side of the model from the right-hand side, and that \u201c+\u201d adds new columns to the design matrix.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Formulas->Removing variables": [
        [
            "The \u201c-\u201d sign can be used to remove columns/variables. For instance, we can remove the intercept from a model by:",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "res = ols(formula=\"Lottery ~ Literacy + Wealth + C(Region) -1 \", data=df).fit()\nprint(res.params)",
            "code"
        ],
        [
            "C(Region)[C]    38.651655\nC(Region)[E]    23.223870\nC(Region)[N]    28.634694\nC(Region)[S]    34.103399\nC(Region)[W]    28.560379\nLiteracy        -0.185819\nWealth           0.451475\ndtype: float64",
            "code"
        ]
    ],
    "Examples->User Notes->Formulas->Multiplicative interactions": [
        [
            "\u201c:\u201d adds a new column to the design matrix with the interaction of the other two columns. \u201c*\u201d will also include the individual columns that were multiplied together:",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "res1 = ols(formula=\"Lottery ~ Literacy : Wealth - 1\", data=df).fit()\nres2 = ols(formula=\"Lottery ~ Literacy * Wealth - 1\", data=df).fit()\nprint(res1.params, \"\\n\")\nprint(res2.params)",
            "code"
        ],
        [
            "Literacy:Wealth    0.018176\ndtype: float64\n\nLiteracy           0.427386\nWealth             1.080987\nLiteracy:Wealth   -0.013609\ndtype: float64",
            "code"
        ],
        [
            "Many other things are possible with operators. Please consult the  to learn more.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Formulas->Functions": [
        [
            "You can apply vectorized functions to the variables in your model:",
            "markdown"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "res = smf.ols(formula=\"Lottery ~ np.log(Literacy)\", data=df).fit()\nprint(res.params)",
            "code"
        ],
        [
            "Intercept           115.609119\nnp.log(Literacy)    -20.393959\ndtype: float64",
            "code"
        ],
        [
            "Define a custom function:",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "def log_plus_1(x):\n    return np.log(x) + 1.0\n\n\nres = smf.ols(formula=\"Lottery ~ log_plus_1(Literacy)\", data=df).fit()\nprint(res.params)",
            "code"
        ],
        [
            "Intercept               136.003079\nlog_plus_1(Literacy)    -20.393959\ndtype: float64",
            "code"
        ],
        [
            "Any function that is in the calling namespace is available to the formula.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Formulas->Using formulas with models that do not (yet) support them": [
        [
            "Even if a given statsmodels function does not support formulas, you can still use patsy\u2019s formula language to produce design matrices. Those matrices can then be fed to the fitting function as endog and exog arguments.",
            "markdown"
        ],
        [
            "To generate numpy arrays:",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "import patsy\n\nf = \"Lottery ~ Literacy * Wealth\"\ny, X = patsy.dmatrices(f, df, return_type=\"matrix\")\nprint(y[:5])\nprint(X[:5])",
            "code"
        ],
        [
            "[[41.]\n [38.]\n [66.]\n [80.]\n [79.]]\n[[1.000e+00 3.700e+01 7.300e+01 2.701e+03]\n [1.000e+00 5.100e+01 2.200e+01 1.122e+03]\n [1.000e+00 1.300e+01 6.100e+01 7.930e+02]\n [1.000e+00 4.600e+01 7.600e+01 3.496e+03]\n [1.000e+00 6.900e+01 8.300e+01 5.727e+03]]",
            "code"
        ],
        [
            "To generate pandas data frames:",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "f = \"Lottery ~ Literacy * Wealth\"\ny, X = patsy.dmatrices(f, df, return_type=\"dataframe\")\nprint(y[:5])\nprint(X[:5])",
            "code"
        ],
        [
            "Lottery\n0     41.0\n1     38.0\n2     66.0\n3     80.0\n4     79.0\n   Intercept  Literacy  Wealth  Literacy:Wealth\n0        1.0      37.0    73.0           2701.0\n1        1.0      51.0    22.0           1122.0\n2        1.0      13.0    61.0            793.0\n3        1.0      46.0    76.0           3496.0\n4        1.0      69.0    83.0           5727.0",
            "code"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "print(sm.OLS(y, X).fit().summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                Lottery   R-squared:                       0.309\nModel:                            OLS   Adj. R-squared:                  0.283\nMethod:                 Least Squares   F-statistic:                     12.06\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           1.32e-06\nTime:                        17:10:39   Log-Likelihood:                -377.13\nNo. Observations:                  85   AIC:                             762.3\nDf Residuals:                      81   BIC:                             772.0\nDf Model:                           3\nCovariance Type:            nonrobust\n===================================================================================\n                      coef    std err          t      P|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nIntercept          38.6348     15.825      2.441      0.017       7.149      70.121\nLiteracy           -0.3522      0.334     -1.056      0.294      -1.016       0.312\nWealth              0.4364      0.283      1.544      0.126      -0.126       0.999\nLiteracy:Wealth    -0.0005      0.006     -0.085      0.933      -0.013       0.012\n==============================================================================\nOmnibus:                        4.447   Durbin-Watson:                   1.953\nProb(Omnibus):                  0.108   Jarque-Bera (JB):                3.228\nSkew:                          -0.332   Prob(JB):                        0.199\nKurtosis:                       2.314   Cond. No.                     1.40e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.4e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
            "code"
        ]
    ],
    "Examples->User Notes->Prediction": [
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "code"
        ]
    ],
    "Examples->User Notes->Prediction->Artificial data": [
        [
            "[3]:",
            "code"
        ],
        [
            "nsample = 50\nsig = 0.25\nx1 = np.linspace(0, 20, nsample)\nX = np.column_stack((x1, np.sin(x1), (x1 - 5) ** 2))\nX = sm.add_constant(X)\nbeta = [5.0, 0.5, 0.5, -0.02]\ny_true = np.dot(X, beta)\ny = y_true + sig * np.random.normal(size=nsample)",
            "code"
        ]
    ],
    "Examples->User Notes->Prediction->Estimation": [
        [
            "[4]:",
            "code"
        ],
        [
            "olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "code"
        ],
        [
            "OLS Regression Results\n==============================================================================\nDep. Variable:                      y   R-squared:                       0.986\nModel:                            OLS   Adj. R-squared:                  0.985\nMethod:                 Least Squares   F-statistic:                     1064.\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           1.74e-42\nTime:                        17:06:17   Log-Likelihood:                 4.4594\nNo. Observations:                  50   AIC:                           -0.9188\nDf Residuals:                      46   BIC:                             6.729\nDf Model:                           3\nCovariance Type:            nonrobust\n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          4.9488      0.079     62.924      0.000       4.791       5.107\nx1             0.5124      0.012     42.246      0.000       0.488       0.537\nx2             0.6093      0.048     12.778      0.000       0.513       0.705\nx3            -0.0216      0.001    -20.250      0.000      -0.024      -0.019\n==============================================================================\nOmnibus:                        6.272   Durbin-Watson:                   2.752\nProb(Omnibus):                  0.043   Jarque-Bera (JB):                5.171\nSkew:                           0.723   Prob(JB):                       0.0754\nKurtosis:                       3.625   Cond. No.                         221.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ]
    ],
    "Examples->User Notes->Prediction->In-sample prediction": [
        [
            "[5]:",
            "code"
        ],
        [
            "ypred = olsres.predict(X)\nprint(ypred)",
            "code"
        ],
        [
            "[ 4.40969647  4.94512482  5.43363276  5.84201378  6.14904543  6.34897612\n  6.45247004  6.48485512  6.48196167  6.48423561  6.53009328  6.64960978\n  6.85957769  7.16074809  7.53770719  7.96140876  8.39394633  8.79478112\n  9.12740246  9.36532639  9.49644688  9.52502482  9.470988    9.36665699\n  9.25143498  9.16533394  9.14240078  9.20512354  9.36073664  9.60003232\n  9.89887343 10.22215909 10.52959169 10.78229764 10.94921423 11.01219312\n 10.96898204 10.83359489 10.63400924 10.40756972 10.19485242 10.03299831\n  9.94961111  9.95822208 10.05606687 10.22453968 10.43225015 10.64018073\n 10.80809711 10.90115863]",
            "code"
        ]
    ],
    "Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot": [
        [
            "[6]:",
            "code"
        ],
        [
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "code"
        ],
        [
            "[10.87960032 10.69855331 10.38191219  9.98412967  9.57688468  9.23153285\n  9.00163619  8.90984922  8.94237236  9.05233055]",
            "code"
        ]
    ],
    "Examples->User Notes->Prediction->Plot comparison": [
        [
            "[7]:",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.plot(x1, y, \"o\", label=\"Data\")\nax.plot(x1, y_true, \"b-\", label=\"True\")\nax.plot(np.hstack((x1, x1n)), np.hstack((ypred, ynewpred)), \"r\", label=\"OLS prediction\")\nax.legend(loc=\"best\")",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "&lt;matplotlib.legend.Legend at 0x7fc0436f7010\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_predict_12_1.png\" src=\"../../../_images/examples_notebooks_generated_predict_12_1.png\"/>",
            "code"
        ]
    ],
    "Examples->User Notes->Prediction->Predicting with Formulas": [
        [
            "Using formulas can make both estimation and prediction a lot easier",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "from statsmodels.formula.api import ols\n\ndata = {\"x1\": x1, \"y\": y}\n\nres = ols(\"y ~ x1 + np.sin(x1) + I((x1-5)**2)\", data=data).fit()",
            "code"
        ],
        [
            "We use the I to indicate use of the Identity transform. Ie., we do not want any expansion magic from using **2",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "res.params",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "Intercept           4.948842\nx1                  0.512418\nnp.sin(x1)          0.609303\nI((x1 - 5) ** 2)   -0.021566\ndtype: float64",
            "code"
        ],
        [
            "Now we only have to pass the single variable and we get the transformed right-hand side variables automatically",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "res.predict(exog=dict(x1=x1n))",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "0    10.879600\n1    10.698553\n2    10.381912\n3     9.984130\n4     9.576885\n5     9.231533\n6     9.001636\n7     8.909849\n8     8.942372\n9     9.052331\ndtype: float64",
            "code"
        ]
    ],
    "Examples->User Notes->Forecasting in statsmodels": [
        [
            "This notebook describes forecasting using time series models in statsmodels.",
            "markdown"
        ],
        [
            "<strong>Note</strong>: this notebook applies only to the state space model classes, which are:",
            "markdown"
        ],
        [
            "sm.tsa.SARIMAX",
            "markdown"
        ],
        [
            "sm.tsa.UnobservedComponents",
            "markdown"
        ],
        [
            "sm.tsa.VARMAX",
            "markdown"
        ],
        [
            "sm.tsa.DynamicFactor",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nmacrodata = sm.datasets.macrodata.load_pandas().data\nmacrodata.index = pd.period_range('1959Q1', '2009Q3', freq='Q')",
            "code"
        ]
    ],
    "Examples->User Notes->Forecasting in statsmodels->Basic example": [
        [
            "A simple example is to use an AR(1) model to forecast inflation. Before forecasting, let\u2019s take a look at the series:",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "&lt;AxesSubplot: \n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_forecasting_3_1.png\" src=\"../../../_images/examples_notebooks_generated_statespace_forecasting_3_1.png\"/>",
            "code"
        ]
    ],
    "Examples->User Notes->Forecasting in statsmodels->Basic example->Constructing and estimating the model": [
        [
            "The next step is to formulate the econometric model that we want to use for forecasting. In this case, we will use an AR(1) model via the SARIMAX class in statsmodels.",
            "markdown"
        ],
        [
            "After constructing the model, we need to estimate its parameters. This is done using the fit method. The summary method produces several convenient tables showing the results.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "# Construct the model\nmod = sm.tsa.SARIMAX(endog, order=(1, 0, 0), trend='c')\n# Estimate the parameters\nres = mod.fit()\n\nprint(res.summary())",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            3     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  2.32873D+00    |proj g|=  8.23649D-03\n\nAt iterate    5    f=  2.32864D+00    |proj g|=  1.41994D-03\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    3      8     10      1     0     0   5.820D-06   2.329D+00\n  F =   2.3286389358138591\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n                               SARIMAX Results\n==============================================================================\nDep. Variable:                   infl   No. Observations:                  203\nModel:               SARIMAX(1, 0, 0)   Log Likelihood                -472.714\nDate:                Wed, 02 Nov 2022   AIC                            951.427\nTime:                        17:06:31   BIC                            961.367\nSample:                    03-31-1959   HQIC                           955.449\n                         - 09-30-2009\nCovariance Type:                  opg\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      1.3962      0.254      5.488      0.000       0.898       1.895\nar.L1          0.6441      0.039     16.482      0.000       0.568       0.721\nsigma2         6.1519      0.397     15.487      0.000       5.373       6.930\n===================================================================================\nLjung-Box (L1) (Q):                   8.43   Jarque-Bera (JB):                68.45\nProb(Q):                              0.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               1.47   Skew:                            -0.22\nProb(H) (two-sided):                  0.12   Kurtosis:                         5.81\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ]
    ],
    "Examples->User Notes->Forecasting in statsmodels->Basic example->Forecasting": [
        [
            "Out-of-sample forecasts are produced using the forecast or get_forecast methods from the results object.",
            "markdown"
        ],
        [
            "The forecast method gives only point forecasts.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "# The default is to get a one-step-ahead forecast:\nprint(res.forecast())",
            "code"
        ],
        [
            "2009Q4    3.68921\nFreq: Q-DEC, dtype: float64",
            "code"
        ],
        [
            "The get_forecast method is more general, and also allows constructing confidence intervals.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "# Here we construct a more complete results object.\nfcast_res1 = res.get_forecast()\n\n# Most results are collected in the `summary_frame` attribute.\n# Here we specify that we want a confidence level of 90%\nprint(fcast_res1.summary_frame(alpha=0.10))",
            "code"
        ],
        [
            "infl       mean   mean_se  mean_ci_lower  mean_ci_upper\n2009Q4  3.68921  2.480302      -0.390523       7.768943",
            "code"
        ],
        [
            "The default confidence level is 95%, but this can be controlled by setting the alpha parameter, where the confidence level is defined as \\((1 - \\alpha) \\times 100\\%\\). In the example above, we specified a confidence level of 90%, using alpha=0.10.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Forecasting in statsmodels->Basic example->Specifying the number of forecasts": [
        [
            "Both of the functions forecast and get_forecast accept a single argument indicating how many forecasting steps are desired. One option for this argument is always to provide an integer describing the number of steps ahead you want.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "print(res.forecast(steps=2))",
            "code"
        ],
        [
            "2009Q4    3.689210\n2010Q1    3.772434\nFreq: Q-DEC, Name: predicted_mean, dtype: float64",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "fcast_res2 = res.get_forecast(steps=2)\n# Note: since we did not specify the alpha parameter, the\n# confidence level is at the default, 95%\nprint(fcast_res2.summary_frame())",
            "code"
        ],
        [
            "infl        mean   mean_se  mean_ci_lower  mean_ci_upper\n2009Q4  3.689210  2.480302      -1.172092       8.550512\n2010Q1  3.772434  2.950274      -2.009996       9.554865",
            "code"
        ],
        [
            "However, <strong>if your data included a Pandas index with a defined frequency</strong> (see the section at the end on Indexes for more information), then you can alternatively specify the date through which you want forecasts to be produced:",
            "markdown"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "print(res.forecast('2010Q2'))",
            "code"
        ],
        [
            "2009Q4    3.689210\n2010Q1    3.772434\n2010Q2    3.826039\nFreq: Q-DEC, Name: predicted_mean, dtype: float64",
            "code"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "fcast_res3 = res.get_forecast('2010Q2')\nprint(fcast_res3.summary_frame())",
            "code"
        ],
        [
            "infl        mean   mean_se  mean_ci_lower  mean_ci_upper\n2009Q4  3.689210  2.480302      -1.172092       8.550512\n2010Q1  3.772434  2.950274      -2.009996       9.554865\n2010Q2  3.826039  3.124571      -2.298008       9.950087",
            "code"
        ]
    ],
    "Examples->User Notes->Forecasting in statsmodels->Basic example->Plotting the data, forecasts, and confidence intervals": [
        [
            "Often it is useful to plot the data, the forecasts, and the confidence intervals. There are many ways to do this, but here\u2019s one example",
            "markdown"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "fig, ax = plt.subplots(figsize=(15, 5))\n\n# Plot the data (here we are subsetting it to get a better look at the forecasts)\nendog.loc['1999':].plot(ax=ax)\n\n# Construct the forecasts\nfcast = res.get_forecast('2011Q4').summary_frame()\nfcast['mean'].plot(ax=ax, style='k--')\nax.fill_between(fcast.index, fcast['mean_ci_lower'], fcast['mean_ci_upper'], color='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_forecasting_20_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_forecasting_20_0.png\"/>",
            "code"
        ]
    ],
    "Examples->User Notes->Forecasting in statsmodels->Basic example->Note on what to expect from forecasts": [
        [
            "The forecast above may not look very impressive, as it is almost a straight line. This is because this is a very simple, univariate forecasting model. Nonetheless, keep in mind that these simple forecasting models can be extremely competitive.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Forecasting in statsmodels->Prediction vs Forecasting": [
        [
            "The results objects also contain two methods that all for both in-sample fitted values and out-of-sample forecasting. They are predict and get_prediction. The predict method only returns point predictions (similar to forecast), while the get_prediction method also returns additional results (similar to get_forecast).",
            "markdown"
        ],
        [
            "In general, if your interest is out-of-sample forecasting, it is easier to stick to the forecast and get_forecast methods.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Forecasting in statsmodels->Cross validation": [
        [
            "<strong>Note</strong>: some of the functions used in this section were first introduced in statsmodels v0.11.0.",
            "markdown"
        ],
        [
            "A common use case is to cross-validate forecasting methods by performing h-step-ahead forecasts recursively using the following process:",
            "markdown"
        ],
        [
            "Fit model parameters on a training sample",
            "markdown"
        ],
        [
            "Produce h-step-ahead forecasts from the end of that sample",
            "markdown"
        ],
        [
            "Compare forecasts against test dataset to compute error rate",
            "markdown"
        ],
        [
            "Expand the sample to include the next observation, and repeat",
            "markdown"
        ],
        [
            "Economists sometimes call this a pseudo-out-of-sample forecast evaluation exercise, or time-series cross-validation.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Forecasting in statsmodels->Cross validation->Example": [
        [
            "We will conduct a very simple exercise of this sort using the inflation dataset above. The full dataset contains 203 observations, and for expositional purposes we\u2019ll use the first 80% as our training sample and only consider one-step-ahead forecasts.",
            "markdown"
        ],
        [
            "A single iteration of the above procedure looks like the following:",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "# Step 1: fit model parameters w/ training sample\ntraining_obs = int(len(endog) * 0.8)\n\ntraining_endog = endog[:training_obs]\ntraining_mod = sm.tsa.SARIMAX(\n    training_endog, order=(1, 0, 0), trend='c')\ntraining_res = training_mod.fit()\n\n# Print the estimated parameters\nprint(training_res.params)",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            3     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  2.23132D+00    |proj g|=  1.09171D-02\n\nAt iterate    5    f=  2.23109D+00    |proj g|=  3.93607D-05\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    3      6      8      1     0     0   7.066D-07   2.231D+00\n  F =   2.2310884444664749\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\nintercept    1.162076\nar.L1        0.724242\nsigma2       5.051600\ndtype: float64",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "# Step 2: produce one-step-ahead forecasts\nfcast = training_res.forecast()\n\n# Step 3: compute root mean square forecasting error\ntrue = endog.reindex(fcast.index)\nerror = true - fcast\n\n# Print out the results\nprint(pd.concat([true.rename('true'),\n                 fcast.rename('forecast'),\n                 error.rename('error')], axis=1))",
            "code"
        ],
        [
            "true  forecast    error\n1999Q3  3.35   2.55262  0.79738",
            "code"
        ],
        [
            "To add on another observation, we can use the append or extend results methods. Either method can produce the same forecasts, but they differ in the other results that are available:",
            "markdown"
        ],
        [
            "append is the more complete method. It always stores results for all training observations, and it optionally allows refitting the model parameters given the new observations (note that the default is not to refit the parameters).",
            "markdown"
        ],
        [
            "extend is a faster method that may be useful if the training sample is very large. It only stores results for the new observations, and it does not allow refitting the model parameters (i.e.\u00a0you have to use the parameters estimated on the previous sample).",
            "markdown"
        ],
        [
            "If your training sample is relatively small (less than a few thousand observations, for example) or if you want to compute the best possible forecasts, then you should use the append method. However, if that method is infeasible (for example, because you have a very large training sample) or if you are okay with slightly suboptimal forecasts (because the parameter estimates will be slightly stale), then you can consider the extend method.",
            "markdown"
        ],
        [
            "A second iteration, using the append method and refitting the parameters, would go as follows (note again that the default for append does not refit the parameters, but we have overridden that with the refit=True argument):",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "# Step 1: append a new observation to the sample and refit the parameters\nappend_res = training_res.append(endog[training_obs:training_obs + 1], refit=True)\n\n# Print the re-estimated parameters\nprint(append_res.params)",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            3     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  2.22839D+00    |proj g|=  2.38555D-03\n\nAt iterate    5    f=  2.22838D+00    |proj g|=  9.78329D-08\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    3      5      8      1     0     0   9.783D-08   2.228D+00\n  F =   2.2283821699856365\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\nintercept    1.171544\nar.L1        0.723152\nsigma2       5.024580\ndtype: float64",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ],
        [
            "Notice that these estimated parameters are slightly different than those we originally estimated. With the new results object, append_res, we can compute forecasts starting from one observation further than the previous call:",
            "markdown"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "# Step 2: produce one-step-ahead forecasts\nfcast = append_res.forecast()\n\n# Step 3: compute root mean square forecasting error\ntrue = endog.reindex(fcast.index)\nerror = true - fcast\n\n# Print out the results\nprint(pd.concat([true.rename('true'),\n                 fcast.rename('forecast'),\n                 error.rename('error')], axis=1))",
            "code"
        ],
        [
            "true  forecast     error\n1999Q4  2.85  3.594102 -0.744102",
            "code"
        ],
        [
            "Putting it altogether, we can perform the recursive forecast evaluation exercise as follows:",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "# Setup forecasts\nnforecasts = 3\nforecasts = {}\n\n# Get the number of initial training observations\nnobs = len(endog)\nn_init_training = int(nobs * 0.8)\n\n# Create model for initial training sample, fit parameters\ninit_training_endog = endog.iloc[:n_init_training]\nmod = sm.tsa.SARIMAX(training_endog, order=(1, 0, 0), trend='c')\nres = mod.fit()\n\n# Save initial forecast\nforecasts[training_endog.index[-1]] = res.forecast(steps=nforecasts)\n\n# Step through the rest of the sample\nfor t in range(n_init_training, nobs):\n    # Update the results by appending the next observation\n    updated_endog = endog.iloc[t:t+1]\n    res = res.append(updated_endog, refit=False)\n\n    # Save the new set of forecasts\n    forecasts[updated_endog.index[0]] = res.forecast(steps=nforecasts)\n\n# Combine all forecasts into a dataframe\nforecasts = pd.concat(forecasts, axis=1)\n\nprint(forecasts.iloc[:5, :5])",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            3     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  2.23132D+00    |proj g|=  1.09171D-02\n\nAt iterate    5    f=  2.23109D+00    |proj g|=  3.93607D-05\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    3      6      8      1     0     0   7.066D-07   2.231D+00\n  F =   2.2310884444664749\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n          1999Q2    1999Q3    1999Q4    2000Q1    2000Q2\n1999Q3  2.552620       NaN       NaN       NaN       NaN\n1999Q4  3.010790  3.588286       NaN       NaN       NaN\n2000Q1  3.342616  3.760863  3.226165       NaN       NaN\n2000Q2       NaN  3.885850  3.498599  3.885225       NaN\n2000Q3       NaN       NaN  3.695908  3.975918  4.196649",
            "code"
        ],
        [
            "We now have a set of three forecasts made at each point in time from 1999Q2 through 2009Q3. We can construct the forecast errors by subtracting each forecast from the actual value of endog at that point.",
            "markdown"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "# Construct the forecast errors\nforecast_errors = forecasts.apply(lambda column: endog - column).reindex(forecasts.index)\n\nprint(forecast_errors.iloc[:5, :5])",
            "code"
        ],
        [
            "1999Q2    1999Q3    1999Q4    2000Q1    2000Q2\n1999Q3  0.797380       NaN       NaN       NaN       NaN\n1999Q4 -0.160790 -0.738286       NaN       NaN       NaN\n2000Q1  0.417384 -0.000863  0.533835       NaN       NaN\n2000Q2       NaN  0.304150  0.691401  0.304775       NaN\n2000Q3       NaN       NaN -0.925908 -1.205918 -1.426649",
            "code"
        ],
        [
            "To evaluate our forecasts, we often want to look at a summary value like the root mean square error. Here we can compute that for each horizon by first flattening the forecast errors so that they are indexed by horizon and then computing the root mean square error fore each horizon.",
            "markdown"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "# Reindex the forecasts by horizon rather than by date\ndef flatten(column):\n    return column.dropna().reset_index(drop=True)\n\nflattened = forecast_errors.apply(flatten)\nflattened.index = (flattened.index + 1).rename('horizon')\n\nprint(flattened.iloc[:3, :5])",
            "code"
        ],
        [
            "1999Q2    1999Q3    1999Q4    2000Q1    2000Q2\nhorizon\n1        0.797380 -0.738286  0.533835  0.304775 -1.426649\n2       -0.160790 -0.000863  0.691401 -1.205918 -0.311464\n3        0.417384  0.304150 -0.925908 -0.151602 -2.384952",
            "code"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "# Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)",
            "code"
        ],
        [
            "horizon\n1    3.292700\n2    3.421808\n3    3.280012\ndtype: float64",
            "code"
        ]
    ],
    "Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend": [
        [
            "We can check that we get similar forecasts if we instead use the extend method, but that they are not exactly the same as when we use append with the refit=True argument. This is because extend does not re-estimate the parameters given the new observation.",
            "markdown"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "# Setup forecasts\nnforecasts = 3\nforecasts = {}\n\n# Get the number of initial training observations\nnobs = len(endog)\nn_init_training = int(nobs * 0.8)\n\n# Create model for initial training sample, fit parameters\ninit_training_endog = endog.iloc[:n_init_training]\nmod = sm.tsa.SARIMAX(training_endog, order=(1, 0, 0), trend='c')\nres = mod.fit()\n\n# Save initial forecast\nforecasts[training_endog.index[-1]] = res.forecast(steps=nforecasts)\n\n# Step through the rest of the sample\nfor t in range(n_init_training, nobs):\n    # Update the results by appending the next observation\n    updated_endog = endog.iloc[t:t+1]\n    res = res.extend(updated_endog)\n\n    # Save the new set of forecasts\n    forecasts[updated_endog.index[0]] = res.forecast(steps=nforecasts)\n\n# Combine all forecasts into a dataframe\nforecasts = pd.concat(forecasts, axis=1)\n\nprint(forecasts.iloc[:5, :5])",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            3     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  2.23132D+00    |proj g|=  1.09171D-02\n\nAt iterate    5    f=  2.23109D+00    |proj g|=  3.93607D-05\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    3      6      8      1     0     0   7.066D-07   2.231D+00\n  F =   2.2310884444664749\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL",
            "code"
        ],
        [
            "This problem is unconstrained.",
            "code"
        ],
        [
            "1999Q2    1999Q3    1999Q4    2000Q1    2000Q2\n1999Q3  2.552620       NaN       NaN       NaN       NaN\n1999Q4  3.010790  3.588286       NaN       NaN       NaN\n2000Q1  3.342616  3.760863  3.226165       NaN       NaN\n2000Q2       NaN  3.885850  3.498599  3.885225       NaN\n2000Q3       NaN       NaN  3.695908  3.975918  4.196649",
            "code"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "# Construct the forecast errors\nforecast_errors = forecasts.apply(lambda column: endog - column).reindex(forecasts.index)\n\nprint(forecast_errors.iloc[:5, :5])",
            "code"
        ],
        [
            "1999Q2    1999Q3    1999Q4    2000Q1    2000Q2\n1999Q3  0.797380       NaN       NaN       NaN       NaN\n1999Q4 -0.160790 -0.738286       NaN       NaN       NaN\n2000Q1  0.417384 -0.000863  0.533835       NaN       NaN\n2000Q2       NaN  0.304150  0.691401  0.304775       NaN\n2000Q3       NaN       NaN -0.925908 -1.205918 -1.426649",
            "code"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "# Reindex the forecasts by horizon rather than by date\ndef flatten(column):\n    return column.dropna().reset_index(drop=True)\n\nflattened = forecast_errors.apply(flatten)\nflattened.index = (flattened.index + 1).rename('horizon')\n\nprint(flattened.iloc[:3, :5])",
            "code"
        ],
        [
            "1999Q2    1999Q3    1999Q4    2000Q1    2000Q2\nhorizon\n1        0.797380 -0.738286  0.533835  0.304775 -1.426649\n2       -0.160790 -0.000863  0.691401 -1.205918 -0.311464\n3        0.417384  0.304150 -0.925908 -0.151602 -2.384952",
            "code"
        ],
        [
            "[22]:",
            "code"
        ],
        [
            "# Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)",
            "code"
        ],
        [
            "horizon\n1    3.292700\n2    3.421808\n3    3.280012\ndtype: float64",
            "code"
        ],
        [
            "By not re-estimating the parameters, our forecasts are slightly worse (the root mean square error is higher at each horizon). However, the process is faster, even with only 200 datapoints. Using the %%timeit cell magic on the cells above, we found a runtime of 570ms using extend versus 1.7s using append with refit=True. (Note that using extend is also faster than using append with refit=False).",
            "markdown"
        ]
    ],
    "Examples->User Notes->Forecasting in statsmodels->Indexes": [
        [
            "Throughout this notebook, we have been making use of Pandas date indexes with an associated frequency. As you can see, this index marks our data as at a quarterly frequency, between 1959Q1 and 2009Q3.",
            "markdown"
        ],
        [
            "[23]:",
            "code"
        ],
        [
            "print(endog.index)",
            "code"
        ],
        [
            "PeriodIndex(['1959Q1', '1959Q2', '1959Q3', '1959Q4', '1960Q1', '1960Q2',\n             '1960Q3', '1960Q4', '1961Q1', '1961Q2',\n             ...\n             '2007Q2', '2007Q3', '2007Q4', '2008Q1', '2008Q2', '2008Q3',\n             '2008Q4', '2009Q1', '2009Q2', '2009Q3'],\n            dtype='period[Q-DEC]', length=203)",
            "code"
        ],
        [
            "In most cases, if your data has an associated data/time index with a defined frequency (like quarterly, monthly, etc.), then it is best to make sure your data is a Pandas series with the appropriate index. Here are three examples of this:",
            "markdown"
        ],
        [
            "[24]:",
            "code"
        ],
        [
            "# Annual frequency, using a PeriodIndex\nindex = pd.period_range(start='2000', periods=4, freq='A')\nendog1 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog1.index)",
            "code"
        ],
        [
            "PeriodIndex(['2000', '2001', '2002', '2003'], dtype='period[A-DEC]')",
            "code"
        ],
        [
            "[25]:",
            "code"
        ],
        [
            "# Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)",
            "code"
        ],
        [
            "DatetimeIndex(['2000-01-01', '2000-04-01', '2000-07-01', '2000-10-01'], dtype='datetime64[ns]', freq='QS-JAN')",
            "code"
        ],
        [
            "[26]:",
            "code"
        ],
        [
            "# Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)",
            "code"
        ],
        [
            "DatetimeIndex(['2000-01-31', '2000-02-29', '2000-03-31', '2000-04-30'], dtype='datetime64[ns]', freq='M')",
            "code"
        ],
        [
            "In fact, if your data has an associated date/time index, it is best to use that even if does not have a defined frequency. An example of that kind of index is as follows - notice that it has freq=None:",
            "markdown"
        ],
        [
            "[27]:",
            "code"
        ],
        [
            "index = pd.DatetimeIndex([\n    '2000-01-01 10:08am', '2000-01-01 11:32am',\n    '2000-01-01 5:32pm', '2000-01-02 6:15am'])\nendog4 = pd.Series([0.2, 0.5, -0.1, 0.1], index=index)\nprint(endog4.index)",
            "code"
        ],
        [
            "DatetimeIndex(['2000-01-01 10:08:00', '2000-01-01 11:32:00',\n               '2000-01-01 17:32:00', '2000-01-02 06:15:00'],\n              dtype='datetime64[ns]', freq=None)",
            "code"
        ],
        [
            "You can still pass this data to statsmodels\u2019 model classes, but you will get the following warning, that no frequency data was found:",
            "markdown"
        ],
        [
            "[28]:",
            "code"
        ],
        [
            "mod = sm.tsa.SARIMAX(endog4)\nres = mod.fit()",
            "code"
        ],
        [
            "RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  1.37900D-01    |proj g|=  4.66940D-01\n\nAt iterate    5    f=  1.32476D-01    |proj g|=  6.00133D-06\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      5     10      1     0     0   6.001D-06   1.325D-01\n  F =  0.13247641992895681\n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n This problem is unconstrained.",
            "code"
        ],
        [
            "What this means is that you cannot specify forecasting steps by dates, and the output of the forecast and get_forecast methods will not have associated dates. The reason is that without a given frequency, there is no way to determine what date each forecast should be assigned to. In the example above, there is no pattern to the date/time stamps of the index, so there is no way to determine what the next date/time should be (should it be in the morning of 2000-01-02? the afternoon? or\nmaybe not until 2000-01-03?).",
            "markdown"
        ],
        [
            "For example, if we forecast one-step-ahead:",
            "markdown"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "res.forecast(1)",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:834: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n  return get_prediction_index(",
            "code"
        ],
        [
            "[29]:",
            "code"
        ],
        [
            "4    0.011866\ndtype: float64",
            "code"
        ],
        [
            "The index associated with the new forecast is 4, because if the given data had an integer index, that would be the next value. A warning is given letting the user know that the index is not a date/time index.",
            "markdown"
        ],
        [
            "If we try to specify the steps of the forecast using a date, we will get the following exception:",
            "markdown"
        ],
        [
            "KeyError: 'The `end` argument could not be matched to a location related to the index of the data.'",
            "code"
        ],
        [
            "[30]:",
            "code"
        ],
        [
            "# Here we'll catch the exception to prevent printing too much of\n# the exception trace output in this notebook\ntry:\n    res.forecast('2000-01-03')\nexcept KeyError as e:\n    print(e)",
            "code"
        ],
        [
            "'The `end` argument could not be matched to a location related to the index of the data.'",
            "code"
        ],
        [
            "Ultimately there is nothing wrong with using data that does not have an associated date/time frequency, or even using data that has no index at all, like a Numpy array. However, if you can use a Pandas series with an associated frequency, you\u2019ll have more options for specifying your forecasts and get back results with a more useful index.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Generic Maximum Likelihood": [
        [
            "This tutorial explains how to quickly implement new maximum likelihood models in statsmodels. We give two examples:",
            "markdown"
        ],
        [
            "Probit model for binary dependent variables",
            "markdown"
        ],
        [
            "Negative binomial model for count data",
            "markdown"
        ],
        [
            "The GenericLikelihoodModel class eases the process by providing tools such as automatic numeric differentiation and a unified interface to scipy optimization functions. Using statsmodels, users can fit new MLE models simply by \u201cplugging-in\u201d a log-likelihood function.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model": [
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.base.model import GenericLikelihoodModel",
            "code"
        ],
        [
            "The Spector dataset is distributed with statsmodels. You can access a vector of values for the dependent variable (endog) and a matrix of regressors (exog) like this:",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "data = sm.datasets.spector.load_pandas()\nexog = data.exog\nendog = data.endog\nprint(sm.datasets.spector.NOTE)\nprint(data.exog.head())",
            "code"
        ],
        [
            "::\n\n    Number of Observations - 32\n\n    Number of Variables - 4\n\n    Variable name definitions::\n\n        Grade - binary variable indicating whether or not a student's grade\n                improved.  1 indicates an improvement.\n        TUCE  - Test score on economics test\n        PSI   - participation in program\n        GPA   - Student's grade point average\n\n    GPA  TUCE  PSI\n0  2.66  20.0  0.0\n1  2.89  22.0  0.0\n2  3.28  24.0  0.0\n3  2.92  12.0  0.0\n4  4.00  21.0  0.0",
            "code"
        ],
        [
            "Them, we add a constant to the matrix of regressors:",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "exog = sm.add_constant(exog, prepend=True)",
            "code"
        ],
        [
            "To create your own Likelihood Model, you simply need to overwrite the loglike method.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "class MyProbit(GenericLikelihoodModel):\n    def loglike(self, params):\n        exog = self.exog\n        endog = self.endog\n        q = 2 * endog - 1\n        return stats.norm.logcdf(q*np.dot(exog, params)).sum()",
            "code"
        ],
        [
            "Estimate the model and print a summary:",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "sm_probit_manual = MyProbit(endog, exog).fit()\nprint(sm_probit_manual.summary())",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.400588\n         Iterations: 292\n         Function evaluations: 494\n                               MyProbit Results\n==============================================================================\nDep. Variable:                  GRADE   Log-Likelihood:                -12.819\nModel:                       MyProbit   AIC:                             33.64\nMethod:            Maximum Likelihood   BIC:                             39.50\nDate:                Wed, 02 Nov 2022\nTime:                        17:02:03\nNo. Observations:                  32\nDf Residuals:                      28\nDf Model:                           3\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -7.4523      2.542     -2.931      0.003     -12.435      -2.469\nGPA            1.6258      0.694      2.343      0.019       0.266       2.986\nTUCE           0.0517      0.084      0.617      0.537      -0.113       0.216\nPSI            1.4263      0.595      2.397      0.017       0.260       2.593\n==============================================================================",
            "code"
        ],
        [
            "Compare your Probit implementation to statsmodels\u2019 \u201ccanned\u201d implementation:",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "sm_probit_canned = sm.Probit(endog, exog).fit()",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.400588\n         Iterations 6",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "print(sm_probit_canned.params)\nprint(sm_probit_manual.params)",
            "code"
        ],
        [
            "const   -7.452320\nGPA      1.625810\nTUCE     0.051729\nPSI      1.426332\ndtype: float64\n[-7.45233176  1.62580888  0.05172971  1.42631954]",
            "code"
        ],
        [
            "[8]:",
            "code"
        ],
        [
            "print(sm_probit_canned.cov_params())\nprint(sm_probit_manual.cov_params())",
            "code"
        ],
        [
            "const       GPA      TUCE       PSI\nconst  6.464166 -1.169668 -0.101173 -0.594792\nGPA   -1.169668  0.481473 -0.018914  0.105439\nTUCE  -0.101173 -0.018914  0.007038  0.002472\nPSI   -0.594792  0.105439  0.002472  0.354070\n[[ 6.46416769e+00 -1.16966616e+00 -1.01173181e-01 -5.94788997e-01]\n [-1.16966616e+00  4.81472112e-01 -1.89134585e-02  1.05438225e-01]\n [-1.01173181e-01 -1.89134585e-02  7.03758394e-03  2.47189243e-03]\n [-5.94788997e-01  1.05438225e-01  2.47189243e-03  3.54069512e-01]]",
            "code"
        ],
        [
            "Notice that the GenericMaximumLikelihood class provides automatic differentiation, so we did not have to provide Hessian or Score functions in order to calculate the covariance estimates.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data": [
        [
            "Consider a negative binomial regression model for count data with log-likelihood (type NB-2) function expressed as:\n\n\\[\\mathcal{L}(\\beta_j; y, \\alpha) = \\sum_{i=1}^n y_i ln\n\\left ( \\frac{\\alpha exp(X_i'\\beta)}{1+\\alpha exp(X_i'\\beta)} \\right ) -\n\\frac{1}{\\alpha} ln(1+\\alpha exp(X_i'\\beta)) + ln \\Gamma (y_i + 1/\\alpha) - ln \\Gamma (y_i+1) - ln \\Gamma (1/\\alpha)\\]",
            "markdown"
        ],
        [
            "with a matrix of regressors \\(X\\), a vector of coefficients \\(\\beta\\), and the negative binomial heterogeneity parameter \\(\\alpha\\).",
            "markdown"
        ],
        [
            "Using the nbinom distribution from scipy, we can write this likelihood simply as:",
            "markdown"
        ],
        [
            "[9]:",
            "code"
        ],
        [
            "import numpy as np\nfrom scipy.stats import nbinom",
            "code"
        ],
        [
            "[10]:",
            "code"
        ],
        [
            "def _ll_nb2(y, X, beta, alph):\n    mu = np.exp(np.dot(X, beta))\n    size = 1/alph\n    prob = size/(size+mu)\n    ll = nbinom.logpmf(y, size, prob)\n    return ll",
            "code"
        ]
    ],
    "Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->New Model Class": [
        [
            "We create a new model class which inherits from GenericLikelihoodModel:",
            "markdown"
        ],
        [
            "[11]:",
            "code"
        ],
        [
            "from statsmodels.base.model import GenericLikelihoodModel",
            "code"
        ],
        [
            "[12]:",
            "code"
        ],
        [
            "class NBin(GenericLikelihoodModel):\n    def __init__(self, endog, exog, **kwds):\n        super(NBin, self).__init__(endog, exog, **kwds)\n\n    def nloglikeobs(self, params):\n        alph = params[-1]\n        beta = params[:-1]\n        ll = _ll_nb2(self.endog, self.exog, beta, alph)\n        return -ll\n\n    def fit(self, start_params=None, maxiter=10000, maxfun=5000, **kwds):\n        # we have one additional parameter and we need to add it for summary\n        self.exog_names.append('alpha')\n        if start_params == None:\n            # Reasonable starting values\n            start_params = np.append(np.zeros(self.exog.shape[1]), .5)\n            # intercept\n            start_params[-2] = np.log(self.endog.mean())\n        return super(NBin, self).fit(start_params=start_params,\n                                     maxiter=maxiter, maxfun=maxfun,\n                                     **kwds)",
            "code"
        ],
        [
            "Two important things to notice:",
            "markdown"
        ],
        [
            "nloglikeobs: This function should return one evaluation of the negative log-likelihood function per observation in your dataset (i.e.\u00a0rows of the endog/X matrix).",
            "markdown"
        ],
        [
            "start_params: A one-dimensional array of starting values needs to be provided. The size of this array determines the number of parameters that will be used in optimization.",
            "markdown"
        ],
        [
            "That\u2019s it! You\u2019re done!",
            "markdown"
        ]
    ],
    "Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Usage Example": [
        [
            "The  dataset is hosted in CSV format at the . We use the read_csv function from the  to load the data in memory. We then print the first few columns:",
            "markdown"
        ],
        [
            "[13]:",
            "code"
        ],
        [
            "import statsmodels.api as sm",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "medpar = sm.datasets.get_rdataset(\"medpar\", \"COUNT\", cache=True).data\n\nmedpar.head()",
            "code"
        ],
        [
            "[14]:",
            "code"
        ],
        [
            "The model we are interested in has a vector of non-negative integers as dependent variable (los), and 5 regressors: Intercept, type2, type3, hmo, white.",
            "markdown"
        ],
        [
            "For estimation, we need to create two variables to hold our regressors and the outcome variable. These can be ndarrays or pandas objects.",
            "markdown"
        ],
        [
            "[15]:",
            "code"
        ],
        [
            "y = medpar.los\nX = medpar[[\"type2\", \"type3\", \"hmo\", \"white\"]].copy()\nX[\"constant\"] = 1",
            "code"
        ],
        [
            "Then, we fit the model and extract some information:",
            "markdown"
        ],
        [
            "[16]:",
            "code"
        ],
        [
            "mod = NBin(y, X)\nres = mod.fit()",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 3.209014\n         Iterations: 805\n         Function evaluations: 1238",
            "code"
        ],
        [
            "/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/base/model.py:2694: UserWarning: df_model + k_constant differs from nparams\n  warnings.warn(\"df_model + k_constant differs from nparams\")\n/opt/hostedtoolcache/Python/3.10.8/x64/lib/python3.10/site-packages/statsmodels/base/model.py:2696: UserWarning: df_resid differs from nobs - nparams\n  warnings.warn(\"df_resid differs from nobs - nparams\")",
            "code"
        ],
        [
            "Extract parameter estimates, standard errors, p-values, AIC, etc.:",
            "markdown"
        ],
        [
            "[17]:",
            "code"
        ],
        [
            "print('Parameters: ', res.params)\nprint('Standard errors: ', res.bse)\nprint('P-values: ', res.pvalues)\nprint('AIC: ', res.aic)",
            "code"
        ],
        [
            "Parameters:  [ 0.2212642   0.70613942 -0.06798155 -0.12903932  2.31026565  0.44575147]\nStandard errors:  [0.05059259 0.07613047 0.05326095 0.06854137 0.06794697 0.01981542]\nP-values:  [1.22298069e-005 1.76979353e-020 2.01818957e-001 5.97480106e-002\n 2.15240570e-253 4.62688812e-112]\nAIC:  9604.95320583016",
            "code"
        ],
        [
            "As usual, you can obtain a full list of available information by typing dir(res). We can also look at the summary of the estimation results.",
            "markdown"
        ],
        [
            "[18]:",
            "code"
        ],
        [
            "print(res.summary())",
            "code"
        ],
        [
            "NBin Results\n==============================================================================\nDep. Variable:                    los   Log-Likelihood:                -4797.5\nModel:                           NBin   AIC:                             9605.\nMethod:            Maximum Likelihood   BIC:                             9632.\nDate:                Wed, 02 Nov 2022\nTime:                        17:02:03\nNo. Observations:                1495\nDf Residuals:                    1490\nDf Model:                           4\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntype2          0.2213      0.051      4.373      0.000       0.122       0.320\ntype3          0.7061      0.076      9.275      0.000       0.557       0.855\nhmo           -0.0680      0.053     -1.276      0.202      -0.172       0.036\nwhite         -0.1290      0.069     -1.883      0.060      -0.263       0.005\nconstant       2.3103      0.068     34.001      0.000       2.177       2.443\nalpha          0.4458      0.020     22.495      0.000       0.407       0.485\n==============================================================================",
            "code"
        ]
    ],
    "Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing": [
        [
            "We can check the results by using the statsmodels implementation of the Negative Binomial model, which uses the analytic score function and Hessian.",
            "markdown"
        ],
        [
            "[19]:",
            "code"
        ],
        [
            "res_nbin = sm.NegativeBinomial(y, X).fit(disp=0)\nprint(res_nbin.summary())",
            "code"
        ],
        [
            "NegativeBinomial Regression Results\n==============================================================================\nDep. Variable:                    los   No. Observations:                 1495\nModel:               NegativeBinomial   Df Residuals:                     1490\nMethod:                           MLE   Df Model:                            4\nDate:                Wed, 02 Nov 2022   Pseudo R-squ.:                 0.01215\nTime:                        17:02:04   Log-Likelihood:                -4797.5\nconverged:                       True   LL-Null:                       -4856.5\nCovariance Type:            nonrobust   LLR p-value:                 1.404e-24\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntype2          0.2212      0.051      4.373      0.000       0.122       0.320\ntype3          0.7062      0.076      9.276      0.000       0.557       0.855\nhmo           -0.0680      0.053     -1.276      0.202      -0.172       0.036\nwhite         -0.1291      0.069     -1.883      0.060      -0.263       0.005\nconstant       2.3103      0.068     34.001      0.000       2.177       2.443\nalpha          0.4457      0.020     22.495      0.000       0.407       0.485\n==============================================================================",
            "code"
        ],
        [
            "[20]:",
            "code"
        ],
        [
            "print(res_nbin.params)",
            "code"
        ],
        [
            "type2       0.221218\ntype3       0.706173\nhmo        -0.067987\nwhite      -0.129053\nconstant    2.310279\nalpha       0.445748\ndtype: float64",
            "code"
        ],
        [
            "[21]:",
            "code"
        ],
        [
            "print(res_nbin.bse)",
            "code"
        ],
        [
            "type2       0.050592\ntype3       0.076131\nhmo         0.053261\nwhite       0.068541\nconstant    0.067947\nalpha       0.019815\ndtype: float64",
            "code"
        ],
        [
            "Or we could compare them to results obtained using the MASS implementation for R:",
            "markdown"
        ],
        [
            "url = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/csv/COUNT/medpar.csv'\nmedpar = read.csv(url)\nf = los~factor(type)+hmo+white\n\nlibrary(MASS)\nmod = glm.nb(f, medpar)\ncoef(summary(mod))\n                 Estimate Std. Error   z value      Pr(|z|)\n(Intercept)    2.31027893 0.06744676 34.253370 3.885556e-257\nfactor(type)2  0.22124898 0.05045746  4.384861  1.160597e-05\nfactor(type)3  0.70615882 0.07599849  9.291748  1.517751e-20\nhmo           -0.06795522 0.05321375 -1.277024  2.015939e-01\nwhite         -0.12906544 0.06836272 -1.887951  5.903257e-02",
            "code"
        ]
    ],
    "Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Numerical precision": [
        [
            "The statsmodels generic MLE and R parameter estimates agree up to the fourth decimal. The standard errors, however, agree only up to the second decimal. This discrepancy is the result of imprecision in our Hessian numerical estimates. In the current context, the difference between MASS and statsmodels standard error estimates is substantively irrelevant, but it highlights the fact that users who need very precise estimates may not always want to rely on default settings when\nusing numerical derivatives. In such cases, it is better to use analytical derivatives with the LikelihoodModel class.",
            "markdown"
        ]
    ],
    "Examples->User Notes->Dates in Time-Series Models": [
        [
            "[1]:",
            "code"
        ],
        [
            "import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.ar_model import AutoReg, ar_select_order\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "code"
        ]
    ],
    "Examples->User Notes->Dates in Time-Series Models->Getting started": [
        [
            "[2]:",
            "code"
        ],
        [
            "data = sm.datasets.sunspots.load()",
            "code"
        ],
        [
            "Right now an annual date series must be datetimes at the end of the year.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "from datetime import datetime\n\ndates = pd.date_range(\"1700-1-1\", periods=len(data.endog), freq=\"A-DEC\")",
            "code"
        ]
    ],
    "Examples->User Notes->Dates in Time-Series Models->Using Pandas": [
        [
            "Make a pandas TimeSeries or DataFrame",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "data.endog.index = dates\nendog = data.endog\nendog",
            "code"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "1700-12-31     5.0\n1701-12-31    11.0\n1702-12-31    16.0\n1703-12-31    23.0\n1704-12-31    36.0\n              ...\n2004-12-31    40.4\n2005-12-31    29.8\n2006-12-31    15.2\n2007-12-31     7.5\n2008-12-31     2.9\nFreq: A-DEC, Name: SUNACTIVITY, Length: 309, dtype: float64",
            "code"
        ],
        [
            "Instantiate the model",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "selection_res = ar_select_order(endog, 9, old_names=False, seasonal=True, period=11)\npandas_ar_res = selection_res.model.fit()",
            "code"
        ],
        [
            "Out-of-sample prediction",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "pred = pandas_ar_res.predict(start=\"2005\", end=\"2027\")\nprint(pred)",
            "code"
        ],
        [
            "2005-12-31    25.907501\n2006-12-31    29.024373\n2007-12-31    18.855729\n2008-12-31    21.392012\n2009-12-31    25.695092\n2010-12-31    49.981886\n2011-12-31    76.437566\n2012-12-31    86.244709\n2013-12-31    89.184663\n2014-12-31    72.251362\n2015-12-31    48.447259\n2016-12-31    32.292122\n2017-12-31    22.374099\n2018-12-31    18.581657\n2019-12-31    22.935657\n2020-12-31    34.513168\n2021-12-31    47.828469\n2022-12-31    65.487766\n2023-12-31    76.551575\n2024-12-31    84.185043\n2025-12-31    72.029019\n2026-12-31    52.603134\n2027-12-31    39.330875\nFreq: A-DEC, dtype: float64",
            "code"
        ],
        [
            "[7]:",
            "code"
        ],
        [
            "fig = pandas_ar_res.plot_predict(start=\"2005\", end=\"2027\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\"/>",
            "code"
        ]
    ],
    "Examples->User Notes->Least squares fitting of models to data": [
        [
            "This is a quick introduction to statsmodels for physical scientists (e.g.\u00a0physicists, astronomers) or engineers.",
            "markdown"
        ],
        [
            "Why is this needed?",
            "markdown"
        ],
        [
            "Because most of statsmodels was written by statisticians and they use a different terminology and sometimes methods, making it hard to know which classes and functions are relevant and what their inputs and outputs mean.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm",
            "code"
        ]
    ],
    "Examples->User Notes->Least squares fitting of models to data->Linear models": [
        [
            "Assume you have data points with measurements y at positions x as well as measurement errors y_err.",
            "markdown"
        ],
        [
            "How can you use statsmodels to fit a straight line model to this data?",
            "markdown"
        ],
        [
            "For an extensive discussion see  \u2026 we\u2019ll use the example data given by them in Table 1.",
            "markdown"
        ],
        [
            "So the model is f(x) = a * x + b and on Figure 1 they print the result we want to reproduce \u2026 the best-fit parameter and the parameter errors for a \u201cstandard weighted least-squares fit\u201d for this data are: * a = 2.24 +- 0.11 * b = 34 +- 18",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "data = \"\"\"\n  x   y y_err\n201 592    61\n244 401    25\n 47 583    38\n287 402    15\n203 495    21\n 58 173    15\n210 479    27\n202 504    14\n198 510    30\n158 416    16\n165 393    14\n201 442    25\n157 317    52\n131 311    16\n166 400    34\n160 337    31\n186 423    42\n125 334    26\n218 533    16\n146 344    22\n\"\"\"\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\ndata = pd.read_csv(StringIO(data), delim_whitespace=True).astype(float)\n\n# Note: for the results we compare with the paper here, they drop the first four points\ndata.head()",
            "code"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "To fit a straight line use the weighted least squares class  \u2026 the parameters are called: * exog = sm.add_constant(x) * endog = y * weights = 1 / sqrt(y_err)",
            "markdown"
        ],
        [
            "Note that exog must be a 2-dimensional array with x as a column and an extra column of ones. Adding this column of ones means you want to fit the model y = a * x + b, leaving it off means you want to fit the model y = a * x.",
            "markdown"
        ],
        [
            "And you have to use the option cov_type='fixed scale' to tell statsmodels that you really have measurement errors with an absolute scale. If you do not, statsmodels will treat the weights as relative weights between the data points and internally re-scale them so that the best-fit model will have chi**2 / ndf = 1.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "exog = sm.add_constant(data[\"x\"])\nendog = data[\"y\"]\nweights = 1.0 / (data[\"y_err\"] ** 2)\nwls = sm.WLS(endog, exog, weights)\nresults = wls.fit(cov_type=\"fixed scale\")\nprint(results.summary())",
            "code"
        ],
        [
            "WLS Regression Results\n==============================================================================\nDep. Variable:                      y   R-squared:                       0.400\nModel:                            WLS   Adj. R-squared:                  0.367\nMethod:                 Least Squares   F-statistic:                     193.5\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           4.52e-11\nTime:                        17:02:51   Log-Likelihood:                -119.06\nNo. Observations:                  20   AIC:                             242.1\nDf Residuals:                      18   BIC:                             244.1\nDf Model:                           1\nCovariance Type:          fixed scale\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        213.2735     14.394     14.817      0.000     185.062     241.485\nx              1.0767      0.077     13.910      0.000       0.925       1.228\n==============================================================================\nOmnibus:                        0.943   Durbin-Watson:                   2.901\nProb(Omnibus):                  0.624   Jarque-Bera (JB):                0.181\nSkew:                          -0.205   Prob(JB):                        0.914\nKurtosis:                       3.220   Cond. No.                         575.\n==============================================================================\n\nNotes:\n[1] Standard Errors are based on fixed scale",
            "code"
        ]
    ],
    "Examples->User Notes->Least squares fitting of models to data->Linear models->Check against scipy.optimize.curve_fit": [
        [
            "[4]:",
            "code"
        ],
        [
            "# You can use `scipy.optimize.curve_fit` to get the best-fit parameters and parameter errors.\nfrom scipy.optimize import curve_fit\n\n\ndef f(x, a, b):\n    return a * x + b\n\n\nxdata = data[\"x\"]\nydata = data[\"y\"]\np0 = [0, 0]  # initial parameter estimate\nsigma = data[\"y_err\"]\npopt, pcov = curve_fit(f, xdata, ydata, p0, sigma, absolute_sigma=True)\nperr = np.sqrt(np.diag(pcov))\nprint(\"a = {0:10.3f} +- {1:10.3f}\".format(popt[0], perr[0]))\nprint(\"b = {0:10.3f} +- {1:10.3f}\".format(popt[1], perr[1]))",
            "code"
        ],
        [
            "a =      1.077 +-      0.077\nb =    213.273 +-     14.394",
            "code"
        ]
    ],
    "Examples->User Notes->Least squares fitting of models to data->Linear models->Check against self-written cost function": [
        [
            "[5]:",
            "code"
        ],
        [
            "# You can also use `scipy.optimize.minimize` and write your own cost function.\n# This does not give you the parameter errors though ... you'd have\n# to estimate the HESSE matrix separately ...\nfrom scipy.optimize import minimize\n\n\ndef chi2(pars):\n    \"\"\"Cost function.\"\"\"\n    y_model = pars[0] * data[\"x\"] + pars[1]\n    chi = (data[\"y\"] - y_model) / data[\"y_err\"]\n    return np.sum(chi ** 2)\n\n\nresult = minimize(fun=chi2, x0=[0, 0])\npopt = result.x\nprint(\"a = {0:10.3f}\".format(popt[0]))\nprint(\"b = {0:10.3f}\".format(popt[1]))",
            "code"
        ],
        [
            "a =      1.077\nb =    213.274",
            "code"
        ]
    ],
    "Examples->User Notes->Least squares fitting of models to data->Non-linear models": [
        [
            "[6]:",
            "code"
        ],
        [
            "# TODO: we could use the examples from here:\n# http://probfit.readthedocs.org/en/latest/api.html#probfit.costfunc.Chi2Regression",
            "code"
        ]
    ],
    "Examples->User Notes->Distributed Estimations": [
        [
            "This notebook goes through a couple of examples to show how to use distributed_estimation. We import the DistributedModel class and make the exog and endog generators.",
            "markdown"
        ],
        [
            "[1]:",
            "code"
        ],
        [
            "import numpy as np\nfrom scipy.stats.distributions import norm\nfrom statsmodels.base.distributed_estimation import DistributedModel\n\n\ndef _exog_gen(exog, partitions):\n    \"\"\"partitions exog data\"\"\"\n\n    n_exog = exog.shape[0]\n    n_part = np.ceil(n_exog / partitions)\n\n    ii = 0\n    while ii &lt; n_exog:\n        jj = int(min(ii + n_part, n_exog))\n        yield exog[ii:jj, :]\n        ii += int(n_part)\n\n\ndef _endog_gen(endog, partitions):\n    \"\"\"partitions endog data\"\"\"\n\n    n_endog = endog.shape[0]\n    n_part = np.ceil(n_endog / partitions)\n\n    ii = 0\n    while ii &lt; n_endog:\n        jj = int(min(ii + n_part, n_endog))\n        yield endog[ii:jj]\n        ii += int(n_part)",
            "code"
        ],
        [
            "Next we generate some random data to serve as an example.",
            "markdown"
        ],
        [
            "[2]:",
            "code"
        ],
        [
            "X = np.random.normal(size=(1000, 25))\nbeta = np.random.normal(size=25)\nbeta *= np.random.randint(0, 2, size=25)\ny = norm.rvs(loc=X.dot(beta))\nm = 5",
            "code"
        ],
        [
            "This is the most basic fit, showing all of the defaults, which are to use OLS as the model class, and the debiasing procedure.",
            "markdown"
        ],
        [
            "[3]:",
            "code"
        ],
        [
            "debiased_OLS_mod = DistributedModel(m)\ndebiased_OLS_fit = debiased_OLS_mod.fit(\n    zip(_endog_gen(y, m), _exog_gen(X, m)), fit_kwds={\"alpha\": 0.2}\n)",
            "code"
        ],
        [
            "Then we run through a slightly more complicated example which uses the GLM model class.",
            "markdown"
        ],
        [
            "[4]:",
            "code"
        ],
        [
            "from statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.families import Gaussian\n\ndebiased_GLM_mod = DistributedModel(\n    m, model_class=GLM, init_kwds={\"family\": Gaussian()}\n)\ndebiased_GLM_fit = debiased_GLM_mod.fit(\n    zip(_endog_gen(y, m), _exog_gen(X, m)), fit_kwds={\"alpha\": 0.2}\n)",
            "code"
        ],
        [
            "We can also change the estimation_method and the join_method. The below example show how this works for the standard OLS case. Here we using a naive averaging approach instead of the debiasing procedure.",
            "markdown"
        ],
        [
            "[5]:",
            "code"
        ],
        [
            "from statsmodels.base.distributed_estimation import _est_regularized_naive, _join_naive\n\n\nnaive_OLS_reg_mod = DistributedModel(\n    m, estimation_method=_est_regularized_naive, join_method=_join_naive\n)\nnaive_OLS_reg_params = naive_OLS_reg_mod.fit(\n    zip(_endog_gen(y, m), _exog_gen(X, m)), fit_kwds={\"alpha\": 0.2}\n)",
            "code"
        ],
        [
            "Finally, we can also change the results_class used. The following example shows how this work for a simple case with an unregularized model and naive averaging.",
            "markdown"
        ],
        [
            "[6]:",
            "code"
        ],
        [
            "from statsmodels.base.distributed_estimation import (\n    _est_unregularized_naive,\n    DistributedResults,\n)\n\n\nnaive_OLS_unreg_mod = DistributedModel(\n    m,\n    estimation_method=_est_unregularized_naive,\n    join_method=_join_naive,\n    results_class=DistributedResults,\n)\nnaive_OLS_unreg_params = naive_OLS_unreg_mod.fit(\n    zip(_endog_gen(y, m), _exog_gen(X, m)), fit_kwds={\"alpha\": 0.2}\n)",
            "code"
        ]
    ],
    "Getting Started": [
        [
            "This very simple case-study is designed to get you up-and-running quickly with\nstatsmodels. Starting from raw data, we will show the steps needed to\nestimate a statistical model and to draw a diagnostic plot. We will only use\nfunctions provided by statsmodels or its pandas and patsy\ndependencies.",
            "markdown"
        ]
    ],
    "Getting Started->Loading modules and functions": [
        [
            "After , we load a\nfew modules and functions:",
            "markdown"
        ],
        [
            "In [1]: import statsmodels.api as sm\n\nIn [2]: import pandas\n\nIn [3]: from patsy import dmatrices",
            "code"
        ],
        [
            "builds on numpy arrays to provide\nrich data structures and data analysis tools. The pandas.DataFrame function\nprovides labelled arrays of (potentially heterogenous) data, similar to the\nR \u201cdata.frame\u201d. The pandas.read_csv function can be used to convert a\ncomma-separated values file to a DataFrame object.",
            "markdown"
        ],
        [
            "is a Python library for describing\nstatistical models and building  using R-like formulas.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This example uses the API interface.  See  for information on\nthe difference between importing the API interfaces (statsmodels.api and\nstatsmodels.tsa.api) and directly importing from the module that defines\nthe model.",
            "markdown"
        ]
    ],
    "Getting Started->Data": [
        [
            "We download the , a\ncollection of historical data used in support of Andre-Michel Guerry\u2019s 1833\nEssay on the Moral Statistics of France. The data set is hosted online in\ncomma-separated values format (CSV) by the  repository.\nWe could download the file locally and then load it using read_csv, but\npandas takes care of all of this automatically for us:",
            "markdown"
        ],
        [
            "In [4]: df = sm.datasets.get_rdataset(\"Guerry\", \"HistData\").data",
            "code"
        ],
        [
            "The  shows how to import from various\nother formats.",
            "markdown"
        ],
        [
            "We select the variables of interest and look at the bottom 5 rows:",
            "markdown"
        ],
        [
            "In [5]: vars = ['Department', 'Lottery', 'Literacy', 'Wealth', 'Region']\n\nIn [6]: df = df[vars]\n\nIn [7]: df[-5:]\nOut[7]: \n      Department  Lottery  Literacy  Wealth Region\n81        Vienne       40        25      68      W\n82  Haute-Vienne       55        13      67      C\n83        Vosges       14        62      82      E\n84         Yonne       51        47      30      C\n85         Corse       83        49      37    NaN",
            "code"
        ],
        [
            "Notice that there is one missing observation in the Region column. We\neliminate it using a DataFrame method provided by pandas:",
            "markdown"
        ],
        [
            "In [8]: df = df.dropna()\n\nIn [9]: df[-5:]\nOut[9]: \n      Department  Lottery  Literacy  Wealth Region\n80        Vendee       68        28      56      W\n81        Vienne       40        25      68      W\n82  Haute-Vienne       55        13      67      C\n83        Vosges       14        62      82      E\n84         Yonne       51        47      30      C",
            "code"
        ]
    ],
    "Getting Started->Substantive motivation and model": [
        [
            "We want to know whether literacy rates in the 86 French departments are\nassociated with per capita wagers on the Royal Lottery in the 1820s. We need to\ncontrol for the level of wealth in each department, and we also want to include\na series of dummy variables on the right-hand side of our regression equation to\ncontrol for unobserved heterogeneity due to regional effects. The model is\nestimated using ordinary least squares regression (OLS).",
            "markdown"
        ]
    ],
    "Getting Started->Design matrices (endog & exog)": [
        [
            "To fit most of the models covered by statsmodels, you will need to create\ntwo design matrices. The first is a matrix of endogenous variable(s) (i.e.\ndependent, response, regressand, etc.). The second is a matrix of exogenous\nvariable(s) (i.e. independent, predictor, regressor, etc.). The OLS coefficient\nestimates are calculated as usual:\n\n\\[\\hat{\\beta} = (X'X)^{-1} X'y\\]",
            "markdown"
        ],
        [
            "where \\(y\\) is an \\(N \\times 1\\) column of data on lottery wagers per\ncapita (Lottery). \\(X\\) is \\(N \\times 7\\) with an intercept, the\nLiteracy and Wealth variables, and 4 region binary variables.",
            "markdown"
        ],
        [
            "The patsy module provides a convenient function to prepare design matrices\nusing R-like formulas. You can find more information .",
            "markdown"
        ],
        [
            "We use patsy\u2019s dmatrices function to create design matrices:",
            "markdown"
        ],
        [
            "In [10]: y, X = dmatrices('Lottery ~ Literacy + Wealth + Region', data=df, return_type='dataframe')",
            "code"
        ],
        [
            "The resulting matrices/data frames look like this:",
            "markdown"
        ],
        [
            "In [11]: y[:3]\nOut[11]: \n   Lottery\n0     41.0\n1     38.0\n2     66.0\n\nIn [12]: X[:3]\nOut[12]: \n   Intercept  Region[T.E]  Region[T.N]  ...  Region[T.W]  Literacy  Wealth\n0        1.0          1.0          0.0  ...          0.0      37.0    73.0\n1        1.0          0.0          1.0  ...          0.0      51.0    22.0\n2        1.0          0.0          0.0  ...          0.0      13.0    61.0\n\n[3 rows x 7 columns]",
            "code"
        ],
        [
            "Notice that dmatrices has",
            "markdown"
        ],
        [
            "split the categorical Region variable into a set of indicator variables.",
            "markdown"
        ],
        [
            "added a constant to the exogenous regressors matrix.",
            "markdown"
        ],
        [
            "returned pandas DataFrames instead of simple numpy arrays. This is useful because DataFrames allow statsmodels to carry-over meta-data (e.g. variable names) when reporting results.",
            "markdown"
        ],
        [
            "The above behavior can of course be altered. See the .",
            "markdown"
        ]
    ],
    "Getting Started->Model fit and summary": [
        [
            "Fitting a model in statsmodels typically involves 3 easy steps:",
            "markdown"
        ],
        [
            "Use the model class to describe the model",
            "markdown"
        ],
        [
            "Fit the model using a class method",
            "markdown"
        ],
        [
            "Inspect the results using a summary method",
            "markdown"
        ],
        [
            "For OLS, this is achieved by:",
            "markdown"
        ],
        [
            "In [13]: mod = sm.OLS(y, X)    # Describe model\n\nIn [14]: res = mod.fit()       # Fit model\n\nIn [15]: print(res.summary())   # Summarize model\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                Lottery   R-squared:                       0.338\nModel:                            OLS   Adj. R-squared:                  0.287\nMethod:                 Least Squares   F-statistic:                     6.636\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):           1.07e-05\nTime:                        17:12:43   Log-Likelihood:                -375.30\nNo. Observations:                  85   AIC:                             764.6\nDf Residuals:                      78   BIC:                             781.7\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      38.6517      9.456      4.087      0.000      19.826      57.478\nRegion[T.E]   -15.4278      9.727     -1.586      0.117     -34.793       3.938\nRegion[T.N]   -10.0170      9.260     -1.082      0.283     -28.453       8.419\nRegion[T.S]    -4.5483      7.279     -0.625      0.534     -19.039       9.943\nRegion[T.W]   -10.0913      7.196     -1.402      0.165     -24.418       4.235\nLiteracy       -0.1858      0.210     -0.886      0.378      -0.603       0.232\nWealth          0.4515      0.103      4.390      0.000       0.247       0.656\n==============================================================================\nOmnibus:                        3.049   Durbin-Watson:                   1.785\nProb(Omnibus):                  0.218   Jarque-Bera (JB):                2.694\nSkew:                          -0.340   Prob(JB):                        0.260\nKurtosis:                       2.454   Cond. No.                         371.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "code"
        ],
        [
            "The res object has many useful attributes. For example, we can extract\nparameter estimates and r-squared by typing:",
            "markdown"
        ],
        [
            "In [16]: res.params\nOut[16]: \nIntercept      38.651655\nRegion[T.E]   -15.427785\nRegion[T.N]   -10.016961\nRegion[T.S]    -4.548257\nRegion[T.W]   -10.091276\nLiteracy       -0.185819\nWealth          0.451475\ndtype: float64\n\nIn [17]: res.rsquared\nOut[17]: 0.337950869192882",
            "code"
        ],
        [
            "Type dir(res) for a full list of attributes.",
            "markdown"
        ],
        [
            "For more information and examples, see the",
            "markdown"
        ]
    ],
    "Getting Started->Diagnostics and specification tests": [
        [
            "statsmodels allows you to conduct a range of useful .  For instance,\napply the Rainbow test for linearity (the null hypothesis is that the\nrelationship is properly modelled as linear):",
            "markdown"
        ],
        [
            "In [18]: sm.stats.linear_rainbow(res)\nOut[18]: (0.8472339976156916, 0.6997965543621643)",
            "code"
        ],
        [
            "Admittedly, the output produced above is not very verbose, but we know from\nreading the \n(also, print(sm.stats.linear_rainbow.__doc__)) that the\nfirst number is an F-statistic and that the second is the p-value.",
            "markdown"
        ],
        [
            "statsmodels also provides graphics functions. For example, we can draw a\nplot of partial regression for a set of regressors by:",
            "markdown"
        ],
        [
            "In [19]: sm.graphics.plot_partregress('Lottery', 'Wealth', ['Region', 'Literacy'],\n   ....:                              data=df, obs_labels=False)\n   ....: \neval_env: 1\nOut[19]: &lt;Figure size 640x480 with 1 Axes\n\n\n<img alt=\"_images/gettingstarted_0.png\" src=\"_images/gettingstarted_0.png\"/>",
            "code"
        ]
    ],
    "Getting Started->Documentation": [
        [
            "Documentation can be accessed from an IPython session\nusing .",
            "markdown"
        ]
    ],
    "Getting Started->More": [
        [
            "Congratulations! You\u2019re ready to move on to other topics in the",
            "markdown"
        ]
    ]
}