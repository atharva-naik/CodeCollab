{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d9eb263",
   "metadata": {},
   "source": [
    "# 6.1 Machine Learning Fundamentals\n",
    "\n",
    "__[pretty display of variables](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)__\n",
    "\n",
    "```python\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from pydataset import data\n",
    "quakes = data('quakes')\n",
    "quakes.head()\n",
    "quakes.tail()\n",
    "```\n",
    "\n",
    "\n",
    "#### 1. Introduction To K-Nearest Neighbors\n",
    "    \n",
    "    Learn the basics of machine learning to suggest optimal AirBnB list prices\n",
    "    * The basics of the machine learning workflow\n",
    "    * How the k-nearest neighbors algorithm works\n",
    "    * The role of Euclidan distance in machine learning\n",
    "    \n",
    "#### 2. Evaluation Model Performance\n",
    "\n",
    "    Learn how to test models using error metrics and simple validation\n",
    "    * How to evaluate model accuracy using MSE and RMSE\n",
    "    * How to compare MSE and RMSE values\n",
    "\n",
    "#### 3. Multivariate K-Nearest Neighbors (Feature Selection)\n",
    "\n",
    "    Improve your predictions by using more features\n",
    "    * How to use multiple variables in machine learning models\n",
    "    * How to prepare columns by normalizeing and handling missing values\n",
    "\n",
    "#### 4. Hyperparameter Optimization (Optimal K value)\n",
    "\n",
    "    Vary the k value to improve performance\n",
    "    * How a model's hyoerparameters affect the models's performance\n",
    "    * How to use grid search to try different hyperparameter values\n",
    "\n",
    "#### 5. Cross Validation\n",
    "\n",
    "    Learn how to use k-fold cross validation to perform more rigorous testing\n",
    "    * How cross-validation lets us more accuractely understand model performance\n",
    "    * The difference between holdout and k-fold cross validation\n",
    "    * How to perform cross-validation in scikit-learn\n",
    "\n",
    "#### 6. Guided Project: Predicting Car Prices\n",
    "\n",
    "    Practice the machine learning workflow using k-nearest neighbors to predict car prices\n",
    "    * Cleaning data in preparation for machine learning\n",
    "    * iterating on k-nearest neighbors models\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75d4c9",
   "metadata": {},
   "source": [
    "## 6.1.1 Introduction to K-Nearest Neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d86d0",
   "metadata": {},
   "source": [
    "### 6.1.1.1 Problem definition\n",
    "\n",
    "AirBnB is a marketplace for short term rentals that allows you to list part or all of your living space for others to rent. You can rent everything from a room in an apartment to your entire house on AirBnB. Because most of the listings are on a short-term basis, AirBnB has grown to become a __popular alternative to hotels__. The company itself has grown from it's founding in 2008 to a __[30 billion dollar valuation in 2016](https://www.bloomberg.com/news/articles/2016-08-05/airbnb-files-to-raise-850-million-at-30-billion-valuation)__ and is currently worth more than any hotel chain in the world.\n",
    "\n",
    "One challenge that hosts looking to rent their living space face is __determining the optimal nightly rent price__. In many areas, renters are presented with a good selection of listings and can filter on criteria like price, number of bedrooms, room type and more. Since AirBnB is a marketplace, the amount a host can charge on a nightly basis is closely linked to the __dynamics of the marketplace__. Here's a screenshot of the search experience on AirBnB:\n",
    "\n",
    "![img alt](https://s3.amazonaws.com/dq-content/airbnb.png)\n",
    "\n",
    "As a host, if we try to charge above market price for a living space we'd like to rent, then renters will select more affordable alternatives which are similar to ours.. If we set our nightly rent price too low, we'll miss out on potential revenue.\n",
    "\n",
    "One strategy we could use is to:\n",
    "\n",
    "* find a few listings that are similar to ours,\n",
    "* average the listed price for the ones most similar to ours,\n",
    "* set our listing price to this __calculated average price__.\n",
    "\n",
    "The process of __discovering patterns__ in existing data to make a __prediction__ is called __machine learning__. In our case, we want to use data on local listings to predict the optimal price for us to set. In this mission, we'll explore a specific machine learning technique called __k-nearest neighbors__, which mirrors the strategy we just described. Before we dive further into machine learning and k-nearest neighbors, let's get familiar with the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b32a60",
   "metadata": {},
   "source": [
    "### 6.1.1.2 Introduction to the data\n",
    "\n",
    "While AirBnB doesn't release any data on the listings in their marketplace, a separate group named __[Inside AirBnB](http://insideairbnb.com/get-the-data.html)__ has extracted data on a sample of the listings for many of the major cities on the website. In this post, we'll be working with their dataset from October 3, 2015 on the listings from Washington, D.C., the capital of the United States. Here's a __[direct link to that dataset](http://data.insideairbnb.com/united-states/dc/washington-dc/2015-10-03/data/listings.csv.gz)__. Each row in the dataset is a specific listing that's available for renting on AirBnB in the Washington, D.C. area\n",
    "\n",
    "To make the dataset less cumbersome to work with, we've removed many of the columns in the original dataset and renamed the file to __dc_airbnb.csv__. Here are the columns we kept:\n",
    "\n",
    "* host_response_rate: the response rate of the host\n",
    "* host_acceptance_rate: number of requests to the host that convert to rentals\n",
    "* host_listings_count: number of other listings the host has\n",
    "* latitude: latitude dimension of the geographic coordinates\n",
    "* longitude: longitude part of the coordinates\n",
    "* zipcode: the zip code the living space resides\n",
    "* state: the state the living space resides\n",
    "* accommodates: the number of guests the rental can accommodate\n",
    "* room_type: the type of living space (Private room, Shared room or Entire home/apt\n",
    "* bedrooms: number of bedrooms included in the rental\n",
    "* bathrooms: number of bathrooms included in the rental\n",
    "* beds: number of beds included in the rental\n",
    "* price: nightly price for the rental\n",
    "* cleaning_fee: additional fee used for cleaning the living space after the guest leaves\n",
    "* security_deposit: refundable security deposit, in case of damages\n",
    "* minimum_nights: minimum number of nights a guest can stay for the rental\n",
    "* maximum_nights: maximum number of nights a guest can stay for the rental\n",
    "* number_of_reviews: number of reviews that previous guests have left\n",
    "\n",
    "Let's read the dataset into Pandas and become more familiar with it.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Read dc_airbnb.csv into a Dataframe named dc_listings.\n",
    "* Use the print function to display the first row in dc_listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b794fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dc_listings = pd.read_csv(\"Data/dc_airbnb.csv\")\n",
    "dc_listings.head(1)\n",
    "# print(dc_listings.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33aa3e",
   "metadata": {},
   "source": [
    "### 6.1.1.3 K-nearest neighbors\n",
    "\n",
    "Here's the strategy we wanted to use:\n",
    "\n",
    "* Find a few similar listings.\n",
    "* Calculate the average nightly rental price of these listings.\n",
    "* Set the average price as the price for our listing.\n",
    "\n",
    "The k-nearest neighbors algorithm is similar to this strategy. Here's an overview:\n",
    "\n",
    "![img alt](https://s3.amazonaws.com/dq-content/knn_infographic.png)\n",
    "\n",
    "There are 2 things we need to unpack in more detail:\n",
    "\n",
    "* the similarity metric\n",
    "* how to choose the k value\n",
    "\n",
    "In this mission, we'll define what similarity metric we're going to use. Then, we'll implement the k-nearest neighbors algorithm and use it to suggest a price for a new, unpriced listing. We'll use a __k value of 5__ in this mission. In later missions, we'll learn how to evaluate how good the suggested prices are, how to choose the __optimal k value__, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39183663",
   "metadata": {},
   "source": [
    "### 6.1.1.4 Euclidean distance\n",
    "\n",
    "The __similarity metric__ works by comparing a fixed set of numerical features, another word for attributes, between 2 observations, or living spaces in our case. When trying to predict a continuous value, like price, the main similarity metric that's used is __Euclidean distance__. Here's the general formula for Euclidean distance:\n",
    "\n",
    "$d = \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \\cdots + (q_n-p_n)^2}$\n",
    "\n",
    "where $p_1$ to $p_n$ represent the feature values for one observation and $q_1$ to $q_n$ represent the feature values for the other observation. Here's a diagram that breaks down the Euclidean distance between the first 2 observations in the dataset using only the host_listings_count, accommodates, bedrooms, bathrooms, and beds columns:\n",
    "\n",
    "![img alt](https://s3.amazonaws.com/dq-content/euclidean_distance_five_features.png)\n",
    "\n",
    "In this mission, we'll use just one feature in this mission to keep things simple as you become familiar with the machine learning workflow. Since we're only using one feature, this is known as the __univariate case__. Here's how the formula looks like for the univariate case:\n",
    "\n",
    "$d = \\sqrt{(q_1 - p_1)^2} $\n",
    "\n",
    "The square root and the squared power cancel and the formula simplifies to:\n",
    "\n",
    "$d = | q_1 - p_1 | $\n",
    "\n",
    "The living space that we want to rent can accommodate 3 people. Let's first calculate the distance, using just the __accommodates feature__, between the first living space in the dataset and our own.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Calculate the Euclidean distance between our living space, which can accommodate 3 people, and the first living space in the dc_listings Dataframe.\n",
    "* Assign the result to first_distance and display the value using the print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e425c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dc_listings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_acc_value = 3\n",
    "first_living_space_value = dc_listings.iloc[0]['accommodates']\n",
    "first_distance = np.abs(first_living_space_value - our_acc_value)\n",
    "print(first_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849643c",
   "metadata": {},
   "source": [
    "### 6.1.1.5 Calculate distance for all observations\n",
    "\n",
    "The Euclidean distance between the first row in the dc_listings Dataframe and our own living space is 1. How do we know if this is high or low? If you look at the Euclidean distance equation itself, the lowest value you can achieve is 0. This happens when the value for the feature is exactly the same for both observations you're comparing. If $p_1 = q_1$ then $d=|q_1 - p_1|$ which results in $d=0$. The closer to 0 the distance the more similar the living spaces are.\n",
    "\n",
    "If we wanted to calculate the Euclidean distance between each living space in the dataset and a living space that accommodates 8 people, here's a preview of what that would look like.\n",
    "\n",
    "![img alt](https://s3.amazonaws.com/dq-content/distance_between_rows_and_ours.png)\n",
    "\n",
    "Then, we can rank the existing living spaces by ascending distance values, the proxy for similarity.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Calculate the distance between each value in the accommodates column from dc_listings and the value 3, which is the number of people our listing accommodates:\n",
    "    * Use the apply method to calculate the absolute value between each value in accommodates and 3 and return a new Series containing the distance values.\n",
    "* Assign the distance values to the distance column.\n",
    "* Use the Series method value_counts and the print function to display the unique value counts for the distance column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_listings = (dc_listings - dc_listings.mean())/dc_listings.std()\n",
    "normalized_listings['price'] = dc_listings['price']\n",
    "\n",
    "normalized_listings.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6691be",
   "metadata": {},
   "source": [
    "### 6.1.3.5 Euclidean distance for multivariate case\n",
    "\n",
    "In the last mission, we trained 2 __univariate__ k-nearest neighbors models. The first one used the __accommodates__ attribute while the second one used the __bathrooms__ attribute. Let's now train a model that uses both attributes when determining how similar 2 living spaces are. Let's refer to the Euclidean distance equation again to see what the distance calculation __using 2 attributes__ would look like:\n",
    "\n",
    "$d = \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \\cdots + (q_n-p_n)^2}$\n",
    "\n",
    "Since we're using 2 attributes, the distance calculation would look like:\n",
    "\n",
    "$d = \\sqrt{(accommodates_1-accommodates_2)^2 + (bathrooms_1-bathrooms_2)^2 }$\n",
    "\n",
    "To find the distance between 2 living spaces, we need to calculate the squared difference between both accommodates values, the squared difference between both bathrooms values, add them together, and then take the square root of the resulting sum. Here's what the Euclidean distance between the first 2 rows in normalized_listings looks like:\n",
    "\n",
    "![img alt](https://s3.amazonaws.com/dq-content/distance_two_features.png)\n",
    "\n",
    "\n",
    "So far, we've been calculating Euclidean distance ourselves by writing the logic for the equation ourselves. We can instead use the __[distance.euclidean() functio from scipy.spatial](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.euclidean.html)__, which takes in 2 vectors as the parameters and calculates the Euclidean distance between them. The euclidean() function expects:\n",
    "\n",
    "* both of the vectors to be represented using a __list-like__ object (Python list, NumPy array, or pandas Series)\n",
    "* both of the vectors must be 1-dimensional and have the same number of elements\n",
    "\n",
    "Here's a simple example:\n",
    "\n",
    "```python\n",
    "from scipy.spatial import distance\n",
    "first_listing = [-0.596544, -0.439151]\n",
    "second_listing = [-0.596544, 0.412923]\n",
    "dist = distance.euclidean(first_listing, second_listing)\n",
    "```\n",
    "\n",
    "Let's use the __euclidean()__ function to calculate the Euclidean distance between 2 rows in our dataset to practice.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Calculate the Euclidean distance using only the accommodates and bathrooms features between the first row and fifth row in normalized_listings using the distance.euclidean() function.\n",
    "* Assign the distance value to first_fifth_distance and display using the print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "##first_listing = normalized_listings.iloc[0][['accommodates', 'bathrooms']]\n",
    "##fifth_listing = normalized_listings.iloc[4][['accommodates', 'bathrooms']]\n",
    "first_fifth_distance = distance.euclidean(normalized_listings[['accommodates', 'bathrooms']].iloc[4], normalized_listings[['accommodates', 'bathrooms']].iloc[0])\n",
    "print(first_fifth_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4dda67",
   "metadata": {},
   "source": [
    "### 6.1.3.6 Introduction to scikit-learn\n",
    "\n",
    "* <span style=\"color:red\">Regression: predict numerical values;</span>\n",
    "* <span style=\"color:red\">Classification: predict a label;</span>\n",
    "\n",
    "So far, we've been writing functions from scratch to train the k-nearest neighbor models. While this is helpful deliberate practice to understand how the mechanics work, you can be more productive and iterate quicker by using a library that handles most of the implementation. In this screen, we'll learn about the __[scikit-learn library](http://scikit-learn.org/stable/)__, which is the most popular machine learning in Python. Scikit-learn contains functions for all of the major machine learning algorithms and a simple, unified workflow. Both of these properties allow data scientists to be incredibly productive when training and testing different models on a new dataset.\n",
    "\n",
    "The scikit-learn workflow consists of 4 main steps:\n",
    "\n",
    "* instantiate the specific machine learning model you want to use\n",
    "* fit the model to the training data\n",
    "* use the model to make predictions\n",
    "* evaluate the accuracy of the predictions\n",
    "\n",
    "We'll focus on the first 3 steps in this screen and the next screen. Each model in scikit-learn is implemented as a __[separate class](http://scikit-learn.org/dev/modules/classes.html)__ and the first step is to identify the class we want to create an instance of. In our case, we want to use the __[KNeighborsRegressor class](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor)__.\n",
    "\n",
    "Any model that helps us __predict numerical values__, like listing price in our case, is known as a __regression model__. The other main class of machine learning models is called __classification__, where we're trying to __predict a label__ from a fixed set of labels (e.g. blood type or gender). The word regressor from the class name KNeighborsRegressor refers to the regression model class that we just discussed.\n",
    "\n",
    "Scikit-learn uses a similar object-oriented style to Matplotlib and you need to instantiate an empty model first by calling the constructor:\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor()\n",
    "```\n",
    "\n",
    "If you refer to the __[documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor)__, you'll notice that by default:\n",
    "\n",
    "* n_neighbors: the number of neighbors, is set to 5\n",
    "* algorithm: for computing nearest neighbors, is set to auto\n",
    "* p: set to 2, corresponding to Euclidean distance\n",
    "\n",
    "Let's set the __algorithm parameter__ to __brute__ and leave the __n_neighbors value as 5__, which matches the implementation we wrote in the last mission. If we leave the algorithm parameter set to the default value of auto, scikit-learn will try to use __tree-based optimizations__ to improve performance (which are outside of the scope of this mission):\n",
    "\n",
    "```python\n",
    "knn = KNeighborsRegressor(algorithm='brute')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd29577",
   "metadata": {},
   "source": [
    "### 6.1.3.7 Fitting a model and making predictions\n",
    "\n",
    "Now, we can fit the model to the data using the __[fit method](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor.fit)__. For all models, the fit method takes in 2 required parameters:\n",
    "\n",
    "* matrix-like object, containing the feature columns we want to use from the training set.\n",
    "* list-like object, containing correct target values.\n",
    "\n",
    "Matrix-like object means that the method is flexible in the input and either a Dataframe or a NumPy 2D array of values is accepted. This means you can select the columns you want to use from the Dataframe and use that as the first parameter to the fit method.\n",
    "\n",
    "If you recall from earlier in the mission, all of the following are acceptable list-like objects:\n",
    "\n",
    "* NumPy array\n",
    "* Python list\n",
    "* pandas Series object (e.g. when selecting a column)\n",
    "\n",
    "You can select the target column from the Dataframe and use that as the second parameter to the fit method:\n",
    "\n",
    "```python\n",
    "# Split full dataset into train and test sets.\n",
    "train_df = normalized_listings.iloc[0:2792]\n",
    "test_df = normalized_listings.iloc[2792:]\n",
    "# Matrix-like object, containing just the 2 columns of interest from training set.\n",
    "train_features = train_df[['accommodates', 'bathrooms']]\n",
    "# List-like object, containing just the target column, `price`.\n",
    "train_target = train_df['price']\n",
    "# Pass everything into the fit method.\n",
    "knn.fit(train_features, train_target)\n",
    "```\n",
    "\n",
    "When the __fit() method__ is called, scikit-learn stores the training data we specified within the __KNearestNeighbors instance (knn)__. If you try passing in data containing missing values or non-numerical values into the fit method, scikit-learn will return an __error__. Scikit-learn contains many such features that help prevent us from making common mistakes.\n",
    "\n",
    "Now that we specified the training data we want used to make predictions, we can use the __[predict method](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor.predict)__ to make predictions on the test set. The predict method has only one required parameter:\n",
    "\n",
    "* matrix-like object, containing the feature columns from the dataset we want to make predictions on\n",
    "\n",
    "The number of feature columns you use during both training and testing need to __match__ or scikit-learn will return an error:\n",
    "\n",
    "```python\n",
    "predictions = knn.predict(test_df[['accommodates', 'bathrooms']])\n",
    "```\n",
    "\n",
    "The predict() method returns a __NumPy array__ containing the predicted price values for the test set. You now have everything you need to practice the entire scikit-learn workflow.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Create an instance of the __[KNeighborsRegressor class](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor)__ with the following parameters:\n",
    "\n",
    "    * n_neighbors: 5\n",
    "    * algorithm: brute\n",
    "\n",
    "* Use the fit method to specify the data we want the k-nearest neighbor model to use. Use the following parameters:\n",
    "\n",
    "    * training data, feature columns: just the accommodates and bathrooms columns, in that order, from train_df.\n",
    "    * training data, target column: the price column from train_df.\n",
    "\n",
    "* Call the predict method to make predictions on:\n",
    "\n",
    "    * the accommodates and bathrooms columns from test_df\n",
    "    * assign the resulting NumPy array of predicted price values to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "train_df = normalized_listings.iloc[0:2792]\n",
    "test_df = normalized_listings.iloc[2792:]\n",
    "train_columns = ['accommodates', 'bathrooms']\n",
    "\n",
    "# Instantiate ML model.\n",
    "knn = KNeighborsRegressor(n_neighbors=5, algorithm='brute')\n",
    "\n",
    "# Fit model to data.\n",
    "knn.fit(train_df[train_columns], train_df['price'])\n",
    "\n",
    "# Use model to make predictions.\n",
    "predictions = knn.predict(test_df[train_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8be7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447bc52d",
   "metadata": {},
   "source": [
    "### 6.1.3.8 Calculating MSE using Scikit-Learn\n",
    "\n",
    "Earlier in this mission, we calculated the __MSE and RMSE__ values using the pandas arithmetic operators to compare each predicted value with the actual value from the price column of our test set. Alternatively, we can instead use the __[sklearn.metrics.mean_squared_error function()](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)__. Once you become familiar with the different machine learning concepts, unifying your workflow using scikit-learn helps save you a lot of time and avoid mistakes.\n",
    "\n",
    "The __mean_squared_error() function__ takes in 2 inputs:\n",
    "\n",
    "* list-like object, representing the true values\n",
    "* list-like object, representing the predicted values using the model\n",
    "\n",
    "For this function, we won't show any sample code and will leave it to you to understand the function __[from the documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)__ itself to calculate the MSE and RMSE values for the predictions we just made.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Use the mean_squared_error function to calculate the MSE value for the predictions we made in the previous screen.\n",
    "* Assign the MSE value to two_features_mse.\n",
    "* Calculate the RMSE value by taking the square root of the MSE value and assign to two_features_rmse.\n",
    "* Display both of these error scores using the print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03daaf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "two_features_mse = mean_squared_error(predictions, test_df['price'])\n",
    "two_features_rmse = np.sqrt(two_features_mse)\n",
    "print(two_features_mse)\n",
    "print(two_features_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55799ce9",
   "metadata": {},
   "source": [
    "### 6.1.3.9 Using more features\n",
    "\n",
    "Here's a table comparing the MSE and RMSE values for the 2 univariate models from the last mission and the multivariate model we just trained:\n",
    "\n",
    "<table>\n",
    "<tbody><tr>\n",
    "<th>feature(s)</th>\n",
    "<th>MSE</th>\n",
    "<th>RMSE</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>accommodates</td>\n",
    "<td>18646.5</td>\n",
    "<td>136.6</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>bathrooms</td>\n",
    "<td>17333.4</td>\n",
    "<td>131.7</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>accommodates, bathrooms</td>\n",
    "<td>15660.4</td>\n",
    "<td>125.1</td>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "\n",
    "As you can tell, the model we trained using both features __ended up performing better__ (lower error score) than either of the univariate models from the last mission. Let's now train a model using the following 4 features:\n",
    "\n",
    "* accommodates\n",
    "* bedrooms\n",
    "* bathrooms\n",
    "* number_of_reviews\n",
    "\n",
    "Scikit-learn makes it incredibly easy to swap the columns used during training and testing. We're going to leave this for you as a challenge to train and test a k-nearest neighbors model using these columns instead. Use the code you wrote in the last screen as a guide.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Create a new instance of the KNeighborsRegressor class with the following parameters:\n",
    "\n",
    "    * n_neighbors: 5\n",
    "    * algorithm: brute\n",
    "\n",
    "* Fit a model that uses the following columns from our training set (train_df):\n",
    "\n",
    "    * accommodates\n",
    "    * bedrooms\n",
    "    * bathrooms\n",
    "    * number_of_reviews\n",
    "\n",
    "* Use the model to make predictions on the test set (test_df) using the same columns. Assign the NumPy array of predictions to four_predictions.\n",
    "\n",
    "* Use the mean_squared_error() function to calculate the MSE value for these predictions by comparing four_predictions with the price column from test_df. Assign the computed MSE value to four_mse.\n",
    "* Calculate the RMSE value and assign to four_rmse.\n",
    "* Display four_mse and four_rmse using the print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b62421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['accommodates', 'bedrooms', 'bathrooms', 'number_of_reviews']\n",
    "knn = KNeighborsRegressor(n_neighbors=5, algorithm='brute')\n",
    "\n",
    "knn.fit(train_df[features], train_df['price'])\n",
    "four_predictions = knn.predict(test_df[features])\n",
    "four_mse = mean_squared_error(test_df['price'], four_predictions)\n",
    "four_rmse = np.sqrt(four_mse)\n",
    "print(four_mse)\n",
    "print(four_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5997d181",
   "metadata": {},
   "source": [
    "### 6.1.3.10 Using all features\n",
    "\n",
    "So far so good! As we increased the features the model used, we observed lower MSE and RMSE values:\n",
    "\n",
    "<table>\n",
    "<tbody><tr>\n",
    "<th>feature(s)</th>\n",
    "<th>MSE</th>\n",
    "<th>RMSE</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>accommodates</td>\n",
    "<td>18646.5</td>\n",
    "<td>136.6</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>bathrooms</td>\n",
    "<td>17333.4</td>\n",
    "<td>131.7</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>accommodates, bathrooms</td>\n",
    "<td>15660.4</td>\n",
    "<td>125.1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>accommodates, bathrooms, bedrooms, number_of_reviews</td>\n",
    "<td>13320.2</td>\n",
    "<td>115.4</td>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "\n",
    "Let's take this to the extreme and use all of the potential features. We should expect the error scores to decrease since so far adding more features has helped do so.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Use all of the columns, except for the price column, to train a k-nearest neighbors model using the same parameters for the KNeighborsRegressor class as the ones from the last few screens.\n",
    "* Use the model to make predictions on the test set and assign the resulting NumPy array of predictions to all_features_predictions.\n",
    "* Calculate the MSE and RMSE values and assign to all_features_mse and all_features_rmse accordingly.\n",
    "* Use the print function to display both error scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a735a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "features = ['accommodates', 'bedrooms', 'bathrooms', 'number_of_reviews']\n",
    "hyper_params = range(1, 6)\n",
    "mse_values = []\n",
    "for k in hyper_params:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, algorithm='brute')\n",
    "    knn.fit(train_df[features], train_df['price'])\n",
    "    predictions = knn.predict(test_df[features])\n",
    "    mse = mean_squared_error(predictions, test_df['price'])\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "print(mse_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9a815",
   "metadata": {},
   "source": [
    "### 6.1.4.3 Expanding grid search\n",
    "\n",
    "Since our dataset is small and scikit-learn has been developed with performance in mind, the code ran quickly. As we increased the k value from 1 to 5, the MSE value fell from approximately 26364 to approximately 14090:\n",
    "\n",
    "<table>\n",
    "<tbody><tr>\n",
    "<th>k</th>\n",
    "<th>MSE</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>26364.928327645051</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>15100.522468714449</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td>\n",
    "<td>14579.597901655923</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td>\n",
    "<td>16212.300767918088</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>5</td>\n",
    "<td>14090.011649601822</td>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "\n",
    "Let's expand grid search all the way to a k value of 20. While __20__ may seem like an arbitrary ending point for our grid search, we can always expand the values we try if we're unconvinced that the lowest MSE value is associated with one of the hyperparamter values we tried so far.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Change the list of hyperparameter values, hyper_params, so it ranges from 1 to 20.\n",
    "* Create an empty list and assign to mse_values.\n",
    "* Use a for loop to iterate over hyper_params and in each iteration:\n",
    "    * Instantiate a KNeighborsRegressor object with the following parameters:\n",
    "        * n_neighbors: the current value for the iterator variable,\n",
    "        * algorithm: brute\n",
    "    * Fit the instantiated k-nearest neighbors model to the following columns from train_df:\n",
    "        * accommodates\n",
    "        * bedrooms\n",
    "        * bathrooms\n",
    "        * number_of_reviews\n",
    "    * Use the trained model to make predictions on the same columns from test_df and assign to predictions.\n",
    "    * Use the mean_squared_error function to calculate the MSE value between predictions and the price column from test_df.\n",
    "    * Append the MSE value to mse_values.\n",
    "* Display mse_values using the print() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db300dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['accommodates', 'bedrooms', 'bathrooms', 'number_of_reviews']\n",
    "hyper_params = range(1, 21)\n",
    "mse_values = []\n",
    "for k in hyper_params:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, algorithm='brute')\n",
    "    knn.fit(train_df[features], train_df['price'])\n",
    "    predictions = knn.predict(test_df[features])\n",
    "    mse = mean_squared_error(predictions, test_df['price'])\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "print(mse_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d3d27",
   "metadata": {},
   "source": [
    "### 6.1.4.4 Visualizing hyperparameter values\n",
    "\n",
    "As we increased the k value from 1 to 6, the MSE value decreased from approximately 26364 to approximately 13657. However, as we increased the k value from 7 to 20, the MSE value didn't decrease further but instead hovered between approximately 14288 and 14870. This means that the optimal k value is 6, since it resulted in the lowest MSE value.\n",
    "\n",
    "This pattern is something you'll notice while performing grid search across other models as well. As you increase k at first, the error rate decreases until a certain point, but then rebounds and increases again. Let's confirm this behavior visually using a scatter plot.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Use the scatter() method from matplotlib.pyplot to generate a line plot with:\n",
    "\n",
    "    * hyper_params on the x-axis,\n",
    "    * mse_values on the y-axis.\n",
    "    \n",
    "* Use plt.show() to display the line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(hyper_params, mse_values)\n",
    "plt.scatter(hyper_params, mse_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4efe07",
   "metadata": {},
   "source": [
    "### 6.1.4.5 Varying features and hyperparameters\n",
    "\n",
    "From the scatter plot, you can tell that the lowest MSE value was achieved at the k value of __6__. As we increased k past 6, the MSE actually increased and hovered but never decreased below 13657 (the approximate MSE value when k was 6).\n",
    "\n",
    "Since varying the k value decreased the MSE value for this model, you may be wondering if repeating the grid search process for one of the models from the last mission that performed poorly when we fixed k to 5 would result in a lower MSE value. Let's try it out!\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Use a for loop to iterate over hyper_params and in each iteration:\n",
    "    * Instantiate a KNeighborsRegressor object with the following parameters:\n",
    "        * n_neighbors: the current value for the iterator variable,\n",
    "        * algorithm: brute\n",
    "    * Fit the instantiated k-nearest neighbors model to all of the columns, except for the price column, from train_df\n",
    "    * Use the trained model to make predictions on the same columns from test_df and assign to predictions.\n",
    "    * Use the mean_squared_error function to calculate the MSE value between predictions and the price column from test_df.\n",
    "    * Append the MSE value to mse_values.\n",
    "* Use the scatter() method from matplotlib.pyplot to generate a line plot with:\n",
    "    * hyper_params on the x-axis,\n",
    "    * mse_values on the y-axis.\n",
    "* Use plt.show() to display the line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211b7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## features = train_df.columns.tolist()\n",
    "## features.remove('price')\n",
    "features = [col for col in train_df.columns if col != 'price']\n",
    "hyper_params = range(1, 21)\n",
    "mse_values = list()\n",
    "\n",
    "for hp in hyper_params:\n",
    "    knn = KNeighborsRegressor(n_neighbors=hp, algorithm='brute')\n",
    "    knn.fit(train_df[features], train_df['price'])\n",
    "    predictions = knn.predict(test_df[features])\n",
    "    mse = mean_squared_error(predictions, test_df['price'])\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "plt.scatter(hyper_params, mse_values)\n",
    "plt.plot(hyper_params, mse_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402da076",
   "metadata": {},
   "source": [
    "### 6.1.4.6 Practice the workflow\n",
    "\n",
    "You may have noticed that the general workflow for finding the best model is:\n",
    "\n",
    "* select relevant features to use for predicting the target column.\n",
    "* use grid search to find the optimal hyperparameter value for the selected features.\n",
    "* evaluate the model's accuracy and repeat the process.\n",
    "\n",
    "Let's now practice this workflow.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* While using only the accommodates and bathrooms columns:\n",
    "\n",
    "    * Train a model for each k value between 1 and 20 using the training data.\n",
    "    * Use each model to make predictions on the test set (using just the accommodates and bathrooms columns).\n",
    "    * Calculate each model's MSE value by comparing each set of predictions to the true price values.\n",
    "    * Find the k value that obtained the lowest MSE value.\n",
    "    * Create a dictionary named two_hyp_mse that contains 1 key-value pair:\n",
    "        * key: k value that resulted in lowest MSE value.\n",
    "        * value: corresponding MSE value.\n",
    "* Repeat this process while using only the accommodates, bathrooms, and bedrooms columns:\n",
    "\n",
    "    * Create a dictionary named three_hyp_mse that contains 1 key-value pair:\n",
    "        * key: k value that resulted in lowest MSE value.\n",
    "        * value: corresponding MSE value.\n",
    "* Display both two_hyp_mse and three_hyp_mse using the print() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e809b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_features = ['accommodates', 'bathrooms']\n",
    "three_features = ['accommodates', 'bathrooms', 'bedrooms']\n",
    "hyper_params = [x for x in range(1,21)]\n",
    "# Append the first model's MSE values to this list.\n",
    "two_mse_values = list()\n",
    "# Append the second model's MSE values to this list.\n",
    "three_mse_values = list()\n",
    "two_hyp_mse = dict()\n",
    "three_hyp_mse = dict()\n",
    "for hp in hyper_params:\n",
    "    knn = KNeighborsRegressor(n_neighbors=hp, algorithm='brute')\n",
    "    knn.fit(train_df[two_features], train_df['price'])\n",
    "    predictions = knn.predict(test_df[two_features])\n",
    "    mse = mean_squared_error(test_df['price'], predictions)\n",
    "    two_mse_values.append(mse)\n",
    "\n",
    "two_lowest_mse = two_mse_values[0]\n",
    "two_lowest_k = 1\n",
    "\n",
    "## two_hyp_mse[np.argmin(two_mse_values)+1] = min(two_mse_values)\n",
    "for k,mse in enumerate(two_mse_values):\n",
    "    if mse < two_lowest_mse:\n",
    "        two_lowest_mse = mse\n",
    "        two_lowest_k = k + 1\n",
    "    \n",
    "for hp in hyper_params:\n",
    "    knn = KNeighborsRegressor(n_neighbors=hp, algorithm='brute')\n",
    "    knn.fit(train_df[three_features], train_df['price'])\n",
    "    predictions = knn.predict(test_df[three_features])\n",
    "    mse = mean_squared_error(test_df['price'], predictions)\n",
    "    three_mse_values.append(mse)\n",
    "    \n",
    "three_lowest_mse = three_mse_values[0]\n",
    "three_lowest_k = 1\n",
    "\n",
    "## three_hyp_mse[np.argmin(three_mse_values)+1] = min(three_mse_values)\n",
    "for k,mse in enumerate(three_mse_values):\n",
    "    if mse < three_lowest_mse:\n",
    "        three_lowest_mse = mse\n",
    "        three_lowest_k = k + 1\n",
    "\n",
    "two_hyp_mse[two_lowest_k] = two_lowest_mse\n",
    "three_hyp_mse[three_lowest_k] = three_lowest_mse\n",
    "\n",
    "print(two_hyp_mse)\n",
    "print(three_hyp_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92028758",
   "metadata": {},
   "source": [
    "### 6.1.4.7 Next Steps\n",
    "\n",
    "The first model, which used the accommodates and bathrooms columns, was able to achieve an MSE value of approximately 14790. The second model, which added the bedrooms column, was able to achieve an MSE value of approximately 13522.9, which is even lower than the lowest MSE value we achieved using the best model from the last mission (which used the accommodates, bedrooms, bathrooms, and number_of_reviews columns). Hopefully this demonstrates that using just one lever to find the best model isn't enough and you really want to use both levers in conjunction.\n",
    "\n",
    "In this mission, we learned about hyperparameter optimization and the workflow of finding the optimal model to make predictions. Next in this course is a challenge, where you'll practice the concepts you've learned so far on a completely new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c464a5e",
   "metadata": {},
   "source": [
    "## 6.1.5 Cross Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af622fe",
   "metadata": {},
   "source": [
    "### 6.1.5.1 Introduction\n",
    "\n",
    "In an earlier mission, we learned about train/test validation, a simple technique for testing a machine learning model's accuracy on new data that the model wasn't trained on. In this mission, we'll focus on __more robust techniques__.\n",
    "\n",
    "To start, we'll focus on the __holdout validation__ technique, which involves:\n",
    "\n",
    "* splitting the full dataset into 2 partitions:\n",
    "* a training set\n",
    "* a test set\n",
    "* training the model on the training set,\n",
    "* using the trained model to predict labels on the test set,\n",
    "* computing an error metric to understand the model's effectiveness,\n",
    "* switch the training and test sets and repeat,\n",
    "* average the errors.\n",
    "\n",
    "In holdout validation, we usually use a __50/50 split instead of the 75/25 split__ from train/test validation. This way, we remove number of observations as a potential source of variation in our model performance.\n",
    "\n",
    "![img alt](https://s3.amazonaws.com/dq-content/holdout_validation.png)\n",
    "\n",
    "\n",
    "Let's start by splitting the data set into 2 nearly equivalent halves.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Use the numpy.random.permutation() function to shuffle the ordering of the rows in dc_listings.\n",
    "* Select the first 1862 rows and assign to split_one.\n",
    "* Select the remaining 1861 rows and assign to split_two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_listing = 3\n",
    "dc_listings['distance'] = dc_listings['accommodates'].apply(lambda x: np.abs(x - new_listing))\n",
    "# dc_listings['distance'] = np.abs(dc_listings['accommodates'] - 3)\n",
    "print(dc_listings['distance'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078dc7d",
   "metadata": {},
   "source": [
    "### 6.1.1.6 Randomizing, and sorting\n",
    "\n",
    "It looks like there are quite a few, __461__ to be precise, living spaces that can accommodate 3 people just like ours. This means the 5 \"nearest neighbors\" we select after sorting all will have a distance value of 0. If we sort by the distance column and then just select the first 5 living spaces, we would be __biasing__ the result to the ordering of the dataset.\n",
    "\n",
    "```python\n",
    ">> dc_listings[dc_listings[\"distance\"] == 0][\"accommodates\"]\n",
    "26      3\n",
    "34      3\n",
    "36      3\n",
    "40      3\n",
    "44      3\n",
    "45      3\n",
    "48      3\n",
    "65      3\n",
    "66      3\n",
    "71      3\n",
    "75      3\n",
    "86      3\n",
    "...\n",
    "```\n",
    "\n",
    "Let's instead __randomize the ordering__ of the dataset and then sort the Dataframe by the distance column. This way, all of the living spaces with the same number of bedrooms will still be at the top of the Dataframe but will be in random order across the first 461 rows. We've already done the first step of setting the random seed, so we can perform answer checking on our end.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Randomize the order of the rows in dc_listings:\n",
    "* Use the np.random.permutation() function to return a NumPy array of shuffled index values.\n",
    "* Use the Dataframe method loc[] to return a new Dataframe containing the shuffled order.\n",
    "* Assign the new Dataframe back to dc_listings.\n",
    "* After randomization, sort dc_listings by the distance column.\n",
    "* Display the first 10 values in the price column using the print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83171544",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "dc_listings = dc_listings.loc[np.random.permutation(len(dc_listings))]\n",
    "dc_listings.sort_values(by='distance', inplace=True)\n",
    "print(dc_listings.iloc[0:10]['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1220b",
   "metadata": {},
   "source": [
    "### 6.1.1.7 Averge price\n",
    "\n",
    "Before we can select the 5 most similar living spaces and compute the average price, we need to clean the price column. Right now, the price column contains comma characters (,) and dollar sign characters and is formatted as a text column instead of a numeric one. We need to remove these values and convert the entire column to the float datatype. Then, we can calculate the average price.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Remove the commas (,) and dollar sign characters (\\$) from the price column:\n",
    "    * Use the __[str accessor](http://pandas.pydata.org/pandas-docs/version/0.22/api.html#string-handling)__ so we can apply string methods to each value in the column followed by the string method replace to replace all comma characters with the empty character:  stripped_commas = dc_listings['price'].str.replace(',', '')\n",
    "    * Repeat to remove the dollar sign characters as well.\n",
    "* Convert the new Series object containing the cleaned values to the float datatype and assign back to the price column in dc_listings.\n",
    "* Calculate the mean of the first 5 values in the price column and assign to mean_price.\n",
    "* Use the print function or the variable inspector below to display mean_price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80059db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dc_listings.iloc[0:2792]\n",
    "test_df = dc_listings.iloc[2792:]\n",
    "\n",
    "def predict_price(new_listing):\n",
    "    temp_df = train_df.copy()\n",
    "    temp_df['distance'] = temp_df['bathrooms'].apply(lambda x: np.abs(x - new_listing))\n",
    "    temp_df = temp_df.sort_values('distance')\n",
    "    nearest_neighbors_prices = temp_df.iloc[0:5]['price']\n",
    "    predicted_price = nearest_neighbors_prices.mean()\n",
    "    return predicted_price\n",
    "\n",
    "test_df['predicted_price'] = test_df['bathrooms'].apply(predict_price)\n",
    "test_df['squared_error'] = np.power(test_df['price'] - test_df['predicted_price'], 2)\n",
    "mse = test_df['squared_error'].mean()\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5697886f",
   "metadata": {},
   "source": [
    "### 6.1.2.5 Root Mean Squared Error\n",
    "\n",
    "While comparing MSE values helps us identify which model performs better on a relative basis, it doesn't help us understand if the performance is good enough in general. This is because the units of the MSE metric are squared (in this case, dollars squared). An MSE value of 16377.5 dollars squared doesn't give us an intuitive sense of how far off the model's predictions are systematically off from the true price value in dollars.\n",
    "\n",
    "__Root mean squared error__ is an error metric whose units are the base unit (in our case, dollars). RMSE for short, this error metric is calculated by taking the square root of the MSE value:\n",
    "\n",
    "$RMSE = \\sqrt{MSE}$\n",
    "\n",
    "Since the RMSE value uses the __same units__ as the target column, we can understand how far off in real dollars we can expect the model to perform.\n",
    "\n",
    "Let's calculate the RMSE value of the model we trained using the __bathrooms__ column.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Calculate the RMSE value of the model we trained using the bathrooms column and assign it to rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8af058",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233dbc68",
   "metadata": {},
   "source": [
    "### 6.1.2.6 Comparing MAE and RMSE\n",
    "\n",
    "The model achieved an RMSE value of approximately 135.6, which implies that we should expect for the model to be off by 135.6 dollars on average for the predicted price values. Given that most of the living spaces are listed at just a few hundred dollars, we need to reduce this error as much as possible to improve the model's usefulness.\n",
    "\n",
    "We discussed a few different error metrics we can use to understand a model's performance. As we mentioned earlier, these individual error metrics are helpeful for comparing models. To better understand a specific model, we can compare multiple error metrics for the same model. This requires a better understanding of the mathematical properties of the error metrics.\n",
    "\n",
    "If you look at the equation for MAE:\n",
    "\n",
    "$MAE = \\frac{1}{n} \\sum_{k=1}^{n} \\lvert (actual_1 - predicted_1) \\rvert + \\cdots + \\lvert (actual_n - predicted_n) \\rvert$\n",
    "\n",
    "you'll notice that that the differences between predicted and actual values grow linearly. A prediction that's off by 10 dollars has a 10 times higher error than a prediction that's off by 1 dollar. If you look at the equation for RMSE, however:\n",
    "\n",
    "$ RMSE = \\sqrt { \\frac{ \\sum_{k=1}^{n} \\lvert (actual_1 - predicted_1) \\rvert + \\cdots + \\lvert (actual_n - predicted_n) \\rvert } {n} } $\n",
    "\n",
    "you'll notice that each error is squared before the square root of the sum of all the errors is taken. This means that the individual errors grows quadratically and has a different effect on the final RMSE value.\n",
    "\n",
    "Let's look at an example using different data entirely. We've created 2 Series objects containing 2 sets of errors and assigned to errors_one and errors_two.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Calculate the MAE for errors_one and assign to mae_one.\n",
    "* Calculate the RMSE for errors_one and assign to rmse_one.\n",
    "* Calculate the MAE for errors_two and assign to mae_two.\n",
    "* Calculate the RMSE for errors_two and assign to rmse_two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3078d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_one = pd.Series([5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10])\n",
    "errors_two = pd.Series([5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 1000])\n",
    "\n",
    "mae_one = np.abs(errors_one).mean()\n",
    "rmse_one = np.sqrt(np.power(errors_one, 2).mean())\n",
    "\n",
    "mae_two = np.abs(errors_two).mean()\n",
    "rmse_two = np.sqrt(np.power(errors_two, 2).mean())\n",
    "\n",
    "mae = [mae_one, mae_two]\n",
    "rmse = [rmse_one, rmse_two]\n",
    "\n",
    "pd.DataFrame(data={\"mae\":mae, \"rmse\":rmse}, index=[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc6074",
   "metadata": {},
   "source": [
    "### 6.1.2.7 Next Steps\n",
    "\n",
    "While the MAE (7.5) to RMSE (7.9056941504209481) ratio was about 1:1 for the first list of errors, the MAE (62.5) to RMSE (235.82302686548658) ratio was closer to 1:4 for the second list of errors. In general, we should expect that __the MAE value be much less than the RMSE value__. The only difference between the 2 sets of errors is the extreme 1000 value in errors_two instead of 10. When we're working with larger data sets, we can't inspect each value to understand if there's one or some outliers or if all of the errors are systematically higher. Looking at the __ratio of MAE to RMSE__ can help us understand if there are __large but infrequent errors__. You can read more about comparing MAE and RMSE in __[this wonderful post](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d#.lyc8od1ix)__.\n",
    "\n",
    "In this mission, we learned how to test our machine learning models using basic cross validation and different metrics. In the next 2 missions, we'll explore how __adding more features__ to the machine learning model and selecting a more __optimal k value__ can help improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f07cb",
   "metadata": {},
   "source": [
    "## 6.1.3 Multivariate K-Nearest Neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d932b4",
   "metadata": {},
   "source": [
    "### 6.1.3.1 Recap\n",
    "\n",
    "In the last mission, we explored how to use a simple k-nearest neighbors machine learning model that used __just one feature__, or attribute, of the listing to predict the rent price. We first relied on the __accommodates column__, which describes the number of people a living space can comfortably accommodate. Then, we switched to the __bathrooms column__ and observed an improvement in accuracy. While these were good features to become familiar with the basics of machine learning, it's clear that using just a single feature to compare listings doesn't reflect the reality of the market. An apartment that can accommodate 4 guests in a popular part of Washington D.C. will rent for much higher than one that can accommodate 4 guests in a crime ridden area.\n",
    "\n",
    "There are 2 ways we can tweak the model to try to improve the accuracy (decrease the RMSE during validation):\n",
    "\n",
    "* increase the number of attributes the model uses to calculate similarity when ranking the closest neighbors\n",
    "* increase k, the number of nearby neighbors the model uses when computing the prediction\n",
    "\n",
    "In this mission, we'll focus on increasing the number of attributes the model uses. When selecting more attributes to use in the model, we need to watch out for columns that don't work well with the distance equation. This includes columns containing:\n",
    "\n",
    "* non-numerical values (e.g. city or state)\n",
    "    * Euclidean distance equation expects numerical values\n",
    "* missing values\n",
    "    * distance equation expects a value for each observation and attribute\n",
    "* non-ordinal values (e.g. latitude or longitude)\n",
    "    * ranking by Euclidean distance doesn't make sense if all attributes aren't ordinal\n",
    "\n",
    "In the following code screen, we've read the __dc_airbnb.csv__ dataset from the last mission into pandas and brought over the data cleaning changes we made. Let's first look at the first row's values to identify any columns containing non-numerical or non-ordinal values. In the next screen, we'll drop those columns and then look for missing values in each of the remaining columns.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Use the __[DataFrame.info]()(http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)__ method to return the number of non-null values in each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "dc_listings = pd.read_csv('Data/dc_airbnb.csv')\n",
    "dc_listings = dc_listings.loc[np.random.permutation(len(dc_listings))]\n",
    "stripped_commas = dc_listings['price'].str.replace(',', '')\n",
    "stripped_dollars = stripped_commas.str.replace('$', '')\n",
    "dc_listings['price'] = stripped_dollars.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bffa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_listings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_listings.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8803e81",
   "metadata": {},
   "source": [
    "### 6.1.3.2 Removing features\n",
    "\n",
    "The following columns contain __non-numerical__ values:\n",
    "\n",
    "* room_type: e.g. Private room\n",
    "* city: e.g. Washington\n",
    "* state: e.g. DC\n",
    "\n",
    "while these columns contain numerical but __non-ordinal__ values:\n",
    "\n",
    "* latitude: e.g. 38.913458\n",
    "* longitude: e.g. -77.031\n",
    "* zipcode: e.g. 20009\n",
    "\n",
    "__Geographic values__ like these aren't ordinal, because a smaller numerical value doesn't directly correspond to a smaller value in a meaningful way. For example, the zip code 20009 isn't smaller or larger than the zip code 75023 and instead both are unique, identifier values. Latitude and longitude value pairs describe a point on a geographic coordinate system and different equations are used in those cases (e.g. __[haversine](https://en.wikipedia.org/wiki/Haversine_formula)__).\n",
    "\n",
    "While we could convert the host_response_rate and host_acceptance_rate columns to be numerical (right now they're object data types and contain the % sign), these columns describe the host and not the living space itself. Since a host could have many living spaces and we don't have enough information to uniquely group living spaces to the hosts themselves, let's avoid using any columns that don't directly describe the living space or the listing itself:\n",
    "\n",
    "* host_response_rate\n",
    "* host_acceptance_rate\n",
    "* host_listings_count\n",
    "\n",
    "Let's remove these 9 columns from the Dataframe.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Remove the 9 columns we discussed above from dc_listings:\n",
    "* 3 containing non-numerical values\n",
    "* 3 containing numerical but non-ordinal values\n",
    "* 3 describing the host instead of the living space itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['room_type', 'city', 'state', 'latitude', 'longitude', 'zipcode', 'host_response_rate', 'host_acceptance_rate', 'host_listings_count']\n",
    "dc_listings = dc_listings.drop(drop_columns, axis=1)\n",
    "dc_listings.isnull().sum() ### Neat!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89fe23",
   "metadata": {},
   "source": [
    "### 6.1.3.3 Handling missing values ( different scenarios for dropping columns or rows)\n",
    "\n",
    "Of the remaining columns, 3 columns have a few missing values (less than 1% of the total number of rows):\n",
    "\n",
    "* bedrooms\n",
    "* bathrooms\n",
    "* beds\n",
    "\n",
    "Since the number of rows containing missing values for one of these 3 columns is low, we can select and __remove those rows__ without losing much information. There are also 2 columns that have a large number of missing values:\n",
    "\n",
    "* cleaning_fee - 37.3% of the rows\n",
    "* security_deposit - 61.7% of the rows\n",
    "\n",
    "and we can't handle these easily. We can't just remove the rows containing missing values for these 2 columns because we'd miss out on the majority of the observations in the dataset. Instead, let's __remove these 2 columns entirely__ from consideration.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Drop the cleaning_fee and security_deposit columns from dc_listings.\n",
    "* Then, remove all rows that contain a missing value for the bedrooms, bathrooms, or beds column from dc_listings.\n",
    "* You can accomplish this by using the __[Dataframe method dropna]()(http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html)__ and setting the axis parameter to 0.\n",
    "* Since only the bedrooms, bathrooms, and beds columns contain any missing values, rows containing missing values in these columns will be removed.\n",
    "* Display the null value counts for the updated dc_listings Dataframe to confirm that there are no missing values left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['cleaning_fee', 'security_deposit']\n",
    "dc_listings.drop(drop_columns, inplace=True, axis=1)\n",
    "dc_listings.dropna(axis=0, inplace=True)\n",
    "dc_listings.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0d57fb",
   "metadata": {},
   "source": [
    "### 6.1.3.4 Normalizing columns\n",
    "\n",
    "Here's how the dc_listings Dataframe looks like after all the changes we made:\n",
    "\n",
    "<table>\n",
    "<tbody><tr>\n",
    "<th>accommodates</th>\n",
    "<th>bedrooms</th>\n",
    "<th>bathrooms</th>\n",
    "<th>beds</th>\n",
    "<th>price</th>\n",
    "<th>minimum_nights</th>\n",
    "<th>maximum_nights</th>\n",
    "<th>number_of_reviews</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>1.0</td>\n",
    "<td>1.0</td>\n",
    "<td>1.0</td>\n",
    "<td>125.0</td>\n",
    "<td>1</td>\n",
    "<td>4</td>\n",
    "<td>149</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>1.0</td>\n",
    "<td>1.5</td>\n",
    "<td>1.0</td>\n",
    "<td>85.0</td>\n",
    "<td>1</td>\n",
    "<td>30</td>\n",
    "<td>49</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>1.0</td>\n",
    "<td>0.5</td>\n",
    "<td>1.0</td>\n",
    "<td>50.0</td>\n",
    "<td>1</td>\n",
    "<td>1125</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>1.0</td>\n",
    "<td>1.0</td>\n",
    "<td>1.0</td>\n",
    "<td>209.0</td>\n",
    "<td>4</td>\n",
    "<td>730</td>\n",
    "<td>2</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>12</td>\n",
    "<td>5.0</td>\n",
    "<td>2.0</td>\n",
    "<td>5.0</td>\n",
    "<td>215.0</td>\n",
    "<td>2</td>\n",
    "<td>1825</td>\n",
    "<td>34</td>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "\n",
    "You may have noticed that while the accommodates, bedrooms, bathrooms, beds, and minimum_nights columns hover between 0 and 12 (at least in the first few rows), the values in the maximum_nights and number_of_reviews columns span much larger ranges. For example, the maximum_nights column has values as low as 4 and high as 1825, in the first few rows itself. If we use these 2 columns as part of a k-nearest neighbors model, these attributes could end up having an outsized effect on the distance calculations because of the largeness of the values.\n",
    "\n",
    "For example, 2 living spaces could be identical across every attribute but be vastly different just on the maximum_nights column. If one listing had a maximum_nights value of 1825 and the other a maximum_nights value of 4, because of the way Euclidean distance is calculated, these listings would be considered very far apart because of the outsized effect the largeness of the values had on the overall Euclidean distance. To prevent any single column from having too much of an impact on the distance, we can __normalize all of the columns to have a mean of 0 and a standard deviation of 1__.\n",
    "\n",
    "Normalizing the values in each column to the __[standard normal distribution](https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution)__ (mean of 0, standard deviation of 1) preserves the distribution of the values in each column while aligning the scales. To normalize the values in a column to the standard normal distribution, you need to:\n",
    "\n",
    "* from each value, subtract the mean of the column\n",
    "* divide each value by the standard deviation of the column\n",
    "\n",
    "Here's the mathematical formula describing the transformation that needs to be applied for all values in a column:\n",
    "\n",
    "$x=\\frac{x-\\mu}{\\sigma}$\n",
    "\n",
    "where $x$ is a value in a specific column, $\\mu$ is the mean of all the values in the column, and $\\sigma$ is the standard deviation of all the values in the column. Here's what the corresponding code, using pandas, looks like:\n",
    "\n",
    "```python\n",
    "# Subtract each value in the column by the mean.\n",
    "first_transform = dc_listings['maximum_nights'] - dc_listings['maximum_nights'].mean()\n",
    "# Divide each value in the column by the standard deviation.\n",
    "normalized_col = first_transform / first_transform.std()\n",
    "```\n",
    "\n",
    "It should be noted that you can also do the following:\n",
    "\n",
    "```python\n",
    "normalized_col = first_transform / dc_listings['maximum_nights'].std()\n",
    "```\n",
    "\n",
    "and get the same answer as above.\n",
    "\n",
    "This is because first_transform is merely shifting the mean of the distribution and has no effect on the shape or scaling of the distribution In other words, the variance of dc_listings is the same as the variance of first_transform.\n",
    "\n",
    "To apply this transformation across all of the columns in a Dataframe, you can use the corresponding Dataframe methods mean() and std():\n",
    "\n",
    "```python\n",
    "normalized_listings = (dc_listings - dc_listings.mean()) / (dc_listings.std())\n",
    "```\n",
    "\n",
    "These methods were written with mass column transformation in mind and when you call mean() or std(), the appropriate column means and column standard deviations are used for each value in the Dataframe. Let's now normalize all of the feature columns in dc_listings.\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "* Normalize all of the feature columns in dc_listings and assign the new Dataframe containing just the normalized feature columns to normalized_listings.\n",
    "* Add the price column from dc_listings to normalized_listings.\n",
    "* Display the first 3 rows in normalized_listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r\"[\\$,]\" ## regular expression\n",
    "dc_listings['price'] = dc_listings['price'].str.replace(pat, '').astype('float')\n",
    "mean_price = dc_listings['price'][:5].mean()\n",
    "print(mean_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e29e14",
   "metadata": {},
   "source": [
    "### 6.1.1.8 Function to make predictions\n",
    "\n",
    "Congrats! You've just made your first prediction! Based on the average price of other listings that accommdate 3 people, we should charge 156.6 dollars per night for a guest to stay at our living space. In the next mission, we'll dive into evaluating how good of a prediction this is.\n",
    "\n",
    "Let's write a more general function that can suggest the optimal price for other values of the accommodates column. The dc_listings Dataframe has information specific to our living space, e.g. the distance column. To save you time, we've reset the dc_listings Dataframe to a clean slate and only kept the data cleaning and randomization we did since those weren't unique to the prediction we were making for our living space.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Write a function named __predict_price__ that can use the k-nearest neighbors machine learning technique to calculate the suggested price for any value for accommodates. This function should:\n",
    "\n",
    "    * Take in a single parameter, new_listing, that describes the number of bedrooms.\n",
    "    * We've added code that assigns dc_listings to a new Dataframe named temp_df. We used the __[pandas.DataFrame.copy() method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.copy.html)__ so the underlying dataframe is assigned to temp_df, instead of just a reference to dc_listings.\n",
    "    * Calculate the distance between each value in the accommodates column and the new_listing value that was passed in. Assign the resulting Series object to the distance column in temp_df.\n",
    "    * Sort temp_df by the distance column and select the first 5 values in the price column. Don't randomize the ordering of temp_df.\n",
    "    * Calculate the mean of these 5 values and use that as the return value for the entire predict_price function.\n",
    "* Use the predict_price function to suggest a price for a living space that:\n",
    "\n",
    "    * accommodates 1 person, assign the suggested price to acc_one.\n",
    "    * accommodates 2 people, assign the suggested price to acc_two.\n",
    "    * accommodates 4 people, assign the suggested price to acc_four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brought along the changes we made to the `dc_listings` Dataframe.\n",
    "dc_listings = pd.read_csv('Data/dc_airbnb.csv')\n",
    "stripped_commas = dc_listings['price'].str.replace(',', '')\n",
    "stripped_dollars = stripped_commas.str.replace('$', '')\n",
    "dc_listings['price'] = stripped_dollars.astype('float')\n",
    "np.random.seed(1)\n",
    "dc_listings = dc_listings.loc[np.random.permutation(len(dc_listings))]\n",
    "\n",
    "def predict_price(new_listing):\n",
    "    temp_df = dc_listings.copy()\n",
    "    ## Complete the function.\n",
    "    temp_df['distance'] = temp_df['accommodates'].apply(lambda x: np.abs(x-new_listing))\n",
    "    temp_df.sort_values(by='distance', inplace=True)\n",
    "    predicted_price = temp_df['price'][:5].mean()\n",
    "    return predicted_price\n",
    "\n",
    "acc_one = predict_price(1)\n",
    "acc_two = predict_price(2)\n",
    "acc_four = predict_price(4)\n",
    "\n",
    "prediction_list = [(x, predict_price(x)) for x in range(20)]\n",
    "print(prediction_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424a886",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In this mission, we explored the problem of predicting the optimal price to list an AirBnB rental for based on the price of similar listings on the site. We stepped through the entire machine learning workflow, from selecting a feature to testing the model. To explore the basics of machine learning, we limited ourselves to only using one feature (the univariate case) and a fixed __k value of 5__.\n",
    "\n",
    "In the next mission, we'll learn how to evaluate a model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04568f75",
   "metadata": {},
   "source": [
    "## 6.1.2 Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50652b0c",
   "metadata": {},
   "source": [
    "### 6.1.2.1 Testing quality of predictions\n",
    "\n",
    "We now have a function that can predict the price for any living space we want to list as long as we know the number of people it can accommodate. The function we wrote represents a __machine learning model__, which means that it outputs a prediction based on the input to the model.\n",
    "\n",
    "A simple way to test the quality of your model is to:\n",
    "\n",
    "* split the dataset into 2 partitions:\n",
    "\n",
    "    * the training set: contains the majority of the rows (75%)\n",
    "    * the test set: contains the remaining minority of the rows (25%)\n",
    "    \n",
    "* use the rows in the training set to predict the price value for the rows in the test set\n",
    "\n",
    "    * add new column named predicted_price to the test set\n",
    "    \n",
    "* compare the predicted_price values with the actual price values in the test set to see how accurate the predicted values were.\n",
    "\n",
    "This __validation process__, where we use the training set to make predictions and the test set to predict values for, is known as __train/test validation__. Whenever you're performing machine learning, you want to perform validation of some kind to ensure that your machine learning model can make good predictions on new data. While train/test validation isn't perfect, we'll use it to understand the validation process, to select an error metric, and then we'll dive into a more robust validation process later in this course.\n",
    "\n",
    "Let's modify the predict_price function to use only the rows in the training set, instead of the full dataset, to find the nearest neighbors, average the price values for those rows, and return the predicted price value. Then, we'll use this function to predict the price for just the rows in the test set. Once we have the predicted price values, we can compare with the true price values and start to understand the model's effectiveness in the next screen.\n",
    "\n",
    "To start, we've gone ahead and assigned the first 75% of the rows in dc_listings to train_df and the last 25% of the rows to test_df. Here's a diagram explaining the split:\n",
    "\n",
    "![img alt](https://s3.amazonaws.com/dq-content/train_test_split.png)\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Within the predict_price function, change the Dataframe that temp_df is assigned to. Change it from dc_listings to train_df, so only the training set is used.\n",
    "* Use the Series method apply to pass all of the values in the accommodates column from test_df through the predict_price function.\n",
    "* Assign the resulting Series object to the predicted_price column in test_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67728de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dc_listings = pd.read_csv(\"Data/dc_airbnb.csv\")\n",
    "stripped_commas = dc_listings['price'].str.replace(',', '') ## Could use regex as well. pat = r\"[\\$,]\"\n",
    "stripped_dollars = stripped_commas.str.replace('$', '')\n",
    "dc_listings['price'] = stripped_dollars.astype('float')\n",
    "train_df = dc_listings.iloc[0:2792] ##??? No randomization???\n",
    "test_df = dc_listings.iloc[2792:]\n",
    "\n",
    "def predict_price(new_listing):\n",
    "    ## DataFrame.copy() performs a deep copy\n",
    "    temp_df = train_df.copy()\n",
    "    temp_df['distance'] = temp_df['accommodates'].apply(lambda x: np.abs(x - new_listing))\n",
    "    temp_df = temp_df.sort_values('distance')\n",
    "    nearest_neighbor_prices = temp_df.iloc[0:5]['price']\n",
    "    predicted_price = nearest_neighbor_prices.mean()\n",
    "    return predicted_price\n",
    "\n",
    "test_df['predicted_price'] = test_df['accommodates'].apply(predict_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['accommodates', 'price', 'predicted_price']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9513fb",
   "metadata": {},
   "source": [
    "### Error Metrics\n",
    "\n",
    "We now need a metric that quantifies how good the predictions were on the test set. This class of metrics is called an __error metric__. As the name suggests, an error metric quantifies how inaccurate our predictions were from the actual values. In our case, the error metric tells us __how off__ our predicted price values were from the actual price values for the living spaces in the test dataset.\n",
    "\n",
    "We could start by calculating the difference between each predicted and actual value and then averaging these differences. This is referred to as __mean error__ but isn't an effective error metric for most cases. Mean error treats a positive difference differently than a negative difference, but we're really interested in how far off the prediction is in either the positive or negative direction. If the true price was 200 dollars and the model predicted 210 or 190 it's off by 10 dollars either way.\n",
    "\n",
    "We can instead use the __mean absolute error__, where we compute the absolute value of each error before we average all the errors.\n",
    "\n",
    " $MAE = \\frac{1}{n} \\sum_{k=1}^{n} \\lvert (actual_1 - predicted_1) \\rvert + \\cdots + \\lvert (actual_n - predicted_n) \\rvert$ \n",
    " \n",
    "#### Instructions\n",
    "\n",
    "* Use numpy.absolute() to calculate the mean absolute error between predicted_price and price.\n",
    "* Assign the MAE to mae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dc_listings = pd.read_csv(\"Data/dc_airbnb.csv\")\n",
    "#### regex: dc_listings['price'] = dc_listings['price'].str.replace(r\"[\\$,]\", '').astype('float')\n",
    "stripped_commas = dc_listings['price'].str.replace(',', '')\n",
    "stripped_dollars = stripped_commas.str.replace('$', '')\n",
    "dc_listings['price'] = stripped_dollars.astype('float')\n",
    "np.random.seed(1)\n",
    "shuffled_index = np.random.permutation(dc_listings.index)\n",
    "dc_listings = dc_listings.reindex(shuffled_index)\n",
    "\n",
    "split_one = dc_listings.iloc[0:1862] ### if using .lic for slicing operation, both boundary sides are included, which may throw an eror here\n",
    "split_two = dc_listings.iloc[1862:] ### Also, .loc is label based selection, here the label is not in order any more due to the permutation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a59ee28",
   "metadata": {},
   "source": [
    "### 6.1.5.2 Holdout Validation\n",
    "\n",
    "Now that we've split our data set into 2 dataframes, let's:\n",
    "\n",
    "* train a k-nearest neighbors model on the first half,\n",
    "* test this model on the second half,\n",
    "* train a k-nearest neighbors model on the second half,\n",
    "* test this model on the first half.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Train a k-nearest neighbors model using the default algorithm (auto) and the default number of neighbors (5) that:\n",
    "    * Uses the accommodates column from train_one for training and\n",
    "    * Tests it on test_one.\n",
    "* Assign the resulting RMSE value to iteration_one_rmse.\n",
    "* Train a k-nearest neighbors model using the default algorithm (auto) and the default number of neighbors (5) that:\n",
    "    * Uses the accommodates column from train_two for training and\n",
    "    * Tests it on test_two.\n",
    "* Assign the resulting RMSE value to iteration_two_rmse.\n",
    "* Use numpy.mean() to calculate the average of the 2 RMSE values and assign to avg_rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0583f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_one = split_one\n",
    "test_one = split_two\n",
    "train_two = split_two\n",
    "test_two = split_one\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5, algorithm='auto')\n",
    "knn.fit(train_one[['accommodates']], train_one['price'])\n",
    "predicted_price = knn.predict(test_one[['accommodates']])\n",
    "iteration_one_rmse = np.sqrt(mean_squared_error(predicted_price, test_one['price']))\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5, algorithm='auto')\n",
    "knn.fit(train_two[['accommodates']], train_two['price'])\n",
    "predicted_price = knn.predict(test_two[['accommodates']])\n",
    "iteration_two_rmse = np.sqrt(mean_squared_error(predicted_price, test_two['price']))\n",
    "avg_rmse = np.mean([iteration_one_rmse, iteration_two_rmse])\n",
    "avg_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3456d31",
   "metadata": {},
   "source": [
    "### 6.1.5.3 K-Fold Cross Validation\n",
    "\n",
    "If we average the two RMSE values from the last step, we get an RMSE value of approximately 128.96. Holdout validation is actually a specific example of a larger class of validation techniques called __k-fold cross-validation__. While holdout validation is better than train/test validation because the model isn't repeatedly biased towards a specific subset of the data, both models that are trained only use half the available data. K-fold cross validation, on the other hand, takes advantage of a larger proportion of the data during training while still __rotating through__ different subsets of the data to avoid the issues of train/test validation.\n",
    "\n",
    "Here's the algorithm from k-fold cross validation:\n",
    "\n",
    "* splitting the full dataset into k equal length partitions.\n",
    "    * selecting k-1 partitions as the training set and\n",
    "    * selecting the remaining partition as the test set\n",
    "* training the model on the training set.\n",
    "* using the trained model to predict labels on the test fold.\n",
    "* computing the test fold's error metric.\n",
    "* repeating all of the above steps k-1 times, until each partition has been used as the test set for an iteration.\n",
    "* calculating the mean of the k error values.\n",
    "\n",
    "__Holdout validation__ is essentially a version of k-fold cross validation when __k is equal to 2__. Generally, 5 or 10 folds is used for k-fold cross-validation. Here's a diagram describing each iteration of 5-fold cross validation:\n",
    "\n",
    "![img alt](https://s3.amazonaws.com/dq-content/kfold_cross_validation.png)\n",
    "\n",
    "As you increase the number the folds, the number of observations in each fold decreases and the variance of the fold-by-fold errors increases. Let's start by manually partitioning the data set into 5 folds. Instead of splitting into 5 dataframes, let's add a column that specifies which fold the row belongs to. This way, we can easily select\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Add a new column to dc_listings named fold that contains the fold number each row belongs to:\n",
    "    * Fold 1 should have rows from index 0 up to745, not including 745.\n",
    "    * Fold 2 should have rows from index 745 up to 1490, not including 1490.\n",
    "    * Fold 3 should have rows from index 1490 up to 2234, not including 2234.\n",
    "    * Fold 4 should have rows from index 2234 up to 2978, not including 2978.\n",
    "    * Fold 5 should have rows from index 2978 up to 3723, not including 3723.\n",
    "* Display the unique value counts for the fold column to confirm that each fold has roughly the same number of elements.\n",
    "* Display the number of missing values in the fold column to confirm we didn't miss any rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c11fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## continue with last mission where dc_listings has been randomly permutated with seed=1\n",
    "dc_listings.loc[dc_listings.index[0:745], \"fold\"] = 1 #### Neat!!!\n",
    "dc_listings.loc[dc_listings.index[745:1490], \"fold\"] = 2\n",
    "dc_listings.loc[dc_listings.index[1490:2234], \"fold\"] = 3\n",
    "dc_listings.loc[dc_listings.index[2234:2978], \"fold\"] = 4\n",
    "dc_listings.loc[dc_listings.index[2978:3723], \"fold\"] = 5\n",
    "\n",
    "print(dc_listings['fold'].value_counts())\n",
    "print(\"\\n Num of missing values: \", dc_listings['fold'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca23a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Testing Broadcasting while assign values to Series\n",
    "df1=pd.DataFrame({'A':[1, 2, 3, 4, 5]})\n",
    "display(df1)\n",
    "df1['B'] = 0\n",
    "df1['C'] = None\n",
    "df1.loc[df1.index[0:1], 'D'] = 1\n",
    "display(df1)\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d06833",
   "metadata": {},
   "source": [
    "### 6.1.5.4 First iteration\n",
    "\n",
    "Let's start by performing the first iteration of k-fold cross validation on a simple, univariate model.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Train a k-nearest neighbors model using the accommodates column as the sole feature from folds 2 to 5 as the training set.\n",
    "* Use the model to make predictions on the test set (accommodates column from fold 1) and assign the predicted labels to labels.\n",
    "* Calculate the RMSE value by comparing the price column with the predicted labels.\n",
    "* Assign the RMSE value to iteration_one_rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ffd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Training\n",
    "model = KNeighborsRegressor()\n",
    "train_iteration_one = dc_listings[dc_listings[\"fold\"] != 1]\n",
    "test_iteration_one = dc_listings[dc_listings[\"fold\"] == 1]\n",
    "model.fit(train_iteration_one[[\"accommodates\"]], train_iteration_one[\"price\"])\n",
    "\n",
    "# Predicting\n",
    "labels = model.predict(test_iteration_one[[\"accommodates\"]])\n",
    "test_iteration_one[\"predicted_price\"] = labels\n",
    "iteration_one_mse = mean_squared_error(test_iteration_one[\"price\"], test_iteration_one[\"predicted_price\"])\n",
    "iteration_one_rmse = np.sqrt(iteration_one_mse)\n",
    "display(iteration_one_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4f38b7",
   "metadata": {},
   "source": [
    "### 6.1.5.5 Function for training models\n",
    "\n",
    "From the first iteration, we achieved an RMSE value of 107. Let's calculate the RMSE values for the remaining iterations. To make the iteration process easier, let's __wrap the code__ we wrote in the previous screen in a function.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Write a function named train_and_validate that takes in a dataframe as the first parameter (df) and a list of fold values (1 to 5 in our case) as the second parameter (folds). This function should:\n",
    "\n",
    "    * Train n models (where n is number of folds) and perform k-fold cross validation (using n folds). Use the default k value for the KNeighborsRegressor class.\n",
    "    * Return a list of RMSE values, where the first element is the RMSE for when fold 1 was the test set, the second element is the RMSE for when fold 2 was the test set, and so on.\n",
    "\n",
    "* Use the train_and_validate function to return the list of RMSE values for the dc_listings Dataframe and assign to rmses.\n",
    "\n",
    "* Calculate the mean of these values and assign to avg_rmse.\n",
    "* Display both rmses and avg_rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf0f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use np.mean to calculate the mean.\n",
    "import numpy as np\n",
    "fold_ids = [1,2,3,4,5]\n",
    "def train_and_validate(df, folds):\n",
    "    fold_rmses = []\n",
    "    for fold in folds:\n",
    "        # Train\n",
    "        model = KNeighborsRegressor()\n",
    "        train = df[df[\"fold\"] != fold]\n",
    "        test = df[df[\"fold\"] == fold]\n",
    "        model.fit(train[[\"accommodates\"]], train[\"price\"])\n",
    "        # Predict\n",
    "        labels = model.predict(test[[\"accommodates\"]])\n",
    "        test[\"predicted_price\"] = labels\n",
    "        mse = mean_squared_error(test[\"price\"], test[\"predicted_price\"])\n",
    "        rmse = mse**(1/2)\n",
    "        fold_rmses.append(rmse)\n",
    "    return(fold_rmses)\n",
    "\n",
    "rmses = train_and_validate(dc_listings, fold_ids)\n",
    "print(rmses)\n",
    "avg_rmse = np.mean(rmses)\n",
    "print(avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea72e58",
   "metadata": {},
   "source": [
    "### 6.1.5.6 Performing K-Fold Cross Validation Using Scikit-Learn\n",
    "\n",
    "While the average RMSE value was approximately 130.2, the RMSE values ranged from 107 to 153. This large amount of variability between the RMSE values means that we're __either using a poor model or a poor evaluation criteria (or a bit of both!)__. By implementing your own k-fold cross-validation function, you hopefully acquired a good understanding of the inner workings of the technique. The function we wrote, however, has many limitations. If we want to now change the number of folds we want to use, we need to make the function more general so it can also handle randomizing the ordering of the rows in the dataframe and splitting into folds.\n",
    "\n",
    "In machine learning, we're interested in building a good model and accurately understanding how well it will perform. To build a better k-nearest neighbors model, we can change the features it uses or tweak the number of neighbors (a hyperparameter). To accurately understand a model's performance, we can perform k-fold cross validation and select the proper number of folds. We've learned how scikit-learn makes it easy for us to quickly experiment with these different knobs when it comes to building a better model. Let's now dive into how we can use scikit-learn to handle cross-validation as well.\n",
    "\n",
    "First, we instantiate an instance of the __[KFold class](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)__ from sklearn.model_selection:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits, shuffle=False, random_state=None)\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "* n_splits is the number of folds you want to use,\n",
    "* shuffle is used to toggle shuffling of the ordering of the observations in the dataset,\n",
    "* random_state is used to specify the random seed value if shuffle is set to True.\n",
    "\n",
    "You'll notice here that no parameters depend on the data set at all. This is because the KFold class returns an __iterator__ object which we use in conjunction with the __[cross_val_score() function](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)__, also from __sklearn.model_selection__. Together, these 2 functions allow us to compactly train and test using k-fold cross validation:\n",
    "\n",
    "Here are the relevant parameters for the __cross_val_score function__:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(estimator, X, Y, scoring=None, cv=None)\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "* estimator is a sklearn model that implements the fit method (e.g. instance of KNeighborsRegressor),\n",
    "* X is the list or 2D array containing the features you want to train on,\n",
    "* y is a list containing the values you want to predict (target column),\n",
    "* scoring is a string describing the scoring criteria (list of accepted values __[here](http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)__).\n",
    "* cv describes the number of folds. Here are some examples of accepted values:\n",
    "    * an instance of the KFold class,\n",
    "    * an integer representing the number of folds.\n",
    "\n",
    "Depending on the scoring criteria you specify, a single total value is returned for each fold. Here's the general workflow for performing k-fold cross-validation using the classes we just described:\n",
    "\n",
    "* instantiate the scikit-learn model class you want to fit,\n",
    "* instantiate the KFold class and using the parameters to specify the k-fold cross-validation attributes you want,\n",
    "* use the cross_val_score() function to return the scoring metric you're interested in.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Create a new instance of the KFold class with the following properties:\n",
    "\n",
    "    * 5 folds,\n",
    "    * shuffle set to True,\n",
    "    * random seed set to 1 (so we can answer check using the same seed),\n",
    "    * assigned to the variable kf.\n",
    "\n",
    "* Create a new instance of the KNeighborsRegressor class and assign to knn.\n",
    "\n",
    "* Use the cross_val_score() function to perform k-fold cross-validation:\n",
    "\n",
    "    * using the KNeighborsRegressor instance knn,\n",
    "    * using the accommodates column for training,\n",
    "    * using the price column as the target column,\n",
    "    * returning an array of MSE values (one value for each fold).\n",
    "\n",
    "* Assign the resulting list of MSE values to mses. Then, take the absolute value followed by the square root of each MSE value. Then, calculate the average of the resulting RMSE values and assign to avg_rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08718b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in normalized_listings.columns if col != 'price'] ## select all columns but price with for loop\n",
    "knn = KNeighborsRegressor(n_neighbors=5, algorithm='brute')\n",
    "knn.fit(train_df[features], train_df['price'])\n",
    "all_features_predictions = knn.predict(test_df[features])\n",
    "all_features_mse = mean_squared_error(all_features_predictions, test_df['price'])\n",
    "all_features_rmse = np.sqrt(all_features_mse)\n",
    "\n",
    "print(all_features_mse)\n",
    "print(all_features_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c79ff8",
   "metadata": {},
   "source": [
    "### 6.1.3.11 Next Steps\n",
    "\n",
    "Interestingly enough, the RMSE value actually increased to 125.1 when we used all of the features available to us. This means that __selecting the right features__ is important and that using more features doesn't automatically improve prediction accuracy. We should re-phrase the lever we mentioned earlier from:\n",
    "\n",
    "* increase the number of attributes the model uses to calculate similarity when ranking the closest neighbors\n",
    "\n",
    "to:\n",
    "\n",
    "* select the __relevant attributes__ the model uses to calculate similarity when ranking the closest neighbors\n",
    "\n",
    "The process of selecting features to use in a model is known as __feature selection__.\n",
    "\n",
    "In this mission, we prepared the data to be able to use more features, trained a few models using multiple features, and evaluated the different performance tradeoffs. We explored __how using more features doesn't always improve the accuracy__ of a k-nearest neighbors model. In the next mission, we'll explore another knob for tuning k-nearest neighbor models - __the k value__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d63d78",
   "metadata": {},
   "source": [
    "## 6.1.4 Hyperparameter Optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61153e2",
   "metadata": {},
   "source": [
    "### 6.1.4.1 Recap\n",
    "\n",
    "In the last mission, we focused on increasing the number of attributes the model uses. We saw how, in general, adding more attributes generally lowered the error of the model. This is because the model is able to do a better job identifying the living spaces from the training set that are the most similar to the ones from the test set. However, we also observed how using all of the available features didn't actually improve the model's accuracy automatically and that some of the features were probably __not relevant for similarity ranking__. We learned that selecting relevant features was the right lever when improving a model's accuracy, not just increasing the features used in the absolute.\n",
    "\n",
    "In this mission, we'll focus on the impact of increasing k, the number of nearby neighbors the model uses to make predictions. We exported both the training (train_df) and test sets (test_df) from the last missions to CSV files, __dc_airbnb_train.csv__ and __dc_airbnb_test.csv__ respectively. Let's read both these CSV's into Dataframes.\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Read dc_airbnb_train.csv into a Dataframe and assign to train_df.\n",
    "* Read dc_airbnb_test.csv into a Dataframe and assign to test_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd34771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('Data/dc_airbnb_train.csv') ## This dataset is already normalized\n",
    "test_df = pd.read_csv('Data/dc_airbnb_test.csv') ## This dataset is already normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e19565",
   "metadata": {},
   "source": [
    "### 6.1.4.2 Hyperparameter optimization\n",
    "\n",
    "When we vary the features that are used in the model, we're affecting the data that the model uses. On the other hand, varying the k value affects the behavior of the model independently of the actual data that's used when making predictions. In other words, we're impacting how the model performs __without trying to change the data that's used__.\n",
    "\n",
    "Values that affect the behavior and performance of a model that are __unrelated to the data__ that's used are referred to as __hyperparameters__. The process of finding the __optimal hyperparameter value__ is known as __[hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)__. A simple but common hyperparameter optimization technique is known as __[grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search)__, which involves:\n",
    "\n",
    "* selecting a subset of the possible hyperparameter values,\n",
    "* training a model using each of these hyperparameter values,\n",
    "* evaluating each model's performance,\n",
    "* selecting the hyperparameter value that resulted in the lowest error value.\n",
    "\n",
    "Grid search essentially boils down to evaluating the model performance at different k values and selecting the k value that resulted in the lowest error. While grid search can __take a long time__ when working with large datasets, the data we're working with in this mission is small and this process is relatively quick.\n",
    "\n",
    "Let's confirm that grid search will work quickly for the dataset we're working with by first observing how the model performance changes as we increase the k value from 1 to 5. If you recall, we set 5 as the k value for the last 2 missions. Let's use the features from the last mission that resulted in the best model accuracy:\n",
    "\n",
    "* accommodates\n",
    "* bedrooms\n",
    "* bathrooms\n",
    "* number_of_reviews\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Create a list containing the integer values 1, 2, 3, 4, and 5, in that order, and assign to hyper_params.\n",
    "* Create an empty list and assign to mse_values.\n",
    "* Use a for loop to iterate over hyper_params and in each iteration:\n",
    "    * Instantiate a KNeighborsRegressor object with the following parameters:\n",
    "        * n_neighbors: the current value for the iterator variable,\n",
    "        * algorithm: brute\n",
    "    * Fit the instantiated k-nearest neighbors model to the following columns from train_df:\n",
    "        * accommodates\n",
    "        * bedrooms\n",
    "        * bathrooms\n",
    "        * number_of_reviews\n",
    "    * Use the trained model to make predictions on the same columns from test_df and assign to predictions.\n",
    "    * Use the mean_squared_error function to calculate the MSE value between predictions and the price column from test_df.\n",
    "    * Append the MSE value to mse_values.\n",
    "* Display mse_values using the print() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be454ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.abs(test_df['price'] - test_df['predicted_price']).mean()\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae3d027",
   "metadata": {},
   "source": [
    "### 6.1.2.3 Mean Squared Error\n",
    "\n",
    "For many prediction tasks, we want to __penalize__ predicted values that are __further away__ from the actual value much more than those that are closer to the actual value.\n",
    "\n",
    "We can instead take the mean of the squared error values, which is called the __mean squared error or MSE__ for short. The MSE makes the gap between the predicted and actual values more clear. A prediction that's off by 100 dollars will have an error (of 10,000) that's 100 times more than a prediction that's off by only 10 dollars (which will have an error of 100).\n",
    "\n",
    "Here's the formula for MSE:\n",
    "\n",
    "$ MAE = \\frac{1}{n} \\sum_{k=1}^{n} (actual_1 - predicted_1)^{2} + \\cdots + (actual_n - predicted_n)^{2} $\n",
    "\n",
    "where n represents the number of rows in the test set. Let's calculate the MSE value for the predictions we made on the test set.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Calculate the MSE value between the predicted_price and price columns and assign to mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd07b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.power(test_df['price'] - test_df['predicted_price'], 2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759e496",
   "metadata": {},
   "source": [
    "### 6.1.2.4 Training another model\n",
    "\n",
    "The model we trained achieved a mean squared error of around __18646.5__. Is this a high or a low mean squared error value? What does this tell us about the __quality of the predictions__ and the model? By itself, the mean squared error value for a single model isn't all that useful.\n",
    "\n",
    "The units of mean squared error in our case is dollars squared (not dollars), which makes it hard to reason about __intuitively__ as well. We can, however, train another model and then compare the mean squared error values to see which model performs better __on a relative basis__. Recall that a low error metric means that the gap between the predicted list price and actual list price values is low while a high error metric means the gap is high.\n",
    "\n",
    "Let's train another model, this time using the __bathrooms column__, and compare MSE values.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Modify the predict_price function to the right to use the bathrooms column instead of the accommodates column to make predictions.\n",
    "* Apply the function to test_df and assign the resulting Series object containing the predicted price values to the predicted_price column in test_df.\n",
    "* Calculate the squared error between the price and predicted_price columns in test_df and assign the resulting Series object to the squared_error column in test_df.\n",
    "* Calculate the mean of the squared_error column in test_df and assign to mse.\n",
    "* Use the print function or the variables inspector to display the MSE value."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
