{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b83c34",
   "metadata": {},
   "source": [
    "# B- Response Variable Wrangling (KMZ to Raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b1e11",
   "metadata": {},
   "source": [
    "#### First, I had to install: gdal, rasterstats, shapely, rasterio, fiona... manually from binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517059db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterstats\n",
    "import numpy as np\n",
    "from rasterstats import zonal_stats\n",
    "import gdal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0957e50e",
   "metadata": {},
   "source": [
    "#### The purpose of this notebook is to show how to get a numpy array containing site locations from their original format, which was a Google Earth kmz file of oulines of each site.  This array is the response variable in the project.\n",
    "\n",
    "####  To begin, we download the kmz file from Menze and Ur's Dataverse webpage using Selenium (the file can only be downloaded by clicking a download button):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1352f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Use chromedriver.exe in the current directory with selenium to go to data website for Menze and Ur 2012 article in Chrome.\n",
    "    This code opens up a Chrome window and clicks the download button for Ur and Menze's site location kmz file.  Then, it closes\n",
    "    the Chrome window when the download is complete (or if the file has previously been downloaded). \n",
    "'''\n",
    "from selenium import webdriver\n",
    "import os, time\n",
    "\n",
    "#Set Chrome's download directory to the present working directory:\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "pwd = %pwd\n",
    "profile={\"download.default_directory\":pwd}\n",
    "chromeOptions.add_experimental_option(\"prefs\", profile)\n",
    "\n",
    "#start up a Chrome window using the settings above and go to Menze and Ur's dataverse page:\n",
    "driver = webdriver.Chrome(chrome_options=chromeOptions)\n",
    "driver.get(\"https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/17731#724\")\n",
    "\n",
    "#Click download button containing site location kmz file:\n",
    "driver.find_element_by_id(\"datasetForm:tabView:filesTable:3:j_idt724\").click()\n",
    "\n",
    "#While file is downloading (i.e. temp files are active), keep Chrome up; when the file is done downloading, close the window:\n",
    "while True:\n",
    "        if os.path.exists(pwd+'\\menze-ur_khabur_sites-outlines.kmz.part'):\n",
    "            time.sleep(10)\n",
    "        elif os.path.exists(pwd+'\\menze-ur_khabur_sites-outlines.kmz'):\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(10)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775d46b",
   "metadata": {},
   "source": [
    "#### We would like to clip the site outlines in the kmz file of the site locations to a raster, and then transform them from shape outlines into array values.  The storage of these locations is fundamentally different from the other data types because they are stored as individual shapes with coordinates and geometry, not as values in an array-like format that covers the study area.  The zonal_stats() function from rasterstats can be used to clip shapes to a raster by setting the raster_out parameter to True.  Unfortunately, gdal doesn't support the kmz file format, so the first thing we need to do is convert the file into a format that can be used by gdal.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04fadc7",
   "metadata": {},
   "source": [
    "### The first step is to convert the KMZ to a KML file by unzipping KMZ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16072d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_arrays[90]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0446dca",
   "metadata": {},
   "source": [
    "### Notice that we get negative index values.  That is because some of the polygons are outside of the image that I intersected with (that is, our study area).  I will remove any polygons that are out of bounds before proceeding.\n",
    "### Out of bounds to the North and West are simple: these are negative values.  To figure out which polygons are out of bounds to the South or East is more tricky.  These will have indices that are greater than or equal to the dimensions of the landsat_clip array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d75c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum dimensions\n",
    "clip_max_row = np.shape(landsat_clip.ReadAsArray())[0]\n",
    "clip_max_col = np.shape(landsat_clip.ReadAsArray())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99464384",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of sites before removing out of bounds ones: \" + str(len(mini_arrays)))\n",
    "\n",
    "new_mini_arrays = []\n",
    "for i in mini_arrays:\n",
    "    shape = np.shape(i[2])\n",
    "    height = shape[0]\n",
    "    width = shape[1]   \n",
    "    row_idx = i[0]\n",
    "    col_idx = i[1]\n",
    "    \n",
    "    # Check if out of bounds left\n",
    "    if col_idx + width <= 0: \n",
    "        continue                         # Completely out of bounds, no portion of site is in array.  Next array\n",
    "    elif col_idx < 0:                    # Straddles the border of the study area\n",
    "        i[2] = i[2][:,-col_idx:]         # Take only the overlapping portion\n",
    "        i[1] = 0                         # Update column coordinate\n",
    "        col_idx = 0\n",
    "    \n",
    "    # Check if out of bounds top\n",
    "    if row_idx + height <= 0: \n",
    "        continue\n",
    "    elif row_idx < 0:\n",
    "        i[2] = i[2][-row_idx:,:]\n",
    "        i[0] = 0\n",
    "        row_idx = 0\n",
    "        \n",
    "    # Check if out of bounds right\n",
    "    if col_idx >= clip_max_col: \n",
    "        continue\n",
    "    elif col_idx + width >= clip_max_col:  # Overlapping right edge\n",
    "        i[2] = i[2][:,:(clip_max_col - col_idx)]\n",
    "    \n",
    "    # Check if out of bounds bottom\n",
    "    if row_idx >= clip_max_row: \n",
    "        continue\n",
    "    elif row_idx + height >= clip_max_row:  # Overlapping bottom edge\n",
    "        i[2] = i[2][:(clip_max_row - row_idx),:]\n",
    "    \n",
    "    # If the iteration made it to this point, some portion of the mini-raster is in bounds and has been trimmed\n",
    "    #     if it overlapped, so it can be appended.  \n",
    "    new_mini_arrays.append(i)\n",
    "     \n",
    "    \n",
    "mini_arrays = new_mini_arrays\n",
    "    \n",
    "print(\"Number of sites after removing out of bounds ones: \" + str(len(mini_arrays)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57381c4b",
   "metadata": {},
   "source": [
    "### Here is an example of an array that overlapped the left boundary.  Notice the flat left edge of the site boundary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_arrays[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af4a62d",
   "metadata": {},
   "source": [
    "### Now the empty landsat_clip array can be used to add all of the mini_arrays to create an array of all of the sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d3013",
   "metadata": {},
   "source": [
    "###I need to pad each mini-array with False above, below, to the right, and to the left to make it the size of land_sat_clip, and pad these False values in a way that corresponds with the mini-array's position in landsat_clip.  To do this, I use the row and column indices for the top and left pads, and I subtract these indices and the widths or heights to form the bottom pads.  This produces a full-sized array for each site that has the same dimensions as the empty landsat clip.  \n",
    "### These arrays are too large (9325 arrays that are 2603 by 5638) to store in an list (trying causes memory errors), so at each step I use a logical_or() function to combine the new boolean arrays one-by-one to a running aggregation.  OR takes care of overlapping arrays nicely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "padded_mini_arrays = []\n",
    "# Empty array of the study area that I add the sites to\n",
    "site_array = landsat_clip.ReadAsArray()\n",
    "\n",
    "for i in (mini_arrays):\n",
    "    # Find the indexes of the upper left corners of the mini-array\n",
    "    row_idx = i[0]\n",
    "    col_idx = i[1]\n",
    "    \n",
    "    # Find the dimensions of the mini-array\n",
    "    shape = np.shape(i[2])\n",
    "    height = shape[0]\n",
    "    width = shape[1]\n",
    "   \n",
    "    # Add padding to each mini-array to fit into the correct portion of the study area array\n",
    "    \n",
    "    # First, have to define how much to pad on each side based on the position of the corner and the dimensions\n",
    "    #      of the mini-array\n",
    "    top_pad    = row_idx\n",
    "    bottom_pad = clip_max_row - row_idx - height\n",
    "    left_pad   = col_idx\n",
    "    right_pad  = clip_max_col - col_idx - width\n",
    "    \n",
    "    # Next, pad the array\n",
    "    padded = np.pad(i[2], [[top_pad, bottom_pad],[left_pad, right_pad]], 'constant', constant_values=False)\n",
    "    \n",
    "    # Finally, add the padded array to the ongoing copy of the study area\n",
    "    site_array = np.logical_or(site_array, padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed618dea",
   "metadata": {},
   "source": [
    "### And finally we have our response variable in a tidy array!  It contains 871496 True values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a845cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(site_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155aeca",
   "metadata": {},
   "source": [
    "### Here is what the study area looks like.  Sites are white:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "ogr2ogr -t_srs EPSG:32637 -s_srs EPSG:4326 Menze2_UTM37N.shp Menze2.shp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae593b",
   "metadata": {},
   "source": [
    "### Then, clip the site polygons to the Landsat clip of our study area (which is in the same projection).  Now, I will use zonal_stats to output the masked arrays.  I use all_touched = True, which means that every pixel that contains any portion of a site will be considered True, and not just pixels that are completely sites.  Setting raster_out = True, we get mini masked array for each site.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stats = zonal_stats('Menze2_UTM37N.shp', 'landsat_clip.tif', all_touched = True, raster_out = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eb006e",
   "metadata": {},
   "source": [
    "### This output is a list of 12746 dictionaries, with each dictionary corresponding to a site.  Each site has a mini masked array that shows where a site is present within a given subrange of the raster.   There is also an affine transformation that gives the position of the upper left corner of the mini array.  This can be used to put all of these mini-arrays together into one large array.   \n",
    "\n",
    "###Here is an example of the output for one of the 12746 sites.  Notice that the interior is filled.  The mask portion of the masked_array object portion of the output will next be used to put all of the sites together into a single large array with correct positions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[46]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a0ef82",
   "metadata": {},
   "source": [
    "### The affine transformation of each mini-array is in the same reference as the raster that the shapefile was clipped to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba171e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[10]['mini_raster_affine']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a849b577",
   "metadata": {},
   "source": [
    "### Gdal puts these values in a different order from rasterstats, but they give the same information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_clip = gdal.Open('landsat_clip.tif')\n",
    "landsat_clip.GetGeoTransform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b504e",
   "metadata": {},
   "source": [
    "### These six values are the width of each pixel, the height of each pixel (in this case, 30 meters), two values for the rotation of the image (0 if North is up), and the x and y coordinate of the Northwest corner of the array.  These will be used to put all of the mini-arrays together.  \n",
    "### First, I need the X and Y UTM coordinates of the upper left corner of the landsat clip that I created the mini-arrays with.  These can be extracted with the GetGeotransform() method on the landsat clip.  These will be subtracted from each corresponding coordinate value of the affine transformation for each mini-array.  They will then be divided by the width/height of the pixels (30m), and transformed to integer values to give array positions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd783cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_x = landsat_clip.GetGeoTransform()[0]\n",
    "clip_y = landsat_clip.GetGeoTransform()[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22af685",
   "metadata": {},
   "source": [
    "### For each mini-raster, we only need the mask values and the transformation, and we wish to reverse the True/False values within the mask because False actually corresponds to a site being present in an array position. \n",
    "### I extract just the information I need from the output, which is the mask and the affine transformation.  I also reverse the mask and convert the UTM coordinates to array position indexes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(site_array, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49481c4c",
   "metadata": {},
   "source": [
    "###Zooming in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815543e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(site_array[200:500,500:800], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e712d4c",
   "metadata": {},
   "source": [
    "### Save this array so that it can be re-used in other notebooks without re-creating it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aedbbb",
   "metadata": {},
   "source": [
    "### Therefore, we need to convert the site outlines in the KML file from line features to polygon features so that the intersection will include the interior of the sites.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1789a7c8",
   "metadata": {},
   "source": [
    "###The kml file is reminiscent of html, and has opening and closing tags that indicate the geometry type.  Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\t<Placemark>\n",
    "\t\t\t<description>Digitizer Tool Line</description>\n",
    "\t\t\t<styleUrl>#line10</styleUrl>\n",
    "\t\t\t<LineString>\n",
    "\t\t\t\t<coordinates>\n",
    "\t\t\t\t\t39.8706552,36.74484027,0 39.8706552,36.74484027,...\n",
    "                    ...\n",
    "                    ... 0 39.87068397,36.74490767,0 39.8706552,36.74484027,0 \n",
    "\t\t\t\t</coordinates>\n",
    "\t\t\t</LineString>\n",
    "\t\t</Placemark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4a138",
   "metadata": {},
   "source": [
    "### All we have to do is replace every \"LineString\" encoding with the encoding for defining a polygon from its outline.  This can be done programatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477186ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fileinput\n",
    "kml=open('doc.kml', 'r')\n",
    "kmldata=kml.read()\n",
    "kml.close()\n",
    "\n",
    "newdata=kmldata.replace(\"<LineString>\", \"<Polygon><outerBoundaryIs><LinearRing>\")\n",
    "newdata1=newdata.replace(\"</LineString>\", \"</LinearRing></outerBoundaryIs></Polygon>\")\n",
    "\n",
    "f=open('fileout.kml', 'w')\n",
    "f.write(newdata1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981fcf8",
   "metadata": {},
   "source": [
    "### Now we convert the KML file of polygons and convert it to Shapefile of Site Polygons with Grass 7.0.2 Command Line\n",
    "(This should be run locally pointing to specific folders Grass and its projection data are saved to, following the same form as below for Jon's machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a71fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "rem #Open up Grass 7.0.2 from command line and convert KML polyline features to Shapefile Polygons in folder devoted to WGS84 projection:\n",
    "C:\\\"Program Files (x86)\"\\\"Grass GIS 7.0.2\"\\grass70 C:\\Users\\Jon\\Drive\\Cleopythons\\WGS84_1\\Jon\n",
    "\n",
    "rem #Import KML polylines 'fileout.kml' as a Grass layer \"Line_Features\"\n",
    "v.in.ogr --overwrite input=C:\\Users\\Jon\\Drive\\Cleopythons\\fileout.kml output=Line_Features\n",
    "\n",
    "rem #Output Site Polygons as shapefile \"Menze2.shp\"\n",
    "v.out.ogr -c --overwrite input=Line_Features output=C:\\Users\\Jon\\Drive\\Cleopythons format=ESRI_Shapefile output_layer=Menze2 output_type=boundary\n",
    "\n",
    "rem #Exit Grass Command Prompt\n",
    "exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb87aea",
   "metadata": {},
   "source": [
    "### Another issue is that because the SRTM file was in the WGS84 projection, the affine transformation values are in degrees, which is not convenient for putting these mini-arrays into one array.  So I convert the projection of Menze2.shp to UTM 37N:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd089ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I first create an empty array to append results to.  \n",
    "mini_arrays = []\n",
    "# Next I iterate over the list of results and append just a list of the reversed mask and array indexes for each mini-array.  \n",
    "for i in stats: \n",
    "    affine = i['mini_raster_affine']  # the array of affine transformation values in the dictionary\n",
    "    col_idx = np.int((affine[2] - clip_x)/30)        # Subtract the affine coordinate value from the image corner values and \n",
    "    row_idx = np.int((clip_y - affine[5])/30)        #    divide by the pixel width to get an array position index for x and y.  \n",
    "    # The subtraction is reversed in the y case because I want the y-index descending\n",
    "    mini_arrays.append([row_idx, col_idx, np.invert(i['mini_raster_array'].mask)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557bd78a",
   "metadata": {},
   "source": [
    "### Here is an example of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84923ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First rename file extension .kmz to .zip\n",
    "import os\n",
    "os.renames('menze-ur_khabur_sites-outlines.kmz', 'menze.zip')\n",
    "\n",
    "#Then, unzip menze.zip to the current directory to reveal the file 'doc.kml':\n",
    "import zipfile\n",
    "zipped = zipfile.ZipFile('menze.zip', 'r')\n",
    "zipped.extractall()\n",
    "zipped.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7878ae1",
   "metadata": {},
   "source": [
    "### In earlier attempts of this process, we found that the fact that the sites are stored as outlines causes a problem with our array output.  The problem is that we compute the intersection of the outline and the site raster, so we only catch the boundaries of the sites and miss the interiors.  We don't want this because we want our classifier to be able to predict the interior of sites, not just their edges.  An example of this output: masked_array output shows hollow sites, that is, blobs of \"-- --\" corresponding to the interior of the sites which should be filled.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a423f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    'mini_raster_array': masked_array(data =\n",
    "   [[0 0 0 0 0 0 0 0 0 --]\n",
    "   [0 0 -- -- 0 0 0 -- 0 0]\n",
    "   [-- 0 -- -- -- -- -- -- -- 0]\n",
    "   [-- 0 0 -- -- -- -- -- 0 0]\n",
    "   [-- -- 0 0 -- -- -- -- 0 --]\n",
    "   [-- -- -- 0 -- -- -- -- 0 --]\n",
    "   [-- -- -- 0 -- -- -- -- 0 --]\n",
    "   [-- -- -- 0 0 0 0 0 0 --]\n",
    "   [-- -- -- 0 0 -- -- -- -- --]],"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
